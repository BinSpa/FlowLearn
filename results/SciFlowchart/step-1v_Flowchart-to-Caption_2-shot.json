{
    "2204.00424v1-Figure6-1.png": {
        "prediction": "The flowchart illustrates a process that involves multiple steps related to cloud detection and segmentation. The process starts with two sets of images, S1 and S2, which are fed into an encoder (E) at different time steps (t-1, t, t+1). The outputs of the encoders are then concatenated and passed through a decoder (D) to generate a new set of images, S2, at time step t. The process is repeated for the next time step (t+1). Finally, the output images are compared with the ground truth cloud-free images, S2t+1, to calculate the loss or metrics.",
        "reference": "Figure 6. MSOPunet network. (S1t\u22121, S2t\u22121), (S1t, S2t) and (S1t+1, S2t+1) denotes the input pairs of optical and SAR images. S2t\u2032 denotes the reference image used to compute the loss (during training) and the metrics (at inference time from test data) from the reconstructed optical image S\u03022t."
    },
    "2202.03651v2-Figure1-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of using a perception model to identify objects in a scene, and then applying a learned scene graph edit to modify the scene. The flowchart shows that the perception model is trained on a dataset of images and their corresponding scene graphs, which are graphical representations of the objects and their relationships in the scene. The perception model is then used to identify objects in a new image, and the learned scene graph edit is applied to modify the scene graph of the new image. The modified scene graph is then used to generate a new image that is consistent with the modified scene graph.",
        "reference": "Figure 1: Instead of retrospectively discovering individual failure cases for perception, we actively search for causal interventions (edits) to existing scenes that consistently result in perception failures. The middle shows an example of a single intervention causing perception failure, which we attribute to the intervention, as opposed to the left where a combinatorial set of factors could explain the error. Consistent failures through this type of intervention constitute a challenging group for the perception model as seen on the right."
    },
    "2011.15102v2-Figure1-1.png": {
        "prediction": "The image contains a flowchart that illustrates a continuous improvement loop between a learner and a tester. The learner, represented by a person with a necktie, is shown to be taking harder tests, which are indicated by the arrows pointing from the learner to the test bank. The tester, represented by a person sitting at a desk, is shown to be providing better solutions, as indicated by the arrows pointing from the tester back to the learner. This suggests a process where the learner is continuously challenged by more difficult tests, leading to the tester providing better solutions, which in turn helps the learner to improve. The flowchart emphasizes the idea of ongoing learning and improvement through a cycle of testing and feedback.",
        "reference": "Figure 1: Learning by passing tests. A tester model creates tests with increasing levels of difficulty from a test bank to evaluate a learner model. The learner continuously improves its learning ability to deliver better solutions for passing those difficult tests."
    },
    "14078997-Figure2-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of using a convolutional neural network (CNN) for video stabilization. The flowchart starts with the input frames from the video, which are then passed through a CNN to predict the affine transforms. These predicted affine transforms are then applied to the input frames to reconstruct the stabilized video. The flowchart also shows a mechanism for extracting the ground truth affine transforms, which are then compared to the predicted affine transforms using mean squared error (MSE) as the loss function. The CNN is trained to minimize this loss function, thereby improving the accuracy of the predicted affine transforms and the stabilization of the video.",
        "reference": "Figure 2: Outline of the transformation-based model. The model is a CNN that takes as input a sequence of consecutive affine transforms between pairs of adjacent video frames. It predicts the affine transform between the last input frame and the next one in the sequence. We compute affine transforms (6 parameters per patch) for overlapping patches of size 8 \u00d7 8 in each video frame. Learning operates in the space of transformations as shown inside the dashed box. The front-end on the left is a module that estimates the affine transforms between pairs of consecutive input frames. The post-processor on the right reconstructs a frame from the predicted set of affine transforms and it is only used at test time."
    },
    "14078997-Figure3-1.png": {
        "prediction": "The image contains a flowchart that represents a sequence of CNN (Convolutional Neural Network) layers. Each CNN layer is represented by a box, and arrows indicate the flow of data between these layers. The flowchart shows that the output of one CNN layer is fed as input to the next CNN layer, indicating a sequential processing of data through the layers.",
        "reference": "Figure 3: Outline of the system predicting 4 frames ahead in time. Only affine transforms A1, A2 and A3 are provided, and the model predicts A\u03034, A\u03035, A\u03036 and A\u03037, which are used to reconstruct the next 4 frames. Since affine parameters are continuous values and the whole chain of CNNs is differentiable, the whole unrolled system can be trained by back-propagation of the error. Note that CNNs all share the same parameters"
    },
    "2204.10509v1-Figure2-1.png": {
        "prediction": "The image contains a flowchart that illustrates the architecture of the PEGE model.",
        "reference": "Figure 2: Illustration of our PEGE Model"
    },
    "2202.05735v2-Figure1-1.png": {
        "prediction": "The image contains a flowchart that outlines the process of feature extraction and analysis from a photoplethysmography (PPG) signal. The flowchart starts with a waveform (WAV) input, which is then processed through a photoplethysmography (PPG) signal. The output of the PPG signal is a derived time series (DTS), which is further analyzed to obtain instantaneous pulse rate (IPR). The IPR is then subjected to feature engineering (FE) to extract pulse rate variability (PRV) and morphological measures (MOR). The extracted features are then fed into a neural network for further processing.",
        "reference": "Fig. 1: Three approaches to ML for sleep staging from PPG. Our new model, SleepPPG-Net takes as input the PPG waveform (WAV).The derived time series (DTS) and feature engineering (FE) approaches allow comparison with SOTA algorithms described in the literature."
    },
    "2101.02429v1-Figure6-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Figure 6: Illustration of the neural decomposition procedure for d = 2 (i.e. Z\u2217 \u2208 R3\u00d73). The dashed red arrows correspond to the eigenvectors of Z\u2217 (q1, q2, q3) and the solid blue arrows show the decomposed vectors p1 and p2. In this example, the rank of Z \u2217 is 2 where q1 and q2 are its two principal eigenvectors. The eigenvalue corresponding to the eigenvector q1 is zero. The light blue colored surface shows the Lorentz cones z = \u221a x2 + y2 and z = \u2212 \u221a x2 + y2. We observe that the decomposed vectors p1 and p2 lie on the boundary of Lorentz cones."
    },
    "1246860-Figure1-1.png": {
        "prediction": "The image contains a flowchart that illustrates a hierarchical approach to sentiment analysis. The flowchart starts with \"Document Sentiment\" at the top, indicating the overall sentiment of a document. Below this, there are two paths: one for \"Sentence Sentiment\" and another for \"Word Sentiment\". The \"Sentence Sentiment\" path leads to \"MIL Transfer\", which stands for Multiple Instance Learning Transfer, and then to \"sentence\", indicating the sentiment of individual sentences in the document. Similarly, the \"Word Sentiment\" path leads to \"MIL Transfer\" and then to \"word\", representing the sentiment of individual words in the document. The flowchart also shows that the output can be either \"Labels Exist\" or \"No Labels\", depending on the availability of sentiment labels for the input.",
        "reference": "Figure 1: Deep multi-instance transfer learning approach for review data."
    },
    "2102.10828v2-Figure7-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of analyzing execution traces and temporal relations.",
        "reference": "Figure 7. Illustration of Equation (4.1)."
    },
    "1022460-Figure1-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of using a convolutional neural network (CNN) for image classification. The flowchart starts with an input image of a road bike, which is then processed through a series of convolutional layers. Each convolutional layer is represented by a rectangle in the flowchart. The output of each convolutional layer is passed through a pooling layer, which is represented by a circle in the flowchart. The final output of the CNN is passed through a fully connected layer, which is represented by a trapezoid in the flowchart. The output of the fully connected layer is then passed through a softmax activation function, which is represented by a triangle in the flowchart. The final output of the softmax activation function is a probability distribution over the classes in the dataset. The flowchart also shows the loss function being calculated at each iteration of training, and the gradients being backpropagated through the network to update the weights.",
        "reference": "Figure 1. A feedback based learning model. The basic idea is to make predictions in an iterative manner based on a notion of the thus-far outcome. This provides several core advantages: I. enabling early predictions (given total inference time T , early predictions are made in fractions of T ); II. naturally conforming to a taxonomy in the output space; and III. better grounds for curriculum learning."
    },
    "2110.03031v3-Figure1-1.png": {
        "prediction": "The image contains a flowchart that represents a process or algorithm. The flowchart consists of a series of rectangles, which are likely representing steps or actions, and arrows that connect these rectangles, indicating the flow of the process.",
        "reference": "Figure 1. RieszNet architecture."
    },
    "2101.11189v1-Figure2-1.png": {
        "prediction": "The image contains a flowchart that outlines a process. The flowchart starts with a \"Start\" symbol and ends with a \"End\" symbol. The flowchart includes multiple steps, such as \"Step 1\", \"Step 2\", \"Step 3\", \"Step 4\", \"Step 5\", \"Step 6\", \"Step 7\", \"Step 8\", and \"Step 9\". The flowchart also includes decision points, represented by diamonds, where the process branches based on certain conditions. The flowchart is a visual representation of a workflow or a set of instructions, showing the steps as boxes of various colors and their order by connecting these with arrows.",
        "reference": "Fig. 2: The overall framework of our arbitrary-oriented ship detection method. Feature maps are first generated by using a fully convolutional network. Afterward, the peaks of the center feature map are selected as center points. Then, the center points offsets, object sizes and head regression locations are regressed on the corresponding feature maps on the position of each center point. The potential head points are collected by extracting peaks with confidence scores larger than 0.1 on the head feature map. The final head location is obtained by assigning each regressed location to its nearest potential head points."
    },
    "2204.01715v1-Figure7-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of data ingestion, transformation, and modeling in a distributed computing environment.",
        "reference": "Figure 7. BigDL-Orca architecture"
    },
    "2202.07508v3-Figure3-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Figure 3. Architecture of the dynamic deep linear kernel."
    },
    "2202.11824v1-Figure1-1.png": {
        "prediction": "The image contains a flowchart that illustrates a DNA-Sequencing Service. The flowchart starts with the \"BioCybernetic Hacking Alert\" (step 1) and proceeds to \"Input Control, CNN\" (step 2). It then moves to \"Shotgun Sequencing\" (step 3) and \"DNA Assembler\" (step 4). The flowchart ends with \"Activate Trojan SW\" (step 5) and \"Attacker\" (step 6).",
        "reference": "Fig. 1: Hybrid Trojan Bio-Cyber Hacking Attack. Steps 1 - 3 indicate a typical genetic sequencing operation for patients. Steps 4 - 6 indicate a situation where a hacker has embedded their IP address and Port number into a DNA that will trigger a remote connection from a Trojan-horse infected software tool leading to a connection to the attacker in Step 8. Our proposed approach utilizes Deep-Learning to detect Trojan payload in digital data using encoded into DNA strands that can prevent the attack."
    },
    "2011.04105v1-Figure3.5-1.png": {
        "prediction": "The flowchart illustrates a process with the following steps:\n\n1. Network-Wide Views: This is the starting point of the process.\n2. Discovery Plane: This step is connected to the Network-Wide Views and is likely a part of the process that follows.\n3. Data Plane: This step is also connected to the Network-Wide Views, indicating that it is another component of the process.\n4. Dissemination Plane: This step is connected to both the Discovery Plane and the Data Plane, suggesting that it integrates information from both of these planes.\n5. Decision Plane: This step is connected to the Dissemination Plane, indicating that it uses the information disseminated to make decisions.\n6. Direct Control: This step is also connected to the Dissemination Plane, implying that it receives information from the Dissemination Plane.\n7. Network-level objectives: This is the final step in the process, connected to both the Decision Plane and Direct Control, indicating that the network-level objectives are achieved through the decisions made and the direct control exercised.",
        "reference": "Figure 3.5: Network-level objectives."
    },
    "2202.01326v1-Figure5-1.png": {
        "prediction": "A flowchart is depicted with the following steps:\n\n1. Start by asking if triplets from SF are enhancing the PV device.\n2. If the answer is no, proceed to check if the features match SF material absorption.\n3. If the answer is yes, then check if IQE is greater than 100%.\n4. If the answer is yes, then triplets are playing a role and it is very likely from SF.\n5. If the answer is no, then proceed to MPC.\n6. At MPC, check if a triplet effect is seen.\n7. If the answer is yes, then triplets are playing a role.\n8. If the answer is no, then check if there is evidence for an effect.\n9. If the answer is yes, then triplets are playing a role.\n10. If the answer is no, then check if a triplet effect is seen.\n11. If the answer is yes, then triplets are playing a role.\n12. If the answer is no, then check if singles are playing a role.\n13. If the answer is yes, then singles are playing a role.\n14. If the answer is no, then there is no evidence for an effect.\n15. End the flowchart.",
        "reference": "Figure 5: Flowchart describing the process for determining if SF is improving photocurrent in a device."
    },
    "2204.08696v2-Figure1-1.png": {
        "prediction": "The image contains a flowchart that outlines a process. The flowchart starts with a box labeled \"Input\" and ends with a box labeled \"Output\". Between these two boxes, there are several other boxes and arrows that represent different steps or components of the process. Some of the boxes are labeled with acronyms or abbreviations, such as \"Encoder\", \"Decoder\", \"LGCM\", \"FEU\", \"MFU\", and \"FAU\". The arrows indicate the flow of information or processing between these components.",
        "reference": "Fig. 1: The complete structure of the proposed CNN-Transformer Cooperation Network (CTCNet). CTCNet is a U-shaped symmetrical hierarchical network with three stages: encoding stag, bottleneck stage, and decoding stage. Among them, the encoding stage is designed to extract local and global features with different scales, and the decoding stage is designed for feature fusion and image reconstruction."
    },
    "2101.08366v2-Figure2-1.png": {
        "prediction": "The flowchart illustrates a Quadratic Residual Network (QRes) Layer. It takes an input x and processes it through two linear transformations represented by W1 and W2. The outputs of these transformations are then combined using an element-wise addition operation, followed by a non-linear activation function, typically represented by the symbol \u03c3. The final output y is obtained after passing the combined result through the activation function.",
        "reference": "Figure 2: Overview of our proposed Quadratic Residual Network (QRes) layer in comparison with plain DNN layer. Blue rectangular boxes represent trainable parameters and round boxes represent operations (purple \u201c\u00d7\u201d: multiplication, orange \u201c+\u201d: addition, green \u201d\u00b7\u201d: Hadamard product, and cyan \u201d\u03c3\u201d: activation operator)."
    },
    "2205.09510v2-Figure6.4-1.png": {
        "prediction": "The flowchart illustrates a process that involves three main components: quantum data, an average block, and a classical optimizer. The process begins with quantum data, which is represented by a cloud icon. This data is then passed through an average block, which is represented by a rectangle with an arrow pointing to a circle. The output of the average block is then fed into a classical optimizer, which is represented by another rectangle. The flowchart does not show any decision points or loops, indicating a linear process flow.",
        "reference": "Figure 6.4: An illustration of the \u201cQC\u201d setting of quantum machine learning, in which data are quantum and processing is classical."
    },
    "2205.09510v2-Figure6.11-1.png": {
        "prediction": "The flowchart illustrates a process that involves a classical optimizer and an average function. The process starts with a classical optimizer, which is represented by the first box. The output of the classical optimizer is then passed through an average function, as indicated by the arrow pointing to the average function. The average function then produces an output, which is the final result of the process.",
        "reference": "Figure 6.11: Illustration of the operation of a VQE. The classical optimizer aims at minimizing the expected value \u3008F \u3009\u03c8(\u03b8)\u3009 = \u3008\u03c8(\u03b8)|F |\u03c8(\u03b8)\u3009 of the observable F ."
    },
    "2206.05240v1-Figure2-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of posterior sampling in an uncertain environment. The flowchart includes the following steps:\n\n1. The process begins with the agent receiving an observation from the environment.\n2. The agent then selects an action based on the observation and the current state of the environment.\n3. The agent executes the selected action in the environment.\n4. The environment provides a reward to the agent based on the action taken.\n5. The agent updates its beliefs about the environment's dynamics based on the new observation and reward.\n6. The agent samples a new action from the updated posterior distribution over actions.\n7. The process repeats, with the agent continuing to interact with the environment and updating its beliefs until a stopping criterion is met.\n\nThroughout the process, the agent is guided by a curriculum that provides a sequence of tasks or goals to be achieved. The curriculum is designed to gradually increase the level of difficulty and uncertainty in the tasks, allowing the agent to learn and adapt to the changing environment.",
        "reference": "Figure 2: Model Overview. We propose CBRL for RCB. Guided by a curriculum sequence, the agent learns to adaptively trade-off constraints and the objective in non-stationarymarkets. During deployment, the agent updates its belief over the market dynamics based on its past experience, and acts through posterior sampling."
    },
    "2210.02172v1-Figure2-1.png": {
        "prediction": "The flowchart illustrates a process that involves the interaction between a target neural network and an evolution neural network. The process begins with the target neural network receiving an experience memory, which contains information about the current state, action, and reward. The target neural network then processes this information to evaluate the Q-value, which represents the quality of the action taken in the current state.\n\nThe evolution neural network is updated based on the approximation error between the target Q-value and the Q-value calculated by the target neural network. This process of updating the evolution neural network continues until an optimal policy is achieved. The optimal policy is then used to guide the actions of the system in the environment, which consists of small-cell and macro-cell base stations (BS) and internet of radio and things (IRT) devices.",
        "reference": "Fig. 2: An illustration of deep Q-learning model for the ISRE-based 6G vehicular aided HetNet environment. The model uses a deep neural network to find the approximate Q-values. The model includes experience replay mechanism (to remove the correlation between different observations), a feature set (given as input to the deep neural network), a target Q-network for updating the primary Q-network and the simulation environment for extracting different parameters."
    },
    "14844989-Figure2-1.png": {
        "prediction": "The image contains a flowchart that illustrates the architecture of a multi-task network for ranking and classification tasks. The network is designed to be cross-domain, meaning it can be applied to different domains or tasks. The architecture consists of a convolutional layer, a fully connected layer, and a softmax loss function. The output of the network is then used to calculate triplet loss and contrastive loss. The flowchart shows the data flow and processing steps within the network.",
        "reference": "Figure 2: The framework of the proposed multi-task deep network and the cross-domain architecture. The crossdomain architecture is only used when an auxiliary dataset is needed for training."
    },
    "2205.03661v1-Figure1-1.png": {
        "prediction": "The flowchart illustrates the process of designing baseline deep networks and methods for binarizing them. The process starts with preprocessing the 1-D ECG signal. The design of baseline deep networks involves designing basic blocks, determining network depth, and determining the number of basic blocks. The methods of binarizing baseline networks include replacing the Sign(x) function with the piecewise function F(x), introducing the learnable parameter a, and maintaining the accuracy of the network. The goals of these methods are to enhance computing speed, reduce memory overhead, and maintain the accuracy of the network.",
        "reference": "Fig. 1. Overview of methods adopted"
    },
    "162169-Figure3-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of event detection and classification in noisy environments.",
        "reference": "Figure 3 Partial illustration of frames, roles, and lexical units related to the CAUSE TO MAKE NOISE frame, from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are unfilled bars. No particular significance is ascribed to the ordering of a frame\u2019s roles in its lexicon entry (the selection and ordering of roles above is for illustrative convenience). CAUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here."
    },
    "2202.03344v2-Figure2-1.png": {
        "prediction": "The flowchart outlines a process for selecting the type of latent variable in a model. It begins with an input, which is a model with a latent variable. The first decision point is to evaluate the degree of improvement in the model's performance. If the improvement is significant, the process proceeds to the next step, which is to initialize the latent variable. If there is no significant improvement, the process moves to a decision point about whether to evaluate the model's adaptivity. If the model is adaptive, the process continues to the next step, which is to initialize the latent variable. If the model is not adaptive, the process moves to a decision point about whether to cross-validate the model. If the model is cross-validated, the process proceeds to the next step, which is to update the latent variable. If the model is not cross-validated, the process ends. The output of the process is a model with a latent variable.",
        "reference": "Figure 2: Flow chart of the procedure to adaptively build a stochastic PCE"
    },
    "2207.12764v1-Figure1-1.png": {
        "prediction": "The image contains a flowchart that outlines a process for data analysis. The flowchart starts with \"OCEL Extraction\" and proceeds to \"Enrich OCEL with complex object-centric process model related attributes.\" The next step is \"Select the desired object type for clustering.\" Following this, the flowchart leads to \"Clustering\" which is further divided into two paths: \"K-means\" and \"Agglomerative.\" The \"K-means\" path leads to \"Assigning Events to the Cluster\" which has two conditions: \"Existence: The cluster contains sub-OCEL\" and \"All the sub-OCEL objects that belong to the event.\" The \"Agglomerative\" path also leads to \"Assigning Events to the Cluster\" with the same conditions.",
        "reference": "Fig. 1: Overview of the proposed framework."
    },
    "2103.13446v3-Figure2-1.png": {
        "prediction": "The image contains a flowchart that illustrates a process involving multiple steps. The flowchart starts with an \"Observations\" step, which is represented by a diamond-shaped box. The output of this step is fed into a function \"f(input)\". The output of this function is then received by multiple neighbors, as indicated by the arrows pointing to multiple \"Receive from Neighbors\" boxes. Each of these boxes has an output that is fed into another function \"f(com)\". The outputs of these functions are then received by multiple neighbors again, as indicated by the arrows pointing to multiple \"Receive from Neighbors\" boxes. This process is repeated for a certain number of iterations, as indicated by the ellipsis. The outputs of the final \"Receive from Neighbors\" boxes are then concatenated and fed into a \"Node Input\" box, which is the final step of the flowchart.",
        "reference": "Fig. 2: ModGNN\u2019s message aggregation module. In this diagram, the system is shown from a centralized point of view. First, the raw observation or output from the last layer is transformed by finput. Then, for each transmission up to K hops, the data from the neighboring agents is passed through fcom and then aggregated. The output is the set of the data from each k-hop neighborhood up to K."
    },
    "2107.06007v1-Figure14-1.png": {
        "prediction": "The image contains a flowchart that illustrates the architecture of a distributed control system. The flowchart shows the interaction between various components of the system, including SQL servers, transfer PCs, PLCs, on-site control PCs, and off-site control PCs. The system is designed to collect and process data from different locations, such as the Waste Isolation Pilot Plant in Carlsbad, NM, and the SLAC National Accelerator Lab, and provide control parameters and system data output to worldwide user computers.",
        "reference": "Figure 14. The EXO-200 slow control architecture consisted of three major parts- the MySQL backbone, the embedded system, and the distributed system."
    },
    "2107.12859v2-Figure3-1.png": {
        "prediction": "The image contains a flowchart that illustrates the architecture of a neural network.",
        "reference": "Figure 3: One iteration of our Recurrent Graph Learning framework. (a) We process part features and compute a graph message. (b) The message is encoded sequentially in our bidirectional GRU framework. (c) The features generated by the forward and reverse GRU are used to regress part-pose. We use three such iterations in our framework."
    },
    "2012.00248v1-Figure1-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Fig. 1. Schematic diagram for synthesizing the digital Fourier hologram. The letter objects are placed at a distance d in front of the Fourier lens in the direction of propagation of light. The real-valued hologram are drawn in the logarithmic scale."
    },
    "2010.06310v1-Figure2-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of entity and trigger extraction. The flowchart starts with the input of ground-truth joint annotation sequences, which are then passed through a sequence-to-sequence labeling process. The output of this process is an annotated tag sequence that includes both entity and trigger information.\n\nNext, the flowchart branches into two main paths: one for entity extraction and the other for trigger extraction. Both paths involve the use of a recurrent neural network (RNN) to encode the input sequence and a softmax layer to generate the predicted distribution over the vocabulary.\n\nFor entity extraction, the predicted distribution is compared to the ground-truth entity distribution using KL divergence, and the resulting loss is backpropagated through the network to update the parameters. Similarly, for trigger extraction, the predicted distribution is compared to the ground-truth trigger distribution using KL divergence, and the resulting loss is backpropagated through the network to update the parameters.\n\nFinally, the losses from both the entity extraction and trigger extraction paths are combined to form the final loss, which is used to update the parameters of the network. The flowchart ends with the output of the final loss, which represents the overall performance of the model on the given task.",
        "reference": "Fig. 2: The framework of the joint-event-extraction model with our proposed cross-supervised mechanism."
    },
    "2202.05336v1-Figure1-1.png": {
        "prediction": "The image contains a flowchart that outlines a process. The flowchart starts with an \"Input Image i\" box, which is connected to a \"Background Subtraction\" box. The output of the background subtraction is labeled as \"B_i\". The \"B_i\" box is connected to an \"Inverse Warping\" box, and the output of the inverse warping is labeled as \"F_i\". The \"F_i\" box is connected to an \"Initial Binary Segmentation\" box. The output of the initial binary segmentation is labeled as \"S_init\". The \"S_init\" box is connected to a \"Motion Estimation\" box. The output of the motion estimation is labeled as \"M_i\". The \"M_i\" box is connected to a \"Warping of Input Images\" box, and the output of the warping is labeled as \"warp(I,M)\". The \"warp(I,M)\" box is connected to a \"Background Generation\" box, and the output of the background generation is labeled as \"B_gen\". The \"B_gen\" box is connected to a \"Dynamic Entropy Map Computation\" box, and the output of the dynamic entropy map computation is labeled as \"C(x)\". The \"C(x)\" box is connected to a \"Threshold Computation\" box, and the output of the threshold computation is labeled as \"S_postProc\". The \"S_postProc\" box is connected to a \"Binary Segmented Image i\" box.",
        "reference": "Fig. 1. Block diagram of DBSGen"
    },
    "2107.03026v1-Figure7-1.png": {
        "prediction": "The flowchart illustrates a continuous cycle of influence between Ecological Systems and Social Economic Systems. The cycle starts with Ecological Systems having an influence on Social Economic Systems, which is then followed by Social Economic Systems having an influence back on Ecological Systems. This bidirectional influence continues indefinitely, indicating a continuous interplay between the two systems.",
        "reference": "Figure 7: Influence matrix schematic graph, based on [5, Figure 5]"
    },
    "1381187-Figure1-1.png": {
        "prediction": "Figure 1: Illustration of the encoder-decoder architecture of the proposed method.",
        "reference": "Fig. 1: Illustration of the SegNet architecture applied to EO data."
    },
    "13567899-Figure3-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Figure 3. V2V Architecture for Voxel Prediction. The lower part (below dashed line) consists of layers from C3D [28]. Connected to these layers we have three 3D convolution layers: Conv3c,Conv4c,Conv-pre use filters of size 3\u00d7 3\u00d7 3 with stride 1\u00d7 1\u00d7 1. Both Deconv5 and Deconv4 are deconvolutional layers employing kernels of size 4 \u00d7 4 \u00d7 4 with output stride of 2 \u00d7 2 \u00d7 2. Deconv3 has kernel size 8 \u00d7 4 \u00d7 4 and output stride of 4 \u00d7 2 \u00d7 2. The numbers inside the boxes represent the number of learning filters in that layer, while the numbers near the boxes (above or below) represent the size of output signals produced by that layer. The part inside the thick-dashed box is application-dependent."
    },
    "2204.10374v1-Figure1-1.png": {
        "prediction": "The flowchart illustrates a hierarchical reinforcement learning architecture for gesture recognition and generation on mobile devices. The architecture consists of three reinforcement learning agents: Gesture RL agent 0, Gesture GVF selection RL agent 1, and Gesture class selection RL agent 2. The agents interact with the Android Dev action and the Gesture action, which in turn provide pixel observations and rewards.",
        "reference": "Figure 1 | Gesture Hierarchy. The architecture used for the Android applications is based on a 3-layer hierarchy: (1) The lowest level operates over GVFs corresponding to all supported gestures; (2) The middle layer selects a gesture GVF given the latest pixel image in AndroidEnv and its agent is trained to maximize the return associated with the task that the agent is trained on; and (3) The top layer selects a single gesture class for the task and the agent is trained to maximize the average per step reward. All levels are operated by distributed DQN agents."
    },
    "2010.00041v1-Figure3-1.png": {
        "prediction": "The flowchart illustrates a genetic algorithm process. It begins with the random creation of the initial population, followed by evaluating the fitness of the population. If the stopping criteria are not satisfied, the process proceeds to passing elites to the next population, selecting operators based on fitness, performing crossover, and mutation to create a new population. The process then loops back to evaluate the fitness of the new population. If the stopping criteria are satisfied, the algorithm ends and returns the best individual.",
        "reference": "Fig. 3. The optimization flowchart for the inverse material design."
    },
    "2103.14005v2-Figure9-1.png": {
        "prediction": "The image contains a flowchart that outlines a process. The flowchart starts with a \"Start\" box and ends with an \"End\" box. In between, there are several boxes labeled with various tasks or steps, such as \"Step 1\", \"Step 2\", and so on. Arrows connect these boxes, indicating the flow from one step to the next.",
        "reference": "Figure 9. The Multi Input Fusion Classifier end task architecture. The orange box shows the frozen encoder."
    },
    "2110.14764v2-Figure1-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of document classification. The flowchart starts with \"Raw Documents\" and proceeds to \"First-tier Classifiers.\" From there, it moves to \"Vectors of Calibrated Posterior Probabilities\" and then to \"Stacking.\" Finally, it ends with \"Meta-Classifier.\"",
        "reference": "Fig. 1. The Fun architecture, exemplified with |L|=3 languages (Chinese, Italian, English). Note that the different term-document matrices in the 1st-tier may contain different numbers of documents and/or different numbers of terms. The three grey diamonds on the left represent calibrated classifiers that map the original vectors (e.g., TFIDF vectors) into |Y|-dimensional spaces. The resulting vectors are thus aligned and can all be used for training the meta-classifier, which is represented by the grey diamond on the right."
    },
    "1422748-Figure1-1.png": {
        "prediction": "The flowchart illustrates a process that starts with \"Document Text\" and proceeds through several steps including \"Text Preprocessing,\" \"Language Detection,\" \"Text Normalization,\" and \"Sentence Breaking.\" The final step is \"Entity Extraction\" which leads to \"Entity Disambiguation and Linking.\"",
        "reference": "Figure 1: Overview of the Lithium NLP pipeline"
    },
    "2101.07004v1-Figure2-1.png": {
        "prediction": "The flowchart illustrates a process involving uplink pilot transmission, processing, and downlink data transmission. The process starts with the transmission of an uplink pilot signal, which is then processed. Following the processing, downlink data is transmitted.",
        "reference": "Figure 2. Block diagram of one transmission block."
    },
    "1061647-Figure5-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of image analysis using a convolutional neural network (CNN). The flowchart starts with a 256x256 image as input. The image then passes through a series of convolutional layers, each followed by a deconvolutional layer. The final output of the flowchart is a 60x60 heat map, which represents the activation of the network at the output layer.",
        "reference": "Fig. 5. Architecture of the pose estimation network. First 5 layers copied from VGGM, followed by a deconvolution layer. A 1x1 convolution layer is then used to predict each output heat map."
    },
    "2103.06081v1-Figure2-1.png": {
        "prediction": "The image contains a flowchart that illustrates the architecture of BERT in pretraining. The flowchart shows the process of predicting masked tokens and the next sentence prediction task.",
        "reference": "Figure 2. Illustration of the illumination scheme of double SIM using far-field optics. (a) Schematic of one possible approach to generate the illumination patterns with structured electric energy density and OC simultaneously. Structured OC patterns formed by the interference of (b) two L-CPL beams ( )C x+ and (c) two R-CPL beams ( )C x\u2212 . (d, e) Structured patterns of ( )eU x . The OC patterns in (b) and (c) possess the opposite handedness. The eU patterns in (d) and (e) are the same. Line-cut profile of the (f) OC and (g) eU along the dashed lines in (b-e). Scale bar in (b) is 02\u03c0/k and applicable for (d-e)."
    },
    "2202.03373v1-Figure5-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Figure 5. An illustration of Curve Non-Linear Unit. This layer can be seen as a learnable non-linear activation function between 0 and 1. Based on Eq. 3, the learned function always follows concave down increasing curves to increase feature intensities."
    },
    "2207.07828v1-Figure2-1.png": {
        "prediction": "Figure 1 for Structural Prior Learning for Weakly Supervised Semantic Segmentation, which was originally presented at CVPR 2019.",
        "reference": "Fig. 2. Overview of the proposed Structural Prior guided Generative Adversarial Transformers (SPGAT). The SPGAT contains one generator, two discriminators, and one structural prior estimator. The generator is used to explore non-local information with the guidance of a structural prior estimator (SPE) for better clear image restoration. We build the skip connections between the generator and discriminators so that the discriminators can better discriminate between real and fake features in the generator for generating more natural images. Meanwhile, SPE is also utilized to guide the discriminators for better estimations. The generator takes a low-light image L as the input and outputs an enhanced image E\u0302. The SPE is input the corresponding structure S of L by gradient operation and outputs the estimated structural prior P\u0302. Both the two discriminators take the concatenation of the enhanced image E\u0302 and estimated structural P\u0302 as the input, which is regarded as the fake label. However, there is no real label feature. To that aim, we generate real label features by inputting the normal-light image to the generator and its corresponding structure to SPE. PW-STB illustrated in Fig. 3 is the basic block of the generator, discriminators, and SPE."
    },
    "2102.08827v1-Figure3-1.png": {
        "prediction": "The image contains a flowchart that illustrates the relationship between a scene ontology and a skill ontology. The scene ontology is divided into four layers: L1-road-level, L2-traffic-in-infrastructure, L3-temporary manipulation of L1 and L2, and L4-objects. Each layer is connected to a scene element, which is further connected to a skill ontology. The skill ontology consists of various skills such as system skill, behavioral skill, planning skill, perception skill, action skill, data acquisition skill, and actuation skill. The flowchart shows that each skill ontology depends on the environment and the layer it is associated with.",
        "reference": "Fig. 3. Class diagram of the connections between skills and scene elements."
    },
    "2204.08805v1-Figure5-1.png": {
        "prediction": "The flowchart illustrates a process with three main sections: User Interface, Interaction, and Feedback. The User Interface section shows a series of images and icons, possibly representing different options or views that a user can interact with. The Interaction section highlights two main activities: viewpoint navigation and attributes editing. These activities likely involve the user adjusting the perspective of the images or modifying certain characteristics of the objects in the images. \n\nThe Feedback section outlines three processes that occur as a result of the Interaction: timeline overview generation, body model augmentation, and suggestive viewpoint. These processes seem to involve creating a summary of the user's actions, enhancing the visual representation of the body in the images, and providing suggestions for optimal viewing angles, respectively. \n\nThe central part of the flowchart, labeled \"video processing,\" encompasses two main activities: 3D pose reconstruction and temporal segmentation. These activities likely involve analyzing the user's interactions to understand the poses and movements captured in the video, and then organizing these poses and movements into distinct segments over time. \n\nFinally, the flowchart also includes a \"pose analysis\" section, which breaks down into sequence alignment and attributes retrieval. These processes seem to involve aligning the different poses and movements captured in the video into a coherent sequence and extracting specific attributes or characteristics of these poses, respectively.",
        "reference": "Fig. 5. The architecture of VCoach, which comprises five main modules."
    },
    "2207.13865v1-Figure3-1.png": {
        "prediction": "The image contains a flowchart. The flowchart starts with a \"Randomly sampling\" block, which leads to two paths. One path goes to a \"Ground set of domains\" block, and the other path goes to an \"InvDANN\" block. The \"InvDANN\" block leads to an \"Initial model\" block, which then leads to an \"ERM\" block. The \"Ground set of domains\" block leads to a \"Level-one sampling\" block, which then leads to a \"Batches inside sampled domains\" block. The \"Batches inside sampled domains\" block leads to a \"featurizer\" block, which then leads to a \"Level-two sampling\" block. The \"Level-two sampling\" block leads to another \"featurizer\" block, which then leads to an \"ERM\" block.",
        "reference": "Figure 3: Illustration of the sampling framework of DOMI. The solid arrow indicates the actual sampling flow, while the dotted arrow only indicates the difference between randomly sampling and DOMI."
    },
    "1828713-Figure2-1.png": {
        "prediction": "The image contains a flowchart that represents a sequence of processing steps. The flowchart starts with a \"Softmax\" layer, which is followed by a \"Pooling\" layer. The output of the \"Pooling\" layer is then passed through a \"Convolution\" layer, which is connected to a \"Tanh\" layer. The output of the \"Tanh\" layer is then fed into a \"Linear\" layer, which is connected to a \"Lookup\" layer. The flowchart then splits into two branches, with one branch leading to a \"Softmax\" layer and the other branch leading to a \"Pooling\" layer. The output of the \"Softmax\" layer in the first branch is labeled \"gold rating = 2\", indicating that it corresponds to a specific rating or classification. The output of the \"Pooling\" layer in the second branch is labeled \"pj\", which could be an intermediate output or a final prediction.",
        "reference": "Figure 2: An illustration of the neural network approach for sentiment classification. wi means the i-th word of a review text. uk and pj are continuous vector representations of user k and product j for capturing user-sentiment and product-sentiment consistencies. Uk and Pj are continuous matrix representations of user k and product j for capturing user-text and product-text consistencies."
    },
    "53776814-Figure4-1.png": {
        "prediction": "The image contains a flowchart that illustrates the architecture of a neural network.",
        "reference": "Figure 4. Illustration of stacked hourglass STGCN with two levels."
    },
    "2107.01209v1-Figure5-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Figure 5. Left: The DM particle enters the bulk of the sun with an angle \u03b8, collides with solar electrons and finally escapes from the Sun. For the purpose of illustration, the DM trajectory is projected onto a single plane. Right: The flux of reflected DM particles can be approximated as propagating along the radial direction from the center of the Sun. See text for details."
    },
    "2110.14160v2-Figure2-1.png": {
        "prediction": "",
        "reference": "Figure 2: Components analyzed in our deep learning-based DR grading framework. The evaluation process of a framework can be divided into two parts: training (top) and testing (bottom). In the training phase, we first fix the architecture of the selected network (ResNet-50). Then we examine a collection of designs with respect to the training setting including preprocessing (image resizing and enhancement), training strategies (compositions of data augmentation (DA) and sampling strategies) and optimization configurations (objective functions and learning rate (LR) schedules). In the testing phase, we apply the same preprocessing as in the training phase and employ paired feature fusion to make use of the correlation between the two eyes (the training step of the fusion network is omitted in this figure). Then, we select the best ensemble method for the final prediction."
    },
    "2205.12022v1-Figure1-1.png": {
        "prediction": "The image contains a flowchart that outlines a process with various steps and decision points. The flowchart starts with an initial step labeled \"Start\" and ends with a final step labeled \"End\". In between, there are multiple steps and decision points represented by rectangles and diamonds, respectively. The flow of the process is indicated by arrows connecting the different steps and decision points.",
        "reference": "Fig. 1. Overview of our model. Our model consists of parsing generator and image generator, training parsing generator requires a pair of source-target images IS , IT , then obtaining human keypoints KS ,KT and human parsing map PS , PT respectively by using openpose and PGN framework. We concatenate KS , PS ,KT as the input of parsing generator, then the input is fed into an Unet-type network that generates a target parsing map with the same size of PT , which contains body shape information. To get the vivid image with detailed texture (e.g.style of clothing), we extract the Per-region feature information PSj from the source image via VGG type network, then we concatenate the parsing map to the above KS , PS ,KT and normalize it along with the region information PSj to combine the information of source texture and target parsing map. Finally, the target image can be generated by spatial normalization and decoder."
    },
    "2207.06080v2-Figure2-1.png": {
        "prediction": "The image contains a flowchart that outlines the process of image classification using a convolutional neural network (CNN). The flowchart starts with the input image, which is then passed through a series of convolutional layers with non-linear activations. The output of these layers is then fed into a dense layer, which produces logits. Finally, the logits are passed through a softmax activation function to produce a probability distribution over the classes, and the predicted class is obtained by taking the argmax of this distribution.",
        "reference": "Fig. 2: Illustration of feature and classification embeddings, using the Resnet 32 architecture. Feature embeddigs (FE) are extracted by the CNN\u2019s non-linear layers and are combined with the final classification layer weights to form classification layer embeddings (CLE), before they are summed."
    },
    "2204.09225v1-Figure1-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Fig. 1. Illustration of the proposed Twin-Transformers framework. (a) shows the overall picture of the proposed model, and (b) shows the details of the Twin-Transformers. The Twin-Transformers take the identical brain signal matrix as input. The spatial Transformer focuses on the space dimension and takes non-overlapping spatial patches as tokens to build attention in the spatial dimension and generate spatial features. Conversely, the temporal Transformer focuses on the temporal dimension and the non-overlapping temporal patches are used as tokens. Correspondingly, the temporal Transformer is designed to build attention in the time dimension and generate temporal features. The twin Transformers are designed for disentangling temporal and spatial features of the input signal matrix."
    },
    "2102.09837v1-Figure2-1.png": {
        "prediction": "The flowchart illustrates a process with three main states: \"Ready\", \"Calibrating\", and \"Calibrated\". The process starts in the \"Ready\" state and proceeds to the \"Init\" step, where the system initializes the calibration process. After the \"Init\" step, the system enters the \"Calibrating\" state, which is a loop that continues until the calibration is complete. Within the \"Calibrating\" loop, there are two paths: one for the entity \"e\" and another for the entity \"s\". Both paths involve a calibration action and a timer \"tp\" that is incremented by 5. The loop continues until the timer reaches a certain threshold, at which point the system transitions to the \"Calibrated\" state. In the \"Calibrated\" state, the system has completed the calibration process and is ready for the next step.",
        "reference": "Figure 2: The platform model of a robot arm."
    },
    "2202.01032v2-Figure5-1.png": {
        "prediction": "The image contains a flowchart that illustrates a service management and orchestration framework for non-real-time radio intelligence centers (RIC). The flowchart is divided into several layers, each representing a different component or functionality of the framework.\n\nAt the top layer, there is a \"Non-real-time RIC\" box, which likely refers to the overall system or platform being described. Below this, there are multiple boxes labeled \"xApp 1,\" \"xApp 2,\" and so on, up to \"xApp N.\" These xApps are likely individual applications or services that run on the RIC platform.\n\nIn the middle layer, there are several functionalities or services that support the xApps. These include \"API/SDK to support xApps,\" \"Internal messaging infrastructure,\" \"Conflict mitigation,\" \"Subscription Management,\" and \"Security.\" These services likely provide the necessary infrastructure and functionalities for the xApps to operate effectively and securely.\n\nAt the bottom layer, there is a box labeled \"RAN,\" which stands for Radio Access Network. This likely represents the network infrastructure that the RIC platform manages or interfaces with.",
        "reference": "Fig. 5: Near-RT RIC architecture. The near-RT RIC connects to the RAN through the E2 interface, at the bottom of the figure (yellow), and to the non-RT RIC/SMO through the A1 and O1 interfaces, at the top of the figures (orange and green, respectively). The communication among the RIC components (in light blue) is mediated by an internal messaging infrastructure. The near-RT RIC can onboard custom logic as xApps (dark blue)."
    },
    "2202.01032v2-Figure6-1.png": {
        "prediction": "The image contains a flowchart that represents a Service Management and Orchestration (SMO) Framework for Non-real-time Radio Intelligent Control (RIC). The flowchart outlines the various components and interactions within the system.",
        "reference": "Fig. 6: Non-RT RIC and SMO architecture. The SMO functionalities (in green) enable connectivity to the O-Cloud (through the O2 interface) and the other RAN components (through O1) for management and orchestration. The non-RT RIC features custom logic (rApps, in red), and a termination of the A1 interface to the near-RT RIC (orange). Shared functionalities between the non-RT RIC and the SMO are in yellow."
    },
    "2202.02070v2-Figure2-1.png": {
        "prediction": "The image contains a flowchart that outlines a process. The flowchart starts with a \"Start\" symbol and ends with a \"Finish\" symbol. In between, there are several rectangles representing steps in the process. The steps are labeled with text and connected by arrows, indicating the flow of the process from one step to the next.",
        "reference": "Fig. 2. The architecture of the proposed CGiS-Net."
    },
    "1319230-Figure1-1.png": {
        "prediction": "The flowchart illustrates a process that involves three main steps: Query, Document, and Answer Generation. The process starts with a Query (x) and a Document (d). The Query and Document are then processed through a Sentence Selection (Latent) step, which leads to the creation of a Document Summary (d'). The Document Summary is then fed into an Answer Generation (RNN) step, which ultimately produces an Answer (y).",
        "reference": "Figure 1: Hierarchical question answering: the model first selects relevant sentences that produce a document summary (d\u0302) for the given query (x), and then generates an answer (y) based on the summary (d\u0302) and the query x."
    },
    "210713891-Figure1-1.png": {
        "prediction": "The image contains a flowchart that represents the architecture of a system for the AVSD task. The flowchart consists of several components, including an image input, several feature extraction layers (i3d-flow, i3d-rgb, and vggish), and an answer generation component. The feature extraction layers process the input image to extract various features, which are then passed to the answer generation component. The answer generation component uses the extracted features to generate a response to the AVSD task.",
        "reference": "Figure 1: An illustration of DSTC8 AVSD task. The goal of the task is generating accurate answer based on multiple modalities."
    },
    "2206.00726v1-Figure1-1.png": {
        "prediction": "The flowchart illustrates a process that involves random sampling, dynamics classification, and trajectory generation for multiple vehicles. The process starts with random sampling, which is followed by dynamics classification for each vehicle. Based on the dynamics classification, trajectories are generated for each vehicle. These trajectories are then checked for collisions and collision avoidance. The process also includes a multi-agent trajectory generation step, which leads to an acquisition function evaluation. Based on the evaluation, the surrogate model is updated.",
        "reference": "Fig. 1: Overview of our proposed algorithm. The BayesOpt model is composed of multiple Gaussian process models, which represent dynamic feasibility and collision avoidance constraints."
    },
    "2101.09353v2-Figure1-1.png": {
        "prediction": "The image contains a flowchart that illustrates a system for monitoring the environment using a drone. The flowchart includes the following steps:\n\n1. The drone is equipped with radioprobes that collect environmental data.\n2. The radioprobes transmit the collected data to a base station.\n3. The base station receives the data from the radioprobes.\n4. The base station processes the received data.\n5. The processed data is transmitted to a processing machine.\n6. The processing machine receives the processed data from the base station.\n7. The processing machine analyzes the received data and generates reports or alerts as necessary.\n8. The processing machine stores the analyzed data in a database for future reference.\n\nThe flowchart shows the data flow and processing steps involved in using a drone for environmental monitoring.",
        "reference": "Figure 1. Working principle and radioprobe system architecture."
    },
    "2101.01418v1-Figure2-1.png": {
        "prediction": "The image contains a flowchart that outlines the process of image processing. The flowchart starts with the \"Problem Domain\" box, which is connected to the \"Image Acquisition\" box. From there, the flowchart branches into two paths: one for \"Pre-processing\" and the other for \"Low Level Processing.\" The \"Pre-processing\" path includes \"Contrast Enhancement\" and \"Noise Removal\" boxes, while the \"Low Level Processing\" path includes \"Segmentation\" and \"Threshold Based\" boxes. The \"Threshold Based\" box is further connected to \"Region Based,\" \"Gradient Based,\" \"Classification Based,\" and \"Hybrid Based\" boxes. The flowchart then converges back into a single path with the \"Intermediate Level Processing\" box, which is connected to the \"High Level Processing\" box. The \"High Level Processing\" path includes \"Representation,\" \"Description,\" \"Recognition,\" and \"Interpretation\" boxes, which are all connected to the \"Results\" box.",
        "reference": "Figure 2: Different levels in image processing process."
    },
    "2101.05151v2-Figure5-1.png": {
        "prediction": "Figure 1: Illustration of the iterative prediction process. The prediction process is initialized with the inference input H(t) and iteratively updates the hidden state H(t+1) for k steps to obtain the final inference output H(t+k).",
        "reference": "Figure 5: Graphical illustration of long horizontal link forecasting. Given a sequence of graph snapshots G = {G(t\u2212tk), ...,G(t)}, whose length is k, test quadruples at t+ \u2206T are to be predicted."
    },
    "1482745-Figure1-1.png": {
        "prediction": "The image contains a flowchart that illustrates the architecture of a GAN discriminator and a PacGAN2 discriminator.",
        "reference": "Figure 1: PacGAN(m) augments the input layer by a factor of m. The number of edges between the first two layers are increased accordingly to preserve the connectivity of the mother architecture (typically fully-connected). Packed samples are fed to the input layer in a concatenated fashion; the grid-patterned nodes represent input nodes for the second input sample."
    },
    "2210.08812v1-Figure2-1.png": {
        "prediction": "The image contains a flowchart that illustrates the architecture of BERT in pretraining. The flowchart shows how the model processes an input sequence to predict masked tokens and perform next sentence prediction.",
        "reference": "Fig. 2. Illustration of aggregation based explicit transformer and modulation based implicit transformer."
    },
    "2101.07910v1-Figure2-1.png": {
        "prediction": "The flowchart illustrates a process with the following steps:\n\n1. Start: The process begins with an initial decision to check if the number of iterations is less than N.\n2. If the number of iterations is less than N, proceed to the next step. If not, the process moves to the Final Generated Code Snippets.\n3. Calculate the Mutation Score: Determine the mutation score for the current input population.\n4. Is sample elite? Make a decision based on the mutation score.\n   - If the answer is Yes, add the sample to the next generation.\n   - If the answer is No, refactor the input with a mutation rate.\n5. Input Population: Update the input population based on the previous step.\n6. Code Snippet: Generate a code snippet from the input population.\n7. Repeat the process from step 1 until the Final Generated Code Snippets are obtained.\n\nThe flowchart represents a iterative process for generating code snippets, where each iteration involves calculating a mutation score, deciding whether to add a sample to the next generation or refactor the input, and updating the input population. The process continues until a stopping criterion is met, which is when the number of iterations reaches N.",
        "reference": "Fig. 2. The guided mutation (GM) process for code adversarial generation."
    },
    "2012.06182v1-Figure1-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Fig. 1: Illustration of a multilayered SN with satellites, HAPs, and UAVs."
    },
    "2012.06182v1-Figure8-1.png": {
        "prediction": "The image contains a flowchart that illustrates the architecture of BERT in pretraining. The flowchart shows the process of predicting masked tokens and the next sentence prediction task.",
        "reference": "Fig. 8: An architecture of HAPs network with P2P HAP-to-HAP and backhauling links."
    },
    "2103.04910v1-Figure6-1.png": {
        "prediction": "The flowchart consists of a system block labeled \"System\" with an input \"u(t)\" and an output \"y(t)\". Inside the system block, there are three sequential blocks: \"Regulator h(t)\", \"Control Design k(t)\", and \"Recursive Identifier \u03b8(t)\". The arrows indicate the flow of information from the input \"u(t)\" to the output \"y(t)\" through these three blocks.",
        "reference": "Figure 6: Model building approach"
    },
    "2202.06208v1-Figure1-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of feature alignment and metric learning for prediction boundary in a dynamic hierarchical tree.",
        "reference": "Figure 1. The architecture of our proposed BROT. Sub-figure (a) depicts the OT process to align features from different domains. Sub-figure (b) shows that the metric learning aims to seek better decision boundaries with a dynamic hierarchical tree. The darkness of blue colors reflects the magnitude of molecular properties."
    },
    "2107.01104v1-Figure1-1.png": {
        "prediction": "The image contains a flowchart that illustrates a process involving slices and particles.",
        "reference": "Figure 1: Distribution of real-space data between MPI processes in TurTLE. Fields are split into slabs and distributed between P MPI processes along the x3 direction. The Np particles are also distributed, with each MPI process storing Pp particles on average. Within each MPI process the particle data is sorted according to its x3 location. This leads to a direct association between each of the Ps field slices to contiguous regions of the particle data arrays \u2014 in turn simplifying the interpolation procedure (see text for details). On average, S p particles are held within each such contiguous region."
    },
    "51865-Figure3-1.png": {
        "prediction": "The image contains a flowchart that illustrates the architecture of a convolutional encoder-decoder (CED) network. The flowchart shows the data input going through a convolutional layer, followed by a pooling layer, and then another convolutional layer with another pooling layer. The output of the pooling layer is then passed through a fully connected (FC) layer, followed by another FC layer and a softmax layer. The final output is represented by the variable \"Z\".",
        "reference": "Figure 3. Overview of DCEC. The network is built by cascading the CNN module and the CEC."
    },
    "2012.02113v1-Figure8.1-1.png": {
        "prediction": "The image contains a Venn diagram with four overlapping circles.",
        "reference": "Figure 8.1 Venn diagram showing entropic quantities associated with a pair of random variables taking values in different sets: the Shannon entropies H(X) and H(Y), the joint entropy H(X,Y), the conditional entropies H(X | Y) and H(Y | X), and the mutual information I(X; Y)."
    },
    "2012.02113v1-Figure7.4-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Figure 7.4 Schematic illustration of Lemma 7.4.6."
    },
    "2204.12696v1-Figure2-1.png": {
        "prediction": "Figure 1: Illustration of the proposed method. Given a set of identities and a style of micromotion, the method generates a sequence of images that exhibit the desired micromotion.",
        "reference": "Figure 2: A tensor illustration of our low-rank micromotion subspace hypothesis. In the StyleGAN latent space, we hypothesize the same type of micromotion, at different quantitative levels but for the same identity, can be approximated by a low-rank subspace. We further hypothesize that subspaces for the same type of micromotion found at different identities are extremely similar to each other, and can hence be transferred across identities."
    },
    "206593096-Figure3-1.png": {
        "prediction": "The flowchart illustrates the architecture of a convolutional neural network (CNN) for image segmentation. The network takes a scaled input image as input and processes it through a series of convolutional layers, followed by a segment proposal layer that generates segment proposals. The segment proposals are then passed through a region-wise computation module, which applies a spatial pyramid pooling (SPP) layer and fully connected (FC) layers to produce a feature map. The feature map is then concatenated with the output of the image conv layer and passed through another set of FC layers and a classifier to produce the final segmentation output.",
        "reference": "Figure 3: Two network designs in this paper. The input image is processed as a whole at the convolutional layers from conv1 to conv5. Segments are exploited at a deeper hierarchy by: (Left) applying CFM on the feature map of conv5, where \u201c b\u201d means for \u201cbounding boxes\u201d and \u201c s\u201d means for segments; (Right) applying CFM on the finest feature map of the spatial pyramid pooling layer."
    },
    "2101.10868v1-Figure1-1.png": {
        "prediction": "The flowchart illustrates a data flow and processing system involving multiple entities, including the government, user, social media, and data processing. The government collects data and issues orders or restrictions, which are monitored. This data is then processed and shared with social media platforms, leading to unknown outcomes. The user's behavior is changed as a result of this data flow, and multiple information aggregation occurs.",
        "reference": "Figure 1: A Systematic Overview of Privacy Threats from Multiple Domains Related to the COVID-19 Pandemic"
    },
    "2102.04539v2-Figure10-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Figure 10: Illustration for 2-Diam GBP. Here, V q denotes all sets V{q,\u00b7}."
    },
    "2102.04539v2-Figure6-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Figure 6: Illustration for the construction in the proof of Proposition 8 for 2-Reach GBP with r = 1. In this example, U = {u1, . . . , un} and we have {u1, ui, uj , un} = F \u2208 F ."
    },
    "2102.04539v2-Figure4-1.png": {
        "prediction": "Figure 1: Illustration of execution traces and temporal relations.",
        "reference": "Figure 4: Illustration to Constructions 3 & 4. Part (a) shows an exemplary directed graph which is a yes-instance for DHP. Applying Construction 3 on (a) yields (b). Applying Construction 4 on (b) yields the instance whose graph is depicted in (c) and two habitats of which are depicted in (d) and (e). Vertices marked yellow in (d) are contained in the habitat Xout. Vertices marked red in (e) are contained in the habitat Yout. The graph induced by Yout contains the red edges."
    },
    "2110.06487v1-Figure5-1.png": {
        "prediction": "The image contains a flowchart that illustrates a process involving a victim, a Covid Unemployment Claim Site, a government, a scanner, and various sources such as the dark web and online sales.",
        "reference": "Fig. 5. Covid unemployment threat model"
    },
    "2103.00907v1-Figure4-1.png": {
        "prediction": "A flowchart that illustrates the architecture of a neural network.",
        "reference": "Figure 4. Diagram of the neural network(NN2) for the turbulent channel flow. The inputs are the pressure and velocity gradient and the output is the pressure strain term. The FCFF has 5 layers with 10 neurons in each layer."
    },
    "2206.00162v2-Figure2-1.png": {
        "prediction": "The image contains a flowchart that illustrates a process. The flowchart starts with a \"Start\" symbol and ends with a \"End\" symbol. The flowchart includes multiple decision points represented by diamonds, and the decisions are labeled with \"Yes\" and \"No\". The flowchart also includes multiple rectangles, which represent tasks or actions. The tasks are labeled with descriptive text. Additionally, the flowchart includes arrows that connect the different symbols, indicating the flow of the process.",
        "reference": "Figure 2: Overview of PAGER generation method."
    },
    "2011.03148v2-Figure4-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of object detection and classification.",
        "reference": "Fig. 4. Diagram of perception consistency loss computation. An EfficientDet object detector predicts boxes and classes. Consistency of predictions between images is captured by losses similar to those in object detection training."
    },
    "2110.09749v2-Figure1-1.png": {
        "prediction": "The image contains a flowchart that outlines a process for candidate keyphrase extraction and keyphrase importance estimation. The flowchart begins with the input of a source document, which is then processed by a RoBERTa model. The output of the RoBERTa model is fed into a candidate keyphrase extraction phase that generates uni-grams, bi-grams, and up to N-grams. These candidate keyphrases are then evaluated for their importance using several criteria, including syntactic accuracy, semantic saliency, and concept consistency. The final output of the flowchart is a set of predictions based on the evaluation of the candidate keyphrases.",
        "reference": "Figure 1: The KIEMP model architecture."
    },
    "2103.07592v2-Figure1-1.png": {
        "prediction": "The flowchart illustrates a process with three main components: DM, H, and SM. DM is connected to H, and H is connected to SM.",
        "reference": "Figure 1: Schematic diagram for flux-mediated dark matter."
    },
    "2107.05680v1-Figure2-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of image generation using a deep neural network. The flowchart starts with an input image represented by the variable \"Z\" and proceeds through a series of transformations to generate the final output image.\n\nThe first step in the process is the convolution of the input image \"Z\" with a set of weights represented by \"W1\" to produce an intermediate feature map \"X1\". This is followed by a non-linear activation function, which is not explicitly shown in the flowchart but is typically applied after each convolutional layer. The output of the activation function is then fed into a pooling layer, which downsamples the feature map to produce a smaller feature map \"D1\".\n\nThe process is repeated for several layers, with each layer having a larger number of filters and producing a smaller feature map. The flowchart shows that the number of filters doubles at each layer, starting with 4 filters in the first layer and reaching 64 filters in the last layer. The final feature map \"D5\" is then transformed back into an image using a series of upsampling and convolutional layers, with the final output being the generated image.\n\nThroughout the process, the flowchart also indicates that the weights are being updated using a gradient descent algorithm, which is represented by the \"Z*\" and \"W*\" notation. This suggests that the network is being trained on a set of images and their corresponding labels, with the goal of minimizing the difference between the generated images and the ground truth images.",
        "reference": "Figure 2: A modified architecture for progressive training of convex GANs (ProCoGAN). At each stage i, a linear generator Wi is used to model images at a given resolution Xi, attempting to fool quadratic-activation discriminator Di, for which the optimal solution can be found in closed-form via (15). Once stage i is trained, the input to stage i + 1 is given as the output of the previous stage with learned weights W\u2217i , which is then used to model higher-resolution images Xi+1. The procedure continues until high-resolution images can be generated from successive application of linear generators."
    },
    "2107.05214v2-Figure6-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of generating binary masks from a tensor. The flowchart starts with a tensor as the input, which is then passed through an MLP (Multilayer Perceptron) layer. The output of the MLP layer is a tensor, which is then passed through a Binarize layer. The Binarize layer converts the tensor into a binary mask. The flowchart ends with the binary mask as the output.",
        "reference": "Figure 6: The illustration of the attention mechanism. The prediction of current hidden state h\u0302t and the grid-level features E is used as query and key, respectively."
    },
    "2102.12058v2-Figure17-1.png": {
        "prediction": "The flowchart illustrates a federated voting process. It starts with a transaction being in an uncommitted state. If the transaction is valid, it proceeds to the voting stage where nodes in each slice vote on the transaction. Once a quorum of nodes votes for or accepts the transaction, it moves to the accepted state. Finally, when a quorum of nodes confirms the transaction, it reaches the confirmed state, completing the process.",
        "reference": "Fig. 17. Federated voting process [46] ."
    },
    "2103.05447v1-Figure2-1.png": {
        "prediction": "The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Figure 2. Experiment schematic."
    },
    "2010.00041v3-Figure2-1.png": {
        "prediction": "The flowchart illustrates a genetic algorithm optimization process. It begins with the random creation of the initial population. The evaluation step calculates the fitness of the population. If the stopping criteria are not satisfied, the process proceeds to passing elites to the next population. Roulette selection of operators based on fitness is performed, followed by crossover and mutation to create a new population. The process then returns to the evaluation step. If the stopping criteria are satisfied, the best individual is returned.",
        "reference": "Fig. 2. The optimization flowchart for the inverse material design."
    },
    "2202.00846v1-Figure2-1.png": {
        "prediction": "The flowchart illustrates a process that involves estimating the Click-Through Rate (CTR) for each group, based on a Click/Purchase Log. The process starts with the CTR Estimation for each group, which is then followed by the computation of the Assignment Probability for Group 1. This step is repeated for Group 2 and so on, up to Group n. The process continues until a stopping rule is met, at which point the results are declared and displayed to the user.",
        "reference": "Figure 2: Method Overview"
    },
    "2101.07424v2-Figure1-1.png": {
        "prediction": "The image contains a flowchart that illustrates the process of scene encoding and measurement. The flowchart starts with the \"Scene\" box, which is the input. The next step is \"Encoded aperture,\" where the scene is encoded. Following this, the encoded scene is shifted, as indicated by the \"Shifted encoded scene\" box. The final step is \"Measurements,\" where measurements are taken based on the shifted encoded scene.",
        "reference": "Fig. 1. Physical sensing phenomena in CASSI, which is the CSI prototype used to validate the proposed approach."
    },
    "2204.03873v1-Figure2-1.png": {
        "prediction": "The image contains a flowchart that illustrates the architecture of a neural network. The flowchart starts with an input layer labeled \"X input\" and proceeds to a \"Batch norm\" layer. The output of the batch norm layer is then passed through two \"TCN\" layers, followed by a \"ST\" layer. The output of the second \"ST\" layer is fed into a \"Batch norm\" layer, followed by an \"Avg-Pooling\" layer. The output of the avg-pooling layer is then passed through a \"FC\" layer, which leads to the final output layer labeled \"fout embedding\".",
        "reference": "Figure 2: Structure of gait-TR. TCN is the temporal convolutional network module, and ST is the spatial transformer module. FC denotes full connect layer. Batch-norm is BatchNorm2D for input Xtv \u2208 \u211d C\u00d7T\u00d7V , while Batch-norm* denotes BatchNorm1D for input Xtv \u2208 \u211d C\u2217V \u00d7T ."
    },
    "2204.10426v1-Figure1-1.png": {
        "prediction": "The flowchart illustrates a simple state transition model with three states: Healthy (State 0), Cognitive impairment (State 1), and Death (State 2). The arrows indicate the possible transitions between these states. Starting from the Healthy state, there is a direct transition to the Cognitive impairment state. From the Cognitive impairment state, there are two possible transitions: one back to the Healthy state and another to the Death state.",
        "reference": "Figure 1: Three-state illness-death model"
    }
}