{
    "2204.00424v1-Figure6-1.png": {
        "prediction": "1. The loss function is defined as the difference between the predicted and actual values of the output layer. 2. The metrics are used to evaluate the performance of the model. 3. The model is trained using the loss function and the metrics are used to adjust the model's parameters. 4. The model is evaluated on a separate set of data to determine its accuracy. 5. The model is then used to make predictions on new data.",
        "reference": "Figure 6. MSOPunet network. (S1t\u22121, S2t\u22121), (S1t, S2t) and (S1t+1, S2t+1) denotes the input pairs of optical and SAR images. S2t\u2032 denotes the reference image used to compute the loss (during training) and the metrics (at inference time from test data) from the reconstructed optical image S\u03022t."
    },
    "2202.03651v2-Figure1-1.png": {
        "prediction": "1. Figure 1: Overview of the proposed approach for detecting and mitigating failures in autonomous driving systems.",
        "reference": "Figure 1: Instead of retrospectively discovering individual failure cases for perception, we actively search for causal interventions (edits) to existing scenes that consistently result in perception failures. The middle shows an example of a single intervention causing perception failure, which we attribute to the intervention, as opposed to the left where a combinatorial set of factors could explain the error. Consistent failures through this type of intervention constitute a challenging group for the perception model as seen on the right."
    },
    "2011.15102v2-Figure1-1.png": {
        "prediction": "1. Learner: Learner is the person who is trying to learn something new. They are trying to solve a problem or complete a task.\n2. Tester: Tester is the person who is testing the learner's solution. They are trying to see if the solution is correct and if it meets the desired criteria.\n3. Test Bank: Test Bank is a collection of test cases that are used to evaluate the learner's solution. It is a set of questions or scenarios that the learner must answer or complete in order to demonstrate their understanding of the material.\n4. Harder tests: Harder tests are tests that are more difficult or complex than the ones that the learner has already completed. They are designed to challenge the learner and help them improve their skills.\n5. Better solutions: Better solutions are solutions that are more effective or efficient than the ones that the learner has already tried. They are designed to help the learner learn more quickly and effectively.\n6. Learner: Learner is the person who is trying to learn something new. They are trying to solve a problem or complete a task.\n7. Tester: Tester is the person who is testing the learner's solution. They are trying to see if the solution is correct and if it meets the desired criteria.\n8. Test Bank: Test Bank is a collection of test cases that are used to evaluate the learner's solution. It is a set of questions or scenarios that the learner must answer or complete in order to demonstrate their understanding of the material.\n9. Harder tests: Harder tests are tests that are more difficult or complex than the ones that the learner has already completed. They are designed to challenge the learner and help them improve their skills.\n10. Better solutions: Better solutions are solutions that are more effective or efficient than the ones that the learner has already tried. They are designed to help the learner learn more quickly and effectively.",
        "reference": "Figure 1: Learning by passing tests. A tester model creates tests with increasing levels of difficulty from a test bank to evaluate a learner model. The learner continuously improves its learning ability to deliver better solutions for passing those difficult tests."
    },
    "14078997-Figure2-1.png": {
        "prediction": "1. Input frames are fed into the CNN model, which outputs affine transforms.",
        "reference": "Figure 2: Outline of the transformation-based model. The model is a CNN that takes as input a sequence of consecutive affine transforms between pairs of adjacent video frames. It predicts the affine transform between the last input frame and the next one in the sequence. We compute affine transforms (6 parameters per patch) for overlapping patches of size 8 \u00d7 8 in each video frame. Learning operates in the space of transformations as shown inside the dashed box. The front-end on the left is a module that estimates the affine transforms between pairs of consecutive input frames. The post-processor on the right reconstructs a frame from the predicted set of affine transforms and it is only used at test time."
    },
    "14078997-Figure3-1.png": {
        "prediction": "1. The first layer is a convolutional layer, which takes in an image of size 28x28 and outputs a feature map of size 14x14. The convolutional layer uses a 3x3 kernel and applies a ReLU activation function to the output. The output of the convolutional layer is then passed through a max pooling layer, which reduces the spatial size of the feature map to 7x7. The max pooling layer also applies a ReLU activation function to the output.",
        "reference": "Figure 3: Outline of the system predicting 4 frames ahead in time. Only affine transforms A1, A2 and A3 are provided, and the model predicts A\u03034, A\u03035, A\u03036 and A\u03037, which are used to reconstruct the next 4 frames. Since affine parameters are continuous values and the whole chain of CNNs is differentiable, the whole unrolled system can be trained by back-propagation of the error. Note that CNNs all share the same parameters"
    },
    "2204.10509v1-Figure2-1.png": {
        "prediction": "1. The PEGE model architecture consists of a transformer, a positive emotion-guided empathetic loss, a negative emotion loss, and a term loss. The transformer is responsible for learning the representations of the input sequence. The positive emotion-guided empathetic loss is used to encourage the model to learn representations that are more empathetic towards positive emotions. The negative emotion loss is used to encourage the model to learn representations that are less empathetic towards negative emotions. The term loss is used to encourage the model to learn representations that are more aligned with the target term.",
        "reference": "Figure 2: Illustration of our PEGE Model"
    },
    "2202.05735v2-Figure1-1.png": {
        "prediction": "1. Waveform (WAV): A representation of the raw signal, such as a photoplethysmography (PPG) waveform or an electrocardiogram (ECG) waveform.",
        "reference": "Fig. 1: Three approaches to ML for sleep staging from PPG. Our new model, SleepPPG-Net takes as input the PPG waveform (WAV).The derived time series (DTS) and feature engineering (FE) approaches allow comparison with SOTA algorithms described in the literature."
    },
    "2101.02429v1-Figure6-1.png": {
        "prediction": "1. The figure shows the eigenvectors of a matrix and the corresponding neural decomposition. The eigenvectors are represented as blue arrows, and the neural decomposition is represented as red arrows. The figure also shows the projection of the eigenvectors onto the plane defined by the neural decomposition.",
        "reference": "Figure 6: Illustration of the neural decomposition procedure for d = 2 (i.e. Z\u2217 \u2208 R3\u00d73). The dashed red arrows correspond to the eigenvectors of Z\u2217 (q1, q2, q3) and the solid blue arrows show the decomposed vectors p1 and p2. In this example, the rank of Z \u2217 is 2 where q1 and q2 are its two principal eigenvectors. The eigenvalue corresponding to the eigenvector q1 is zero. The light blue colored surface shows the Lorentz cones z = \u221a x2 + y2 and z = \u2212 \u221a x2 + y2. We observe that the decomposed vectors p1 and p2 lie on the boundary of Lorentz cones."
    },
    "1246860-Figure1-1.png": {
        "prediction": "1. Preprocessing: Remove punctuation, lower case, and remove stop words. 2. Tokenization: Split the text into individual words. 3. Part-of-speech tagging: Assign a part-of-speech tag to each word. 4. Lemmatization: Convert words to their base form. 5. Sentence segmentation: Identify sentences in the text. 6. Sentence normalization: Convert sentences to lowercase and remove punctuation. 7. Sentence classification: Assign a sentiment label to each sentence. 8. Sentences to words: Convert sentences to individual words. 9. Sentences to sentences: Convert sentences to individual sentences. 10. Sentences to sentences: Convert sentences to individual sentences. 11. Sentences to sentences: Convert sentences to individual sentences. 12. Sentences to sentences: Convert sentences to individual sentences. 13. Sentences to sentences: Convert sentences to individual sentences. 14. Sentences to sentences: Convert sentences to individual sentences. 15. Sentences to sentences: Convert sentences to individual sentences. 16. Sentences to sentences: Convert sentences to individual sentences. 17. Sentences to sentences: Convert sentences to individual sentences. 18. Sentences to sentences: Convert sentences to individual sentences. 19. Sentences to sentences: Convert sentences to individual sentences. 20. Sentences to sentences: Convert sentences to individual sentences. 21. Sentences to sentences: Convert sentences to individual sentences. 22. Sentences to sentences: Convert sentences to individual sentences. 23. Sentences to sentences: Convert sentences to individual sentences. 24. Sentences to sentences: Convert sentences to individual sentences. 25. Sentences to sentences: Convert sentences to individual sentences. 26. Sentences to sentences: Convert sentences to individual sentences. 27. Sentences to sentences: Convert sentences to individual sentences. 28. Sentences to sentences: Convert sentences to individual sentences. 29. Sentences to sentences: Convert sentences to individual sentences. 30. Sentences to sentences: Convert sentences to individual sentences. 31. Sentences to sentences: Convert sentences to individual sentences. 32. Sentences to sentences: Convert sentences to individual sentences. 33. Sentences to sentences: Convert sentences to individual sentences. 34. Sentences to sentences:",
        "reference": "Figure 1: Deep multi-instance transfer learning approach for review data."
    },
    "2102.10828v2-Figure7-1.png": {
        "prediction": "1. The graph is a directed acyclic graph (DAG) with n nodes and m edges. 2. The nodes are labeled with integers from 0 to n-1. 3. The edges are labeled with integers from 0 to m-1. 4. The degree of node i is denoted by deg(i). 5. The degree of edge (i,j) is denoted by deg(i,j). 6. The degree of node i is the sum of the degrees of all edges incident to i. 7. The degree of edge (i,j) is the sum of the degrees of all edges incident to i and j. 8. The degree of node i is equal to the degree of edge (i,0). 9. The degree of edge (i,j) is equal to the degree of edge (0,j). 10. The degree of node i is equal to the degree of edge (i,n-1). 11. The degree of edge (i,j) is equal to the degree of edge (i,j+1). 12. The degree of node i is equal to the degree of edge (i,j+1). 13. The degree of edge (i,j) is equal to the degree of edge (i,j-1). 14. The degree of node i is equal to the degree of edge (i,j-1). 15. The degree of edge (i,j) is equal to the degree of edge (i,j-2). 16. The degree of node i is equal to the degree of edge (i,j-2). 17. The degree of edge (i,j) is equal to the degree of edge (i,j-3). 18. The degree of node i is equal to the degree of edge (i,j-3). 19. The degree of edge (i,j) is equal to the degree of edge (i,j-4). 20. The degree of node i is equal to the degree of edge (i,j-4). 21. The degree of edge (i,j) is equal to the degree of edge (i,j-5). 22. The degree of node i is equal to the degree of edge (i,j-5). 23. The degree of edge (i,j) is equal to the degree of",
        "reference": "Figure 7. Illustration of Equation (4.1)."
    },
    "1022460-Figure1-1.png": {
        "prediction": "1. The input image is fed into the CNN, which extracts features from the image. 2. The features are passed through a fully connected layer, which outputs a probability distribution over the classes. 3. The softmax function is applied to the output probabilities, which gives us a probability distribution over the classes. 4. The predicted class is the class with the highest probability.",
        "reference": "Figure 1. A feedback based learning model. The basic idea is to make predictions in an iterative manner based on a notion of the thus-far outcome. This provides several core advantages: I. enabling early predictions (given total inference time T , early predictions are made in fractions of T ); II. naturally conforming to a taxonomy in the output space; and III. better grounds for curriculum learning."
    },
    "2110.03031v3-Figure1-1.png": {
        "prediction": "1. The input sequence is encoded into a sequence of hidden states, h1, h2, ..., hL.",
        "reference": "Figure 1. RieszNet architecture."
    },
    "2101.11189v1-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed method for estimating the head position of a person in a video sequence. The backbone network is used to extract features from the video frames. The object size and head point regression are used to estimate the head position. The head point estimation network is used to estimate the head position from the extracted features. The center offset is used to align the head position with the center of the video frame. The offset is used to align the head position with the center of the video frame.",
        "reference": "Fig. 2: The overall framework of our arbitrary-oriented ship detection method. Feature maps are first generated by using a fully convolutional network. Afterward, the peaks of the center feature map are selected as center points. Then, the center points offsets, object sizes and head regression locations are regressed on the corresponding feature maps on the position of each center point. The potential head points are collected by extracting peaks with confidence scores larger than 0.1 on the head feature map. The final head location is obtained by assigning each regressed location to its nearest potential head points."
    },
    "2204.01715v1-Figure7-1.png": {
        "prediction": "1. Data ingestion: User app sends data to the data ingestion layer, which processes the data and sends it to the transform layer.",
        "reference": "Figure 7. BigDL-Orca architecture"
    },
    "2202.07508v3-Figure3-1.png": {
        "prediction": "1. Convolutional filters process: The image is processed by a series of convolutional filters, which are applied to each pixel in the image. The filters are designed to extract features from the image, such as edges, textures, and shapes. The output of each filter is a feature map, which is a representation of the image at a particular scale and location.",
        "reference": "Figure 3. Architecture of the dynamic deep linear kernel."
    },
    "2202.11824v1-Figure1-1.png": {
        "prediction": "1. The attacker sends a malicious email containing a link to a malicious website. 2. The user clicks on the link and is directed to the website. 3. The website loads a malicious script that injects a Trojan SW into the user's computer. 4. The Trojan SW monitors the user's computer for specific keywords or phrases, such as \"cybersecurity\" or \"hacking alert.\" 5. When the keywords or phrases are detected, the Trojan SW sends a request to the DNA-sequencing service. 6. The DNA-sequencing service receives the request and begins sequencing the user's DNA. 7. The DNA-sequencing service sends the results of the sequencing to the Trojan SW. 8. The Trojan SW uses the results to identify the user's identity and location. 9. The attacker can then use this information to carry out their attack.",
        "reference": "Fig. 1: Hybrid Trojan Bio-Cyber Hacking Attack. Steps 1 - 3 indicate a typical genetic sequencing operation for patients. Steps 4 - 6 indicate a situation where a hacker has embedded their IP address and Port number into a DNA that will trigger a remote connection from a Trojan-horse infected software tool leading to a connection to the attacker in Step 8. Our proposed approach utilizes Deep-Learning to detect Trojan payload in digital data using encoded into DNA strands that can prevent the attack."
    },
    "2011.04105v1-Figure3.5-1.png": {
        "prediction": "1. Discovery Plane: Collects data from various sources and aggregates it into a network-wide view. 2. Dissemination Plane: Distributes the network-wide view to all nodes in the network. 3. Decision Plane: Takes actions based on the network-wide view and the local observations of the nodes. 4. Network-level objectives: The objective of the network is to achieve a certain state or behavior.",
        "reference": "Figure 3.5: Network-level objectives."
    },
    "2202.01326v1-Figure5-1.png": {
        "prediction": "1. Are triplets from SF enhancing the PV device? (Yes or No",
        "reference": "Figure 5: Flowchart describing the process for determining if SF is improving photocurrent in a device."
    },
    "2204.08696v2-Figure1-1.png": {
        "prediction": "1. Feature maps are extracted from the input image using a convolutional neural network (CNN). 2. The feature maps are then passed through a local-global feature cooperation module (LGM) to extract global and local features. 3. The global and local features are concatenated and passed through a multi-scale feature fusion unit (MFFU) to generate a refined feature map. 4. The refined feature map is then passed through a facial structure attention unit (FSAU) to generate a facial structure attention map. 5. The facial structure attention map is used to generate a facial structure refinement module (FSRM) that generates a refined facial structure. 6. The refined facial structure is then used to generate a facial structure attention unit (FSAU) that generates a facial structure attention map. 7. The facial structure attention map is used to generate a facial structure refinement module (FSRM) that generates a refined facial structure. 8. The refined facial structure is then used to generate a facial structure attention unit (FSAU) that generates a facial structure attention map. 9. The facial structure attention map is used to generate a facial structure refinement module (FSRM) that generates a refined facial structure. 10. The refined facial structure is then used to generate a facial structure attention unit (FSAU) that generates a facial structure attention map. 11. The facial structure attention map is used to generate a facial structure refinement module (FSRM) that generates a refined facial structure. 12. The refined facial structure is then used to generate a facial structure attention unit (FSAU) that generates a facial structure attention map. 13. The facial structure attention map is used to generate a facial structure refinement module (FSRM) that generates a refined facial structure. 14. The refined facial structure is then used to generate a facial structure attention unit (FSAU) that generates a facial structure attention map. 15. The facial structure attention map is used to generate a facial structure refinement module (FSRM) that generates a refined facial structure. 16. The refined facial structure is then used to generate a facial structure attention unit (FSAU) that generates a facial structure attention map. 17. The facial structure attention map is used to generate a facial structure refinement module (FSRM) that generates a refined facial structure. 18. The refined facial structure is then used to generate a facial structure attention",
        "reference": "Fig. 1: The complete structure of the proposed CNN-Transformer Cooperation Network (CTCNet). CTCNet is a U-shaped symmetrical hierarchical network with three stages: encoding stag, bottleneck stage, and decoding stage. Among them, the encoding stage is designed to extract local and global features with different scales, and the decoding stage is designed for feature fusion and image reconstruction."
    },
    "2101.08366v2-Figure2-1.png": {
        "prediction": "1. The input to the network is a sequence of words, where each word is represented as a vector. The input to the network is a sequence of word vectors.",
        "reference": "Figure 2: Overview of our proposed Quadratic Residual Network (QRes) layer in comparison with plain DNN layer. Blue rectangular boxes represent trainable parameters and round boxes represent operations (purple \u201c\u00d7\u201d: multiplication, orange \u201c+\u201d: addition, green \u201d\u00b7\u201d: Hadamard product, and cyan \u201d\u03c3\u201d: activation operator)."
    },
    "2205.09510v2-Figure6.4-1.png": {
        "prediction": "1. Quantum data is fed into a quantum neural network.",
        "reference": "Figure 6.4: An illustration of the \u201cQC\u201d setting of quantum machine learning, in which data are quantum and processing is classical."
    },
    "2205.09510v2-Figure6.11-1.png": {
        "prediction": "1. The classical optimizer is applied to the input vector, which is the result of the quantum circuit. 2. The output vector is obtained by applying the quantum circuit to the input vector. 3. The average of the output vector is calculated by applying the classical optimizer to the output vector. 4. The result is the average of the output vector, which is the result of the quantum circuit.",
        "reference": "Figure 6.11: Illustration of the operation of a VQE. The classical optimizer aims at minimizing the expected value \u3008F \u3009\u03c8(\u03b8)\u3009 = \u3008\u03c8(\u03b8)|F |\u03c8(\u03b8)\u3009 of the observable F ."
    },
    "2206.05240v1-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed curriculum-based reinforcement learning framework. The figure has the following components:",
        "reference": "Figure 2: Model Overview. We propose CBRL for RCB. Guided by a curriculum sequence, the agent learns to adaptively trade-off constraints and the objective in non-stationarymarkets. During deployment, the agent updates its belief over the market dynamics based on its past experience, and acts through posterior sampling."
    },
    "2210.02172v1-Figure2-1.png": {
        "prediction": "1. Figure 1: Approximation error in reinforcement learning.",
        "reference": "Fig. 2: An illustration of deep Q-learning model for the ISRE-based 6G vehicular aided HetNet environment. The model uses a deep neural network to find the approximate Q-values. The model includes experience replay mechanism (to remove the correlation between different observations), a feature set (given as input to the deep neural network), a target Q-network for updating the primary Q-network and the simulation environment for extracting different parameters."
    },
    "14844989-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of a multi-task network for the tasks of ranking and classification. The auxiliary dataset is used to train the network. The convolutional layer is followed by fully connected layers, which are connected to the softmax loss and the triplet loss. The contrastive loss is used to train the network.",
        "reference": "Figure 2: The framework of the proposed multi-task deep network and the cross-domain architecture. The crossdomain architecture is only used when an auxiliary dataset is needed for training."
    },
    "2205.03661v1-Figure1-1.png": {
        "prediction": "1. Design of Baseline Deep Networks: Determine the number of basic blocks and design the basic blocks. 2. Methods of Binarizing Baseline Networks: Replace Sign(x) with the piecewise function F(x). 3. Enhancing Computing Speed: Introduce the learnable parameter \u03b1. 4. Maintain the accuracy of net: Maintain the accuracy of the network. 5. Reduce memory overhead: Reduce memory overhead.",
        "reference": "Fig. 1. Overview of methods adopted"
    },
    "162169-Figure3-1.png": {
        "prediction": "1. Event: A specific occurrence or happening. 2. Place: A location or area. 3. Time: A period of time. 4. Cause: The reason or origin of something. 5. Agent: The person or thing that causes something. 6. Object: The thing that is caused or influenced by something. 7. Influence: The effect or impact of something. 8. Dependent entity: A thing that is affected by something else. 9. Inheriting relation: A relationship that is inherited from one entity to another. 10. Causative relation: A relationship that is caused by one entity to another. 11. Inheritance relation: A relationship that excludes a causative relation.",
        "reference": "Figure 3 Partial illustration of frames, roles, and lexical units related to the CAUSE TO MAKE NOISE frame, from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are unfilled bars. No particular significance is ascribed to the ordering of a frame\u2019s roles in its lexicon entry (the selection and ordering of roles above is for illustrative convenience). CAUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here."
    },
    "2202.03344v2-Figure2-1.png": {
        "prediction": "1. Initialize the latent variable and the set of truncated sets. 2. Apply hybrid-LAR to select the latent variable. 3. Update the latent variable. 4. Initialize the set of candidate distributions. 5. Initialize the set of candidate distributions. 6. Initialize the set of candidate distributions. 7. Initialize the set of candidate distributions. 8. Initialize the set of candidate distributions. 9. Initialize the set of candidate distributions. 10. Initialize the set of candidate distributions. 11. Initialize the set of candidate distributions. 12. Initialize the set of candidate distributions. 13. Initialize the set of candidate distributions. 14. Initialize the set of candidate distributions. 15. Initialize the set of candidate distributions. 16. Initialize the set of candidate distributions. 17. Initialize the set of candidate distributions. 18. Initialize the set of candidate distributions. 19. Initialize the set of candidate distributions. 20. Initialize the set of candidate distributions. 21. Initialize the set of candidate distributions. 22. Initialize the set of candidate distributions. 23. Initialize the set of candidate distributions. 24. Initialize the set of candidate distributions. 25. Initialize the set of candidate distributions. 26. Initialize the set of candidate distributions. 27. Initialize the set of candidate distributions. 28. Initialize the set of candidate distributions. 29. Initialize the set of candidate distributions. 30. Initialize the set of candidate distributions. 31. Initialize the set of candidate distributions. 32. Initialize the set of candidate distributions. 33. Initialize the set of candidate distributions. 34. Initialize the set of candidate distributions. 35. Initialize the set of candidate distributions. 36. Initialize the set of candidate distributions. 37. Initialize the set of candidate distributions. 38. Initialize the set of candidate distributions. 39. Initialize the set of candidate distributions. 40. Initialize the set of candidate distributions. 41. Initialize the set of candidate distributions. 42. Initialize the set of candidate distributions. 43. Initialize the set of candidate distributions. 44. Initialize the set of candidate distributions. 45. Initialize the set of candidate distributions. 46. Initialize the set of candidate distributions",
        "reference": "Figure 2: Flow chart of the procedure to adaptively build a stochastic PCE"
    },
    "2207.12764v1-Figure1-1.png": {
        "prediction": "1. Extracting OCEL from the event data. 2. Enrich OCEL with complex object-centric process model and related attributes. 3. Select the desired object type for clustering. 4. Assigning events to the cluster. 5. Assigning events to the cluster. 6. Assigning events to the cluster. 7. Assigning events to the cluster. 8. Assigning events to the cluster. 9. Assigning events to the cluster. 10. Assigning events to the cluster. 11. Assigning events to the cluster. 12. Assigning events to the cluster. 13. Assigning events to the cluster. 14. Assigning events to the cluster. 15. Assigning events to the cluster. 16. Assigning events to the cluster. 17. Assigning events to the cluster. 18. Assigning events to the cluster. 19. Assigning events to the cluster. 20. Assigning events to the cluster. 21. Assigning events to the cluster. 22. Assigning events to the cluster. 23. Assigning events to the cluster. 24. Assigning events to the cluster. 25. Assigning events to the cluster. 26. Assigning events to the cluster. 27. Assigning events to the cluster. 28. Assigning events to the cluster. 29. Assigning events to the cluster. 30. Assigning events to the cluster. 31. Assigning events to the cluster. 32. Assigning events to the cluster. 33. Assigning events to the cluster. 34. Assigning events to the cluster. 35. Assigning events to the cluster. 36. Assigning events to the cluster. 37. Assigning events to the cluster. 38. Assigning events to the cluster. 39. Assigning events to the cluster. 40. Assigning events to the cluster. 41. Assigning events to the cluster. 42. Assigning events to the cluster. 43. Assigning events to the cluster. 44. Assigning events to the cluster. 45. Assigning events to the cluster. 46. Assign",
        "reference": "Fig. 1: Overview of the proposed framework."
    },
    "2103.13446v3-Figure2-1.png": {
        "prediction": "1. The input to the network is the concatenated vector of the node input and the concatenated vector of the node output.",
        "reference": "Fig. 2: ModGNN\u2019s message aggregation module. In this diagram, the system is shown from a centralized point of view. First, the raw observation or output from the last layer is transformed by finput. Then, for each transmission up to K hops, the data from the neighboring agents is passed through fcom and then aggregated. The output is the set of the data from each k-hop neighborhood up to K."
    },
    "2107.06007v1-Figure14-1.png": {
        "prediction": "1. Waste Isolation Pilot Plant - Carlsbad, NM",
        "reference": "Figure 14. The EXO-200 slow control architecture consisted of three major parts- the MySQL backbone, the embedded system, and the distributed system."
    },
    "2107.12859v2-Figure3-1.png": {
        "prediction": "1. Figure 1: Overview of the proposed model for pose estimation.",
        "reference": "Figure 3: One iteration of our Recurrent Graph Learning framework. (a) We process part features and compute a graph message. (b) The message is encoded sequentially in our bidirectional GRU framework. (c) The features generated by the forward and reverse GRU are used to regress part-pose. We use three such iterations in our framework."
    },
    "2012.00248v1-Figure1-1.png": {
        "prediction": "1. The image shows the geometry of a hologram. The objects are placed in the x-y plane, and the hologram plane is in the z-plane. The Fourier lens is placed in the x-z plane, and the hologram plane is in the y-z plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane. The hologram plane is parallel to the Fourier lens plane.",
        "reference": "Fig. 1. Schematic diagram for synthesizing the digital Fourier hologram. The letter objects are placed at a distance d in front of the Fourier lens in the direction of propagation of light. The real-valued hologram are drawn in the logarithmic scale."
    },
    "2010.06310v1-Figure2-1.png": {
        "prediction": "1. The Cross-Supervised Mechanism",
        "reference": "Fig. 2: The framework of the joint-event-extraction model with our proposed cross-supervised mechanism."
    },
    "2202.05336v1-Figure1-1.png": {
        "prediction": "1. Fixed Image: Input image i",
        "reference": "Fig. 1. Block diagram of DBSGen"
    },
    "2107.03026v1-Figure7-1.png": {
        "prediction": "1. Ecological systems influence on social-economic systems",
        "reference": "Figure 7: Influence matrix schematic graph, based on [5, Figure 5]"
    },
    "1381187-Figure1-1.png": {
        "prediction": "1. Encoder: conv + BN + ReLU + pooling. 2. Decoder: upampling + conv + conv + BN + ReLU. 3. Output Layer: softmax. 4. Segmentation Layer: sigmoid. 5. Segmentation Loss: cross-entropy. 6. Overall Loss: cross-entropy. 7. Overall Loss Weight: 1.0. 8. Segmentation Weight: 1.0. 9. Segmentation Weight: 1.0. 10. Segmentation Weight: 1.0. 11. Segmentation Weight: 1.0. 12. Segmentation Weight: 1.0. 13. Segmentation Weight: 1.0. 14. Segmentation Weight: 1.0. 15. Segmentation Weight: 1.0. 16. Segmentation Weight: 1.0. 17. Segmentation Weight: 1.0. 18. Segmentation Weight: 1.0. 19. Segmentation Weight: 1.0. 20. Segmentation Weight: 1.0. 21. Segmentation Weight: 1.0. 22. Segmentation Weight: 1.0. 23. Segmentation Weight: 1.0. 24. Segmentation Weight: 1.0. 25. Segmentation Weight: 1.0. 26. Segmentation Weight: 1.0. 27. Segmentation Weight: 1.0. 28. Segmentation Weight: 1.0. 29. Segmentation Weight: 1.0. 30. Segmentation Weight: 1.0. 31. Segmentation Weight: 1.0. 32. Segmentation Weight: 1.0. 33. Segmentation Weight: 1.0. 34. Segmentation Weight: 1.0. 35. Segmentation Weight: 1.0. 36. Segmentation Weight: 1.0. 37. Segmentation Weight: 1.0. 38. Segmentation Weight: 1.0. 39. Segmentation Weight: 1.0. 40. Segmentation",
        "reference": "Fig. 1: Illustration of the SegNet architecture applied to EO data."
    },
    "13567899-Figure3-1.png": {
        "prediction": "1. The input image is resized to 128x128 pixels.",
        "reference": "Figure 3. V2V Architecture for Voxel Prediction. The lower part (below dashed line) consists of layers from C3D [28]. Connected to these layers we have three 3D convolution layers: Conv3c,Conv4c,Conv-pre use filters of size 3\u00d7 3\u00d7 3 with stride 1\u00d7 1\u00d7 1. Both Deconv5 and Deconv4 are deconvolutional layers employing kernels of size 4 \u00d7 4 \u00d7 4 with output stride of 2 \u00d7 2 \u00d7 2. Deconv3 has kernel size 8 \u00d7 4 \u00d7 4 and output stride of 4 \u00d7 2 \u00d7 2. The numbers inside the boxes represent the number of learning filters in that layer, while the numbers near the boxes (above or below) represent the size of output signals produced by that layer. The part inside the thick-dashed box is application-dependent."
    },
    "2204.10374v1-Figure1-1.png": {
        "prediction": "1. Gesture class selection: The system selects the appropriate gesture class based on the user's current action. For example, if the user is holding the phone horizontally, the system selects the 'reward' gesture class. 2. Gesture GVF selection: The system selects the appropriate GVF based on the selected gesture class. For example, if the selected gesture class is 'reward', the system selects the 'reward' GVF. 3. Gesture GVF parameters: The system initializes the parameters of the selected GVF. 4. Gesture GVF reward: The system calculates the reward for the selected GVF based on the parameters. 5. Reward: The system calculates the reward for the selected gesture class based on the reward for the selected GVF. 6. RL agent: The system uses the reward and the reward for the selected gesture class to train the RL agent. 7. RL agent 0: The system uses the trained RL agent to predict the next action of the user. 8. AndroidEnv action: The system takes the predicted action and executes it on the AndroidEnv. 9. AndroidEnv reward: The system receives the reward for the predicted action from the AndroidEnv. 10. Reward: The system calculates the reward for the selected gesture class based on the reward for the selected GVF and the reward for the predicted action. 11. Reward: The system calculates the reward for the selected gesture class based on the reward for the selected GVF and the reward for the predicted action. 12. Reward: The system calculates the reward for the selected gesture class based on the reward for the selected GVF and the reward for the predicted action. 13. Reward: The system calculates the reward for the selected gesture class based on the reward for the selected GVF and the reward for the predicted action. 14. Reward: The system calculates the reward for the selected gesture class based on the reward for the selected GVF and the reward for the predicted action. 15. Reward: The system calculates the reward for the selected gesture class based on the reward for the selected GVF and the reward for the predicted action. 16. Reward: The system calculates the reward for the selected gesture class based on the reward for the selected GVF and the reward for the predicted action. 17. Reward: The system calculates the reward for the selected gesture class based on the reward for the selected G",
        "reference": "Figure 1 | Gesture Hierarchy. The architecture used for the Android applications is based on a 3-layer hierarchy: (1) The lowest level operates over GVFs corresponding to all supported gestures; (2) The middle layer selects a gesture GVF given the latest pixel image in AndroidEnv and its agent is trained to maximize the return associated with the task that the agent is trained on; and (3) The top layer selects a single gesture class for the task and the agent is trained to maximize the average per step reward. All levels are operated by distributed DQN agents."
    },
    "2010.00041v1-Figure3-1.png": {
        "prediction": "1. Start: random creation of the initial population",
        "reference": "Fig. 3. The optimization flowchart for the inverse material design."
    },
    "2103.14005v2-Figure9-1.png": {
        "prediction": "1. The input image is resized to 224x224 pixels.",
        "reference": "Figure 9. The Multi Input Fusion Classifier end task architecture. The orange box shows the frozen encoder."
    },
    "2110.14764v2-Figure1-1.png": {
        "prediction": "1. Preprocessing: The raw documents are preprocessed to remove stop words, punctuation, and special characters. 2. Embedding: The preprocessed documents are converted into vectors using word embeddings. 3. Classification: The vectors are classified into different topics using a multi-class classifier. 4. Stacking: The results of the classification are stacked to form a vector representation of the document. 5. Meta-Classifier: The stacked vectors are passed through a meta-classifier to generate a final topic vector for the document.",
        "reference": "Fig. 1. The Fun architecture, exemplified with |L|=3 languages (Chinese, Italian, English). Note that the different term-document matrices in the 1st-tier may contain different numbers of documents and/or different numbers of terms. The three grey diamonds on the left represent calibrated classifiers that map the original vectors (e.g., TFIDF vectors) into |Y|-dimensional spaces. The resulting vectors are thus aligned and can all be used for training the meta-classifier, which is represented by the grey diamond on the right."
    },
    "1422748-Figure1-1.png": {
        "prediction": "1. Text Preprocessing: This step involves cleaning and normalizing the text data. 2. Language Detection: This step detects the language of the text data. 3. Text Detection: This step detects the presence of text in the document. 4. Text Normalization: This step normalizes the text data. 5. Sentence Detection: This step detects the presence of sentences in the text data. 6. Tokenization: This step breaks the text data into individual tokens. 7. Sentence Breaking: This step breaks the sentences into individual words. 8. Entity Extraction: This step extracts named entities from the text data. 9. Entity Disambiguation: This step disambiguates the extracted entities. 10. Entity Linking: This step links the extracted entities to their corresponding entities in a knowledge base.",
        "reference": "Figure 1: Overview of the Lithium NLP pipeline"
    },
    "2101.07004v1-Figure2-1.png": {
        "prediction": "7. Figure 1: Downlink data transmission. The downlink pilot is transmitted at the beginning of the downlink transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the downlink pilot and the downlink data transmission. The processing time is the time it takes to process the down",
        "reference": "Figure 2. Block diagram of one transmission block."
    },
    "1061647-Figure5-1.png": {
        "prediction": "1. The input image is resized to 256x256 pixels.",
        "reference": "Fig. 5. Architecture of the pose estimation network. First 5 layers copied from VGGM, followed by a deconvolution layer. A 1x1 convolution layer is then used to predict each output heat map."
    },
    "2103.06081v1-Figure2-1.png": {
        "prediction": "1. A schematic of the experimental setup for measuring the refractive index of a sample using a combination of a conventional phase-contrast microscope and a Brillouin light scattering (BLS) system. 2. A diagram of the BLS system, which consists of a laser beam, a sample, and a BLS cell. 3. A diagram of the conventional phase-contrast microscope, which consists of a laser beam, a sample, and a phase-contrast objective. 4. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast objective positioned above it. 5. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast objective positioned below it. 6. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast objective positioned to the side of the sample. 7. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast objective positioned to the front of the sample. 8. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast objective positioned to the back of the sample. 9. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast objective positioned to the side of the sample, with the laser beam focused on the sample. 10. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast objective positioned to the front of the sample, with the laser beam focused on the sample. 11. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast objective positioned to the back of the sample, with the laser beam focused on the sample. 12. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast objective positioned to the side of the sample, with the laser beam focused on the sample. 13. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast objective positioned to the front of the sample, with the laser beam focused on the sample. 14. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast objective positioned to the back of the sample, with the laser beam focused on the sample. 15. A diagram of the sample plane, with the BLS cell and the conventional phase-contrast",
        "reference": "Figure 2. Illustration of the illumination scheme of double SIM using far-field optics. (a) Schematic of one possible approach to generate the illumination patterns with structured electric energy density and OC simultaneously. Structured OC patterns formed by the interference of (b) two L-CPL beams ( )C x+ and (c) two R-CPL beams ( )C x\u2212 . (d, e) Structured patterns of ( )eU x . The OC patterns in (b) and (c) possess the opposite handedness. The eU patterns in (d) and (e) are the same. Line-cut profile of the (f) OC and (g) eU along the dashed lines in (b-e). Scale bar in (b) is 02\u03c0/k and applicable for (d-e)."
    },
    "2202.03373v1-Figure5-1.png": {
        "prediction": "1. The input is a sequence of images, each with a different color channel.",
        "reference": "Figure 5. An illustration of Curve Non-Linear Unit. This layer can be seen as a learnable non-linear activation function between 0 and 1. Based on Eq. 3, the learned function always follows concave down increasing curves to increase feature intensities."
    },
    "2207.07828v1-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed SPGM-Net. The generator and discriminator are composed of multiple convolutional layers, followed by a skip connection and a concatenation layer. The generator and discriminator are trained simultaneously using the adversarial loss. The generator produces a fake image, and the discriminator estimates the probability of the image being real or fake. The generator is trained to fool the discriminator, and the discriminator is trained to distinguish real and fake images. The final output is the sum of the generator and the discriminator.",
        "reference": "Fig. 2. Overview of the proposed Structural Prior guided Generative Adversarial Transformers (SPGAT). The SPGAT contains one generator, two discriminators, and one structural prior estimator. The generator is used to explore non-local information with the guidance of a structural prior estimator (SPE) for better clear image restoration. We build the skip connections between the generator and discriminators so that the discriminators can better discriminate between real and fake features in the generator for generating more natural images. Meanwhile, SPE is also utilized to guide the discriminators for better estimations. The generator takes a low-light image L as the input and outputs an enhanced image E\u0302. The SPE is input the corresponding structure S of L by gradient operation and outputs the estimated structural prior P\u0302. Both the two discriminators take the concatenation of the enhanced image E\u0302 and estimated structural P\u0302 as the input, which is regarded as the fake label. However, there is no real label feature. To that aim, we generate real label features by inputting the normal-light image to the generator and its corresponding structure to SPE. PW-STB illustrated in Fig. 3 is the basic block of the generator, discriminators, and SPE."
    },
    "2102.08827v1-Figure3-1.png": {
        "prediction": "1. Scene and Skill Ontology",
        "reference": "Fig. 3. Class diagram of the connections between skills and scene elements."
    },
    "2204.08805v1-Figure5-1.png": {
        "prediction": "1. User Interface: The user interface consists of a timeline overview generation, body model augmentation, suggestive viewpoint, and video processing. 2. Interaction: The interaction consists of viewpoint navigation, attributes editing, and pose normalization. 3. Feedback: The feedback consists of 3D pose reconstruction, temporal segmentation, attributes retrieval, and pose analysis.",
        "reference": "Fig. 5. The architecture of VCoach, which comprises five main modules."
    },
    "2207.13865v1-Figure3-1.png": {
        "prediction": "1. Randomly sampling from the ground set of domains. 2. Inducing a model on the sampled data. 3. Level-one sampling: Batches inside sampled domains. 4. Level-two sampling: Features. 5. Level-two sampling: Batches inside sampled domains. 6. Level-two sampling: Features. 7. Level-two sampling: Batches inside sampled domains. 8. Level-two sampling: Features. 9. Level-two sampling: Batches inside sampled domains. 10. Level-two sampling: Features. 11. Level-two sampling: Batches inside sampled domains. 12. Level-two sampling: Features. 13. Level-two sampling: Batches inside sampled domains. 14. Level-two sampling: Features. 15. Level-two sampling: Batches inside sampled domains. 16. Level-two sampling: Features. 17. Level-two sampling: Batches inside sampled domains. 18. Level-two sampling: Features. 19. Level-two sampling: Batches inside sampled domains. 20. Level-two sampling: Features. 21. Level-two sampling: Batches inside sampled domains. 22. Level-two sampling: Features. 23. Level-two sampling: Batches inside sampled domains. 24. Level-two sampling: Features. 25. Level-two sampling: Batches inside sampled domains. 26. Level-two sampling: Features. 27. Level-two sampling: Batches inside sampled domains. 28. Level-two sampling: Features. 29. Level-two sampling: Batches inside sampled domains. 30. Level-two sampling: Features. 31. Level-two sampling: Batches inside sampled domains. 32. Level-two sampling: Features. 33. Level-two sampling: Batches inside sampled domains. 34. Level-two sampling: Features. 35. Level-two sampling: Batches inside sampled domains. 36. Level-two sampling: Features. 37. Level-two sampling: Batches inside sampled domains. 38. Level-two sampling: Features. 39. Level-two sampling: Batches inside sampled domains. 40. Level-two sampling: Features. 41. Level-two sampling: Batches inside sampled domains. 42. Level-two sampling: Features. 43",
        "reference": "Figure 3: Illustration of the sampling framework of DOMI. The solid arrow indicates the actual sampling flow, while the dotted arrow only indicates the difference between randomly sampling and DOMI."
    },
    "1828713-Figure2-1.png": {
        "prediction": "1. Softmax: Softmax is a function that maps a vector of real numbers to a vector of real numbers. It is used to compute the probability of each class for a given input. In this case, we are using Softmax to compute the probability of each class for each word in the input sequence.",
        "reference": "Figure 2: An illustration of the neural network approach for sentiment classification. wi means the i-th word of a review text. uk and pj are continuous vector representations of user k and product j for capturing user-sentiment and product-sentiment consistencies. Uk and Pj are continuous matrix representations of user k and product j for capturing user-text and product-text consistencies."
    },
    "53776814-Figure4-1.png": {
        "prediction": "1. The STGCN layer consists of two convolutional layers, followed by a deconvolutional layer. The convolutional layers are used to extract features from the input graph, while the deconvolutional layer is used to upsample the features. The output of the STGCN layer is a feature map of the same size as the input graph.",
        "reference": "Figure 4. Illustration of stacked hourglass STGCN with two levels."
    },
    "2107.01209v1-Figure5-1.png": {
        "prediction": "1. The figure shows the geometry of the Earth and the Sun in the solar system. The Earth is a sphere of radius a, and the Sun is a sphere of radius D. The distance between the Earth and the Sun is labeled as D = 1a.u.",
        "reference": "Figure 5. Left: The DM particle enters the bulk of the sun with an angle \u03b8, collides with solar electrons and finally escapes from the Sun. For the purpose of illustration, the DM trajectory is projected onto a single plane. Right: The flux of reflected DM particles can be approximated as propagating along the radial direction from the center of the Sun. See text for details."
    },
    "2110.14160v2-Figure2-1.png": {
        "prediction": "1. The input resolution selection module selects the resolution of the input image based on the size of the image and the size of the network. 2. The composition of DA selection module selects the data augmentation strategy based on the type of data. 3. The sampling strategy selection module selects the sampling strategy based on the type of data. 4. The training strategies module selects the training strategies based on the type of data. 5. The LR schedule selection module selects the learning rate schedule based on the type of data. 6. The optimization configurations module selects the optimization configurations based on the type of data. 7. The paired feature fusion module fuses the features of the input image and the corresponding label image. 8. The ensemble method selection module selects the ensemble method based on the type of data. 9. The prediction module predicts the label of the input image.",
        "reference": "Figure 2: Components analyzed in our deep learning-based DR grading framework. The evaluation process of a framework can be divided into two parts: training (top) and testing (bottom). In the training phase, we first fix the architecture of the selected network (ResNet-50). Then we examine a collection of designs with respect to the training setting including preprocessing (image resizing and enhancement), training strategies (compositions of data augmentation (DA) and sampling strategies) and optimization configurations (objective functions and learning rate (LR) schedules). In the testing phase, we apply the same preprocessing as in the training phase and employ paired feature fusion to make use of the correlation between the two eyes (the training step of the fusion network is omitted in this figure). Then, we select the best ensemble method for the final prediction."
    },
    "2205.12022v1-Figure1-1.png": {
        "prediction": "1. Parsing Generator with Res FFT Conv Block",
        "reference": "Fig. 1. Overview of our model. Our model consists of parsing generator and image generator, training parsing generator requires a pair of source-target images IS , IT , then obtaining human keypoints KS ,KT and human parsing map PS , PT respectively by using openpose and PGN framework. We concatenate KS , PS ,KT as the input of parsing generator, then the input is fed into an Unet-type network that generates a target parsing map with the same size of PT , which contains body shape information. To get the vivid image with detailed texture (e.g.style of clothing), we extract the Per-region feature information PSj from the source image via VGG type network, then we concatenate the parsing map to the above KS , PS ,KT and normalize it along with the region information PSj to combine the information of source texture and target parsing map. Finally, the target image can be generated by spatial normalization and decoder."
    },
    "2207.06080v2-Figure2-1.png": {
        "prediction": "1. Feature Extraction: The first step is to extract features from the input image. This is typically done using convolutional layers, which are non-linear activations. The output of the convolutional layers is a set of feature maps, each of which represents a different aspect of the image. The size of the feature maps is typically 32 x 32 pixels.",
        "reference": "Fig. 2: Illustration of feature and classification embeddings, using the Resnet 32 architecture. Feature embeddigs (FE) are extracted by the CNN\u2019s non-linear layers and are combined with the final classification layer weights to form classification layer embeddings (CLE), before they are summed."
    },
    "2204.09225v1-Figure1-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed model for the task of temporal similarity estimation. The model consists of two twin transformers, which are identical except for the shared weights. The twin transformers are used to extract spatial and temporal features from the input FMRI data. The spatial features are extracted using the spatial transformer, while the temporal features are extracted using the temporal transformer. The spatial and temporal features are then multiplied and recovered using the matrix multiplication and matrix recovery operations. The recovered features are then projected using linear projection and temporal position. The output of the model is the temporal similarity estimation.",
        "reference": "Fig. 1. Illustration of the proposed Twin-Transformers framework. (a) shows the overall picture of the proposed model, and (b) shows the details of the Twin-Transformers. The Twin-Transformers take the identical brain signal matrix as input. The spatial Transformer focuses on the space dimension and takes non-overlapping spatial patches as tokens to build attention in the spatial dimension and generate spatial features. Conversely, the temporal Transformer focuses on the temporal dimension and the non-overlapping temporal patches are used as tokens. Correspondingly, the temporal Transformer is designed to build attention in the time dimension and generate temporal features. The twin Transformers are designed for disentangling temporal and spatial features of the input signal matrix."
    },
    "2102.09837v1-Figure2-1.png": {
        "prediction": "1. The system starts in the initial state, where it is not calibrated.",
        "reference": "Figure 2: The platform model of a robot arm."
    },
    "2202.01032v2-Figure5-1.png": {
        "prediction": "1. Service Management and Orchestration Framework (SMOF) - A high-level architecture that provides a common framework for managing and orchestrating the execution of xApps. It includes a non-real-time RIC (Non-Real-Time Resource Isolation Container) for managing the execution of xApps, an API/SDK for supporting xApps, and a subscription management system for managing the subscription of xApps to the SMOF.",
        "reference": "Fig. 5: Near-RT RIC architecture. The near-RT RIC connects to the RAN through the E2 interface, at the bottom of the figure (yellow), and to the non-RT RIC/SMO through the A1 and O1 interfaces, at the top of the figures (orange and green, respectively). The communication among the RIC components (in light blue) is mediated by an internal messaging infrastructure. The near-RT RIC can onboard custom logic as xApps (dark blue)."
    },
    "2202.01032v2-Figure6-1.png": {
        "prediction": "1. Service Management and Orchestration (SMO) Framework",
        "reference": "Fig. 6: Non-RT RIC and SMO architecture. The SMO functionalities (in green) enable connectivity to the O-Cloud (through the O2 interface) and the other RAN components (through O1) for management and orchestration. The non-RT RIC features custom logic (rApps, in red), and a termination of the A1 interface to the near-RT RIC (orange). Shared functionalities between the non-RT RIC and the SMO are in yellow."
    },
    "2202.02070v2-Figure2-1.png": {
        "prediction": "1. Pretraining BERT: The architecture of BERT in pretraining. Assume that in the lowercased example sequence consisting of the segment pair 'he starts to speak. the nervous crowd is watch-ing him.' the tokens 'speak' and 'nervous' were sampled to be masked. 'speak' is replaced by the '[MASK]' token and 'nervous' is replaced by the random token 'that'. The model's task is to predict the tokens 'speak' and 'nervous' from the representation vectors it learns at the positions of the input embeddings of '[MASK]' and 'that'. P(B follows A) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Fig. 2. The architecture of the proposed CGiS-Net."
    },
    "1319230-Figure1-1.png": {
        "prediction": "1. Query (x) is sent to the RNN model, which generates an answer (y). 2. The answer is sent to the document summarization model, which generates a document summary (d). 3. The document summary is sent to the sentence selection model, which selects the most relevant sentence(s) from the document summary. 4. The selected sentence(s) are sent to the document summarization model, which generates a document summary (d'). 5. The document summary (d') is sent to the RNN model, which generates an answer (y').",
        "reference": "Figure 1: Hierarchical question answering: the model first selects relevant sentences that produce a document summary (d\u0302) for the given query (x), and then generates an answer (y) based on the summary (d\u0302) and the query x."
    },
    "210713891-Figure1-1.png": {
        "prediction": "1. The AVSD task is to generate a summary of a given image. 2. The input to the model is a sequence of images, each with a corresponding caption. 3. The model is trained to predict the next image in the sequence, given the previous images and captions. 4. The model is also trained to predict the summary of the sequence, given the previous images and captions. 5. The summary is generated by the model and can be used to describe the sequence of images.",
        "reference": "Figure 1: An illustration of DSTC8 AVSD task. The goal of the task is generating accurate answer based on multiple modalities."
    },
    "2206.00726v1-Figure1-1.png": {
        "prediction": "1. The surrogate model is updated with the latest information from the multi-agent trajectory classifier.",
        "reference": "Fig. 1: Overview of our proposed algorithm. The BayesOpt model is composed of multiple Gaussian process models, which represent dynamic feasibility and collision avoidance constraints."
    },
    "2101.09353v2-Figure1-1.png": {
        "prediction": "1. The drone is equipped with radioprobe sensors that collect data from the cloud. 2. The data is transmitted to the base station via radio link. 3. The data is processed by a processing machine. 4. The processed data is sent back to the drone.",
        "reference": "Figure 1. Working principle and radioprobe system architecture."
    },
    "2101.01418v1-Figure2-1.png": {
        "prediction": "1. Low Level Processing: Image Acquisition, Contrast Enhancement, Noise Removal, Segmentation, Representation, Description",
        "reference": "Figure 2: Different levels in image processing process."
    },
    "2101.05151v2-Figure5-1.png": {
        "prediction": "1. The input to the first layer is the sequence of tokens in the input sentence. The output of the first layer is a sequence of hidden states, one for each token in the input sentence.",
        "reference": "Figure 5: Graphical illustration of long horizontal link forecasting. Given a sequence of graph snapshots G = {G(t\u2212tk), ...,G(t)}, whose length is k, test quadruples at t+ \u2206T are to be predicted."
    },
    "1482745-Figure1-1.png": {
        "prediction": "1. GAN Discriminator: A neural network that takes an input image and outputs a probability distribution over the real and fake classes. The discriminator network consists of multiple layers, each of which takes an input image and outputs a probability distribution over the real and fake classes. The output of the last layer is a binary classification that determines whether the input image is real or fake. The discriminator network is trained to distinguish between real and fake images, and it is used to generate fake images that are indistinguishable from real images.",
        "reference": "Figure 1: PacGAN(m) augments the input layer by a factor of m. The number of edges between the first two layers are increased accordingly to preserve the connectivity of the mother architecture (typically fully-connected). Packed samples are fed to the input layer in a concatenated fashion; the grid-patterned nodes represent input nodes for the second input sample."
    },
    "2210.08812v1-Figure2-1.png": {
        "prediction": "1. Content: The content of the image, which includes the object, background, and other features. 2. Coordinates: The coordinates of the object in the image, which are the x and y coordinates of the object's center. 3. Softmax: A function that maps a vector to a probability distribution. 4. Aggregation: The process of combining the content and coordinates of the object to generate a single vector. 5. Transform: A function that maps a vector to a new vector. 6. Modulation: A function that maps a vector to a new vector by multiplying it by a scalar. 7. Explicit Transformer: A neural network that takes the content and coordinates of the object as input and generates a new vector as output. 8. Implicit Transformer: A neural network that takes the content and coordinates of the object as input and generates a new vector as output by applying a function to the content and coordinates.",
        "reference": "Fig. 2. Illustration of aggregation based explicit transformer and modulation based implicit transformer."
    },
    "2101.07910v1-Figure2-1.png": {
        "prediction": "1. Input Population: The initial population of code snippets to be mutated.",
        "reference": "Fig. 2. The guided mutation (GM) process for code adversarial generation."
    },
    "2012.06182v1-Figure1-1.png": {
        "prediction": "1. Figure 1: Overview of the different layers of the Earth observation system.",
        "reference": "Fig. 1: Illustration of a multilayered SN with satellites, HAPs, and UAVs."
    },
    "2012.06182v1-Figure8-1.png": {
        "prediction": "1. A swarm of UAVs flying in the air. 2. IoT applications communicating with the UAVs. 3. A stand-alone HAP communicating with the UAVs. 4. A remote area populated by buildings and vehicles. 5. A RF link connecting the HAP to the UAVs. 6. An optical link connecting the HAP to the UAVs.",
        "reference": "Fig. 8: An architecture of HAPs network with P2P HAP-to-HAP and backhauling links."
    },
    "2103.04910v1-Figure6-1.png": {
        "prediction": "1. The system is modeled as a function $f(t)$, which is the output of the system at time $t$. 2. A regulator $r(t)$ is designed to control the system. 3. The control design is a function $h(t) = k(t)f(t)$, where $k(t)$ is a function of the current state of the system. 4. The recursive identifier $i(t)$ is used to estimate the state of the system. 5. The system is controlled by the regulator $r(t)$, which is designed to minimize the error between the output of the system and the control input. 6. The recursive identifier $i(t)$ is used to estimate the state of the system, which is used to update the control input $h(t)$. 7. The system is controlled by the regulator $r(t)$, which is designed to minimize the error between the output of the system and the control input. 8. The recursive identifier $i(t)$ is used to estimate the state of the system, which is used to update the control input $h(t)$.",
        "reference": "Figure 6: Model building approach"
    },
    "2202.06208v1-Figure1-1.png": {
        "prediction": "1. Dynamic Hierarchical Tree: This is a representation learning algorithm that uses a hierarchical tree structure to represent the data. The tree is dynamically updated as new data is added, and the tree structure is used to make predictions. The tree structure is optimized to minimize the error between the predicted and true values.",
        "reference": "Figure 1. The architecture of our proposed BROT. Sub-figure (a) depicts the OT process to align features from different domains. Sub-figure (b) shows that the metric learning aims to seek better decision boundaries with a dynamic hierarchical tree. The darkness of blue colors reflects the magnitude of molecular properties."
    },
    "2107.01104v1-Figure1-1.png": {
        "prediction": "1. The figure shows a schematic of the proposed method for the segmentation of the brain from MRI data. The method involves the following steps: (1) The MRI data is preprocessed to remove any non-brain tissue. (2) The brain is segmented into slices using a pre-trained CNN. (3) The slices are then processed to extract the spatial distribution of the particles. (4) The particles are then classified into different types based on their spatial distribution. (5) The final segmentation of the brain is obtained by combining the results of the different types of particles.",
        "reference": "Figure 1: Distribution of real-space data between MPI processes in TurTLE. Fields are split into slabs and distributed between P MPI processes along the x3 direction. The Np particles are also distributed, with each MPI process storing Pp particles on average. Within each MPI process the particle data is sorted according to its x3 location. This leads to a direct association between each of the Ps field slices to contiguous regions of the particle data arrays \u2014 in turn simplifying the interpolation procedure (see text for details). On average, S p particles are held within each such contiguous region."
    },
    "51865-Figure3-1.png": {
        "prediction": "1. CNN Module: This module consists of convolutional layers, pooling layers, and FC layers. The convolutional layers extract features from the input data, while the pooling layers reduce the spatial dimensions of the feature maps. The FC layers are used to combine the features from different layers and produce the final output.",
        "reference": "Figure 3. Overview of DCEC. The network is built by cascading the CNN module and the CEC."
    },
    "2012.02113v1-Figure8.1-1.png": {
        "prediction": "1. Venn diagram showing the intersection and union of two sets.",
        "reference": "Figure 8.1 Venn diagram showing entropic quantities associated with a pair of random variables taking values in different sets: the Shannon entropies H(X) and H(Y), the joint entropy H(X,Y), the conditional entropies H(X | Y) and H(Y | X), and the mutual information I(X; Y)."
    },
    "2012.02113v1-Figure7.4-1.png": {
        "prediction": "1. The figure shows the distribution of the parameters of the Dirichlet distribution. The parameters are represented by the points on the x-axis, and the probability of each parameter is represented by the height of the corresponding bar. The probability of each parameter is proportional to the area under the curve, which is represented by the height of the bar. The probability of each parameter is also proportional to the number of parameters that are equal to that value.",
        "reference": "Figure 7.4 Schematic illustration of Lemma 7.4.6."
    },
    "2204.12696v1-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of StyleGAN, which is a generative adversarial network that can generate realistic-looking images. The figure shows the latent space of the network, which is a 3D cube with 1000 dimensions. The figure also shows the strength of micromotion, which is the ability of the network to generate images that are similar to the input images but with slight differences. The figure also shows the identities of the input images, which are the images that the network is trying to generate.",
        "reference": "Figure 2: A tensor illustration of our low-rank micromotion subspace hypothesis. In the StyleGAN latent space, we hypothesize the same type of micromotion, at different quantitative levels but for the same identity, can be approximated by a low-rank subspace. We further hypothesize that subspaces for the same type of micromotion found at different identities are extremely similar to each other, and can hence be transferred across identities."
    },
    "206593096-Figure3-1.png": {
        "prediction": "1. The input image is scaled to a fixed size. 2. The scaled image is fed into a classifier, which outputs a probability distribution over the classes. 3. The classifier's output is fed into a set of convolutional filters, which extract features from the image. 4. The features are concatenated with the scaled image, and passed through a classifier, which outputs a probability distribution over the classes. 5. The classifier's output is fed into a set of convolutional filters, which extract features from the image. 6. The features are concatenated with the scaled image, and passed through a classifier, which outputs a probability distribution over the classes. 7. The classifier's output is fed into a set of convolutional filters, which extract features from the image. 8. The features are concatenated with the scaled image, and passed through a classifier, which outputs a probability distribution over the classes. 9. The classifier's output is fed into a set of convolutional filters, which extract features from the image. 10. The features are concatenated with the scaled image, and passed through a classifier, which outputs a probability distribution over the classes. 11. The classifier's output is fed into a set of convolutional filters, which extract features from the image. 12. The features are concatenated with the scaled image, and passed through a classifier, which outputs a probability distribution over the classes. 13. The classifier's output is fed into a set of convolutional filters, which extract features from the image. 14. The features are concatenated with the scaled image, and passed through a classifier, which outputs a probability distribution over the classes. 15. The classifier's output is fed into a set of convolutional filters, which extract features from the image. 16. The features are concatenated with the scaled image, and passed through a classifier, which outputs a probability distribution over the classes. 17. The classifier's output is fed into a set of convolutional filters, which extract features from the image. 18. The features are concatenated with the scaled image, and passed through a classifier, which outputs a probability distribution over the classes. 19. The classifier's output is fed into a set of convolutional filters, which extract features from the image. 20. The features are concatenated with the scaled image, and passed through a classifier, which outputs a probability distribution over",
        "reference": "Figure 3: Two network designs in this paper. The input image is processed as a whole at the convolutional layers from conv1 to conv5. Segments are exploited at a deeper hierarchy by: (Left) applying CFM on the feature map of conv5, where \u201c b\u201d means for \u201cbounding boxes\u201d and \u201c s\u201d means for segments; (Right) applying CFM on the finest feature map of the spatial pyramid pooling layer."
    },
    "2101.10868v1-Figure1-1.png": {
        "prediction": "1. Government collects data from social media and other sources. 2. Data is processed and shared with unknown parties. 3. Data is aggregated and used to monitor users' behaviors. 4. Users' behaviors are changed by the data sharing and aggregation process. 5. The process continues, with restrictions and mass surveillance increasing the risk of privacy violations.",
        "reference": "Figure 1: A Systematic Overview of Privacy Threats from Multiple Domains Related to the COVID-19 Pandemic"
    },
    "2102.04539v2-Figure10-1.png": {
        "prediction": "1. The input to the network is a sequence of words.",
        "reference": "Figure 10: Illustration for 2-Diam GBP. Here, V q denotes all sets V{q,\u00b7}."
    },
    "2102.04539v2-Figure6-1.png": {
        "prediction": "1. The input to the network is a sequence of words.",
        "reference": "Figure 6: Illustration for the construction in the proof of Proposition 8 for 2-Reach GBP with r = 1. In this example, U = {u1, . . . , un} and we have {u1, ui, uj , un} = F \u2208 F ."
    },
    "2102.04539v2-Figure4-1.png": {
        "prediction": "1. The figure shows the architecture of a convolutional neural network (CNN) with multiple convolutional layers and pooling layers. The input image is passed through the convolutional layers, which apply a set of filters to the image to extract features. The output of each convolutional layer is passed through a pooling layer, which reduces the spatial dimensions of the feature maps. The output of the convolutional layers and pooling layers is then passed through a fully connected layer, which applies a set of linear transformations to the feature maps. The output of the fully connected layer is then passed through a softmax layer, which outputs a probability distribution over the classes.",
        "reference": "Figure 4: Illustration to Constructions 3 & 4. Part (a) shows an exemplary directed graph which is a yes-instance for DHP. Applying Construction 3 on (a) yields (b). Applying Construction 4 on (b) yields the instance whose graph is depicted in (c) and two habitats of which are depicted in (d) and (e). Vertices marked yellow in (d) are contained in the habitat Xout. Vertices marked red in (e) are contained in the habitat Yout. The graph induced by Yout contains the red edges."
    },
    "2110.06487v1-Figure5-1.png": {
        "prediction": "1. Victim accesses a COVID Unemployment Claim Site on the Public Internet. 2. Scammer D/CG sends a message to the Victim. 3. The Victim clicks on a link in the message, which takes them to a Dark Web site. 4. The Victim is tricked into transferring funds to a Fund Transfer. 5. The funds are stolen by the Scammer D/CG.",
        "reference": "Fig. 5. Covid unemployment threat model"
    },
    "2103.00907v1-Figure4-1.png": {
        "prediction": "1. The figure shows a neural network with multiple hidden layers. The input layer is connected to the first hidden layer, which is connected to the second hidden layer, and so on. The output layer is connected to the last hidden layer, which is connected to the input layer. The input layer is labeled as \"Input\" and the output layer is labeled as \"Output.\" The hidden layers are labeled as \"Hidden layers.\" The figure also shows the direction of the arrows, which represent the flow of information from one layer to the next. The figure is labeled as \"Hidden layers.\"",
        "reference": "Figure 4. Diagram of the neural network(NN2) for the turbulent channel flow. The inputs are the pressure and velocity gradient and the output is the pressure strain term. The FCFF has 5 layers with 10 neurons in each layer."
    },
    "2206.00162v2-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed method for generating high-quality masks for face edge detection. The input image is first processed by a core generator to generate a coarse mask. The coarse mask is then refined by a resolution enhancer to generate a high-quality mask. The refined mask is then used to generate a quality score for the image. The quality score is used to select the best mask for the image. The selected mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge image. The boosted edge image is then used to generate a boosted edge mask. The boosted edge mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge residue mask. The boosted edge residue mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge residue mask. The boosted edge residue mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge residue mask. The boosted edge residue mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge residue mask. The boosted edge residue mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge residue mask. The boosted edge residue mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge residue mask. The boosted edge residue mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge residue mask. The boosted edge residue mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge residue mask. The boosted edge residue mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge residue mask. The boosted edge residue mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge residue mask. The boosted edge residue mask is then used to generate a boosted edge residue image. The boosted edge residue image is then used to generate a boosted edge residue mask. The boosted edge residue mask is then used to generate a boosted edge residue image. The boosted edge residue image is",
        "reference": "Figure 2: Overview of PAGER generation method."
    },
    "2011.03148v2-Figure4-1.png": {
        "prediction": "1. The BBox loss is calculated by comparing the predicted bounding box with the ground truth bounding box. The loss is computed using the L1 norm of the difference between the predicted and ground truth bounding boxes. 2. The Huber regression loss is used to penalize large errors in the predicted bounding box. 3. The Focal loss is used to encourage the model to focus on the most important pixels in the bounding box. 4. The Class Logits loss is used to encourage the model to predict the correct class for each bounding box. 5. The Class Similarity loss is used to encourage the model to predict similar bounding boxes for similar objects. 6. The Focal Loss is used to encourage the model to focus on the most important pixels in the bounding box. 7. The Class Logits loss is used to encourage the model to predict the correct class for each bounding box. 8. The Class Similarity loss is used to encourage the model to predict similar bounding boxes for similar objects.",
        "reference": "Fig. 4. Diagram of perception consistency loss computation. An EfficientDet object detector predicts boxes and classes. Consistency of predictions between images is captured by losses similar to those in object detection training."
    },
    "2110.09749v2-Figure1-1.png": {
        "prediction": "1. Candidate Keyphrase Extraction: Extracts candidate keyphrases from the source document using a unit-granularity keyphrase extraction model. 2. Keyphrase Importance Estimation: Estimates the importance of each candidate keyphrase using a keyphrase importance estimation model. 3. Syntactic Accuracy: Measures the syntactic accuracy of the extracted keyphrases using a syntactic accuracy model. 4. Information Saliency: Measures the information saliency of the extracted keyphrases using an information saliency model. 5. Predictions: Generates predictions for the document representation using a concept consistency model.",
        "reference": "Figure 1: The KIEMP model architecture."
    },
    "2103.07592v2-Figure1-1.png": {
        "prediction": "1. The equation for the force of friction between two surfaces in contact, Ff, is given by the product of the normal force, N, and the coefficient of friction, \u03bc. The equation is Ff = \u03bcN.",
        "reference": "Figure 1: Schematic diagram for flux-mediated dark matter."
    },
    "2107.05680v1-Figure2-1.png": {
        "prediction": "1. The input image is resized to 8x8 pixels.",
        "reference": "Figure 2: A modified architecture for progressive training of convex GANs (ProCoGAN). At each stage i, a linear generator Wi is used to model images at a given resolution Xi, attempting to fool quadratic-activation discriminator Di, for which the optimal solution can be found in closed-form via (15). Once stage i is trained, the input to stage i + 1 is given as the output of the previous stage with learned weights W\u2217i , which is then used to model higher-resolution images Xi+1. The procedure continues until high-resolution images can be generated from successive application of linear generators."
    },
    "2107.05214v2-Figure6-1.png": {
        "prediction": "1. The input to the MLP is a sequence of binary vectors, where each vector represents a single word in the sentence. The length of the sequence is equal to the length of the sentence.",
        "reference": "Figure 6: The illustration of the attention mechanism. The prediction of current hidden state h\u0302t and the grid-level features E is used as query and key, respectively."
    },
    "2102.12058v2-Figure17-1.png": {
        "prediction": "1. A node in each slice accepts a. 2. A node in each slice votes for/accepts a. 3. A node in each slice votes for/accepts a. 4. A node in each slice votes for/accepts a. 5. A node in each slice votes for/accepts a. 6. A node in each slice votes for/accepts a. 7. A node in each slice votes for/accepts a. 8. A node in each slice votes for/accepts a. 9. A node in each slice votes for/accepts a. 10. A node in each slice votes for/accepts a. 11. A node in each slice votes for/accepts a. 12. A node in each slice votes for/accepts a. 13. A node in each slice votes for/accepts a. 14. A node in each slice votes for/accepts a. 15. A node in each slice votes for/accepts a. 16. A node in each slice votes for/accepts a. 17. A node in each slice votes for/accepts a. 18. A node in each slice votes for/accepts a. 19. A node in each slice votes for/accepts a. 20. A node in each slice votes for/accepts a. 21. A node in each slice votes for/accepts a. 22. A node in each slice votes for/accepts a. 23. A node in each slice votes for/accepts a. 24. A node in each slice votes for/accepts a. 25. A node in each slice votes for/accepts a. 26. A node in each slice votes for/accepts a. 27. A node in each slice votes for/accepts a. 28. A node in each slice votes for/accepts a. 29. A node in each slice votes for/accepts a. 30. A node in each slice votes for/accepts a. 31. A node in each slice votes for/accepts a. 32. A node in each slice votes for/accepts a. 3",
        "reference": "Fig. 17. Federated voting process [46] ."
    },
    "2103.05447v1-Figure2-1.png": {
        "prediction": "1. The flow meter is installed in the middle of the straight pipe.",
        "reference": "Figure 2. Experiment schematic."
    },
    "2010.00041v3-Figure2-1.png": {
        "prediction": "1. Start: random creation of the initial population",
        "reference": "Fig. 2. The optimization flowchart for the inverse material design."
    },
    "2202.00846v1-Figure2-1.png": {
        "prediction": "1. Click/ Purchase Log: A log containing information about user clicks and purchases.",
        "reference": "Figure 2: Method Overview"
    },
    "2101.07424v2-Figure1-1.png": {
        "prediction": "1. Coded aperture scene: The scene is encoded using a coded aperture, which is a pattern of holes in a plate that can be used to encode the scene. The coded aperture is placed in front of the scene, and the light passing through the holes is collected by a camera. The camera captures the encoded scene, which is a set of pixels with a specific color at each location.",
        "reference": "Fig. 1. Physical sensing phenomena in CASSI, which is the CSI prototype used to validate the proposed approach."
    },
    "2204.03873v1-Figure2-1.png": {
        "prediction": "1. The input is fed into the batch normalization layer, which normalizes the input and helps stabilize the training process. 2. The input is then passed through a convolutional layer, which applies a set of filters to the input and outputs a feature map. 3. The feature map is then passed through a max-pooling layer, which reduces the spatial size of the feature map and extracts the most relevant information. 4. The feature map is then passed through a fully connected layer, which converts the feature map into a set of numbers representing the input. 5. The fully connected layer is then passed through a softmax layer, which converts the numbers into probabilities representing the input. 6. The softmax layer is then used to generate the output probabilities for the input.",
        "reference": "Figure 2: Structure of gait-TR. TCN is the temporal convolutional network module, and ST is the spatial transformer module. FC denotes full connect layer. Batch-norm is BatchNorm2D for input Xtv \u2208 \u211d C\u00d7T\u00d7V , while Batch-norm* denotes BatchNorm1D for input Xtv \u2208 \u211d C\u2217V \u00d7T ."
    },
    "2204.10426v1-Figure1-1.png": {
        "prediction": "1. Cognitive impairment (State 1) -> Death (State 2",
        "reference": "Figure 1: Three-state illness-death model"
    },
    "2204.09537v1-Figure1-1.png": {
        "prediction": "1. The setup of the experiment.",
        "reference": "Figure 1: Top: Schematic of the hydrogen beamline including the electric field ionizer (FI) and the microchannel plate (MCP) particle detectors. The electric and magnetic fields required for optical Rydberg excitation (cf. section 2.2.1) yet need to be experimentally implemented. Bottom: Schematic of the injection seeded Ti:Sa Rydberg excitation laser."
    },
    "2107.06268v1-Figure1-1.png": {
        "prediction": "1. Data cleaning and preprocessing: This step involves cleaning and preprocessing the data to remove any errors or inconsistencies. This can include tasks such as data cleaning, data augmentation, and feature engineering. 2. Holiday adjustment procedure: This step involves adjusting the data to account for holidays and other events that may affect the data. This can involve tasks such as removing data during holidays, adjusting the data to account for holiday effects, and creating new features to capture holiday effects. 3. Train individual forecasting models: This step involves training individual forecasting models for each day of the year. This can involve tasks such as feature engineering, model selection, and model training. 4. Forecast combination by smoothed Bernstein Online Aggregation (BOA): This step involves combining the individual forecasting models using BOA to create a more accurate forecast. This can involve tasks such as feature engineering, model selection, and model training. 5. Final output: The final output of the forecasting model is a forecast for the next day of the year.",
        "reference": "Fig. 1: Structure of forecasting approach used for the forecasting competition."
    },
    "2011.14684v2-Figure4-1.png": {
        "prediction": "1. Residual Reduction Module: This module is responsible for reducing the size of the feature map by removing redundant information. It consists of a SE block, which applies a series of downsampling and upsampling operations to reduce the size of the feature map. The SE block also applies a global average pooling operation to extract the global average feature for each channel. The output of the SE block is then concatenated with the input feature map and passed through a residual connection to the next module.",
        "reference": "Fig. 4: Overview of the REMnet architecture. The input of the model is the K\u00d71 tensor representing the CIR of the measurement. The dimensionality is reduced by N subsequent Residual Reduction Modules (RRM) with a feature attention mechanism. Finally, a fully connected layer composes the high-level extracted features and outputs the range error estimation."
    },
    "2206.08316v1-Figure1-1.png": {
        "prediction": "1. Training Stage: Raw dataset, Augmented Dataset, Pretrained Teacher Model, Extracted Dark Knowledge, Normal Optimizer, DNN Surrogate Model, Adversarial Example Optimizer, Clean Image Optimizer. 2. Generating Stage: Adversarial Example Optimizer, Adversarial Example, Adversarial Example Optimizer, Adversarial Example, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer, Adversarial Example Optimizer",
        "reference": "Fig. 1 An illustration of transfer-based adversarial attack and the proposed method. The two images in Raw Dataset are from ImageNet, labeled as \u201cpersian cat\u201d and \u201cpapillon\u201d respectively. Note that they also have features of other cats and dogs, as well as pillow and car. The normal surrogate model is trained by one-hot labels, and its adversarial transferability is relatively weak. In contrast, the dark surrogate model we proposed is trained with enhanced dark knowledge. Thus, it demonstrates stronger adversarial transferability."
    },
    "2010.03420v1-Figure2-1.png": {
        "prediction": "1. Input Layer: This is the first layer of the neural network and it receives the input data. It is a set of nodes that are connected to the input data. The input data is fed into the input layer and the nodes in the layer process the data. The output of the input layer is the input vector for the next layer.",
        "reference": "Fig 2: Fully connected convolutional neural network with four hidden layers."
    },
    "2107.00465v1-Figure1-1.png": {
        "prediction": "1. The input layer is the first layer of the neural network and it receives the input data. The input data is represented as a vector of features.",
        "reference": "Fig. 1: Illustration of the neural network architecture to predict the optimal generation outputs P\u0302g using the active power demand Pd as input: There are K hidden layers in the neural network withNk neurons each. Where k = 1, ...,K."
    },
    "210701837-Figure2-1.png": {
        "prediction": "1. Training: The model is trained on the modified and unmodified images. The loss function is used to optimize the model. 2. Prediction: The model is used to predict the activation of the brain region. 3. 3D Labeling post-processing: The model is used to segment the brain region in 3D. 4. Consensus: The model is used to generate a consensus segmentation of the brain region.",
        "reference": "Figure 2: The final segmentation volume is generated by taking into account activations from three FCNNs specialized on each 2D orientation. Neighboring slices are taken into account in a multi-channel approach. Full slices are used in prediction time, but training uses patches."
    },
    "2107.09388v2-Figure1-1.png": {
        "prediction": "1. Pretraining BERT: The architecture of BERT in pretraining. Assume that in the lowercased example sequence consisting of the segment pair 'he starts to speak. the nervous crowd is watch-ing him.' the tokens 'speak' and 'nervous' were sampled to be masked. 'speak' is replaced by the '[MASK]' token and 'nervous' is replaced by the random token 'that'. The model's task is to predict the tokens 'speak' and 'nervous' from the representation vectors it learns at the positions of the input embeddings of '[MASK]' and 'that'. P(B follows A) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Figure 1: MHSA model configuration for SELD task."
    },
    "2103.10114v1-Figure10-1.png": {
        "prediction": "1. Input: F2, x, y, z",
        "reference": "Fig. 10: Iteration procedure in dynamical core."
    },
    "2011.02216v2-Figure10-1.png": {
        "prediction": "1. The first round of the game is played. The player with the highest score wins the round and receives the reward. The player with the lowest score loses the round and receives the penalty. The game continues until one player has won n rounds.",
        "reference": "FIG. 10 Example of entanglement detection with bounded configuration space. In this illustration, every measurement in the first n \u2212 1 rounds is a 4-outcome POVM, where one outcome leads directly to guess SEP, and 3 outcomes continue with the measurement process."
    },
    "2102.12327v1-Figure4-1.png": {
        "prediction": "1. Test case 1 failed: Test Suite Diagnosis: These constraints are inconsistent with the following test cases: usage = Scientific, cpu? = CPU, mb? = MBSilver.",
        "reference": "Figure 4: PC recommender knowledge base: result of the diagnosis process presented in WEEVIS."
    },
    "2010.01919v2-Figure6-1.png": {
        "prediction": "1. Pre-training: The model is trained on a large corpus of text data to learn general language understanding. The model is fine-tuned on a smaller corpus of data to learn specific language understanding.",
        "reference": "Fig. 6: Training process. Data augmentation in pre-training. No data augmentation in fine-tuning."
    },
    "2107.12156v1-Figure2-1.png": {
        "prediction": "1. Machine learning (Bayesian optimization) is used to determine the optimal conditions for thin film fabrication.",
        "reference": "Figure 2. Schematic illustration of a machine-learning-integrated closed-loop process to optimize thin film fabrication parameters [13]"
    },
    "2107.00710v1-Figure1-1.png": {
        "prediction": "1. Short Interval Input Example: 11 x 240",
        "reference": "Figure 1. (A) The network\u2019s architecture employed for Mood-State Bipolar classification using short intervals containing 2830 learnable parameters. In this figure, Ri refers to the ith residual block (i \u2208 {1, 2, 3}), while Bj refers to the jth InceptionTime block (j \u2208 {1, 2, 3}). Conv refers to a convolutional layer and GAP refers to the Global Average Pooling operation. Finally, the plus signs refer to an element-wise summation. (B) Short-Long Network\u2019s architecture using 5689 parameters. The features from the Short Network corresponds to the output of the global average pooling operation in the Short Network."
    },
    "1369182-Figure1-1.png": {
        "prediction": "1. Observation (O) is fed into the master action (MA) module, which outputs the master action (MA) and the action (A) probabilities. 2. The action (A) probabilities are used to select the action (A) to be taken. 3. The selected action (A) is then executed.",
        "reference": "Figure 1: Structure of a hierarchical sub-policy agent. \u03b8 represents the master policy, which selects a sub-policy to be active. In the diagram, \u03c63 is the active sub-policy, and actions are taken according to its output."
    },
    "2107.09510v1-Figure2-1.png": {
        "prediction": "1. Self-Attention Block: This block takes in the input sequence and applies self-attention to it. The self-attention mechanism allows the model to focus on different parts of the input sequence at different times. The output of the self-attention block is a sequence of vectors, each representing a different part of the input sequence.",
        "reference": "Fig. 2. The structure of the self-attention network (SAN) in this study. X is the input sequential physiological data. Four layers including one multi-head attention layer, one dense layer with two add & normalization layers form a self-attention block. After positional encoding, the model passes input data through 3 self-attention blocks and outputs the stress estimation result."
    },
    "2103.10312v2-Figure2-1.png": {
        "prediction": "1. Pre-trained ResNet-18 (ImageNet) as the feature extractor.",
        "reference": "Figure 2. The Deep Autofocus architecture for a mini-batch size of one (recall our mini-batch size is thirty-two during training). The network inputs a single-look complex (SLC) image, ge, which is dynamic range compressed (DRC) and passed into a feature extraction network (blue) along with the SLC\u2019s phase. The features are then fed to a regression network (green) which outputs the phase corruption model parameters, which in this case are the coefficients of ten-degree polynomial modeling the phase error. The phase error is then applied in the k-space domain, computed by the fast Fourier transform (FFT), of the SLC and then inverse Fourier transformed back to the spatial domain. Finally, the relative improvement in sharpness between the input and output magnitude images is measured and weights are backpropagated to minimize this quantity (recall minimization of this term equates to maximisation of relative image sharpness, see Eq 8). Our formulation is end-to-end differentiable and trained on a graphics processing unit (GPU). During deployment, only a single forward pass is needed to compute g\u0302 which is represented by the red path."
    },
    "204955470-Figure1-1.png": {
        "prediction": "1. Video Level Classifier: This classifier takes in a video and outputs a topic score for each of the top 20 topics. The topic scores are used to determine the overall topic of the video.",
        "reference": "Figure 1. Solution overview."
    },
    "204955470-Figure6-1.png": {
        "prediction": "1. The model is trained on a large amount of data.",
        "reference": "Figure 6. Overview of a mixture of 3 NeXtVLAD models(MixNeXtVLAD) with online knowledge distillation. The orange arrows indicate the distillation of knowledge from the mixture prediction to the predictions of sub-models."
    },
    "12938495-Figure1-1.png": {
        "prediction": "1. The embedding model is used to embed the question and the answer. 2. The embedding matrix W is used to encode the question and the answer. 3. The embedding matrix W is used to encode the subgraph of the question. 4. The embedding matrix W is used to encode the subgraph of the answer. 5. The embedding matrix W is used to encode the subgraph of the question and the answer. 6. The embedding matrix W is used to encode the subgraph of the question and the answer. 7. The embedding matrix W is used to encode the subgraph of the question and the answer. 8. The embedding matrix W is used to encode the subgraph of the question and the answer. 9. The embedding matrix W is used to encode the subgraph of the question and the answer. 10. The embedding matrix W is used to encode the subgraph of the question and the answer. 11. The embedding matrix W is used to encode the subgraph of the question and the answer. 12. The embedding matrix W is used to encode the subgraph of the question and the answer. 13. The embedding matrix W is used to encode the subgraph of the question and the answer. 14. The embedding matrix W is used to encode the subgraph of the question and the answer. 15. The embedding matrix W is used to encode the subgraph of the question and the answer. 16. The embedding matrix W is used to encode the subgraph of the question and the answer. 17. The embedding matrix W is used to encode the subgraph of the question and the answer. 18. The embedding matrix W is used to encode the subgraph of the question and the answer. 19. The embedding matrix W is used to encode the subgraph of the question and the answer. 20. The embedding matrix W is used to encode the subgraph of the question and the answer. 21. The embedding matrix W is used to encode the subgraph of the question and the answer. 22. The embedding matrix W is used to encode the subgraph of the question and the answer. 23. The embedding matrix W is used to encode the subgraph of the question and the answer. 24. The embedding matrix W is used to encode the subgraph of the question and",
        "reference": "Fig. 1. Illustration of the subgraph embedding model scoring a candidate answer: (i) locate entity in the question; (ii) compute path from entity to answer; (iii) represent answer as path plus all connected entities to the answer (the subgraph); (iv) embed both the question and the answer subgraph separately using the learnt embedding vectors, and score the match via their dot product."
    },
    "2202.06988v2-Figure22-1.png": {
        "prediction": "1. Figure 1: Differentiable Neural Architecture Search",
        "reference": "Figure 22: Visualisation of gradient back-propagation, comparing differentiable and supervised setups; displayed is a 3-step setup; the loss gradients from the last step are propagated through all previous steps and towards all previous network outputs; if the back-propagation is split into subranges, the gradients of the simulation state are set to zero, visualised by \u2019\\\u2019"
    },
    "2102.06744v1-Figure1-1.png": {
        "prediction": "1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.",
        "reference": "Fig. 1. Neural classifier model"
    },
    "2206.05406v1-Figure2-1.png": {
        "prediction": "1. Normal clients connect to the server and send their data to the server. 2. The server aggregates the data from the clients and sends it to the training client. 3. The training client trains a model on the aggregated data. 4. The training client sends the trained model back to the server. 5. The server sends the trained model to the free-rider client. 6. The free-rider client sends the trained model to the clients. 7. The clients use the trained model to make predictions. 8. The server aggregates the predictions from the clients and sends them back to the training client. 9. The training client updates the model based on the aggregated predictions. 10. The training client sends the updated model back to the server. 11. The server sends the updated model to the free-rider client. 12. The free-rider client sends the updated model to the clients. 13. The clients use the updated model to make predictions. 14. The server aggregates the predictions from the clients and sends them back to the training client. 15. The training client updates the model based on the aggregated predictions. 16. The training client sends the updated model back to the server. 17. The server sends the updated model to the free-rider client. 18. The free-rider client sends the updated model to the clients. 19. The clients use the updated model to make predictions. 20. The server aggregates the predictions from the clients and sends them back to the training client. 21. The training client updates the model based on the aggregated predictions. 22. The training client sends the updated model back to the server. 23. The server sends the updated model to the free-rider client. 24. The free-rider client sends the updated model to the clients. 25. The clients use the updated model to make predictions. 26. The server aggregates the predictions from the clients and sends them back to the training client. 27. The training client updates the model based on the aggregated predictions. 28. The training client sends the updated model back to the server. 29. The server sends the updated model to the free-rider client. 30. The free-rider client sends the updated model to the clients. 31",
        "reference": "Figure 2: Illustration of a free-rider attack. The free-rider does not perform normal training, but transmits fake model updates to the server by adding opportune stochastic perturbations \u03b5 based on Gaussian noise N(0, \u03c3). Finally, the global model issued by the server will be distributed to the free-rider."
    },
    "2101.11685v2-Figure3-4-1.png": {
        "prediction": "1. Product memory pipeline",
        "reference": "Figure 3-4: The overview of the modified squeeze-and-excitation block augmented with the memory layer. FGB is the function of global pooling which reduces the dimension of 3 dimensional feature map to the signle dimension, and Fadd is channel-wise addition of a resulting vector from memory to the original feature tensor."
    },
    "214727773-Figure1-1.png": {
        "prediction": "1. The first row shows the original data distribution, which is a mixture of two Gaussians. The second row shows the data distribution after the first layer of the network, which is a mixture of two Gaussians. The third row shows the data distribution after the second layer of the network, which is a mixture of two Gaussians. The fourth row shows the data distribution after the third layer of the network, which is a mixture of two Gaussians. The fifth row shows the data distribution after the fourth layer of the network, which is a mixture of two Gaussians. The sixth row shows the data distribution after the fifth layer of the network, which is a mixture of two Gaussians. The seventh row shows the data distribution after the sixth layer of the network, which is a mixture of two Gaussians. The eighth row shows the data distribution after the seventh layer of the network, which is a mixture of two Gaussians. The ninth row shows the data distribution after the eighth layer of the network, which is a mixture of two Gaussians. The tenth row shows the data distribution after the ninth layer of the network, which is a mixture of two Gaussians. The eleventh row shows the data distribution after the tenth layer of the network, which is a mixture of two Gaussians. The twelfth row shows the data distribution after the eleventh layer of the network, which is a mixture of two Gaussians. The thirteenth row shows the data distribution after the twelfth layer of the network, which is a mixture of two Gaussians. The fourteenth row shows the data distribution after the thirteenth layer of the network, which is a mixture of two Gaussians. The fifteenth row shows the data distribution after the fourteenth layer of the network, which is a mixture of two Gaussians. The sixteenth row shows the data distribution after the fifteenth layer of the network, which is a mixture of two Gaussians. The seventeenth row shows the data distribution after the sixteenth layer of the network, which is a mixture of two Gaussians. The eighteenth row shows the data distribution after the seventeenth layer of the network, which is a mixture of two Gaussians. The nineteenth row shows the data distribution after the eighteenth layer of the network, which is a mixture of two Gaussians. The twentieth row shows the data distribution after the nineteenth layer of the network, which is a mixture of two Gaussians. The twenty-first row shows the",
        "reference": "Figure 1. Illustration of LGKD, t(x,\u03b8) (left) and LTKD, t(x,\u03b8) (right)"
    },
    "2202.13121v1-Figure14-1.png": {
        "prediction": "1. Feature extraction: The CNN model extracts features from the source domain data.",
        "reference": "Figure 14: An approach to achieve generalization."
    },
    "2011.07233v2-Figure3-1.png": {
        "prediction": "1. The diagram shows the forces acting on a ball in a pendulum. The forces are labeled as follows:\n* F1: The force of gravity acting on the ball\n* F2: The force of friction between the ball and the pendulum rod\n* F3: The force of friction between the ball and the pivot point\n* F4: The force of friction between the ball and the pivot point\n* F5: The force of friction between the ball and the pivot point\n* F6: The force of friction between the ball and the pivot point\n* F7: The force of friction between the ball and the pivot point\n* F8: The force of friction between the ball and the pivot point\n* F9: The force of friction between the ball and the pivot point\n* F10: The force of friction between the ball and the pivot point\n* F11: The force of friction between the ball and the pivot point\n* F12: The force of friction between the ball and the pivot point\n* F13: The force of friction between the ball and the pivot point\n* F14: The force of friction between the ball and the pivot point\n* F15: The force of friction between the ball and the pivot point\n* F16: The force of friction between the ball and the pivot point\n* F17: The force of friction between the ball and the pivot point\n* F18: The force of friction between the ball and the pivot point\n* F19: The force of friction between the ball and the pivot point\n* F20: The force of friction between the ball and the pivot point\n* F21: The force of friction between the ball and the pivot point\n* F22: The force of friction between the ball and the pivot point\n* F23: The force of friction between the ball and the pivot point\n* F24: The force of friction between the ball and the pivot point\n* F25: The force of friction between the ball and the pivot point\n* F26: The force of friction between the ball and the pivot point\n* F27: The force of friction between the ball and the pivot point\n* F28: The force of friction between the ball and the pivot point\n* F29: The force of friction between the ball",
        "reference": "Figure 3: On-surface aggregation. A 3D point x on the geometric scaffold \u0393 is seen in a set of source images. Each such image contributes a feature vector fk along a ray vk (green). On-surface aggregation uses a differentiable set network to process this data and produces a feature vector g for the target ray u (red)."
    },
    "2205.08180v1-Figure3-1.png": {
        "prediction": "1. Pre-trained XLS-R Encoder (Frame Level Pooling) - This is the first layer of the model. It takes the pre-trained XLS-R encoder as input and performs frame level pooling to generate a fixed-length sequence of features.",
        "reference": "Fig. 3: An illustration of the multimodal training framework"
    },
    "2010.03990v1-Figure5-1.png": {
        "prediction": "1. Deep Convolutional Neural Network (VGG-16) is used to extract features from the input image. 2. Region Proposal Network (RPN) generates bounding boxes around the object of interest. 3. Regions are filtered using Non-Maximum Suppression (NMS) to reduce the number of redundant bounding boxes. 4. Region of Interest (ROI) pooling is used to extract features from the selected regions. 5. A classifier is used to classify the object of interest, and a regressor is used to estimate the bounding box coordinates. 6. The object score is used to rank the bounding boxes based on their likelihood of containing the object of interest.",
        "reference": "Fig. 5: Architecture of FRCNN [33]"
    },
    "2010.03990v1-Figure6-1.png": {
        "prediction": "1. The base network (VGG-16) is used to extract features from the input image. 2. The predicted class scores and coordinates of each default box are calculated using the base network's features. 3. The non-maximum suppression algorithm is applied to the predicted class scores and coordinates to determine the final predicted class scores and coordinates.",
        "reference": "Fig. 6: Architecture of SSD [21]"
    },
    "2010.03990v1-Figure7-1.png": {
        "prediction": "1. The input image is passed through a convolutional layer with 32 filters and a kernel size of 3x3. The output of this layer is passed through a ReLU activation function.",
        "reference": "Fig. 7: Architecture of UESegNet-1"
    },
    "2202.07376v1-Figure3-1.png": {
        "prediction": "1. The input image is resized to 2D matrices. 2. Rearrange the matrices in sets of 2D matrices. 3. Rearrange the matrices in sets of 2D matrices. 4. Max-pooling is applied to the matrices. 5. Flatten the matrices. 6. Convolutional layers are applied to the flattened matrices. 7. Max-pooling is applied to the flattened matrices. 8. Flatten the matrices. 9. Convolutional layers are applied to the flattened matrices. 10. Max-pooling is applied to the flattened matrices. 11. Flatten the matrices. 12. Convolutional layers are applied to the flattened matrices. 13. Max-pooling is applied to the flattened matrices. 14. Flatten the matrices. 15. Convolutional layers are applied to the flattened matrices. 16. Max-pooling is applied to the flattened matrices. 17. Flatten the matrices. 18. Convolutional layers are applied to the flattened matrices. 19. Max-pooling is applied to the flattened matrices. 20. Flatten the matrices. 21. Convolutional layers are applied to the flattened matrices. 22. Max-pooling is applied to the flattened matrices. 23. Flatten the matrices. 24. Convolutional layers are applied to the flattened matrices. 25. Max-pooling is applied to the flattened matrices. 26. Flatten the matrices. 27. Convolutional layers are applied to the flattened matrices. 28. Max-pooling is applied to the flattened matrices. 29. Flatten the matrices. 30. Convolutional layers are applied to the flattened matrices. 31. Max-pooling is applied to the flattened matrices. 32. Flatten the matrices. 33. Convolutional layers are applied to the flattened matrices. 34. Max-pooling is applied to the flattened matrices. 35. Flatten the matrices. 36. Convolutional layers are applied to the flattened matrices. 37. Max-pooling is applied to the flattened matrices. 38. Flatten the matrices. 39. Convolutional layers are applied to the flattened matrices. 40. Max-pooling",
        "reference": "Figure 3: Our proposed end-to-end QPP model comprising a Siamese network of shared parameters of layered convolutional feature extraction, followed by either i) merge (concatenation) and a fully connected (FC) layer with a Sigmoid loss for pairwise testing (Equation 7) yielding a binary comparison indicator between a pair, or ii) a linear activation layer with pairwise hinge loss for pointwise testing yielding a score for a given query (Equation 8). Since the interaction for MDMQ and SDSQ are matrices with a single row only, the two layers of convolution filter sizes for these approaches are 1 \u00d7 5 and 1 \u00d7 3 (see Section 2.4)."
    },
    "2205.02090v1-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed model for discourse dependency prediction. The model consists of two main components: a sentence encoder and a discourse dependency predictor. The sentence encoder takes a sentence as input and outputs a vector representation of the sentence. The discourse dependency predictor takes the sentence representation and the discourse dependency as input and outputs a vector representation of the discourse dependency. The vector representations are then used to predict the discourse dependency.",
        "reference": "Figure 2: An overview of our model. Intra-sentential dependencies are discovered first and inter-sentential dependencies are constructed after that to form a complete dependency tree."
    },
    "2102.00713v1-Figure3-1.png": {
        "prediction": "1. Convolutional Neural Network (CNN) - A type of deep learning model that is commonly used for image recognition and classification tasks. It consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers use filters to extract features from the input image, while the pooling layers reduce the size of the feature maps to reduce the number of parameters. The fully connected layers are used for classification. CNNs are commonly used in computer vision tasks such as object detection, image segmentation, and image recognition.",
        "reference": "Figure 3: The architecture details of the proposed multi-task CNN. Here n denotes the number of output feature maps."
    },
    "1505637-Figure4-1.png": {
        "prediction": "1. The target kernel is a kernel function that is used to measure the similarity between two data points. It is typically used in machine learning and pattern recognition tasks.",
        "reference": "Figure 4: Method to select the kernel height kh. We project the receptive field of the target kernel to equirectangular projection Ie and increase kh until it is taller than the target kernel in Ie. The kernel width kw is determined using the same procedure after kh is set. We restrict the kernel size kw \u00d7 kh by an upper bound Uk."
    },
    "2011.03197v1-Figure2-1.png": {
        "prediction": "1. The first layer of the network has three neurons, each with a single input. The inputs are the values of the variables x1, x2, and x3. The neurons in the first layer are connected to the neurons in the second layer, which have two inputs each. The connections between the neurons in the first and second layers are represented by the weights w11, w12, and w13. The second layer has three neurons, each with two inputs. The inputs are the values of the variables x1, x2, and x3, and the weights w11, w12, and w13. The neurons in the second layer are connected to the neurons in the third layer, which have two inputs each. The connections between the neurons in the second and third layers are represented by the weights w21, w22, and w23. The third layer has three neurons, each with two inputs. The inputs are the values of the variables x1, x2, and x3, and the weights w21, w22, and w23. The neurons in the third layer are connected to the neurons in the fourth layer, which have two inputs each. The connections between the neurons in the third and fourth layers are represented by the weights w31, w32, and w33. The fourth layer has three neurons, each with two inputs. The inputs are the values of the variables x1, x2, and x3, and the weights w31, w32, and w33. The neurons in the fourth layer are connected to the neurons in the fifth layer, which have two inputs each. The connections between the neurons in the fourth and fifth layers are represented by the weights w41, w42, and w43. The fifth layer has three neurons, each with two inputs. The inputs are the values of the variables x1, x2, and x3, and the weights w41, w42, and w43. The neurons in the fifth layer are connected to the neurons in the sixth layer, which have two inputs each. The connections between the neurons in the fifth and sixth layers are represented by the weights w51, w52, and w53. The sixth layer has three neurons, each with two inputs. The inputs are the values of the variables x1, x2",
        "reference": "Figure 2: Reliability block diagram of series-parallel system."
    },
    "2011.03307v2-Figure11-1.png": {
        "prediction": "1. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black hole. The X-ray corona is emitted from the accretion disk, which is located at the center of the black",
        "reference": "Fig. 11. Illustration of the considered scenario. Above a spinning black hole, X-rays are emitted isotropically. Because of the compact corona very close to the black hole, the majority of the photons either hit the accretion disc or fall into the black hole. From the high- to the low-flux state, a partial coverer is obscuring and increasing part of the emitted X-ray radiation."
    },
    "2010.09125v2-Figure1-1.png": {
        "prediction": "1. The input image is passed through a Fast Annotation network, which generates a set of annotations for the image. 2. The annotations are passed through a Disentangling Mapping network, which learns a mapping between the annotations and the image features. 3. The image features are passed through an Inverse Graphics Network, which generates a stylized image. 4. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 5. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 6. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 7. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 8. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 9. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 10. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 11. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 12. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 13. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 14. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 15. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 16. The stylized image is passed through a Multi-view data over Vocabulary network, which learns a mapping between the stylized image and the input image. 17. The",
        "reference": "Figure 1: We employ two \u201crenderers\u201d: a GAN (StyleGAN in our work), and a differentiable graphics renderer (DIB-R in our work). We exploit StyleGAN as a synthetic data generator, and we label this data extremely efficiently. This \u201cdataset\u201d is used to train an inverse graphics network that predicts 3D properties from images. We use this network to disentangle StyleGAN\u2019s latent code through a carefully designed mapping network."
    },
    "2102.06271v1-Figure5-1.png": {
        "prediction": "1. The model is trained on the original data (X1, Y1) and the intervened data (X1, T). 2. The model is then used to predict the intervened data (X2, T). 3. The model is then used to predict the original data (X2, Y2) based on the predicted intervened data (X2, T). 4. The model is then used to predict the original data (X3, Y3) based on the predicted original data (X3, Y3). 5. The model is then used to predict the original data (X4, Y4) based on the predicted original data (X4, Y4). 6. The model is then used to predict the original data (X5, Y5) based on the predicted original data (X5, Y5). 7. The model is then used to predict the original data (X6, Y6) based on the predicted original data (X6, Y6). 8. The model is then used to predict the original data (X7, Y7) based on the predicted original data (X7, Y7). 9. The model is then used to predict the original data (X8, Y8) based on the predicted original data (X8, Y8). 10. The model is then used to predict the original data (X9, Y9) based on the predicted original data (X9, Y9). 11. The model is then used to predict the original data (X10, Y10) based on the predicted original data (X10, Y10). 12. The model is then used to predict the original data (X11, Y11) based on the predicted original data (X11, Y11). 13. The model is then used to predict the original data (X12, Y12) based on the predicted original data (X12, Y12). 14. The model is then used to predict the original data (X13, Y13) based on the predicted original data (X13, Y13). 15. The model is then used to predict the original data (X14, Y14) based on the predicted original data (X14, Y14).",
        "reference": "Figure 5. Schematic demonstrating calculation of NCI ."
    },
    "212029-Figure2-1.png": {
        "prediction": "1. The image is fed into the network, which extracts features at different scales. 2. The features are then passed through a residual neck, which combines the features from different scales. 3. The residual neck is followed by a sliding window activation module, which slides the window over the residual neck and generates a new feature map. 4. The new feature map is then passed through a downscale module, which downscales the feature map. 5. The downscaled feature map is then passed through a neck module, which combines the features from different scales. 6. The neck module is followed by a sliding window activation module, which slides the window over the neck and generates a new feature map. 7. The new feature map is then passed through a downscale module, which downscales the feature map. 8. The downscaled feature map is then passed through a neck module, which combines the features from different scales. 9. The neck module is followed by a sliding window activation module, which slides the window over the neck and generates a new feature map. 10. The new feature map is then passed through a downscale module, which downscales the feature map. 11. The downscaled feature map is then passed through a neck module, which combines the features from different scales. 12. The neck module is followed by a sliding window activation module, which slides the window over the neck and generates a new feature map. 13. The new feature map is then passed through a downscale module, which downscales the feature map. 14. The downscaled feature map is then passed through a neck module, which combines the features from different scales. 15. The neck module is followed by a sliding window activation module, which slides the window over the neck and generates a new feature map. 16. The new feature map is then passed through a downscale module, which downscales the feature map. 17. The downscaled feature map is then passed through a neck module, which combines the features from different scales. 18. The neck module is followed by a sliding window activation module, which slides the window over the neck and generates a new feature map. 19. The new feature map is then passed through a downscale module, which downscales the feature map. 20. The downscaled feature map is then passed through a neck module, which combines",
        "reference": "Figure 2. An overview of the proposed FastMask architecture."
    },
    "174869-Figure2-1.png": {
        "prediction": "1. Importance evaluation: The importance of each node is calculated based on the contribution of the node to the overall model performance. The importance of each node is represented as a vector of importance values.",
        "reference": "Figure 2. Illustration of \u201cSparse Shrink\u201d algorithm. We evaluate the importance factor of each channel of feature maps f `, and prune the least important channels (dashed box). The pruning operation involves removing corresponding channels in W ` (dashed line), and modifying convolutional kernel W ` (blue line)."
    },
    "2103.05900v1-Figure5-1.png": {
        "prediction": "1. Directed graph and its topology",
        "reference": "Fig. 5: Overview of the DPN model for diagram classification."
    },
    "2107.03491v1-Figure5-1.png": {
        "prediction": "1. The user starts by specifying the rotation matrix (RM) and the number of iterations (N). 2. The algorithm then performs a semi-coherent RM search by generating a set of coherent rotations (RM = N x RM) and a set of coherent rotations (RM = 0). 3. The algorithm then performs a coherent rotation (RM = N x RM) and a coherent rotation (RM = 0) to synthesize the desired rotation matrix (RM = (RM + RM)M).",
        "reference": "Figure 5. A schematic of the semi-coherent RM search routine that offsets the deleterious effects of intra-channel depolarization by coherently rotating to discrete trial RMs and applying RM-synthesis. The separation between trial values, \u03b4RM \u223c 700 rad m\u22122, ensures that no more than 10% depolarization can occur across the semi-coherent search range. This amounts to several thousand coherent de-rotation operations to search out to RM amplitudes as large as 106 radm\u22122."
    },
    "2011.09318v2-Figure1-1.png": {
        "prediction": "1. Cryptocurrency Transaction Network Analysis",
        "reference": "Fig. 1: The outline of cryptocurrency transaction network analysis methods."
    },
    "1294630-Figure2-1.png": {
        "prediction": "1. Softmax function: Computes the softmax function for each word in the input sequence. The softmax function is a function that maps a vector of real numbers to a vector of non-negative real numbers, where the sum of the elements in the vector is equal to 1. The softmax function is used to compute the probability of each word in the input sequence being the next word in the sequence.",
        "reference": "Figure 2: Architecture of our neural EL system. The input to the system are: a document D containing the query mention m and the corresponding Wikipedia candidate link li \u2208 L, where L is the set of all possible links extracted from the fast match step described in Section ."
    },
    "2107.03234v5-Figure5-1.png": {
        "prediction": "1. The figure shows a train track with two platforms, two tracks, and two stations. The train track is divided into two sections, one for each platform. The train track is also divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train track is further divided into two sections, one for each platform. The train track is further divided into two sections, one for each track. The train",
        "reference": "Figure 5: The demonstrative model."
    },
    "2011.06150v1-Figure1-1.png": {
        "prediction": "1. The figure shows a 4x4 grid of tasks, where each task is represented by a 4-tuple of integers. The tasks are arranged in two schedules, S1 and S2, where each schedule is a 4x4 grid of tasks. The figure also shows the 4-tuple of integers that represents the task in the middle of the grid, which is the task that is being scheduled. The figure also shows the 4-tuple of integers that represents the task in the bottom right corner of the grid, which is the task that is being scheduled next.",
        "reference": "Figure 1 An illustration of an application of Algorithm 1. Let the set of cliques be given by"
    },
    "2207.10397v1-Figure1-1.png": {
        "prediction": "1. Dual Execution Agreement: A mechanism that allows two execution environments to agree on the order in which they execute code. This is achieved by having each execution environment execute a portion of the code and then comparing the results to ensure they are consistent.",
        "reference": "Figure 1: The illustration of CODET. Both the code solutions and the test cases are generated by the pre-trained language model. The best code solution is then selected by a dual execution agreement."
    },
    "2110.08422v1-Figure1-1.png": {
        "prediction": "1. Content publisher creates content and publishes it on the blockchain.",
        "reference": "Fig. 1. System and adversary model. A client in the censored region cannot directly access sensitive services and news. Instead, content publishers embed content on commercial blockchains; clients access data with privacy and anonymity through standard Satoshi clients. The adversary can censor select client communications, monitor communications and perform integrity attacks."
    },
    "2011.01713v2-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of a CNN model for image classification. The model has 8 convolutional layers, 2 fully connected layers, and 2 output layers. The input layer has 32 channels, and the output layer has 10 classes. The model uses batch normalization, dropout, and ReLU activation functions. The model is trained using the Adam optimizer and a learning rate of 0.001. The model is evaluated on the CIFAR-10 dataset, and the accuracy is 92.7%.",
        "reference": "Fig. 2. Data-path schematic view of the accelerator core and its embedding into an SoC-level system. The diagram shows the unrolled compute architecture and encoding/decoding blocks, as well as the weight and feature map memories and tile buffer module. The dataflow of the accelerator is scheduled to first buffer full feature map windows in the tilebuffer and then compute the convolution result with pre-loaded weights in the compute units after which they are saved back to the feature map memory."
    },
    "2010.15032v3-Figure1-1.png": {
        "prediction": "1. The invocation controller receives invocations. 2. The invocation controller distributes invocations to the function instances. 3. The scale controller creates and removes instances of the function. 4. The invocation controller invokes the function instances.",
        "reference": "Figure 1: Abstract FaaS architecture."
    },
    "2010.09258v1-Figure3-1.png": {
        "prediction": "1. The data partitioning strategy is applied to the data. 2. The data partitioning strategy is applied to the model. 3. The data partitioning strategy is applied to the parameter server. 4. The data partitioning strategy is applied to the data manager. 5. The data partitioning strategy is applied to the sub-dataset. 6. The data partitioning strategy is applied to the sub-model. 7. The data partitioning strategy is applied to the parameter server. 8. The data partitioning strategy is applied to the data manager. 9. The data partitioning strategy is applied to the sub-dataset. 10. The data partitioning strategy is applied to the sub-model. 11. The data partitioning strategy is applied to the parameter server. 12. The data partitioning strategy is applied to the data manager. 13. The data partitioning strategy is applied to the sub-dataset. 14. The data partitioning strategy is applied to the sub-model. 15. The data partitioning strategy is applied to the parameter server. 16. The data partitioning strategy is applied to the data manager. 17. The data partitioning strategy is applied to the sub-dataset. 18. The data partitioning strategy is applied to the sub-model. 19. The data partitioning strategy is applied to the parameter server. 20. The data partitioning strategy is applied to the data manager. 21. The data partitioning strategy is applied to the sub-dataset. 22. The data partitioning strategy is applied to the sub-model. 23. The data partitioning strategy is applied to the parameter server. 24. The data partitioning strategy is applied to the data manager. 25. The data partitioning strategy is applied to the sub-dataset. 26. The data partitioning strategy is applied to the sub-model. 27. The data partitioning strategy is applied to the parameter server. 28. The data partitioning strategy is applied to the data manager. 29. The data partitioning strategy is applied to the sub-dataset. 30. The data partitioning strategy is applied to the sub-model. 31. The data partitioning strategy is applied to the parameter server. 32. The data partitioning strategy",
        "reference": "FIGURE 3 The architecture and data flow of distributed machine learning"
    },
    "1090603-Figure3-1.png": {
        "prediction": "1. Oracle DNN O: Oracle DNN O is the oracle DNN model that takes the input sentence and produces the correct output sentence. 2. Substitute Dataset Labeling: The substitute dataset is labeled by the oracle DNN O. 3. Substitute Dataset Training: The substitute dataset is used to train the substitute DNN F. 4. Substitute DNN F: The substitute DNN F is trained on the substitute dataset. 5. Jacobian-based Dataset Augmentation: The Jacobian-based dataset augmentation is applied to the substitute dataset. 6. Substitute DNN Architecture Selection: The substitute DNN architecture is selected based on the Jacobian-based dataset augmentation.",
        "reference": "Figure 3: Training of the substitute DNN F : the attacker (1) collects an initial substitute training set S0 and (2) selects an architecture F . Using oracle O\u0303, the attacker (3) labels S0 and (4) trains substitute F . After (5) Jacobian-based dataset augmentation, steps (3) through (5) are repeated for several substitute epochs \u03c1."
    },
    "2210.08554v1-Figure8-1.png": {
        "prediction": "1. Candidate Entities (N = 16, 060",
        "reference": "Figure 8: Overview of Image Wikification (visual entity linking) method in KRAMT. To recognize named visual entities in images, we use available methods such as CRAFT+CRNN, VGG-Face, and Landmark ArcFace for brands, celebrities, and landmarks respectively. Using these experts, we measure similarity against several thousands of reference entities to obtain a set of high ranking candidates. This open-set recognition approaches allow for addition or removal of any number of reference entities without a need to re-train."
    },
    "2107.06007v2-Figure13-1.png": {
        "prediction": "1. The diagram shows the basic components of a cryostat, including the HFE storage, the ballast bottle, the lead wall, and the liquid and gas lines. The ballast bottle is used to balance the weight of the cryostat and the HFE storage. The lead wall is used to provide thermal insulation and to prevent the HFE from leaking out. The liquid and gas lines are used to transport the HFE and the cryostat.",
        "reference": "Figure 13. A simplified schematic of the HFE system. The major system components are the storage dewar, ballast bottle, and inner cryostat vessel (IV)."
    },
    "2107.05307v2-Figure1-1.png": {
        "prediction": "1. The input video is first downsampled to 4K resolution using a stride of 4.",
        "reference": "Fig. 1. Overview of EGVSR network."
    },
    "2103.07052v1-Figure4-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed model for document-level semantic matching. The model consists of two main components: a document-level representation model and a document-level semantic matching model. The document-level representation model is used to compute the document-level representation of each document in the corpus. The document-level semantic matching model is used to compute the semantic similarity between the document-level representations of two documents. The model is trained using a contrastive loss function that minimizes the difference between the predicted and ground-truth document-level representations. The model is evaluated using a variety of evaluation metrics, including precision, recall, and F1 score.",
        "reference": "Figure 4: Network architecture of the DV-Projection method. Vectors \ud835\udc38\ud835\udc40\ud835\udc35, \ud835\udc3f\ud835\udc40 and \ud835\udc37\ud835\udc49 are represented using rounded rectangle shape. Fully connected layers are represented using trapezoid shape. Element-wise math operations are represented using circles."
    },
    "2107.03564v1-Figure2-1.png": {
        "prediction": "1. The model takes in a sequence of words as input and outputs a sequence of vectors. The vectors represent the probability of the words in the sequence.",
        "reference": "Figure 2: The overall architecture of ProxySR."
    },
    "2110.03090v1-Figure3-1.png": {
        "prediction": "1. The input sequence is passed through a ResNet18 network to extract features. 2. The features are passed through a 1D convolutional layer to extract temporal features. 3. The temporal features are passed through a 1D convolutional layer to extract spatial features. 4. The spatial features are passed through a 1D convolutional layer to extract temporal features. 5. The temporal features are passed through a 1D convolutional layer to extract spatial features. 6. The spatial features are passed through a 1D convolutional layer to extract temporal features. 7. The temporal features are passed through a 1D convolutional layer to extract spatial features. 8. The spatial features are passed through a 1D convolutional layer to extract temporal features. 9. The temporal features are passed through a 1D convolutional layer to extract spatial features. 10. The spatial features are passed through a 1D convolutional layer to extract temporal features. 11. The temporal features are passed through a 1D convolutional layer to extract spatial features. 12. The spatial features are passed through a 1D convolutional layer to extract temporal features. 13. The temporal features are passed through a 1D convolutional layer to extract spatial features. 14. The spatial features are passed through a 1D convolutional layer to extract temporal features. 15. The temporal features are passed through a 1D convolutional layer to extract spatial features. 16. The spatial features are passed through a 1D convolutional layer to extract temporal features. 17. The temporal features are passed through a 1D convolutional layer to extract spatial features. 18. The spatial features are passed through a 1D convolutional layer to extract temporal features. 19. The temporal features are passed through a 1D convolutional layer to extract spatial features. 20. The spatial features are passed through a 1D convolutional layer to extract temporal features. 21. The temporal features are passed through a 1D convolutional layer to extract spatial features. 22. The spatial features are passed through a 1D convolutional layer to extract temporal features. 23. The temporal features are passed through a 1D convolutional layer to extract spatial features. 24. The spatial features",
        "reference": "Fig. 3: Network architecture for the player identification model. The networks accepts a player tracklet as input. Each tracklet image is passed through a ResNet18 to obtain time ordered features F . The features F are input into three 1D convolutional blocks, each consisting of a 1D convolutional layer, batch normalization, and ReLU activation. In this figure, k and s are the kernel size and stride of convolution operation. The activations obtained from the convolutions blocks are mean-pooled and passed through a fully connected layer and a softmax layer to output the probability distribution of jersey number pjn."
    },
    "2204.04370v1-Figure1-1.png": {
        "prediction": "1. Preprocessing: The audio input is filtered and feature extraction is performed.",
        "reference": "Figure 1: QuiKo Architecture"
    },
    "2103.01209v3-Figure3-1.png": {
        "prediction": "1. Figure 1: Overview of the GANformer architecture. The GANformer architecture consists of a latent layer, a global latent layer, a residual block, a global latent layer, and a stack of segment embedding layers. The latent layer is used to generate the attention map, which is used to select the region of interest for the global latent layer. The global latent layer is used to generate the image features, which are then passed through the segment embedding layers to generate the image. The attention map is used to select the region of interest for the global latent layer. The latent layer is used to generate the attention map, which is used to select the region of interest for the global latent layer. The global latent layer is used to generate the image features, which are then passed through the segment embedding layers to generate the image. The attention map is used to select the region of interest for the global latent layer. The latent layer is used to generate the attention map, which is used to select the region of interest for the global latent layer. The global latent layer is used to generate the image features, which are then passed through the segment embedding layers to generate the image. The attention map is used to select the region of interest for the global latent layer. The latent layer is used to generate the attention map, which is used to select the region of interest for the global latent layer. The global latent layer is used to generate the image features, which are then passed through the segment embedding layers to generate the image. The attention map is used to select the region of interest for the global latent layer. The latent layer is used to generate the attention map, which is used to select the region of interest for the global latent layer. The global latent layer is used to generate the image features, which are then passed through the segment embedding layers to generate the image. The attention map is used to select the region of interest for the global latent layer. The latent layer is used to generate the attention map, which is used to select the region of interest for the global latent layer. The global latent layer is used to generate the image features, which are then passed through the segment embedding layers to generate the image. The attention map is used to select the region of interest for the global latent layer. The latent layer is used to generate the attention map, which is used to select the region of interest for the global latent layer. The global latent layer is used to generate the image features, which",
        "reference": "Figure 3. Model Overview. Left: The GANformer layer is composed of a bipartite attention operation to propagate information from the latents to the image grid, followed by convolution and upsampling. These are stacked multiple times starting from a 4\u00d74 grid and up to producing a final high-resolution image. Right: The latents and image features attend to each other to capture the scene structure. The GANformer\u2019s compositional latent space contrasts with the StyleGAN monolithic one (where a single latent modulates the whole scene uniformly)."
    },
    "2102.05956v1-Figure3-1.png": {
        "prediction": "1. The original network has 4 filters, each with 10 neurons. The input x is a 1000-dimensional vector. The output y is a 1000-dimensional vector.",
        "reference": "Fig. 3: Monte Carlo dropout performed in fully-connected and convolutional neural networks. Keeping dropout during inference creates an implicit ensemble of models. As shown it requires running the same network with different dropout masks in order to provide uncertainty estimations."
    },
    "2205.10889v1-Figure1-1.png": {
        "prediction": "1. The proposed architecture for a neuromorphic processor with a spiking neural network and a bit-wise majority logic unit. 2. The spiking neural network is composed of a set of controllers, each of which is connected to a set of input neurons. The controllers are responsible for implementing the bit-wise majority logic unit. 3. The input neurons receive input signals from the input layer and generate spikes in response to the input signals. 4. The controllers receive the input spikes from the input neurons and generate output spikes in response to the input spikes. 5. The output spikes from the controllers are used to implement the bit-wise majority logic unit. 6. The bit-wise majority logic unit is implemented using a set of input neurons, which receive the output spikes from the controllers. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 7. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 8. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 9. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 10. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 11. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 12. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 13. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 14. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 15. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 16. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 17. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 18. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 19. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 20. The output spikes from the input neurons are used to implement the bit-wise majority logic unit. 21. The output spikes from the input neurons are used to implement the bit",
        "reference": "Fig. 1: Overview of the proposed many-core wireless-enabled IMC platform. Orange encoders map to our wireless TX, while green IMCs map to our wireless-augmented IMCs. Bit-wise majority operation maps to the wireless OTA computation."
    },
    "2202.10590v1-Figure1-1.png": {
        "prediction": "1. Edge Caching: Stores the data at the edge nodes to reduce the latency and improve the network performance. 2. Routing: Determines the shortest path to route the data from the edge nodes to the user. 3. Root Server: Stores the data at the root server to provide a centralized resource for the network. 4. User Locations: Represents the location of the user in the network. 5. Network: Represents the entire network of nodes and edges.",
        "reference": "Fig. 1. System Model. A network of J caches serves file requests from a set I of user locations. Unserved requests are routed to the Root Server. Caching decisions are aided via the recommendations provided by the rec-sys."
    },
    "17589422-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed model. The model consists of two main components: a document-level convolutional neural network (DocCVM) and a sentence-level convolutional neural network (CVM). The DocCVM is a convolutional neural network that takes as input a document and outputs a document-level representation. The CVM is a convolutional neural network that takes as input a sentence and outputs a sentence-level representation. The two components are connected through a bidirectional attention mechanism, which allows the model to attend to both the document-level and sentence-level representations. The model is trained using a combination of document-level and sentence-level losses.",
        "reference": "Figure 2: Description of a parallel document-level compositional vector model (DOC). The model recursively computes semantic representations for each sentence of a document and then for the document itself, treating the sentence vectors as inputs for a second CVM."
    },
    "2103.03875v1-Figure1-1.png": {
        "prediction": "1. Input: Image of a cat climbing a wall. 2. Frozen layers: The first few layers of the network are frozen, meaning that they are not updated during training. 3. Trainable layers: The remaining layers of the network are trainable, meaning that they can be updated during training. 4. Output: The final layer of the network outputs a probability distribution over the classes, indicating the likelihood that the image contains a cat climbing a wall.",
        "reference": "Figure 1. Transfer CNN model in sandwich-shape encoding, Ls and Le are tunable parameters to determine the boundary of trainable layers"
    },
    "2012.14111v1-Figure6-1.png": {
        "prediction": "1. The device sends a request to the web server (192.168.178.29) through the router (192.168.178.1).",
        "reference": "Figure 6: Logical Diagram showing how DLP solution is deployed within the organization"
    },
    "2204.05751v2-Figure2-1.png": {
        "prediction": "1. Training-Support: He also functioned as a drama critic, allowing him free entry to Broadway and downtown shows. 2. New-Support: Youth Sing Praise performed the show at the National Shrine of Our Lady of the Snows in Belleville. 3. New-Query: The production opened on Broadway at the New Century Theatre where it ran from November. 4. Sup-Span-f.t.: Sup-Span-f.t. 5. MAML-Span-f.t.: MAML-Span-f.t. (Ours) 6. Sup-Span: Sup-Span-f.t. 7. True prediction: True prediction 8. False Prediction: False Prediction",
        "reference": "Figure 2: Case study of span detection. Sup-Span: train a span detector in the fully supervised manner on available data from all training episodes, and then directly use it for span detection. Sup-Span-f.t.: further fine-tune the model learned by Sup-Span as in the proposed approach."
    },
    "2107.04367v2-Figure4-1.png": {
        "prediction": "1. Feature Extraction Layer: This layer extracts features from the input audio clip using a convolutional neural network (CNN). The CNN is trained on a large dataset of audio clips, which are labeled with their corresponding semantic labels. The CNN learns to extract features that are relevant to the semantic labels of the audio clips.",
        "reference": "Fig. 4: Procedure of the proposed feature selection."
    },
    "2107.04367v2-Figure3-1.png": {
        "prediction": "1. The server receives the data from the clients and performs global aggregation.",
        "reference": "Fig. 3: Overview of the proposed LHD framework using heterogeneous federated learning with local adaptation."
    },
    "2107.04367v2-Figure5-1.png": {
        "prediction": "1. Convolution + ReLU: The first convolutional layer is followed by a ReLU activation function. This layer downsamples the input image by a factor of 6.",
        "reference": "Fig. 5: Neural network architecture example at the client."
    },
    "2010.06536v1-Figure5-1.png": {
        "prediction": "1. Annotation tool: A tool used to annotate the 3D model with semantic information such as building facades, footprints, and rectification. 2. Facade Parsing and Rectification: A module that extracts the facade information from the 3D model and rectifies it to match the ground truth. 3. Footprint Extrusion: A module that extracts the footprint information from the 3D model and extrudes it to match the ground truth. 4. Inverse Procedural Modeling: A module that generates the procedural model based on the extracted information. 5. 3D Mesh Generation: A module that generates the 3D mesh based on the procedural model. 6. 3D Render: A module that renders the 3D model based on the generated 3D mesh. 7. Repository: A repository that stores the generated 3D model and the associated semantic information.",
        "reference": "Figure 5. System diagram for the 3D models module."
    },
    "2107.02220v2-Figure2-1.png": {
        "prediction": "1. Feature Extraction: Extract features from the original image and the Re-ID Backbone. 2. Graph Convolution: Apply graph convolution to the global graph to obtain the Re-identification representation. 3. Re-identification: Re-identify the image using the Re-identification representation. 4. Cross-camera Graph: Update the similarity matrix for the cross-camera graph. 5. Cross-camera Graph: Apply graph convolution to the updated cross-camera graph to obtain the updated Re-identification representation. 6. Cross-camera Graph: Re-identify the image using the updated Re-identification representation.",
        "reference": "Fig. 2: The pipeline of the proposed graph convolution based re-ranking (GCR) method."
    },
    "2103.10630v1-Figure2-1.png": {
        "prediction": "1. Volume with orientation and offset (O, t, i, j, k) Euler center angles offset",
        "reference": "Fig. 2. Illustration of the forward model used for the cryo-EM MBIR method. It involves a 3D projection at a fixed orientation (with appropriate offset for the center of rotation) followed by a propagation operator that depends on the contrast transfer function (CTF) of the system. The figure shows an example of the magnitude of the Fourier transform of a typical CTF, illustrating that the CTF typically zeros out several frequency components of the projection data. While this can pose challenges for typical pre-process and reconstruct approaches, we use this model in order to perform the reconstruction."
    },
    "2101.03561v3-Figure1-1.png": {
        "prediction": "1. Proposition 6: Theorem 1",
        "reference": "Figure 1. Main Lemmas and Theorems diagram."
    },
    "2210.01191v1-Figure2-1.png": {
        "prediction": "1. The input to the model is a sequence of tokens, which are converted to vectors using a word embedding layer. 2. The vectors are then passed through a bidirectional LSTM layer, which extracts contextual information from the sequence. 3. The output of the LSTM layer is passed through a fully connected layer, which produces a probability distribution over the possible labels. 4. The loss function is computed by comparing the predicted labels to the true labels. 5. The model is trained using backpropagation, which updates the weights of the network to minimize the loss. 6. The model is then used to make predictions on new sequences of tokens.",
        "reference": "Figure 2: Overview of the proposed end-to-end architecture, centered around the MAC-X Network: On the left, the question (Q), visual frames (V ), dialogue transcript (T ), acoustic input (Ac) as well as correct (A1) and incorrect (A2) answers are shown for the binary task. Their features are encoded with LSTMs, before use in MAC-X or in final classification along with last memory mp. Two identical classifiers make the predictions y1, y2 which are then used to calculate the loss in equation (7)."
    },
    "2101.08621v1-Figure3-1.png": {
        "prediction": "1. The participant is connected to the control server, which activates or deactivates interventions. 2. The participant is connected to the GPU server, which runs the face detection and head pose estimation algorithms. 3. The video stream is sent to the GPU server, which runs the face detection and head pose estimation algorithms. 4. The results are sent back to the control server, which judges if the participant is attentive or not.",
        "reference": "Figure 3: Architecture of the entire system we implemented for the second experiment."
    },
    "2205.10635v1-Figure1-1.png": {
        "prediction": "1. The original neural network has higher response time and higher accuracy.",
        "reference": "Figure 1: Overview of layer and semantic splitting strategies"
    },
    "2103.03189v1-Figure1-1.png": {
        "prediction": "1. The slit lamp is connected to a laser link, which is connected to a contact lens with a piezo transducer. The contact lens is connected to a PC via a DAQ and a CTRL/D/A. The laser link is connected to a pulsed laser with a wavelength of 523 nm. The pulsed laser is connected to a fiber coupler, which is connected to a photo diode. The photo diode is connected to an AOM, which is connected to a diaphragm. The diaphragm is connected to a PC via a DAQ and a CTRL/D/A. The AOM is connected to a 0-order and a 2-order diaphragm. The 0-order diaphragm is connected to a fiber coupler, which is connected to a photo diode. The photo diode is connected to an AOM, which is connected to a diaphragm. The diaphragm is connected to a PC via a DAQ and a CTRL/D/A. The AOM is connected to a 0-order and a 2-order diaphragm. The 0-order diaphragm is connected to a fiber coupler, which is connected to a photo diode. The photo diode is connected to an AOM, which is connected to a diaphragm. The diaphragm is connected to a PC via a DAQ and a CTRL/D/A. The AOM is connected to a 0-order and a 2-order diaphragm. The 0-order diaphragm is connected to a fiber coupler, which is connected to a photo diode. The photo diode is connected to an AOM, which is connected to a diaphragm. The diaphragm is connected to a PC via a DAQ and a CTRL/D/A. The AOM is connected to a 0-order and a 2-order diaphragm. The 0-order diaphragm is connected to a fiber coupler, which is connected to a photo diode. The photo diode is connected to an AOM, which is connected to a diaphragm. The diaphragm is connected to a PC via a DAQ and a CTRL/D/A. The AOM is connected to a 0-order and a 2-order diaphragm. The 0-order diaphragm is connected to a fiber coupler, which is connected to a photo diode. The photo diode is",
        "reference": "Fig. 1. Schematic sketch of the experimental setup."
    },
    "2101.07327v1-Figure5-1.png": {
        "prediction": "1. The system architecture of the video capture and streaming system.",
        "reference": "Fig. 5. The baseline architecture in a modern UVR system"
    },
    "2101.07327v1-Figure7-1.png": {
        "prediction": "1. The OpenUVR architecture consists of a host PC, a mobile user device, and a wireless network.",
        "reference": "Fig. 7. The OpenUVR system architecture"
    },
    "2110.08043v1-Figure14-1.png": {
        "prediction": "1. The figure shows the two-dimensional representation of the discrete Fourier transform (DFT) and its inverse. The DFT is represented as a matrix multiplication, where the input vector is multiplied by the DFT matrix. The inverse DFT is represented as a matrix multiplication, where the output vector is multiplied by the inverse DFT matrix. The DFT and inverse DFT are used in digital signal processing to perform operations such as filtering, sampling, and compression.",
        "reference": "Figure 14: Mode I (left) and Mode I+II (right) for the study of the crack path under thermal expansion and the loading process. Here, the initial damage z\u2217(x) is illustrated by the red initial crack in the figures."
    },
    "1125974-Figure2-1.png": {
        "prediction": "1. The input to the RNN is the concatenation of the MFCC features and the shape information. The MFCC features are represented as a vector of size T x F, where T is the number of frames and F is the number of MFCC features. The shape information is represented as a vector of size 1 x F. The concatenation of the two vectors is represented as a vector of size T x (F + 1).",
        "reference": "Figure 2: Model architecture of RNN."
    },
    "812092-Figure7-1.png": {
        "prediction": "1. Pretraining BERT: The BERT model is pre-trained on a large corpus of text data, such as Wikipedia, to learn representations for words and phrases. The pretraining is done using a masked language modeling task, where the model is trained to predict the masked tokens in a sentence. The masked tokens are replaced with the [MASK] token, and the model is trained to predict the correct token for each masked position. The pretraining is done using a sequence-to-sequence model, where the input is a sequence of words and the output is a sequence of masked words. The model is trained to predict the masked words based on the context of the surrounding words. The pretraining is done using a sequence-to-sequence model, where the input is a sequence of words and the output is a sequence of masked words. The model is trained to predict the masked words based on the context of the surrounding words.",
        "reference": "Fig. 7. Illustration of the decoder model. \u201c<bos>\u201d is a tag denoting the beginning of a sentence, and \u201c<eos>\u201d is the end of a sentence."
    },
    "2101.11032v5-Figure1-1.png": {
        "prediction": "2. Figure 1: The Demon and the Friend.",
        "reference": "FIG. 1. Top: A physicist with limited control of a gas in a box predicts and observes an irreversible (to her) increase of entropy in a spontaneous equilibration process. A cheeky demon with full knowledge of positions and momenta of all particles can reverse the process bringing the gas (and in principle her too) back to the original state. Bottom: Wigner\u2019s friend predicts and observes an irreversible (to her) collapse of the quantum state during a measurement process. A cheeky demon named Wigner with full knowledge of the global quantum state can reverse the process bringing system, measuring device and her back to the original state. In this note we highlight and discuss the analogies between these two setups and their significance for the debate surrounding the original Wigner\u2019s friend gedankenexperiment."
    },
    "2110.13367v1-Figure4-1.png": {
        "prediction": "1. The image contains a flowchart. Generate the description of the flowchart, reflecting the text nodes and arrows as depicted.",
        "reference": "Figure 4: Workflow of the detection (in the third step, red is the output of model and yellow cube is the final output of the method)"
    },
    "2011.06192v3-Figure5-1.png": {
        "prediction": "1. Figure 1: Overview of the proposed method for learning to control a robot with a delay in the communication channel. The robot is controlled by a controller that sends commands to the robot. The robot's response is delayed by a random amount of time. The controller learns to control the robot by imitating the robot's behavior in a simulation environment. The robot's response is delayed by a random amount of time in the simulation environment as well. The controller learns to control the robot by imitating the robot's behavior in the simulation environment. The robot's response is delayed by a random amount of time in the simulation environment as well. The controller learns to control the robot by imitating the robot's behavior in the simulation environment. The robot's response is delayed by a random amount of time in the simulation environment as well. The controller learns to control the robot by imitating the robot's behavior in the simulation environment. The robot's response is delayed by a random amount of time in the simulation environment as well. The controller learns to control the robot by imitating the robot's behavior in the simulation environment. The robot's response is delayed by a random amount of time in the simulation environment as well. The controller learns to control the robot by imitating the robot's behavior in the simulation environment. The robot's response is delayed by a random amount of time in the simulation environment as well. The controller learns to control the robot by imitating the robot's behavior in the simulation environment. The robot's response is delayed by a random amount of time in the simulation environment as well. The controller learns to control the robot by imitating the robot's behavior in the simulation environment. The robot's response is delayed by a random amount of time in the simulation environment as well. The controller learns to control the robot by imitating the robot's behavior in the simulation environment. The robot's response is delayed by a random amount of time in the simulation environment as well. The controller learns to control the robot by imitating the robot's behavior in the simulation environment. The robot's response is delayed by a random amount of time in the simulation environment as well. The controller learns to control the robot by imitating the robot's behavior in the simulation environment. The robot's response is delayed by a random amount of time in the simulation environment as well. The controller learns to control the robot by imitating the robot's behavior in the simulation environment. The robot's response is delayed by a random amount of",
        "reference": "Fig. 5. Overview of general IL and our bilateral control-based IL. In general, the delays caused during the demonstration and autonomous operation are different. Therefore, a general IL can realize only slow motion, which can ignore delays. In the bilateral control-based IL, the delays caused during the demonstration and autonomous operation are the same. Thus, in our bilateral control-based IL, fast motion with delays can be achieved."
    },
    "2107.07277v1-Figure1-1.png": {
        "prediction": "1. The figure shows the equivalent circuit of a microgrid with a DGU connected to it. The DGU is represented by a voltage source connected in series with a capacitor and an inductor. The capacitor is represented by a parallel combination of a capacitor and a resistor. The inductor is represented by a parallel combination of a capacitor and a resistor. The DGU is connected to the microgrid through a resistor. The microgrid is represented by a parallel combination of a capacitor and a resistor. The voltage source is represented by a parallel combination of a capacitor and a resistor. The DGU and the microgrid are connected in series with a resistor. The voltage source is connected in parallel with the microgrid. The DGU and the microgrid are connected in parallel with the resistor. The DGU and the microgrid are connected in parallel with the voltage source. The DGU and the microgrid are connected in parallel with the resistor. The DGU and the microgrid are connected in parallel with the voltage source. The DGU and the microgrid are connected in parallel with the resistor. The DGU and the microgrid are connected in parallel with the voltage source. The DGU and the microgrid are connected in parallel with the resistor. The DGU and the microgrid are connected in parallel with the voltage source. The DGU and the microgrid are connected in parallel with the resistor. The DGU and the microgrid are connected in parallel with the voltage source. The DGU and the microgrid are connected in parallel with the resistor. The DGU and the microgrid are connected in parallel with the voltage source. The DGU and the microgrid are connected in parallel with the resistor. The DGU and the microgrid are connected in parallel with the voltage source. The DGU and the microgrid are connected in parallel with the resistor. The DGU and the microgrid are connected in parallel with the voltage source. The DGU and the microgrid are connected in parallel with the resistor. The DGU and the microgrid are connected in parallel with the voltage source. The DGU and the microgrid are connected in parallel with the resistor. The DGU and the microgrid are connected in parallel with the voltage source. The DGU and the microgrid are connected in parallel with the resistor. The DGU and the microgrid are connected in parallel with the voltage source. The DGU and the microgrid are connected in parallel with the resistor. The",
        "reference": "Figure 1: Electric circuit representing the averaged model of a DC/DC buck converter connected to the microgrid."
    },
    "2011.08946v1-Figure8-1.png": {
        "prediction": "1. Input: User's query and the target information (e.g., gender, age, location, etc.).\n2. Input network: A neural network that takes the query as input and outputs a ranking score for each item in the target information.\n3. Target ratio: The desired proportion of items in the target information that should be ranked highly in the output.\n4. Diffusion process: A process that spreads the information from the input network to the output network. This process can be based on diffusion equations or other mathematical models.\n5. Random seed group: A group of random seeds that are used to initialize the diffusion process. These seeds are used to generate the initial ranking scores for the items in the target information.\n6. Small seed group: A group of small seeds that are used to refine the initial ranking scores generated by the random seeds. These seeds are selected based on their ability to spread the information more effectively.\n7. Deciding seeding ratio: A process that determines the proportion of small seeds that should be selected for the diffusion process. This process can be based on heuristics or other mathematical models.\n8. Scaling function: A function that scales the diffusion process based on the size of the input network. This function can be based on diffusion equations or other mathematical models.\n9. Output: The final ranking scores for the items in the target information. These scores are based on the diffusion process and the scaling function.",
        "reference": "Figure 8: The framework illustration of Disparity Seeding."
    },
    "999120-Figure2-1.png": {
        "prediction": "1. Part Generation: The model generates the part embeddings by applying ROI pooling to the input image. 2. Part Loss Computation: The part embeddings are compared with the part embeddings from the baseline network to compute the part loss. 3. Part Loss Backpropagation: The part loss is backpropagated through the baseline network to update the weights of the network. 4. Global Loss Computation: The global loss is computed by applying the global loss function to the part embeddings from the baseline network. 5. Global Loss Backpropagation: The global loss is backpropagated through the baseline network to update the weights of the network.",
        "reference": "Figure 2. Overview of part loss networks (PL-Net), which is composed of a baseline network and a part loss computation extension. \u201cGAP\u201d denotes the Global Average Pooling. Given an input image, we firstly extract its feature maps X , then compute the global loss and person part loss based on X . The person part loss is computed on K parts generated with an unsupervised method."
    },
    "2012.05825v3-Figure1-1.png": {
        "prediction": "1. Collecting data: The first step is to collect a large dataset of X-ray images and corresponding labels. This can be done by collecting data from various sources such as hospitals, medical imaging centers, and online platforms. The dataset should be diverse and representative of different types of lung diseases. The labels can be manually annotated by experts or automatically extracted from the images using computer vision techniques.",
        "reference": "Figure 1. Novelty detection is challenging since X-rays of novel diseases are remarkably similar to known conditions. The unlabeled batch of inference-time data can be used to adapt a semi-supervised novelty detection approach to emerging novel diseases."
    },
    "200059-Figure4-1.png": {
        "prediction": "1. The input image is passed through a convolutional neural network (CNN) to extract features from the image. 2. The features are then passed through a series of fully connected layers to generate a set of embeddings for each image. 3. The embeddings are then used to train a linear classifier to predict whether the image is from a near-frontal or non-frontal view.",
        "reference": "Figure 4. A genuine pair {x1,x2} that share the same identity but different pose is fed into the recognition network \u03b8r to obtain the rich embedding er1 and er2. By regularizing the self and cross reconstruction, er11 and er21, the identity and non-identity features are eventually disentangled to make the non-frontal peer ei2 to be similar to its near-frontal reference ei1."
    },
    "2207.10758v1-Figure7-1.png": {
        "prediction": "1. Continuous World: 3D point on plane (R, t). 2D point (R, t) is projected onto the 2D plane. 2D point is sampled at a time step (t). 2D pixel is the discrete representation of the 2D point. 2D pixel is projected onto the 2D plane. 3D point on plane (R, t) is projected onto the 2D plane. 2D point (R, t) is sampled at a time step (t). 2D pixel is the discrete representation of the 2D point. 2D pixel is projected onto the 2D plane.",
        "reference": "Fig. 7: Overview of Theorem 1 (Example 13.2 from [30]), which links the planarity and projective transformations in the continuous world."
    },
    "2202.04333v1-Figure7-1.png": {
        "prediction": "1. The user browses the web and interacts with the anchors. 2. The user's browsing behavior is recorded in the user browsing tree. 3. The user's demographic data is recorded in the user demographic dataset. 4. The anchors are retrieved and stored in the anchor retrieval dataset. 5. The anchors are categorized and stored in the anchor category dataset. 6. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 7. The anchors are categorized and stored in the anchor category dataset. 8. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 9. The anchors are categorized and stored in the anchor category dataset. 10. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 11. The anchors are categorized and stored in the anchor category dataset. 12. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 13. The anchors are categorized and stored in the anchor category dataset. 14. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 15. The anchors are categorized and stored in the anchor category dataset. 16. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 17. The anchors are categorized and stored in the anchor category dataset. 18. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 19. The anchors are categorized and stored in the anchor category dataset. 20. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 21. The anchors are categorized and stored in the anchor category dataset. 22. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 23. The anchors are categorized and stored in the anchor category dataset. 24. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 25. The anchors are categorized and stored in the anchor category dataset. 26. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 27. The anchors are categorized and stored in the anchor category dataset. 28. The anchors are broadcasted to the user and stored in the anchor broadcast tree. 29. The anchors are categorized and stored in the",
        "reference": "Figure 7: Online live broadcast recommender system with proposed TWINS model that partly shares similar idea with [20]. The new system lightens the TWINSmodel, and builds tree structures for user browsing histories and anchor histories in a offline fashion to save computation and latency costs for online serving."
    },
    "1358694-Figure1-1.png": {
        "prediction": "1. Input image I_in is warped using a geometric predictor p to produce warped image I_out.",
        "reference": "Figure 1: Network module of Spatial Transformers [7]. The blue arrows indicate information passing of appearance, and the purple one indicate that of geometry. The yellow 3D trapezoid denotes the geometric predictor, which contains the learnable parameters."
    },
    "2202.06670v2-Figure1-1.png": {
        "prediction": "1. Self-supervision: The model learns to predict the correct label for an image based on its features. 2. Auxiliary information: The model learns to predict the correct label for an image based on its features and the auxiliary information (e.g., the image's location in the dataset). 3. Our method: The model learns to predict the correct label for an image based on its features, auxiliary information, and the cluster construction information.",
        "reference": "Figure 1: Left: Self-supervision. Self-supervised learning (SSL) uses self-supervision (the supervision from the data itself) for learning representations. An example of self-supervision is the augmented variant of the original data. Middle: Auxiliary Information. This paper aims to leverage auxiliary information of data for weakly-supervised representation learning. We consider data attributes (e.g., binary indicators of attributes) as auxiliary information. Right: Our Weakly-supervised Contrastive Learning Method. We first construct data clusters according to auxiliary information. We argue the formed clusters can provide valuable structural information of data for learning better representations. Second, we present a contrastive learning approach - the clustering InfoNCE (Cl-InfoNCE) objective to leverage the constructed clusters."
    },
    "2012.14294v1-Figure1-1.png": {
        "prediction": "1. The blockchain network is connected to the local network, which includes the National Institute of Health, Ministry of Public Health, and the Local Network.",
        "reference": "Fig. 1. The proposed I-Health system architecture."
    },
    "2110.06875v2-Figure3-1.png": {
        "prediction": "1. The first step is to define the problem. In this case, we are trying to find the shortest path between two nodes in a graph.",
        "reference": "Fig. 3. Illustration of the possible steps performed during the iteration by HM-Improve. The edges of the current sub-allocation Y are depicted using bold, green lines, while edges of the envy graph are shown by dashed, red lines. Source and sink vertices of Y are depicted with a white black diamond, respectively. Vertices of R as well as all edges incident to them are shown in grey."
    },
    "210911552-Figure3-1.png": {
        "prediction": "1. Encoder: The encoder takes the image as input and encodes it into a representation vector. The encoder consists of a CNN module, a Place CNN module, a Caption LSTM module, and a Tag LSTM module. The CNN module extracts features from the image, the Place CNN module extracts features from the image based on the location of the object, the Caption LSTM module extracts features from the image based on the caption, and the Tag LSTM module extracts features from the image based on the tags. The representation vector is the output of the encoder.",
        "reference": "Figure 3. Multi-Cue Bayesian Moderator Network. We first use a Bayesian CNN/LSTM to obtain the embeddings gi, gp, gc, gt and then fuse those using the Fusion Module to get \u00b5p, \u00b5c, \u00b5t. These embeddings are then passed to the Moderator network. These are then fed to the decoder to get the questions for each image."
    },
    "2205.03371v1-Figure5-1.png": {
        "prediction": "1. The input sequence is split into segments. 2. Each segment is processed by a feedforward neural network (FNN) to produce a representation vector. 3. The representation vectors from all segments are concatenated and passed through a channel-wise sum function. 4. The output of the channel-wise sum function is passed through a softmax function to produce a probability distribution over the possible next words. 5. The next word is chosen based on the highest probability in the distribution.",
        "reference": "Fig. 5: Illustration on the instance representation and the generation of bag probability distribution."
    },
    "1189091-Figure3-1.png": {
        "prediction": "1. Region Proposal Network (RPN) generates proposals for object detection. 2. Region features are extracted from the proposals. 3. Detection scores are computed for each proposal. 4. The proposals with the highest detection scores are selected as the final detections. 5. The selected proposals are passed to the localization and captioning network. 6. The localization and captioning network generates bounding boxes and captions for the selected proposals.",
        "reference": "Figure 3: Our framework consists of two stages: a region detection network and a localization and captioning network."
    },
    "2101.04989v1-Figure2-1.png": {
        "prediction": "1. A: A histological image of a lung cancer sample with a high level of cellularity and a large number of tumor cells. 2. B: A histological image of a lung cancer sample with a low level of cellularity and a smaller number of tumor cells.",
        "reference": "Fig. 2. Steps in processing esophageal biopsy images to produce patches. (A) A typical image of a hemotoxylin and eosin (H&E)-stained esophageal biopsy section obtained from an individual with active EoE. The image was taken at 80X magnification. (B) The same image after background removal with an illustration of tissue coverage criteria per patch size to meet the threshold for inclusion in training or validation sets. Box 1 (red): patch of 224X224 pixels with less than 10% tissue coverage. Box 2 (yellow): patch of 224X224 pixels with greater than 10% tissue coverage. Box 3 (red): patch of 448X448 pixels with less than 10% tissue coverage. Box 4 (yellow): patch of 448X448 pixels with greater than 10% tissue coverage."
    },
    "2011.05643v1-Figure2-1.png": {
        "prediction": "1. The block diagram of the proposed high-gain low-gain (HG/LG) selector for the high-voltage (HV) transimpedance amplifier (TIA) with a high-gain mode (HG) and a low-gain mode (LG).",
        "reference": "Figure 2: Block level schematic diagram of an ASIC channel [8]."
    },
    "2012.06186v1-Figure3-1.png": {
        "prediction": "1. The figure shows the architecture of the neural network used for the task of sentiment analysis. The input is a sentence, which is first tokenized and then passed through a pre-trained language model (BERT) to obtain a representation of the sentence. The representation is then passed through a fully connected layer to obtain a final prediction of the sentiment of the sentence. The figure also shows the different layers of the neural network, including the embedding layer, the hidden layers, and the output layer.",
        "reference": "Fig. 3: CNN architecture composed of the ResNet-20 followed by the NetVLAD layer. Numbers in each rectangle denote kernel size, number of output filters, padding, and size of stride, respectively. The N -way fully connected is dropped and instead the 1\u00d7 1\u00d7 64 output vector is passed to the NetVLAD layer."
    },
    "113704-Figure2-1.png": {
        "prediction": "1. Figure 1: Overview of the neural network architecture for the joint model. The joint model consists of a predictor P and a judge J. The predictor P takes as input the sentence s and the predicted probability p of the judge J. The judge J takes as input the sentence s and the predicted probability p of the predictor P. The joint model outputs the predicted probability p of the judge J. The joint model is trained using the cross-entropy loss function.",
        "reference": "Figure 2: DAN Architecture for Text Classification"
    },
    "2102.07774v2-Figure4-1.png": {
        "prediction": "1. The event horizon of the past event horizon is the same as the event horizon of the future event horizon.",
        "reference": "Figure 4. An illustration of the approach to the limit point Clim, where CA and CB are infinitesimally close to the limiting point."
    },
    "2206.01335v2-Figure1-1.png": {
        "prediction": "1. Source code, documentation, etc. 2. Instance extraction 3. Code generation results 4. Huge code corpus 5. Large-scale training 6. Pre-trained language model 7. Raw completions 8. Post-processing",
        "reference": "Figure 1: Overview of a general framework for generating code analysis tools using few-shot, pre-trained language models."
    },
    "2210.01384v1-Figure3-1.png": {
        "prediction": "1. Edge-friendly search space: A search space that is optimized for edge inference.",
        "reference": "Figure 3: A system-level overview of our proposed methods. We leverage multi-objective, hardware-aware neural architecture search to discover optimal neural components suitable for multi-task dense predictions, while simultaneously ensuring efficient edge inference."
    },
    "2010.06791v3-Figure4-1.png": {
        "prediction": "1. The conventional decoder takes the received signal and decodes it to obtain the message.",
        "reference": "Fig. 4. Comparison of the conventional decoder and the optimal GNNDR, for channels with imperfect receiver CSI. The left block diagram is part of the channel model in Figure 1. The upper right block diagram illustrates the conventional approach for realizing the decoder, where the imperfect receiver CSI is utilized to estimate the true channel state, and the estimated channel state is fed into the NNDR for decoding; the lower right block diagram illustrates the optimal GNNDR, which directly obtains the output processing function and the codeword scaling function, without estimating the channel state. We remark that channel estimation is still a critical module in practical wireless communication systems, and that the optimal GNNDR structure bypassing the channel estimation step is only valid for the specific information transmission system model in this paper."
    },
    "2101.03244v5-Figure2-1.png": {
        "prediction": "1. The input image is processed to extract features using a convolutional neural network (CNN). 2. The features are then fed into a residual classifier, which outputs a probability score for each patch. 3. The patch scores are combined using a weighted average to produce a final probability score for the entire image. 4. The final probability score is used to classify the image as benign or malignant.",
        "reference": "Fig. 2. Proposed end-to-end framework for computing voxel-level detections of csPCa in validation/test samples of prostate bpMRI. The model center-crops two ROIs from the multi-channel concatenation of the patient\u2019s T2W, DWI and ADC scans for the input of its detection and classification 3D CNN sub-models (M1, M2). M1 leverages an anatomical prior P in its input x1 to synthesize spatial priori and generate a preliminary detection y1. M2 infers on a set of overlapping patches x2 and maps them to a set of probabilistic malignancy scores y2. Decision fusion node NDF aggregates y1, y2 to produce the model output yDF in the form of a post-processed csPCa detection map with high sensitivity and reduced false positives."
    },
    "2012.06168v4-Figure4-1.png": {
        "prediction": "1. State Representation: The state representation is a set of features that describe the current state of the game. It includes information such as the player's position, the opponent's position, and the current game state. 2. Action Information Representation: The action information representation is a set of features that describe the action that the player is taking. It includes information such as the player's action, the opponent's action, and the current game state. 3. Card Information Representation: The card information representation is a set of features that describe the card that the player is playing. It includes information such as the card's type, the card's strength, and the current game state. 4. Pseudo Siamese Architecture: The pseudo siamese architecture is a neural network that takes the state representation and the action information representation as input and outputs a representation of the action that the player is taking. 5. Training Losses: The training losses are the loss functions that are used to train the neural network. They include the policy loss, the value loss, and the action probability loss. 6. Policy Loss: The policy loss is the loss function that is used to train the neural network to predict the action that the player should take. 7. Value Loss: The value loss is the loss function that is used to train the neural network to predict the value of the card that the player is playing. 8. Action Probability Loss: The action probability loss is the loss function that is used to train the neural network to predict the probability of the action that the player should take.",
        "reference": "Fig. 4. End-to-end learning architecture of our deep RL based AI ARL."
    },
    "2204.10485v1-Figure2-1.png": {
        "prediction": "1. The architecture of the proposed patch-based feature extraction and fusion network for image restoration.",
        "reference": "Figure 2. Overview of AHIQ. The proposed model takes a pair of the reference image and distortion image as input and then obtains feature maps through ViT [11] and CNN, respectively. The feature maps of reference image from ViT are used as global information to obtain the offset map of the deformable convolution [8]. After the feature fusion module which fuses the feature maps, we use a patch-wise prediction module to predict a score for each image patch. The final output is the weighted sum of the scores."
    },
    "2102.08921v2-Figure2-1.png": {
        "prediction": "1. Evaluation pipeline: The evaluation pipeline consists of two steps: (a) generating a synthetic data stream and (b) auditing the pipeline. In step (a), the model generates a synthetic data stream by sampling from the distribution of the real data. In step (b), the model evaluates the pipeline by evaluating the embedding and checking the authenticity of the generated data. The evaluation metrics used in step (b) are the sample-level metrics and the high-quality sample metrics.",
        "reference": "Figure 2. Illustration for the evaluation and auditing pipelines."
    },
    "2010.11991v2-Figure8-1.png": {
        "prediction": "1. The point cloud processor filters the initial scans to remove noise and outliers. 2. The point cloud down-samples the filtered scans to reduce the number of points. 3. The point cloud extrap",
        "reference": "Fig. 8. Schematic of the point cloud aggregation pipeline. The input data are filtered from redundant points, downsampled to reduce the computational complexity. At the same time, the current agent\u2019s position, and the position for the previous scan is taken from history. The PointCloudExtrapolator splits the entire scan into the smaller batches, and for every batch is calculates linearly interpolated transformation that is proportional to the when the point has been scanned. Finally, all the bathes are aggregated in the PointCloudAggregator"
    },
    "2204.05928v2-Figure3-1.png": {
        "prediction": "1. The model is trained on a large amount of data from multiple domains. 2. The model learns to predict the probability distribution of the intent over the slots in each domain. 3. The model is then fine-tuned on a smaller amount of data from a new domain. 4. The model is able to predict the probability distribution of the intent over the slots in the new domain.",
        "reference": "Figure 3: Proposed action prediction in DDPT using a transformer decoder. In every decoding step, a token embedding for domain, intent or slot informs the model what needs to be predicted and the previous output is fed into the decoder. In case of domain prediction, we propose a domain gate that decides whether to choose a domain that the user currently talks about."
    },
    "2110.02775v1-Figure1-1.png": {
        "prediction": "1.1.1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ",
        "reference": "Figure 1: An example of a network for the MONK-2 dataset. xi are the inputs, y is the output. The red and blue rectangles represent the plot of functions, with input range on the x-axis and output on the y-axis. The green rectangles contain the aggregation function. The numbers in bold represent the thresholds for the step functions."
    },
    "2010.15764v1-Figure4-1.png": {
        "prediction": "1. The input to the model is a sentence, which is represented as a sequence of words. The words are converted to one-hot vectors, and the sequence is padded with zeros to make it of the same length as the longest sentence in the training data.",
        "reference": "Figure 4: The causal diagram for mixed-causal-anticausal domain adaptation."
    },
    "2202.09450v1-Figure38-1.png": {
        "prediction": "1. Mutual Information Maximization: The goal is to maximize the mutual information between the generated sentence and the attribute label. This can be achieved by minimizing the cross-entropy loss between the generated sentence and the attribute label.",
        "reference": "Fig. 38. Illustration of AttGAN extension for attribute style manipulation. Courtesy of [52]"
    },
    "2202.09450v1-Figure59-1.png": {
        "prediction": "1. Input frames: The input frames are the images that are captured by the camera. 2. Depth map: The depth map is a map that shows the distance between the camera and the objects in the scene. 3. Rigid flow: The rigid flow is a map that shows the motion of the objects in the scene. 4. Final flow: The final flow is the result of the rigid flow. 5. Consistency check: The consistency check is a process that ensures that the final flow is consistent with the depth map. 6. Rigid structure reconstructor: The rigid structure reconstructor is a network that takes the depth map and the rigid flow as input and outputs a map that shows the structure of the scene. 7. Non-rigid motion localizer: The non-rigid motion localizer is a network that takes the depth map and the rigid flow as input and outputs a map that shows the motion of the objects in the scene.",
        "reference": "Fig. 59. The overview of GeoNet Framework. It consists of rigid structure reconstructor for estimating static scene geometry and non-rigid motion localizer for capturing dynamic objects. Courtesy of [89]"
    },
    "2202.09450v1-Figure31-1.png": {
        "prediction": "1. The system design of the proposed method.",
        "reference": "Fig. 31. The high-level architecture of . Courtesy of [41]"
    },
    "2202.09450v1-Figure32-1.png": {
        "prediction": "1. The figure shows the architecture of a cycle-consistency loss function. The input image is first transformed by a generator G, which produces a transformed image. The transformed image is then compared with the original image using a cycle-consistency loss function. The cycle-consistency loss function ensures that the transformed image is similar to the original image, and that the generator G is able to produce realistic images.",
        "reference": "Fig. 32. CycleGAN model contains two mapping functions G : X \u2192 Y and F : Y \u2192 X , and associated adversarial discriminators DY and DX . DY encourages G to translate X into outputs indistinguishable from domain Y , and vice versa for DX , F, and X. To further regularize the mappings, they introduced two \u201ccycle consistency losses\u201d that capture the intuition that if they translate from one domain to the other and back again we should arrive where they started. Courtesy of [47]"
    },
    "2202.09450v1-Figure21-1.png": {
        "prediction": "1. Pre-align: Pre-align the images to align the clothing and the person.",
        "reference": "Fig. 21. Overview of M3D-VTON architecture. Courtesy of [20]"
    },
    "2202.09450v1-Figure47-1.png": {
        "prediction": "1. The figure shows the architecture of the 3D gaze estimation model. The model takes as input a video frame and estimates the gaze direction of the person in the frame. The model consists of a dual attention module, a backbone, and a heatmap regression head. The dual attention module consists of a feedforward neural network (FNN) and a residual connection. The FNN takes as input the video frame and the head position of the person, and outputs the gaze direction. The residual connection adds the output of the FNN to the input of the FNN. The backbone consists of a convolutional layer, a depth estimation network, and a depth reweighting network. The convolutional layer takes as input the video frame and the head position of the person, and outputs the depth estimation network. The depth estimation network takes as input the video frame and the head position of the person, and outputs the depth of the person. The depth reweighting network takes as input the depth estimation network output and the head position of the person, and outputs the final gaze estimation. The heatmap regression head takes as input the video frame and the gaze estimation, and outputs the heatmap regression head output. The figure also shows the training process of the model. The model is trained using the binary classification loss, the focal loss, and the focal volume loss. The focal loss is used to reduce the impact of outliers in the training data. The focal volume loss is used to reduce the impact of noise in the training data. The figure also shows the input and output of the model. The input is a video frame, and the output is the gaze estimation of the person in the frame.",
        "reference": "Fig. 47. The Architecture of Dual Attention Guided Gaze Target Detection. Courtesy of [62]"
    },
    "2102.10313v1-Figure1-1.png": {
        "prediction": "1. A 3D model of a room with a drone flying inside it. 2. A drone flying inside a room with a laser beam being projected onto the wall.",
        "reference": "Fig. 1. Left: Illustration of a typical planning run with all planners on the hilo scenario. Red is the proposed, green the DGEO, and brown the RRT*Con planner. Blue shades correspond to the RRT*-Sam, yellow shades to the RRT*-Pro, and pink to CHOMP. Right: Example of an OMAV inspecting a surface using the proposed planner."
    },
    "2207.10392v2-Figure3-1.png": {
        "prediction": "1. Overview of FADE",
        "reference": "Figure 3: Technical pipeline of FADE. From (a) the overview of FADE, feature upsampling is executed by jointly exploiting the encoder and decoder feature with two key modules. In (b) dynamic feature pre-upsampling, they are used to generate upsampling kernels using a semi-shift convolutional operator (Fig. 5). The kernels are then used to reassemble the decoder feature into pre-upsampled feature. In (c) gated feature refinement, the encoder and pre-upsampled features are modulated by a decoder-dependent gating mechanism to enhance detail delineation before generating the final upsampled feature."
    },
    "2011.07831v1-Figure1-1.png": {
        "prediction": "1. The figure shows a block diagram of a system with a feedback loop. The feedback loop is represented by the arrows from the output of the system to the input of the system. The system is also represented by the block diagram with the input and output variables labeled.",
        "reference": "Figure 1: A simplified illustration of our proposed method where \u039b refers to the write mechanism described in section 3.1.1. Ft are the recurrent weights of the FWM which have been generated by the LSTM. The LSTM is a regular slow RNN. The residual connection between the FWM and the LSTM is not depicted."
    },
    "1262057-Figure5-1.png": {
        "prediction": "1. Place recognition: The system recognizes the location of the user based on the user's current location and the user's prior knowledge of the location. 2. Action recognition: The system recognizes the action of the user based on the user's current location and the user's prior knowledge of the location. 3. Incorporating prior knowledge: The system incorporates the user's prior knowledge of the location into the place recognition and action recognition processes. 4. Predicting human motion boundaries using spatial and temporal cues: The system predicts the human motion boundaries using spatial and temporal cues. 5. Corridor network: The system uses a corridor network to detect abnormal human behavior. 6. Places 205VGG network: The system uses a Places 205VGG network to recognize places and probabilities for each action. 7. A matrix of recognized places: The system creates a matrix of recognized places based on the Places 205VGG network. 8. ConvNet: The system uses a ConvNet to detect abnormal human behavior. 9. Abnormal human behavior is detected: The system detects abnormal human behavior when the user's prior knowledge of the location is inconsistent with the user's current location or the user's current location is inconsistent with the user's prior knowledge of the location.",
        "reference": "Figure 5. Overview of our approach for unbiased human action recognition on a sample of the Guiabot robot dataset."
    },
    "2107.00638v1-Figure5-1.png": {
        "prediction": "1. The figure shows the process of a) vacuum evaporation, b) condensation, and c) phase separation of a polymer blend.",
        "reference": "Figure 5: Charge transfer (CT) due to relative orbital positions in metal-organic interfaces. Panel a) shows the situation before contact. The Fermi-level of the metal lies above the LUMO of the adsorbate. Panel b) intermediate stage (purely for illustration, this does not exist in reality). Adsorbate and substrate are in contact, adsorbate orbitals broaden IP and EA are changed due to band gap renormalization, charge is transferred to enforce a common Fermi level. Panel c) Final situation. Charge transfer is finished, result is a common Fermi level and a dipole that shifts the adsorbate levels by \ud835\udee5\ud835\udef7. Reproduced with permission from [62]."
    },
    "2205.15979v1-Figure7-1.png": {
        "prediction": "1. Short-term planning step (STPS) and long-term planning step (LTPS) are two steps in the planning process.",
        "reference": "Figure 7: Overview of the short-term and long-term planning steps for the local trajectory planning."
    },
    "2206.00991v1-Figure3-1.png": {
        "prediction": "1. Occupancy Grid Decoder: This module takes the current agent occupancy as input and predicts the occupancy of the whole scene. It uses the whole-scene features and the current agent occupancy as input. The output is the occupancy of the whole scene.",
        "reference": "Fig. 3. An overview of the StopNet architecture. The encoder processes the input point set P and produces a feature map, which is used to predict both per-agent trajectories and whole-scene occupancy grids for each agent type. Input agent boxes at t = 0 are also rendered in BEV as binary features and fed to the trajectory and occupancy grid decoders."
    },
    "1356654-Figure2-1.png": {
        "prediction": "1. Full image supervision: The model is trained on the entire image, including the background. 2. Image-level supervision: The model is trained on the segmentation map of the image, which is a binary mask that indicates the presence or absence of a specific object. 3. Point-level supervision: The model is trained on the segmentation map of a specific object, which is a binary mask that indicates the presence or absence of a specific object. 4. Objectness prior: The model is trained on the segmentation map of a specific object, but with an additional constraint that the object should be present in the image. This helps the model to focus on the presence of the object, rather than just its segmentation map.",
        "reference": "Fig. 2. (Top): Overview of our semantic segmentation training framework. (Bottom): Different levels of training supervision. For full supervision, the class of every pixel is provided. For image-level supervision, the class labels are known but their locations are not. We introduce point-level supervision, where each class is only associated with one or a few pixels, corresponding to humans pointing to objects of that class. We include an objectness prior in our training loss function to accurately infer the object extent."
    },
    "980236-Figure2-1.png": {
        "prediction": "1. Content and style targets are concatenated and passed through a content loss network (VGG-16) to produce a content loss value.",
        "reference": "Fig. 2. System overview. We train an image transformation network to transform input images into output images. We use a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process."
    },
    "204823930-Figure2-1.png": {
        "prediction": "1. Joint Evidence Reasoning: The model takes the joint evidence of the edge and the node as input and outputs the probability of the claim label. 2. Evidence Selection: The model selects the evidence based on the joint evidence and the evidence selection function. 3. Evidence Reasoning: The model uses the selected evidence to reason about the claim label. 4. Node Kernel MLP: The model uses a node kernel MLP to extract features from the node. 5. Joint Evidence Reasoning: The model takes the joint evidence of the edge and the node as input and outputs the probability of the claim label. 6. Evidence Selection: The model selects the evidence based on the joint evidence and the evidence selection function. 7. Evidence Reasoning: The model uses the selected evidence to reason about the claim label. 8. Node Kernel MLP: The model uses a node kernel MLP to extract features from the node. 9. Claim Label: The model outputs the probability of the claim label. 10. Edge Kernel MLP: The model uses an edge kernel MLP to extract features from the edge. 11. Joint Evidence Reasoning: The model takes the joint evidence of the edge and the node as input and outputs the probability of the claim label. 12. Evidence Selection: The model selects the evidence based on the joint evidence and the evidence selection function. 13. Evidence Reasoning: The model uses the selected evidence to reason about the claim label. 14. Edge Kernel MLP: The model uses an edge kernel MLP to extract features from the edge. 15. Claim Label: The model outputs the probability of the claim label. 16. Node Kernel MLP: The model uses a node kernel MLP to extract features from the node. 17. Joint Evidence Reasoning: The model takes the joint evidence of the edge and the node as input and outputs the probability of the claim label. 18. Evidence Selection: The model selects the evidence based on the joint evidence and the evidence selection function. 19. Evidence Reasoning: The model uses the selected evidence to reason about the claim label. 20. Edge Kernel MLP: The model uses an edge kernel MLP to extract features from the edge. 21. Claim Label: The model outputs the probability of the claim label. 22. Node Kernel MLP: The model uses a node kernel MLP to extract features from the node. ",
        "reference": "Figure 2: KGAT Architecture."
    },
    "336802-Figure1-1.png": {
        "prediction": "1. Input Image: The input image is the image that is being processed by the deep CNN model.",
        "reference": "Fig. 1. Overview over the proposed PHOCNet approach for Query-byExample (QbE) and Query-by-String (QbS) word spotting."
    },
    "2206.10801v3-Figure1-1.png": {
        "prediction": "1. The encoder takes the expression profile as input and encodes it into a latent representation $Z_e$. 2. The categorical latency $Z_e$ is then passed through a decoder to reconstruct the expression profile $G(Z_e)$. 3. The decoder takes the categorical latency $Z_e$ as input and reconstructs the expression profile $G(Z_e)$. 4. The decoder is trained to minimize the reconstruction error between the reconstructed expression profile $G(Z_e)$ and the original expression profile $D$.",
        "reference": "Fig. 1: Overview of the proposed system. D denotes the discriminator, G denotes the generator."
    },
    "2101.04223v2-Figure2-1.png": {
        "prediction": "1. ESN: An example of an ESN architecture with a single hidden layer.",
        "reference": "Figure 2: Single and hierarchical echo-state network (ESN) architectures. A: A single ESN with internally connected nodes with a single set of hyper-parameters \u03b1 and \u03c1. B: A hierarchical ESN composed of 2 connected reservoirs where the input is fed into reservoir 1 only and the connection is unidirectional from R1 to R2, which is identical to the 2 layer DeepESN of [27]. C: A parallel (or unconnected hierarchical) ESN where the network is partitioned into 2 reservoirs, R1 and R2, which each receive the input and provide output but have distinct hyper-parameters."
    },
    "2101.11508v8-Figure2-1.png": {
        "prediction": "1. The image input layer is the first layer in the model. It receives the image data as input.",
        "reference": "Figure 2. U-net architecture. Conv means convolution. ReLU is rectified linear unit. DepthConv is depth concatenation. UpConv means up-convolution or transposed convolution. MaxPool is Max Pooling."
    },
    "2210.09757v1-Figure4-1.png": {
        "prediction": "1. VLS observation: The VLS observation is the observation of the system at a particular time instant. It is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables at that time instant. The VLS observation is represented by the vector of the state variables",
        "reference": "Fig. 4. Illustration of drift RANSAC. Drift RANSAC randomly samples local key frames and calculate errors of each set. Then we estimates T lo \u2032 between VIO and VLS coordinates of least error among T lo,k . Finally we calculate error and weight of each VLS edge."
    },
    "2102.03117v2-Figure2-1.png": {
        "prediction": "1. Figure 1: Illustration of the different types of graphs and their corresponding classes in the matrix class M. 2. Figure 2: Illustration of the different types of graphs and their corresponding classes in the hereditary class \u03b8 of ordered graphs.",
        "reference": "Figure 2 A more detailed proof diagram."
    },
    "2010.11078v2-Figure2-1.png": {
        "prediction": "1. High-level task planner: The high-level task planner is responsible for generating the high-level control commands for the robot. It takes as input the desired task and the current state of the robot, and generates the control commands that the robot should execute to achieve the desired task. The high-level task planner can use a variety of techniques, such as planning algorithms, reinforcement learning, or model-based control.",
        "reference": "Fig. 2. A conceptual illustration of the natural match between the discrete task planner and the low-level distributed trajectory optimization. In the move action without an object in hand, velocity and dynamics constraints are enforced. When the gripper is in the push action, the optimization naturally adds a contact constraint. Note that qgoal,move = qinit,push."
    },
    "2204.06433v1-Figure3-1.png": {
        "prediction": "1. Simulation software (Robot Arm Controller, Hand Controller, and Pathfinder)",
        "reference": "Fig. 3. Implementation Diagram"
    },
    "2204.12346v1-Figure2-1.png": {
        "prediction": "1. The SIR model is a simple mathematical model that describes the spread of an infectious disease in a population. It consists of three types of individuals: susceptible (S), infected (I), and recovered (R). The model assumes that the number of susceptible individuals is equal to the number of infected individuals plus the number of recovered individuals. The model also assumes that the number of susceptible individuals is constant over time.",
        "reference": "Figure 2. A schematic representation of the SIR and the SIRD model."
    },
    "2011.12745v4-Figure3-1.png": {
        "prediction": "1. Feature extractor: Extracts features from the input sparse point cloud.",
        "reference": "Fig. 3. The flowchart of the proposed method. Given a sparse point cloud with N points, it first learns a U -dimensional feature for each point (i.e., the feature learning module) and also embeds the relative position information of K nearest neighbouring (KNN) points into the U -dimensional features (i.e., the distance encoder module). Then the two types of high-dimensional features are concatenated to regress unified and sorted interpolation weights (i.e., the weight learning module), which coarsely interpolate the input sparse point cloud into a dense one. The coarse point cloud is finally refined via a self-attention-based refinement module, which regresses an offset for each point from the corresponding feature (see Fig. 4 for the detailed network architecture). After one-time end-to-end training, the proposed method is capable of handling flexible upsampling factors not greater than the maximum factor Rmax."
    },
    "2210.06361v3-Figure2-1.png": {
        "prediction": "1. Feature Pyramid Encoder: This layer extracts features from the input image using a feature pyramid. The feature pyramid is a hierarchical representation of the image, with features at different scales and resolutions. The features are extracted using a convolutional neural network (CNN) architecture. 2. View Combiner Layer: This layer combines the features from different views of the image to generate a multi-view representation. The view combination is performed using a view fusion module, which takes the features from each view and combines them to generate a single multi-view representation. 3. Co-Attention of Multi-View Fusion: This layer uses co-attention to generate a multi-view representation that is more informative and discriminative. Co-attention is a technique that takes the multi-view representation and generates a new representation that is more relevant to the task at hand. 4. Hierarchical Channel Fusion Decoder: This layer uses a hierarchical channel fusion decoder to generate a multi-view representation that is more discriminative and informative. The channel fusion decoder takes the multi-view representation and generates a new representation that is more relevant to the task at hand. 5. Output Layer: This layer generates the final output of the model, which is a multi-view representation that is more discriminative and informative.",
        "reference": "Figure 2: Overview of our model structure. We generate multiple views (Diagonally and Vertically flipped views, Close looking views) of the data by different transformation methods. The shared pyramid feature encoder is applied to extract hierarchical features of different scales corresponding to different view choices. The view combining layer concatenates features of same level from different views (fDi , f V i , f O i , f C1 i , f C2 i ) channel-wisely and output multi-view feature tensors (mvtensors). The model feeds mv-tensors into CAMVs and obtain multi-view enhanced feature tensor (en-tensor) fi. CAMV is adopted to fuse features and aggregate vital clues between different views by a two-stage co-attention mechanism. The entensors are further decoded and the contextual correlation are exploited by hierarchical channel fusion unit simultaneously. In the end, a probability map of camouflaged object in the input image is computed by several convolutional blocks."
    },
    "2011.09340v3-Figure9-1.png": {
        "prediction": "1. The first step is to determine the parity of the number of bits in the message. In this case, the message has 4 bits, so the parity is 0.",
        "reference": "Figure 9: Process with an entanglement breaking map on at least one of its spaces. If the circuit of a process can be represented with an entanglement breaking (EB) channel on one of its wires, then the resulting comb \u03a5ABC is separable in the corresponding cut. For example, an entanglement breaking channel on the environment R implies that \u03a5ABC is separable in the splitting A : BC. If there are two entanglement breaking channels (independent of what two wires they act on), then the resulting comb is fully separable. For better tracking of the involved spaces, the input and output spaces of the EB channels are labelled differently."
    },
    "2011.03178v1-Figure1-1.png": {
        "prediction": "1. Transductive Information",
        "reference": "Figure 1: A diagram for the pipeline of (transductive) active learning."
    },
    "2101.07079v1-Figure5-1.png": {
        "prediction": "1. The projection of the vector v1 onto the plane defined by the vectors v2 and v3 is given by the cross product of v2 and v3, which is v2 x v3. The projection vector is v1 x (v2 x v3) = (v1 x v2) x v3.",
        "reference": "Figure 5: Illustration for the notations in the beginning of Section 5.1."
    },
    "1580300-Figure2.2-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed model for object detection and classification. The model consists of two main components: a parameter prediction network and a dynamic parameter layer. The parameter prediction network is a sequence of convolutional layers, followed by a fully connected layer, and a softmax layer. The dynamic parameter layer is a fully connected layer that takes the output of the parameter prediction network as input and outputs the parameters for the object detection and classification task. The output of the dynamic parameter layer is then used as input to the classification network, which consists of a sequence of convolutional layers, followed by a fully connected layer, and a softmax layer. The figure also shows an example of how the model can be used to detect and classify objects in an image.",
        "reference": "Figure 2.2: DPPnet, the question is used to learn how to predict parameters for a dynamic parameter layer in the classification network."
    },
    "2207.14299v1-Figure2-1.png": {
        "prediction": "1. Figure 1: Overview of the proposed approach for temporal alignment of video sequences. The input video sequence is first processed by a graph abstraction module to obtain a graph representation. The graph representation is then encoded using a graph encoder to obtain a sequence of graph embeddings. The graph embeddings are then aligned using a temporal alignment module to obtain the temporal alignment of the video sequences. The aligned graph embeddings are then used to generate the final video sequence representation.",
        "reference": "Figure 2: Overview. We extract object bounding boxes from video sequences using an off-the-shelf detector, and construct a graph abstraction of the scene. We model graph-abstracted object interactions using Interaction Networks [5], and learn a reward function by aligning video embeddings temporally. We then train image-based RL policies using our learned reward function, and deploy on a real robot."
    },
    "2107.04225v2-Figure1-1.png": {
        "prediction": "1. The teacher network is trained on the labeled data and the student network is trained on the unlabeled data. 2. The student network is trained to minimize the consistency loss between the teacher network's predictions on the unlabeled data and the student network's predictions on the unlabeled data. 3. The student network is trained to minimize the multi-task supervised loss between the teacher network's predictions on the labeled data and the student network's predictions on the unlabeled data. 4. The student network is trained to minimize the multi-task consistency loss between the teacher network's predictions on the unlabeled data and the student network's predictions on the unlabeled data.",
        "reference": "Fig. 1. Framework for multi-task affective behavior analysis model"
    },
    "2012.04925v1-Figure2-1.png": {
        "prediction": "1. Visual feature space: The visual feature space is a two-dimensional space where each dimension represents a different aspect of the image. The first dimension represents the brightness of the image, while the second dimension represents the contrast. The visual feature space is a high-dimensional space, but it is compact and has a clear structure.",
        "reference": "Figure 2. Conceptual illustration of the three proposed metrics. Given a caption \ud835\udc66\ud835\udc61 generated by a crosslingual image captioning model, we propose WMDRel and CLinRel to measure the semantic relevance between \ud835\udc66\ud835\udc61 and \ud835\udc66\ud835\udc60 , the reference in a source language (English here), and CMedRel to measure the semantic relevance between \ud835\udc66\ud835\udc61 and the visual content. Different from previous works, no reference caption in the target language (Chinese here) is needed."
    },
    "1592947-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of a bidirectional long short-term memory (BiLSTM) model for sequence labeling. The model has two bidirectional LSTM layers, followed by a fully connected layer and a softmax layer. The forward LSTM layer takes the input sequence and outputs a sequence of hidden states. The backward LSTM layer takes the hidden states from the forward LSTM layer and outputs a sequence of hidden states. The forward and backward LSTM layers are connected by bidirectional connections. The bidirectional connections allow the model to capture long-range dependencies in the input sequence. The fully connected layer and softmax layer are used to predict the label for each word in the input sequence.",
        "reference": "Figure 2. A BI-LSTM-CRF model for identifying FS."
    },
    "2011.14903v1-Figure3-1.png": {
        "prediction": "1. Parallel Asset Management Model: A model for minimizing the cost of replacement schedule. 2. Model for Minimizing Cost of Replacement Schedule: A model that incorporates battery degradation in asset management. 3. Nonlinear Capacity Fade Model based on Experimental Data: A battery lifetime prediction model that uses experimental data to estimate battery capacity fade. 4. Battery Degradation in Asset Management Model: A model that incorporates battery degradation into asset management. 5. Asset Management: The process of managing assets to ensure their optimal performance and longevity. 6. Battery Degradation: The gradual decrease in battery capacity over time due to various factors such as aging, temperature, and usage. 7. Parallel Asset Management Model: A model that manages multiple assets simultaneously, taking into account their unique characteristics and requirements. 8. Model for Minimizing Cost of Replacement Schedule: A model that determines the optimal time to replace an asset, taking into account factors such as cost, availability, and performance. 9. Nonlinear Capacity Fade Model based on Experimental Data: A model that uses experimental data to estimate the capacity fade of a battery over time. 10. Battery Lifetime Prediction: A model that predicts the lifetime of a battery, taking into account factors such as usage, temperature, and aging. 11. Asset Management Model: A model that manages assets to ensure their optimal performance and longevity. 12. Battery Degradation Model: A model that predicts the degradation of a battery over time. 13. Parallel Asset Management Model: A model that manages multiple assets simultaneously, taking into account their unique characteristics and requirements. 14. Model for Minimizing Cost of Replacement Schedule: A model that determines the optimal time to replace an asset, taking into account factors such as cost, availability, and performance. 15. Nonlinear Capacity Fade Model based on Experimental Data: A model that uses experimental data to estimate the capacity fade of a battery over time. 16. Battery Lifetime Prediction: A model that predicts the lifetime of a battery, taking into account factors such as usage, temperature, and aging. 17. Asset Management Model: A model that manages assets to ensure their optimal performance and longevity. 18. Battery Degradation Model: A model that predicts the degradation of a battery over time. 19. Parallel Asset Management Model: A model that manages multiple assets simultaneously, taking into account their unique characteristics and requirements",
        "reference": "Fig. 3. Battery Degradation Model and Asset Management Model Framework"
    },
    "2110.15553v2-Figure4-1.png": {
        "prediction": "1. Data-driven generation of realizations: Available data of the four brain substructures are used to generate realizations of the 2D head model. 2. Realizations of the 2D head model: The realizations of the 2D head model are generated using Grassmann manifold projections. 3. Head model outputs: The head model outputs are the final input representations for the output tasks. 4. Training generation: The training generation involves the use of Gaussian processes and geometric harmonics. 5. Surrogate model development: The surrogate model development involves the use of diffusion maps and out-of-sample predictions.",
        "reference": "Figure 4. A schematic illustration of the proposed data-driven surrogate modeling framework for UQ of computational head models: in the first stage (see Section 3.2), the available material properties of each of the four substructures, \ud835\udcb3M \ud835\udc56 \u2208 \u211d4\u00d7\ud835\udc41\ud835\udc56 (\ud835\udc41\ud835\udc56 denotes number of voxels for substructure \ud835\udc56) are used to generate 300 realizations of the input random vector of material properties for the 2D head model (i.e., \ud835\udcb3M). Simulations of these realizations yields input-output (\ud835\udcb3M \u2212 \ud835\udcb4M MAS) data sets for training the surrogate model in the second stage. The surrogate model is developed in three steps (see Section 3.3): 1. perform nonlinear dimensionality reduction on the output via"
    },
    "214611580-Figure1-1.png": {
        "prediction": "1. Representation Learning Network (RLN): This network learns to represent the data in a way that is useful for the downstream tasks. It is trained using the hard n-tuples as input and the output of the N-tuple sampling network as the target. The output of the RLN is used as the input to the downstream tasks.",
        "reference": "Figure 1. The overview of the NeuCrowd framework. The dash line indicates that the SRL network passes its training loss to the n-tuplet sampling network. The blue dots represent original crowdsourced examples and red dots represent the robust anchors."
    },
    "1047598-Figure4-1.png": {
        "prediction": "1. The flowchart shows the steps involved in the proposed method.",
        "reference": "Figure 4. Schematic of recognition in proposed model."
    },
    "2107.07001v1-Figure13-1.png": {
        "prediction": "1. The CSM separates from the S-IVB. 2. The CSM executes a 180-degree rotation. 3. The CSM rendezvous with the berthed LM. 4. The CSM/LM separates from the S-IVB. Our solution method is applied to these phases.",
        "reference": "Fig. 13 Illustration of the Apollo CSM Transposition and Docking maneuver with the LM housed inside the Saturn S-IVB third stage [35, Figure 2-11]."
    },
    "2204.03065v1-Figure2-1.png": {
        "prediction": "1. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 2. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 3. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 4. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 5. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 6. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 7. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 8. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 9. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 10. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 11. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 12. The input items are fed into the extractor, which extracts features from the items. The extracted features are then passed through the task processing G, which transforms the features into the output y. 13. The input items are fed into the extractor, which extracts features from the",
        "reference": "Fig. 2: Generic designs of networks that act on sets of inputs. These cover relevant architectures, e.g. for few-shot-classification and clustering. Left: A generic network for processing a set of input items typically follows the depicted structure: (i) Each item separately goes through a common feature extractor F . (ii) The set of extracted features is the input to a downstream task processing module G. ; Right: A more general structure in which the extracted features undergo a joint processing by a transform T . Our SOT transform (as well as other attention mechanisms) is of this type and its high-level design (within the \u2018green\u2019 module) is detailed in Fig. 1."
    },
    "137199-Figure3-1.png": {
        "prediction": "1. Structured kernel: A kernel function that takes a structured input and produces a structured output.",
        "reference": "Figure 3. Illustration for a exemplary subspace decomposition {\u039b(m)}3m=1."
    },
    "2012.01644v2-Figure5-1.png": {
        "prediction": "1. Input volume: The input volume is a 3D volume of the brain.",
        "reference": "Figure 5: Example of multi-patch sampling procedure with sampled anchor patch, positive child, and negative child."
    },
    "2012.01249v2-Figure16-1.png": {
        "prediction": "1. Input cloud (N=50, H=11). 2. Global features (N=4). 3. Fully connected (16). 4. Graph features (N=4). 5. Attention features (N=4). 6. Fully connected (128). 7. Graph features (N=4). 8. Attention features (N=4). 9. Fully connected (128). 10. Graph features (N=4). 11. Attention features (N=4). 12. Fully connected (128). 13. Graph features (N=4). 14. Attention features (N=4). 15. Fully connected (128). 16. Graph features (N=4). 17. Attention features (N=4). 18. Fully connected (128). 19. Graph features (N=4). 20. Attention features (N=4). 21. Fully connected (128). 22. Graph features (N=4). 23. Attention features (N=4). 24. Fully connected (128). 25. Graph features (N=4). 26. Attention features (N=4). 27. Fully connected (128). 28. Graph features (N=4). 29. Attention features (N=4). 30. Fully connected (128). 31. Graph features (N=4). 32. Attention features (N=4). 33. Fully connected (128). 34. Graph features (N=4). 35. Attention features (N=4). 36. Fully connected (128). 37. Graph features (N=4). 38. Attention features (N=4). 39. Fully connected (128). 40. Graph features (N=4). 41. Attention features (N=4). 42. Fully connected (128). 43. Graph features (N=4). 44. Attention features (N=4). 45. Fully connected (128). 46. Graph features (N=4",
        "reference": "Fig. 16. ABCNet architecture used for pileup identification in Ref. 16. FC layer and encoding node sizes are denoted inside curly brackets. For each graph attention pooling layer (GAPLayer), the number of k-nearest neighbors and attention heads (h) are given."
    },
    "2012.01249v2-Figure12-1.png": {
        "prediction": "1. Encoder: Graph Module, Graph Module, Graph Module, Graph Module, Output Module",
        "reference": "Fig. 12. Graph neural network architecture for particle tracking [21]. The initial latent features of the nodes and edges after the encoder network are named H0. The graph module is applied repeatedly to the latent features. For the ith iteration, the initial features H0 are concatenated with the current features Hi\u22121. After 8 iterations, the output network takes the last latent features H8 to produce classification scores for every edge."
    },
    "2012.01249v2-Figure14-1.png": {
        "prediction": "1. The input to the model is a sequence of tracklets, each consisting of a start and end time, and a track ID. The track IDs are used to identify the tracks in the sequence.",
        "reference": "Fig. 14. The Set2Graph [36, 37] model architecture (top) consisting of a set-to-set component \u03c6, which creates a hidden representation of each track, a broadcasting layer \u03b2, which creates a representation for each directed edge (ordered pair of tracks in the jet), and an edge classifier \u03c8. Edges whose symmetrized edge score is over a certain threshold are connected, resulting in the set partition."
    },
    "140529-Figure1-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed model for semantic segmentation of the input image. The model consists of two main components: a pre-trained BERT model and a fully connected neural network (FCNN) with a softmax layer. The pre-trained BERT model is used to extract the contextual information of the input image, which is then fed into the FCNN to generate the final segmentation map. The model is trained using the cross-entropy loss function.",
        "reference": "Figure 1: (Top) Model architecture: the network is split into two branches after the shared feature extraction layers. The top branch predicts a segmentation mask for the the object located at the center while the bottom branch predicts an object score for the input patch. (Bottom) Examples of training triplets: input patch x, mask m and label y. Green patches contain objects that satisfy the specified constraints and therefore are assigned the label y = 1. Note that masks for negative examples (shown in red) are not used and are shown for illustrative purposes only."
    },
    "2204.08626v1-Figure1-1.png": {
        "prediction": "1. The EEG signal is first filtered using a bank of spatial filters. 2. The CSP is applied to each filtered signal to extract the spatial features. 3. The CSP is applied to the concatenated signals to extract the temporal features. 4. The SAE is used to learn the mapping between the spatial and temporal features. 5. The SAE is used to learn the mapping between the EEG signal and the extracted features.",
        "reference": "Fig. 1: The procedure of extracting features. The EEG signals are bandpass filtered using K different frequency ranges represented in F . The CSP algorithm is applied to the filtered signals to generate a feature vector. These vectors are then fused to feed a supervised autoencoder."
    },
    "2101.08520v3-Figure1-1.png": {
        "prediction": "1. The input sequence is fed into the encoder, which produces a sequence of hidden states. 2. The hidden states are then passed through a feedforward neural network (FFNN) to produce a sequence of logits. 3. The logits are then used to compute the loss for each word in the sequence. 4. The loss is then backpropagated through the FFNN to update the weights. 5. The updated weights are then used to compute the next hidden state in the sequence. 6. The process is repeated for each word in the sequence.",
        "reference": "Figure 1. Pictorial description of the overall architecture."
    },
    "2012.14758v1-Figure1-1.png": {
        "prediction": "1. The user sends a request to enroll in the system. 2. The system generates a random bit string and sends it to the user. 3. The user extracts the bit string and sends it back to the system. 4. The system generates a hash of the bit string and sends it to the user. 5. The user extracts the hash and sends it back to the system. 6. The system checks the hash and the bit string to verify the user's identity. 7. The user is enrolled in the system.",
        "reference": "Fig. 1: Block diagram of the proposed system."
    },
    "2012.14758v1-Figure2-1.png": {
        "prediction": "1. The architecture of the Iris-CNN model.",
        "reference": "Fig. 2: Proposed deep feature extraction and binarization (DFB) model for the fully concatenated architecture (FCA)."
    },
    "2012.05647v1-Figure2-1.png": {
        "prediction": "1. The figure shows the Quasi Reinforcement Learning Loop for the Neural Network. The loop starts with the known input, which is the spatial phase modulation. The beam array training phase pattern is then used to train the Neural Network. The Neural Network is then used to predict the desired phase pattern, which is the Scattering. The Scattering is then used to update the NN parameters. The NN parameters are then used to update the phase pattern. The phase pattern is then used to update the desired phase pattern. The desired phase pattern is then used to update the Neural Network. The Neural Network is then used to predict the Scattering. The Scattering is then used to update the NN parameters. The NN parameters are then used to update the phase pattern. The phase pattern is then used to update the desired phase pattern. The desired phase pattern is then used to update the Neural Network. The Neural Network is then used to predict the Scattering. The Scattering is then used to update the NN parameters. The NN parameters are then used to update the phase pattern. The phase pattern is then used to update the desired phase pattern. The desired phase pattern is then used to update the Neural Network. The Neural Network is then used to predict the Scattering. The Scattering is then used to update the NN parameters. The NN parameters are then used to update the phase pattern. The phase pattern is then used to update the desired phase pattern. The desired phase pattern is then used to update the Neural Network. The Neural Network is then used to predict the Scattering. The Scattering is then used to update the NN parameters. The NN parameters are then used to update the phase pattern. The phase pattern is then used to update the desired phase pattern. The desired phase pattern is then used to update the Neural Network. The Neural Network is then used to predict the Scattering. The Scattering is then used to update the NN parameters. The NN parameters are then used to update the phase pattern. The phase pattern is then used to update the desired phase pattern. The desired phase pattern is then used to update the Neural Network. The Neural Network is then used to predict the Scattering. The Scattering is then used to update the NN parameters. The NN parameters are then used to update the phase pattern. The phase pattern is then used to update the desired phase pattern. The desired phase pattern is then used to update the Neural Network. The Neural Network is then used to predict the Scattering.",
        "reference": "Figure 2: Schematic of the specific reinforcement learning loop used for each phase pattern of the training data set, with a particular example of desired phase pattern."
    },
    "2101.00341v1-Figure1.1-1.png": {
        "prediction": "1. Figure 1: Temporal popularity dynamics of content in a social network.",
        "reference": "Figure 1.1: An illustration of a UDCN and its intrinsic spatio-temporal popularity dynamics. (a) Spatially dynamics of popularity (b) Temporal dynamics where the content popularity changes for long-term and short-term duration. The long-term dynamics are captured by the Chinese restaurant process, which determines the mean popularity for a certain time period of T . During this period, the instantaneous popularity is captured by the mean reversion model following the OU process [43]."
    },
    "2205.13948v1-Figure4-1.png": {
        "prediction": "1. Start\n2. FNN + softmax\n3. [p1] [p2] [p3] [p4] [p5] [p6] [p7] [p8] [p9] [p10] [p11] [p12] [p13] [p14] [p15] [p16] [p17] [p18] [p19] [p20] [p21] [p22] [p23] [p24] [p25] [p26] [p27] [p28] [p29] [p30] [p31] [p32] [p33] [p34] [p35] [p36] [p37] [p38] [p39] [p40] [p41] [p42] [p43] [p44] [p45] [p46] [p47] [p48] [p49] [p50] [p51] [p52] [p53] [p54] [p55] [p56] [p57] [p58] [p59] [p60] [p61] [p62] [p63] [p64] [p65] [p66] [p67] [p68] [p69] [p70] [p71] [p72] [p73] [p74] [p75] [p76] [p77] [p78] [p79] [p80] [p81] [p82] [p83] [p84] [p85] [p86] [p87] [p88] [p89] [p90] [p91] [p92] [p93] [p94] [p95] [p96] [p97] [p98] [p99] [",
        "reference": "Fig. 4. Overview of PEGA."
    },
    "2210.01528v1-Figure3-1.png": {
        "prediction": "1. Read the thermal noise removal file. 2. Apply the orbit file. 3. Read the speckle filter file. 4. Apply the calibration file. 5. Read the terrain correction file. 6. Apply the terrain flattening file. 7. Read the subset file. 8. Apply the linear to from DB file.",
        "reference": "Fig. 3: Illustration of one of the back-scattering SNAP pipeline we have integrated in the system."
    },
    "913274-Figure1-1.png": {
        "prediction": "1. Softmax Layer: This layer is used to compute the probability of each word in the sequence. The softmax function is applied to the output of the previous layer, which is a vector of probabilities for each word in the sequence. The softmax function is used to normalize the probabilities so that they sum to 1. The output of the softmax layer is a vector of probabilities for each word in the sequence.",
        "reference": "Figure 1. An illustration of the Bi-LSTM-RNN model. The example is \u201cHe had headachese1 from molde2 in the bedrooms.\u201d. e1 and e2 denote two target entities."
    },
    "914562-Figure2-1.png": {
        "prediction": "1. Feature extraction: The CNN extracts features from the input image. 2. Metric learning: The CNN learns a metric between the features of different images. 3. Metric matching: The CNN matches the features of the input image with the features of the database images to find the closest match. 4. Classification: The CNN classifies the input image based on the closest match.",
        "reference": "Figure 2: Illustration of our learning formulation. It tends to maximize the distance disparity between the matched pair and mismatched pair for each triplet, in terms of optimization."
    },
    "2101.02047v2-Figure2-1.png": {
        "prediction": "1. Input Image: A hand gesture is captured by a camera.",
        "reference": "Figure 2: A block diagram of the unified gesture recognition and fingertip detection algorithm depicting the CNN architecture with input and output."
    },
    "2202.06065v1-Figure2-1.png": {
        "prediction": "1. The first block represents the input sequence, which is a sequence of words. The second block represents the hidden state of the first layer, which is a vector of word embeddings. The third block represents the hidden state of the second layer, which is a vector of word embeddings. The fourth block represents the hidden state of the third layer, which is a vector of word embeddings. The fifth block represents the hidden state of the fourth layer, which is a vector of word embeddings. The sixth block represents the hidden state of the fifth layer, which is a vector of word embeddings. The seventh block represents the hidden state of the sixth layer, which is a vector of word embeddings. The eighth block represents the hidden state of the seventh layer, which is a vector of word embeddings. The ninth block represents the hidden state of the eighth layer, which is a vector of word embeddings. The tenth block represents the hidden state of the ninth layer, which is a vector of word embeddings. The eleventh block represents the hidden state of the tenth layer, which is a vector of word embeddings. The twelfth block represents the hidden state of the eleventh layer, which is a vector of word embeddings. The thirteenth block represents the hidden state of the twelfth layer, which is a vector of word embeddings. The fourteenth block represents the hidden state of the thirteenth layer, which is a vector of word embeddings. The fifteenth block represents the hidden state of the fourteenth layer, which is a vector of word embeddings. The sixteenth block represents the hidden state of the fifteenth layer, which is a vector of word embeddings. The seventeenth block represents the hidden state of the sixteenth layer, which is a vector of word embeddings. The eighteenth block represents the hidden state of the seventeenth layer, which is a vector of word embeddings. The nineteenth block represents the hidden state of the eighteenth layer, which is a vector of word embeddings. The twentieth block represents the hidden state of the nineteenth layer, which is a vector of word embeddings. The twenty-first block represents the hidden state of the twentieth layer, which is a vector of word embeddings. The twenty-second block represents the hidden state of the twenty-first layer, which is a vector of word embeddings. The twenty-third block represents the hidden state of the twenty-second layer, which is a vector of word embeddings. The twenty-fourth block represents the hidden state of the twenty-third layer, which is a vector of",
        "reference": "Fig. 2. Illustration of the construction of (B , B ) . The straight edges are the five possible types for edges of % . The curvy edge corresponds to an edge of Alice, and the do ed edge to an edge of Bob."
    },
    "2012.15175v3-Figure2-1.png": {
        "prediction": "1. The original graph is represented as a set of nodes and edges. 2. The graph is scaled by a factor of k, resulting in a new set of nodes and edges. 3. The scaled graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 4. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 5. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 6. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 7. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 8. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 9. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 10. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 11. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 12. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 13. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 14. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 15. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 16. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 17. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 18. The graph is re-scaled by a factor of 1/k, resulting in a new set of nodes and edges. 19. The graph is re-scaled by a factor of 1/k, resulting in",
        "reference": "Figure 2. During training, the ground-truth heatmaps are firstly scaled according to predicted scale maps and then are used to supervise the whole model via weight-adaptive loss. During testing, the predicted heatmaps and associative embeddings are used for grouping of individual persons."
    },
    "2103.06446v1-Figure1-1.png": {
        "prediction": "1. Input data: individual students' achievement data across subjects",
        "reference": "Figure 1: The procedures of our approach"
    },
    "18233504-Figure1-1.png": {
        "prediction": "1. Figure 1: Illustration of the architecture of the LSTM model with the attention mechanism. The input sequence is encoded by the LSTM model, and the attention mechanism is used to focus on the most relevant parts of the input sequence. The output of the LSTM model is then passed through a fully connected layer to produce the final output.",
        "reference": "Figure 1: Iteratively decoding with DQN and LSTM; the encoder- coder LSTM network is depicted as gray-filled rectangles on the bottom; the top-leftis the graphical illustration of the DQN with bidirectional LSTMs; the dash arrow line on the right indicates the iteration loop."
    },
    "2010.13114v1-Figure2-1.png": {
        "prediction": "1. The teacher network is trained to predict the labels of the student network's outputs.",
        "reference": "Figure 2: A schematic of the proposed methodology for joint knowledge distillation and open set recognition. The outputs from teacher and student models are their respective representations, T and S, which are used for calculating CRD loss term. Next, we zS and zT represent student and teacher logits, which are used for calculating the KD loss term. Finally, we calculate class probabilities \u03c3S(zS), which are used for computing of cross-entropy loss term."
    },
    "988721-Figure1-1.png": {
        "prediction": "1. The image is processed by a convolutional neural network (CNN) to extract features. 2. The features are passed through a fully connected layer to obtain a representation of the image. 3. The representation is then passed through a CRF layer to obtain a probability distribution over the possible labels. 4. The CRF layer is trained using the negative log-likelihood loss function. 5. The final output is a probability distribution over the possible labels.",
        "reference": "Figure 1. Overview of the proposed method. Multi-scale fully convolutional network is used for image representation. The network consists of 5 different paths and each path extracts feature at a difference scale. At the end of each path, two convolutional layers extract feature for semantic segmentation and depth estimation. These feature maps are scaled and aggregated to form a comprehensive feature map for semantic segmentation and depth estimation. Depth values are estimated using Eq. 3. Estimated depth values along with semantic unaries and the image are passed through the fully connected CRF to get the final probabilities of semantic labels."
    },
    "2202.07568v4-Figure2-1.png": {
        "prediction": "1. What to move: Evaluate each model against attacks by attacker x. Train/construct different models. Set of models selected by defender. Set of models selected by optimizer. 2. What to move: Perform model selection. 3. How to move: Produce strategy vector. Optimize. Set of models selected by defender. Set of models selected by optimizer. 4. When to move: Roll biased die. Single model from Zx is chosen strategically. 5. What to move: Check prediction. 6. When to move: Make a prediction. 7. What to move: Feed relevant data into optimizer. Set of optimizers. 8. When to move: Check prediction. 9. What to move: Roll biased die. 10. When to move: Make a prediction. 11. What to move: Check prediction. 12. When to move: Make a prediction. 13. What to move: Roll biased die. 14. When to move: Make a prediction. 15. What to move: Check prediction. 16. When to move: Make a prediction. 17. What to move: Check prediction. 18. When to move: Make a prediction. 19. What to move: Check prediction. 20. When to move: Make a prediction. 21. What to move: Check prediction. 22. When to move: Make a prediction. 23. What to move: Check prediction. 24. When to move: Make a prediction. 25. What to move: Check prediction. 26. When to move: Make a prediction. 27. What to move: Check prediction. 28. When to move: Make a prediction. 29. What to move: Check prediction. 30. When to move: Make a prediction. 31. What to move: Check prediction. 32. When to move: Make a prediction. 33. What to move: Check prediction. 34. When to move: Make a prediction. 35. What to move: Check prediction. 36. When to move: Make a prediction. 37. What to move: Check prediction. 38. When to move: Make a prediction. 39. What to move: Check prediction.",
        "reference": "Fig. 2: Overview of StratDef."
    },
    "201666925-Figure1-1.png": {
        "prediction": "1. The input image is fed into the GAN model, which generates a fake image. 2. The fake image is then fed into the reconstruction model, which generates a reconstruction image. 3. The reconstruction image is compared to the original image to calculate the reconstruction error. 4. The fake image is then fed into the GAN model again, and the process is repeated. 5. The GAN model is trained to generate more realistic fake images, and the reconstruction error is reduced.",
        "reference": "Fig. 1. Illustration depicting the comparison between the baseline GAN model and our Recon-GLGAN model. In the training phase, the ZF image and the ROI are fed in as inputs to the Recon-GLGAN model, while the baseline GAN only takes the ZF image as input. In the testing stage, the ZF image is fed as input to either model to produce the reconstruction (Note: ROI is not used during testing stage). The reconstruction error of the Recon-GLGAN model is lesser than the baseline GAN model in the ROI"
    },
    "201666925-Figure2-1.png": {
        "prediction": "1. The input image is fed into the encoder, which produces a representation vector. 2. The representation vector is then passed through a classifier, which outputs a probability distribution over the classes. 3. The classifier is trained to minimize the cross-entropy loss between the predicted probabilities and the true labels. 4. The representation vector is then used as input to the decoder, which generates the image.",
        "reference": "Fig. 2. Recon-GLGAN architecture"
    },
    "2107.05180v1-Figure2-1.png": {
        "prediction": "1. Representation Learning: Learning the representation of the data, such as the features of the real estate properties, the transaction events, and the community-level features.",
        "reference": "Figure 2: The framework overview of MugRep."
    },
    "2202.10337v1-Figure1-1.png": {
        "prediction": "1. Domain knowledge embedding in modeling process. 2. Knowledge discovery based on machine learning. 3. Knowledge embedded model. 4. Applications: Prediction and forecasting of physical processes, inverse modeling, data assimilation, determining physics laws, discovery of physics laws, interpretation of observations, nature of physics, equations. 5. Simulation. 6. Vector space. 7. Knowledge embedded model. 8. Knowledge embedded model. 9. Knowledge embedded model. 10. Knowledge embedded model. 11. Knowledge embedded model. 12. Knowledge embedded model. 13. Knowledge embedded model. 14. Knowledge embedded model. 15. Knowledge embedded model. 16. Knowledge embedded model. 17. Knowledge embedded model. 18. Knowledge embedded model. 19. Knowledge embedded model. 20. Knowledge embedded model. 21. Knowledge embedded model. 22. Knowledge embedded model. 23. Knowledge embedded model. 24. Knowledge embedded model. 25. Knowledge embedded model. 26. Knowledge embedded model. 27. Knowledge embedded model. 28. Knowledge embedded model. 29. Knowledge embedded model. 30. Knowledge embedded model. 31. Knowledge embedded model. 32. Knowledge embedded model. 33. Knowledge embedded model. 34. Knowledge embedded model. 35. Knowledge embedded model. 36. Knowledge embedded model. 37. Knowledge embedded model. 38. Knowledge embedded model. 39. Knowledge embedded model. 40. Knowledge embedded model. 41. Knowledge embedded model. 42. Knowledge embedded model. 43. Knowledge embedded model. 44. Knowledge embedded model. 45. Knowledge embedded model. 46. Knowledge embedded model. 47. Knowledge embedded model. 48. Knowledge embedded model. 49. Knowledge embedded model. 50. Knowledge embedded model. 51. Knowledge embedded model. 52. Knowledge embedded model. 53. Knowledge embedded model. 54. Knowledge embedded model. 55. Knowledge embedded model. 56. Knowledge embedded model. 57. Knowledge embedded model. 58. Knowledge embedded model. 59. Knowledge embedded model. 60.",
        "reference": "Figure 1: Schematic diagram of the relationship between knowledge embedding and knowledge discovery"
    },
    "30595348-Figure1-1.png": {
        "prediction": "1. The model takes in an image of a cat and a dog. 2. The model uses a convolutional neural network to extract features from the image. 3. The model uses a reverse attention mechanism to learn what is not a cat or a dog. 4. The model uses a convolutional neural network to learn what is a cat or a dog. 5. The model combines the features from the two networks to produce a final prediction.",
        "reference": "Fig. 1. An illustration of the proposed reversed attention network (RAN), where the lower and upper branches learn features and predictions that are and are not associated with a target class, respectively. The mid-branch focuses on local regions with complicated spatial patterns whose object responses are weaker and provide a mechanism to amplify the response. The predictions of all three branches are fused to yield the final prediction for the segmentation task."
    },
    "1233699-Figure4-1.png": {
        "prediction": "1. Input image is processed to extract features using convolutional neural networks (CNNs). 2. The extracted features are used to predict the next sentence in the sequence. 3. The predicted sentence is compared with the ground truth sentence to calculate the loss. 4. The loss is backpropagated through the network to update the weights. 5. The updated weights are used to predict the next sentence in the sequence. 6. The process is repeated for each sentence in the sequence. 7. The final loss is calculated by averaging the losses for all sentences in the sequence. 8. The final loss is used to train the network to improve its accuracy.",
        "reference": "Fig. 4. The proposed network architecture for skeleton extraction, which is converted from VGG 16-layer net [36]. (a) Multi-task Scale-associated side outputs (SSOs) learning. Our network has 4 stages with SSO layers connected to the convolutional layers. Each stage branches into two sibling SSO layers, one for skeleton localization and the other for scale prediction, denoted by Loc-SSO (the left multi-color blocks) and ScalePred-SSO (the right blue block), respectively. The SSOs in each stage are guided by a scale-associated groundtruth skeleton map (The skeleton pixels with different quantized scales are in different colors. Each block in a Loc-SSO is the activation map for one quantized scale, marked by the corresponding color). (b) Scale-specific fusion. Each Loc-SSO provides a certain number of scale-specific skeleton score maps (identified by stage number-quantized scale value pairs). The score maps of the same scales from different stages will be sliced and concatenated. Five scale-specific weighted-fusion layers are added to automatically fuse outputs from multiple stages."
    },
    "2012.09688v3-Figure3-1.png": {
        "prediction": "1. The input sequence is fed into the Transformer encoder, which produces a sequence of hidden states. 2. The attention mechanism is applied to the hidden states to extract relevant information from the sequence. 3. The attention weights are used to compute the value of each token in the sequence. 4. The value of each token is passed through a Softmax function to produce a probability distribution over the tokens. 5. The probabilities are used to compute the attention features for each token. 6. The attention features are concatenated with the hidden states to produce a new sequence of hidden states. 7. The new sequence of hidden states is passed through a linear layer to produce the output sequence. 8. The output sequence is then passed through a linear layer to produce the LBR score.",
        "reference": "Figure 3. Architecture of Offset-Attention. Numbers above tensors are numbers of dimensions N and feature channels D/Da, with switches showing alternatives of Self-Attention or Offset-Attention: dotted lines indicate Self-Attention branches."
    },
    "2110.10072v1-Figure1-1.png": {
        "prediction": "1. The laser pulse shaper is used to generate a train of laser pulses with a fixed duration. 2. The train of laser pulses is transmitted through the Wheatstone bridge and the strain gauge. 3. The strain gauge measures the strain in the Wheatstone bridge. 4. The strain gauge output is amplified and sent to the oscilloscope. 5. The oscilloscope measures the strain in the Wheatstone bridge. 6. The strain in the Wheatstone bridge is proportional to the strain in the flat disc. 7. The strain in the flat disc is proportional to the pressure in the vessel.",
        "reference": "Fig. 1 Schematic diagram of a Split Hopkinson Pressure bar and top view of the flattened Brazilian disc sample before mounting"
    },
    "688013-Figure7-1.png": {
        "prediction": "1. The input RGB frames are fed into a Region Proposal Network (RPN) to generate region proposals. 2. The region proposals are then fed into a Region of Interest (ROI) pooling layer to extract features. 3. The features are then passed through an average pooling layer to generate a single feature vector. 4. The feature vector is then fed into a classification layer to generate a classification score. 5. The classification score is then used to generate a box refinement layer to refine the bounding box. 6. The refined bounding box is then passed through a box refinement layer to generate a final bounding box.",
        "reference": "Figure 7. Illustration of our approach for spatio-temporal action localization. Region proposals are detected and regressed with Faster-RCNN on RGB keyframes. Spatio-temporal tubes are classified with two-stream I3D convolutions."
    },
    "1379674-Figure1-1.png": {
        "prediction": "1. The input sequence is split into segments. 2. Each segment is processed by a feedforward neural network (FNN) to generate a representation vector. 3. The representation vectors from all segments are concatenated and passed through a feedforward neural network (FNN) to generate a representation vector for the entire sequence. 4. The representation vector is passed through a feedforward neural network (FNN) to generate a representation vector for each time step. 5. The representation vectors for each time step are concatenated and passed through a feedforward neural network (FNN) to generate a representation vector for the entire sequence. 6. The representation vector for the entire sequence is passed through a feedforward neural network (FNN) to generate a representation vector for each time step. 7. The representation vectors for each time step are concatenated and passed through a feedforward neural network (FNN) to generate a representation vector for the entire sequence. 8. The representation vector for the entire sequence is passed through a feedforward neural network (FNN) to generate a representation vector for each time step. 9. The representation vectors for each time step are concatenated and passed through a feedforward neural network (FNN) to generate a representation vector for the entire sequence. 10. The representation vector for the entire sequence is passed through a feedforward neural network (FNN) to generate a representation vector for each time step. 11. The representation vectors for each time step are concatenated and passed through a feedforward neural network (FNN) to generate a representation vector for the entire sequence. 12. The representation vector for the entire sequence is passed through a feedforward neural network (FNN) to generate a representation vector for each time step. 13. The representation vectors for each time step are concatenated and passed through a feedforward neural network (FNN) to generate a representation vector for the entire sequence. 14. The representation vector for the entire sequence is passed through a feedforward neural network (FNN) to generate a representation vector for each time step. 15. The representation vectors for each time step are concatenated and passed through a feedforward neural network (FNN) to generate a representation vector for the entire sequence. 16. The representation vector for the entire sequence is passed through a feedforward neural network (FNN) to generate a representation vector for each time step. 17",
        "reference": "Figure 1. Inference in a 3-Level Pyramid Network [15]: The network G0 computes the residual flow v0 at the highest level of the pyramid (smallest image) using the low resolution images {I10 , I20}. At each pyramid level, the network Gk computes a residual flow vk which propagates to each of the next lower levels of the pyramid in turn, to finally obtain the flow V2 at the highest resolution."
    },
    "2011.05655v1-Figure1-1.png": {
        "prediction": "1. The process flow diagram of the waste water treatment system in Hall #5.",
        "reference": "Figure 1: Schematic of the pilot plant experiment"
    },
    "2101.08003v1-Figure1-1.png": {
        "prediction": "1. The figure shows the geometry of the experiment. The incident light is polarized along the x-axis and the detector is placed along the y-axis. The incident light is reflected by the gold film and passes through the BK7 glass. The angle of incidence is 80 degrees. The distance between the incident light and the detector is 50 mm. The thickness of the gold film is 1 nm. The thickness of the BK7 glass is 1 mm.",
        "reference": "FIG. 1. A schematic of (a) simulation setup showing the major components as depicted. The plane of polarization is XY. The angle of incidence of the monochromatic light (L) is \u03b8i, thickness of each material and detector (D) are shown and (b) a single layer of shape anisotropic molecules (rod shaped) tilted with respect to X-axis along Y-axis on the YZ plane. The projection of the molecules is shown in black. Such projection resembles nematic ordering on 2D plane with optic axis along Y-axis."
    },
    "2202.03587v1-Figure2-1.png": {
        "prediction": "1. Figure 1: Overview of the multimodal transformer architecture. The acoustic word embeddings are first transformed into acoustic token embeddings using a transformer encoder. The acoustic token embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then transformed into acoustic word embeddings using a transformer decoder. The acoustic word embeddings are then",
        "reference": "Figure 2: The acoustic tokens, as output by the SpecTran network pretrained via CALP, are employed together with (text) words in a multimodal transformer. The multimodal transformer encodes the set of tokens with modality and positional information."
    },
    "2010.13130v1-Figure1-1.png": {
        "prediction": "1. The algorithm starts with an empty list of true labels and a prediction of the target variable. 2. The algorithm ingests a new observation and predicts the target variable. 3. The algorithm scores the prediction based on the true labels and the scoring function. 4. The algorithm updates the true labels and the prediction based on the scoring function. 5. The algorithm continues to ingest observations and predict the target variable until the end of the algorithm.",
        "reference": "Figure 1: AutoSpeech Challenge\u2019s evaluation process for one task defined by the 5-tuple: Dtr, D \u2205 te, L,BT , BS . Participants need to submit a strategy implemented by Python scripts which must contain a file named \u201dmodel.py\u201d. forgiving In this file, the two functions named train and test implement the logic of AutoSpeech algorithm. These two functions are called by the ingestion program (defined in ingestion.py) orderly, to train on Dtr and produce a prediction Y t pred on D \u2205 te at the timestamp t respectively. The prediction Y tpred is then compared to true labels Yte in scoring program (defined by score.py and produces a score st. This ingestion/scoring loop is controlled by time budget BT and a flag in model.py, to determine whether the whole program ends. At any time,the score sequence st0, st1, ... is visualized as a learning curve and the area under learning curve is used as the evaluation for this task. Note that only the run time of ingestion program is accumulated, but the time of scoring is not counted for running in parallel."
    },
    "2103.14969v2-Figure7-1.png": {
        "prediction": "1. The input image is resized to 256x200x24x16. 2. The image is then divided into 256x200x24 blocks, each of which is processed separately. 3. The output of each block is then concatenated with the previous block's output. 4. The concatenated output is then processed by a convolutional layer with a 3x3 kernel and stride 2. 5. The output of this layer is then upsampled by a factor of 2x2. 6. The output of this layer is then processed by a convolutional layer with a 3x3 kernel and stride 1. 7. The output of this layer is then upsampled by a factor of 2x2. 8. The output of this layer is then processed by a convolutional layer with a 3x3 kernel and stride 1. 9. The output of this layer is then upsampled by a factor of 2x2. 10. The output of this layer is then processed by a convolutional layer with a 3x3 kernel and stride 1. 11. The output of this layer is then upsampled by a factor of 2x2. 12. The output of this layer is then processed by a convolutional layer with a 3x3 kernel and stride 1. 13. The output of this layer is then upsampled by a factor of 2x2. 14. The output of this layer is then processed by a convolutional layer with a 3x3 kernel and stride 1. 15. The output of this layer is then upsampled by a factor of 2x2. 16. The output of this layer is then processed by a convolutional layer with a 3x3 kernel and stride 1. 17. The output of this layer is then upsampled by a factor of 2x2. 18. The output of this layer is then processed by a convolutional layer with a 3x3 kernel and stride 1. 19. The output of this layer is then upsampled by a factor of 2x2. 20. The output of this layer is then processed by a convolutional layer with a 3x",
        "reference": "Fig. 7. V-Net with autoencoder regularization: \u201cSchematic visualization of the network architecture. Input is a four channel 3D MRI crop, followed by initial 3x3x3 3D convolution with 32 filters. Each green block is a ResNet-like block with the GroupNorm normalization. The output of the segmentation decoder has three channels (with the same spatial size as the input) followed by a sigmoid for segmentation maps of the three tumor subregions (WT, TC, ET). The VAE branch reconstructs the input image into itself, and is used only during training to regularize the shared encoder.\u201d [33]"
    },
    "2010.00702v1-Figure4-1.png": {
        "prediction": "1. The input image is fed into the network, which produces a feature map. 2. The feature map is then passed through a series of operations, including convolution, batch normalization, and ReLU activation. 3. The output of the network is a set of feature maps, each corresponding to a different layer of the network. 4. The feature maps are then passed through a series of operations, including max pooling, which reduces the size of the feature maps. 5. The output of the network is a set of feature maps, each corresponding to a different layer of the network. 6. The feature maps are then passed through a series of operations, including fully connected layers, which produce the final output of the network.",
        "reference": "Figure 4: Our dual-view reflection removal. Given images I1 and I2, our reflection-invariant optical flow network \u03c6flow estimates the motion F1 2 between the unknown transmissive layers of the inputs, which is then used to warp I2 towards I1 to \u201cundo\u201d that motion. Our synthesis network \u03c6syn can then use these aligned images to leverage the parallax between the reflective layers (and the lack thereof between the transmissive layers) to synthesize I1\u2019s transmissive layer T1."
    },
    "2205.04319v1-Figure1-1.png": {
        "prediction": "1. The system consists of two operators, a broker, a customer, and a collection of requests.",
        "reference": "Figure 1: Illustration of an AMOD Broker System"
    },
    "2205.11782v1-Figure1-1.png": {
        "prediction": "1. The user launches an IPA or OPA. 2. The IPA or OPA generates fake data. 3. The LDP Randomizer generates a random number. 4. The LDP Randomizer outputs the fake data. 5. The LDP Randomizer outputs the random number. 6. The LDP Randomizer outputs the fake data. 7. The LDP Randomizer outputs the random number. 8. The LDP Randomizer outputs the fake data. 9. The LDP Randomizer outputs the random number. 10. The LDP Randomizer outputs the fake data. 11. The LDP Randomizer outputs the random number. 12. The LDP Randomizer outputs the fake data. 13. The LDP Randomizer outputs the random number. 14. The LDP Randomizer outputs the fake data. 15. The LDP Randomizer outputs the random number. 16. The LDP Randomizer outputs the fake data. 17. The LDP Randomizer outputs the random number. 18. The LDP Randomizer outputs the fake data. 19. The LDP Randomizer outputs the random number. 20. The LDP Randomizer outputs the fake data. 21. The LDP Randomizer outputs the random number. 22. The LDP Randomizer outputs the fake data. 23. The LDP Randomizer outputs the random number. 24. The LDP Randomizer outputs the fake data. 25. The LDP Randomizer outputs the random number. 26. The LDP Randomizer outputs the fake data. 27. The LDP Randomizer outputs the random number. 28. The LDP Randomizer outputs the fake data. 29. The LDP Randomizer outputs the random number. 30. The LDP Randomizer outputs the fake data. 31. The LDP Randomizer outputs the random number. 32. The LDP Randomizer outputs the fake data. 33. The LDP Randomizer outputs the random number. 34. The LDP Randomizer outputs the fake data. 35. The LDP Randomizer outputs the random number. 36. The LDP Randomizer outputs the fake data. 37. The LDP Random",
        "reference": "Figure 1: Illustration of our fine-grained data poisoning attacks on LDP-based mean/variance estimation."
    },
    "2107.09786v1-Figure1-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed federated learning framework. The client K-1 sends the gradient to the cloud server, which sends the activation to the server. The server sends the gradient to the client, and the client sends the model to the next client. The loss is computed at each epoch.",
        "reference": "Fig. 1. Overview of split learning scheme. (a) Split learning with K clients. (b) Communication and computation at the client level."
    },
    "2210.09964v2-Figure1-1.png": {
        "prediction": "1. Safe-range RC (S3.1) -> RC (S3.1) -> SRNF (S3.3) -> RANF (S3.4) -> RA (S3.5) -> SQL (S3.6.3",
        "reference": "Figure 1: Overview of our translation."
    },
    "2010.06417v2-Figure2.12-1.png": {
        "prediction": "1. The input sequence is passed through a function f, which produces a sequence of hidden states h. 2. The hidden states are then \"unfolded\" into the original input sequence, producing a new sequence of hidden states h' = f(h). 3. The hidden states are then passed through a function g, which produces a new sequence of hidden states h'' = g(h'). 4. The hidden states are then \"unfolded\" into the original input sequence, producing a new sequence of hidden states h''' = g(h''). 5. The process is repeated for each hidden state in the original input sequence, producing a sequence of unfolded hidden states h'''' = g(h'''), h''' = g(h'''), ..., h'' = g(h''). 6. The unfolded hidden states are then concatenated to form a new sequence of hidden states h'''' = [h''', h''', ..., h'']. 7. The new sequence of hidden states is then passed through a function f, which produces a new sequence of hidden states h'' = f(h'''). 8. The process is repeated for each hidden state in the new sequence of hidden states, producing a sequence of unfolded hidden states h''' = f(h'''), h'' = f(h'''), ..., h'' = f(h''). 9. The unfolded hidden states are then concatenated to form a new sequence of hidden states h''' = [h'''', h''', ..., h'']. 10. The new sequence of hidden states is then passed through a function g, which produces a new sequence of hidden states h'' = g(h'''). 11. The process is repeated for each hidden state in the new sequence of hidden states, producing a sequence of unfolded hidden states h''' = g(h'''), h'' = g(h'''), ..., h'' = g(h''). 12. The unfolded hidden states are then concatenated to form a new sequence of hidden states h''' = [h'''', h''', ..., h'']. 13. The new sequence of hidden states is then passed through a function f, which produces a new sequence of hidden states h'' = f(h'''). 14. The process is repeated for each hidden state in the new sequence of hidden states, producing a sequence of unfolded hidden states h''' = f(h'''), h'' = f(h'''), ..., h'' = f(h'",
        "reference": "Figure 2.12: A recurrent network with no outputs. This recurrent network just processes information from the input x by incorporating it into the state h that is passed forward through time. (Left) Circuit diagram. The black square indicates a delay of a single time-step. Right The same network seen as an unfolded computational graph, where each node is now associated with one particular time instance."
    },
    "2110.02729v1-Figure3-1.png": {
        "prediction": "1. The block diagram of the proposed 16-bit DAC with an on-chip DAC/counter.",
        "reference": "Figure 3. The block diagram of the proposed time domain offset cancelation technique."
    },
    "2103.04234v2-Figure6-1.png": {
        "prediction": "1. The validator is an upper layer that extends the benchmarker, which calls the client. The validator also calls the fault injection checker. The validator uses the HTTP server configuration to transport the calls to the client. The validator extends the use case library, which is a lower layer that uses TCP/UDP channels to transport calls. The validator also uses the use case library to transport calls to the data store. The validator uses the use case library to transport calls to the ballot storage. The validator uses the use case library to transport calls to the state machine. The validator uses the use case library to transport calls to the gob/json channel. The validator uses the use case library to transport calls to the gob/bson channel. The validator uses the use case library to transport calls to the gob/protobuf channel. The validator uses the use case library to transport calls to the gob/flatbuffer channel. The validator uses the use case library to transport calls to the gob/protobuf2 channel. The validator uses the use case library to transport calls to the gob/flatbuffer2 channel. The validator uses the use case library to transport calls to the gob/protobuf3 channel. The validator uses the use case library to transport calls to the gob/flatbuffer3 channel. The validator uses the use case library to transport calls to the gob/protobuf4 channel. The validator uses the use case library to transport calls to the gob/flatbuffer4 channel. The validator uses the use case library to transport calls to the gob/protobuf5 channel. The validator uses the use case library to transport calls to the gob/flatbuffer5 channel. The validator uses the use case library to transport calls to the gob/protobuf6 channel. The validator uses the use case library to transport calls to the gob/flatbuffer6 channel. The validator uses the use case library to transport calls to the gob/protobuf7 channel. The validator uses the use case library to transport calls to the gob/flatbuffer7 channel. The validator uses the use case library to transport calls to the gob/protobuf8 channel. The validator uses the use case library to transport calls to the gob/flatbuffer8 channel. The validator uses the use case library to transport calls to the gob/protobuf9 channel. The validator uses the use case library to transport calls to the gob/flatbuffer9 channel. The validator uses the use case library to transport calls to the gob/protobuf10 channel. The validator uses the use",
        "reference": "Fig. 6: The Paxi architecture"
    },
    "2206.10375v1-Figure1-1.png": {
        "prediction": "1. Grayscale conversion to grayscale image. 2. Pixel-wise weighting using Gaussian curve. 3. Dual-Channel CNN Stereo Pair Under-Exposed Architecture. 4. Normalized Exposedness Maps (Ej). 5. Normalized Weight Maps (Wj). 6. Reconstructed Laplacian Pyramid Map (Lj). 7. Laplacian & Median Filter. 8. Obtained Disparity Estimate. 9. Normalized Exposedness Maps (Ej). 10. Normalized Weight Maps (Wj). 11. Reconstructed Disparity Map (Dj). 12. Laplacian Pyramid Map (Lj). 13. Contrast Maps (Cj). 14. Disparity Map (Dj). 15. Laplacian Pyramid Map (Lj). 16. Reconstructed Disparity Map (Dj). 17. Normalized Weight Maps (Wj). 18. Normalized Exposedness Maps (Ej). 19. Grayscale conversion to grayscale image. 20. Pixel-wise weighting using Gaussian curve. 21. Dual-Channel CNN Stereo Pair Over-Exposed Architecture. 22. Normalized Exposedness Maps (Ej). 23. Normalized Weight Maps (Wj). 24. Reconstructed Disparity Map (Dj). 25. Laplacian Pyramid Map (Lj). 26. Contrast Maps (Cj). 27. Disparity Map (Dj). 28. Laplacian Pyramid Map (Lj). 29. Reconstructed Disparity Map (Dj). 30. Normalized Weight Maps (Wj). 31. Normalized Exposedness Maps (Ej). 32. Grayscale conversion to grayscale image. 33. Pixel-wise weighting using Gaussian curve. 34. Dual-Channel CNN Stereo Pair Impaired Exposure Architecture. 35. Normalized Exposedness Maps (Ej). 36. Normalized Weight Maps (Wj). 37. Reconstructed Disparity Map (Dj). 38. Laplacian Pyramid Map (Lj). 39. Contrast Maps (Cj). 40. Disparity Map (Dj). 41. Laplacian Pyramid Map (Lj). ",
        "reference": "Fig. 1. Overview of proposed ME2Stereo-Du2CNN architecture: The input consists of three stereo pair of the same scene captured at different camera exposure level and the output is a refined disparity map (R\u0302). The input is passed through novel Dual CNN architecture to get the disparity estimates at three exposure levels. The estimates are fused using two quality measures: well-exposedness and contrast, which in-turn are obtained respectively from the input left-view image and the disparity estimate of the corresponding stereo pair. The predicted refined disparity map (R\u0302) is a better estimate compared to three intermediate disparity estimates."
    },
    "2101.08386v4-Figure5-1.png": {
        "prediction": "1. The input to the model is a sentence with a blank space in the middle. The model predicts the probability of the next word in the sentence. 2. The model outputs a probability vector for each word in the vocabulary. 3. The model uses the probability vectors to predict the next word in the sentence. 4. The model uses the predicted word to predict the next word in the sentence. 5. The model repeats this process until the end of the sentence is reached. 6. The model outputs a probability vector for each word in the vocabulary. 7. The model uses the probability vectors to predict the next word in the sentence. 8. The model uses the predicted word to predict the next word in the sentence. 9. The model repeats this process until the end of the sentence is reached. 10. The model outputs a probability vector for each word in the vocabulary. 11. The model uses the probability vectors to predict the next word in the sentence. 12. The model uses the predicted word to predict the next word in the sentence. 13. The model repeats this process until the end of the sentence is reached. 14. The model outputs a probability vector for each word in the vocabulary. 15. The model uses the probability vectors to predict the next word in the sentence. 16. The model uses the predicted word to predict the next word in the sentence. 17. The model repeats this process until the end of the sentence is reached. 18. The model outputs a probability vector for each word in the vocabulary. 19. The model uses the probability vectors to predict the next word in the sentence. 20. The model uses the predicted word to predict the next word in the sentence. 21. The model repeats this process until the end of the sentence is reached. 22. The model outputs a probability vector for each word in the vocabulary. 23. The model uses the probability vectors to predict the next word in the sentence. 24. The model uses the predicted word to predict the next word in the sentence. 25. The model repeats this process until the end of the sentence is reached. 26. The model outputs a probability vector for each word in the vocabulary. 27. The model uses the probability vectors to predict the next word in the sentence. 28",
        "reference": "Figure 5: (Handwritten digits) Diagram of the learning model used in the handwritten digits experiment. The model is obtained by concatenating a computer vision (CV) model and an identity effect (IE) model. From right to left: The model takes a pair of images as input (right); then, the CV model classifies them or, equivalently, encodes them as two 10-dimensional probability vectors (center); finally, the IE model assigns a rating from 0 to 1 to the pair of probability (or encoding) vectors to identify whether the images represent identical digits or not (left)."
    },
    "2204.07837v2-Figure3-1.png": {
        "prediction": "1. The encoder takes in the input sequence and encodes it into a sequence of hidden states. 2. The position classifier takes the hidden states and predicts the position of the token in the input sequence. 3. The decoder takes the hidden states and generates the next token in the output sequence. 4. The token classifier takes the hidden states and predicts the token in the input sequence. 5. The loss function is computed based on the predicted positions and the actual positions. 6. The model is trained using backpropagation to minimize the loss function.",
        "reference": "Fig. 3. The illustration of our proposed self-supervised input representation (Section III-C) in sequence-to-sequence learning framework. We add two classifier to predict the token and position of perturbed tokens synthesized by the smooth augmented data generator in Section III-B. The meaning of blue rectangle and green rectangle is the same as in Figure 2. The red rectangles represent disturbed tokens\u2019 intermediate representation produced by the top layer of encoder."
    },
    "11277821-Figure1-1.png": {
        "prediction": "1. Pretraining BERT: The BERT model is pre-trained on a large corpus of text data, such as Wikipedia, to learn representations for words and phrases. The pretraining task is to predict the next word or phrase in a sentence given the previous words. The model is trained using a masked language modeling objective, where some of the words in the sentence are masked and the model is required to predict the masked words. This pretraining helps the model to learn better representations for words and phrases, which can be fine-tuned for downstream tasks such as question-answering.",
        "reference": "Figure 1: Architecture of the Wasserstein Deep Learning: two samples are drawn from the data distribution and set as input of the same network (\u03c6) that computes the embedding. The embedding is learnt such that the squared Euclidean distance in the embedding mimics the Wasserstein distance. The embedded representation of the data is then decoded with a different network (\u03c8), trained with a Kullback-Leibler divergence loss."
    },
    "2010.10246v2-Figure1-1.png": {
        "prediction": "1. The user starts by creating a component, which is a set of configurations, checkpoints/outputs, and other pipelines. 2. The component is then added to the library repository, which is a set of datasets and data cleansing. 3. The library repository is then added to the pipeline repository, which is a set of ML platforms and CNN pipelines. 4. The pipeline repository is then added to the dataset repository, which is a set of configurations, checkpoints/outputs, and other pipelines.",
        "reference": "Fig. 1. The architecture of MLCask for supporting collaborative pipeline development with version control semantics."
    },
    "2202.07638v2-Figure1-1.png": {
        "prediction": "1. The input layer of the first convolutional layer is connected to the input image. 2. The output of the first convolutional layer is connected to the input of the second convolutional layer. 3. The output of the second convolutional layer is connected to the input of the first fully connected layer. 4. The output of the first fully connected layer is connected to the input of the second fully connected layer. 5. The output of the second fully connected layer is connected to the input of the output layer. 6. The output of the output layer is connected to the input of the input layer.",
        "reference": "Fig. 1. Schematic representation of the multiplex network considered in this paper. Black nodes denote the agents while blue nodes represent the (possible) leaders."
    },
    "2101.05329v1-Figure1-1.png": {
        "prediction": "1. Uncompressed Byte Array",
        "reference": "Figure 1: Schematic illustration of the proposed algorithm."
    },
    "2110.06972v1-Figure2-1.png": {
        "prediction": "1. Interaction History: The interaction history is a sequence of actions taken by the user in the past. It is used to model the user's behavior and preferences.",
        "reference": "Figure 2: Proposed ZeUS algorithm. The components shown in green (i.e. observation encoder, context encoder, dynamics model and reward model) are shared across tasks. Components/representations in red or blue belong to separate tasks."
    },
    "2110.14705v2-Figure10-1.png": {
        "prediction": "1. The probability of flipping a spin is proportional to the size of the corresponding bubble.",
        "reference": "FIG. 10. A candidate low-order process leading to the meltdown of the order parameter. In the main text, we prove that energy resonance between the two configurations is not possible under a generic assumption of strong incommensurability of the couplings."
    },
    "2012.06275v1-Figure1-1.png": {
        "prediction": "1. Phonocardiogram (PCG) is a signal that represents the electrical activity of the heart. It is typically recorded using a microphone placed on the chest.",
        "reference": "Fig. 1. The convolutional deep autoencoder (DAE(C)) architecture."
    },
    "2205.13113v1-Figure3-1.png": {
        "prediction": "1. Training and pilot transmission for CE",
        "reference": "Fig. 3. Illustration of the training process."
    },
    "2206.10128v2-Figure1-1.png": {
        "prediction": "1. Query Generation Model: This model generates the query based on the input sentence. It uses the word embeddings to represent the words in the sentence and then uses a cross-encoder to generate the query. The cross-encoder takes the query and the sentence as inputs and generates a query representation that is used to rank the documents. The query generation model is trained using the cross-entropy loss.",
        "reference": "Figure 1: Overview of the proposed DSI-QG indexing framework, where a query generator (yellow box) is used to generate potential queries, which in turn are used to represent documents for indexing. The architecture of the method includes a cross-encoder ranker (gray box) that is used to select only promising queries to be sent for indexing."
    },
    "2202.13078v2-Figure1-1.png": {
        "prediction": "1. Input: The input to the model is a sequence of characters, each represented as a one-hot vector. The input is passed through a ResNet18 network, which is a deep convolutional neural network that is pre-trained on a large dataset. The output of the ResNet18 network is a sequence of feature vectors, each representing a character in the input sequence.",
        "reference": "Fig. 1: Model architecture used in the pretraining phase of the proposed method."
    },
    "2102.00719v1-Figure1-1.png": {
        "prediction": "1. Pretraining BERT: The model is trained on a large corpus of text data using a masked language modeling (MLM) task. The model is then fine-tuned on a downstream task, such as sentiment analysis or question-answering, using a transfer learning approach.",
        "reference": "Figure 1: Video Transformer Network architecture. Connecting three modules: A 2D spatial backbone (f(x)), used for feature extraction. Followed by a temporal attentionbased encoder (Longformer in this work), that uses the feature vectors (\u03c6i) combined with a position encoding. The [CLS] token is processed by a classification MLP head to get the final class prediction."
    },
    "2012.02670v2-Figure8-1.png": {
        "prediction": "1. The server receives the private key (Xpriv) and the public key (Xpub) from the client. 2. The server generates a random number (f) and calculates the ciphertext (Catt) using the public key (Xpub). 3. The server calculates the log of the function (log1) using the private key (Xpriv) and the ciphertext (Catt). 4. The server calculates the log of the function (log1) using the public key (Xpub) and the ciphertext (Catt). 5. The server calculates the difference (D) between the two logs. 6. The server sends the difference (D) to the client.",
        "reference": "Figure 8: Schematic representation of the training process of the server\u2019s networks for the attribute inference attack. In the figure, the network \ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61 substitutes \ud835\udc53 \u22121 and \ud835\udc52\ud835\udc5b refers to a suitable entropy measure for the classification task."
    },
    "2205.10688v1-Figure2-1.png": {
        "prediction": "1. Agent Policy: Define and constrain the policy. 2. PPO Training: Single-agent, step-size policy. 3. Create variants: Create variants of the policy. 4. PPO Training: Group of agents, universal policy. 5. Selection: Select the best policy. 6. Crossover and mutation: Crossover and mutation. 7. New Generation: New generation of agents. 8. Output: Output the new generation of agents.",
        "reference": "Fig. 2. Overview: The agent is either generated randomly or with user support. The user also defined constraints (yellow arrows) (a). The initial Proximal Policy Optimization (PPO) trains the input agent to provide baseline agent policy (b). The system then creates variants of the initial model (c) and trains them all together with universal PPO (d). Selection (e), crossover, and mutation (f) create a new generation that is trained again. The system outputs the best(s) co-designed agents and their policies (g)."
    },
    "1241810-Figure5-1.png": {
        "prediction": "1. Weight and bias training: The model is trained using weight and bias parameters.",
        "reference": "Fig. 5. The framework of proposed bias tuning method."
    },
    "2011.12453v1-Figure1-1.png": {
        "prediction": "1. The input to the model is a sequence of tokens, where each token is a word or a subword. The model is trained to predict the next token in the sequence based on the previous tokens.",
        "reference": "Figure 1: (a) Illustration of graph neural ansatz. (b) Distributed workflow of VMC using ISGO."
    },
    "2202.04161v1-Figure3-1.png": {
        "prediction": "1. Structured Context: The structured context is a set of structured information that is provided to the model. It includes the name, price, rating, and flavor of the item, as well as the name and price of the item that is being compared to the item in the structured context. The structured context is used to provide additional information to the model that can help it make more accurate predictions.",
        "reference": "Figure 3: Task structure for the generative model."
    },
    "2107.00466v1-Figure1-1.png": {
        "prediction": "1. The input image is resized to a smaller size, L0, using a factor of 6. The resized image is then divided by the factor of 6 to obtain the resized image, L1.",
        "reference": "FIG. 1. Illustration of the inverse renormalization group. Inverse transformations are applied to iteratively increase the size L in each dimension by a factor of b, therefore evading the critical slowing down effect."
    },
    "2204.00034v1-Figure1-1.png": {
        "prediction": "1. The user starts by creating a list of tasks. 2. The user then assigns a priority to each task. 3. The user then assigns a deadline to each task. 4. The user then assigns a reward to each task. 5. The user then assigns a penalty to each task. 6. The user then assigns a weight to each task. 7. The user then assigns a cost to each task. 8. The user then assigns a value to each task. 9. The user then assigns a risk to each task. 10. The user then assigns a confidence to each task. 11. The user then assigns a probability to each task. 12. The user then assigns a likelihood to each task. 13. The user then assigns a likelihood ratio to each task. 14. The user then assigns a likelihood ratio test to each task. 15. The user then assigns a likelihood ratio test statistic to each task. 16. The user then assigns a likelihood ratio test p-value to each task. 17. The user then assigns a likelihood ratio test critical value to each task. 18. The user then assigns a likelihood ratio test confidence interval to each task. 19. The user then assigns a likelihood ratio test confidence interval to each task. 20. The user then assigns a likelihood ratio test confidence interval to each task. 21. The user then assigns a likelihood ratio test confidence interval to each task. 22. The user then assigns a likelihood ratio test confidence interval to each task. 23. The user then assigns a likelihood ratio test confidence interval to each task. 24. The user then assigns a likelihood ratio test confidence interval to each task. 25. The user then assigns a likelihood ratio test confidence interval to each task. 26. The user then assigns a likelihood ratio test confidence interval to each task. 27. The user then assigns a likelihood ratio test confidence interval to each task. 28. The user then assigns a likelihood ratio test confidence interval to each task. 29. The user then assigns a likelihood ratio test confidence interval to each task. 30. The user then assigns a likelihood ratio test confidence interval to each task. 31. The user then assigns a likelihood ratio test confidence interval to each task. 32. The user",
        "reference": "Fig. 1. Schematic comparison of sequential proof-of-work (Bitcoin, left) and parallel proof-of-work (proposed, right). Boxes represent blocks, checkmarks represent proof-of-work solutions, and arrows represent hash-references."
    },
    "1508199-Figure2-1.png": {
        "prediction": "1. Spectral Clustering: Assigns each data point to a cluster based on its similarity to other data points in the same cluster. 2. PCA: Principal Component Analysis: A linear transformation that projects the data into a lower-dimensional space while preserving as much of the variance as possible. 3. Individual Clusters: Assigns each data point to a cluster based on its similarity to other data points in the same cluster. 4. Select Point: Selects a point from the data set that is representative of the cluster. 5. Impute Label Information: Assigns a label to each data point based on its similarity to the representative point. 6. Assign z1 to Certain Set: Assigns z1 to the set of data points that are likely to be human. 7. Yes: Assigns z1 to the set of data points that are likely to be human. 8. No: Assigns z1 to the set of data points that are likely to be non-human. 9. Select Query Human: Selects a data point from the set of data points that are likely to be human.",
        "reference": "Figure 2. Diagram of SUPERPAC algorithm for pairwise constrained clustering."
    },
    "2012.12683v1-Figure1-1.png": {
        "prediction": "1. The architecture of the deployment network.",
        "reference": "Figure 1. Left: Network control architecture of SOXS. Right: Components of the SOXS software; red boxes represent software requiring custom configuration or development, green boxes represent VLTSW components that will be used without modifications."
    },
    "2103.07018v1-Figure1-1.png": {
        "prediction": "1. Interleaving learning: The learner is exposed to a sequence of tasks, each task is presented in a random order. The learner is allowed to take breaks between tasks. 2. Block learning: The learner is exposed to a sequence of tasks, each task is presented in a fixed order. The learner is not allowed to take breaks between tasks.",
        "reference": "Figure 1: Comparison between interleaving learning and block learning. In interleaving learning, we perform task 1 for a short while, then move to task 2, then task 3. Afterwards, we move from task 3 back to task 1. This process iterates where each task is performed for a short time period before switching to another task. In contrast, in block learning, we perform task 1 to the very end, then move to task 2, and later task 3. Each task is performed for a long time period until it is completed. Once a task is finished, it will not be performed again."
    },
    "2107.02704v1-Figure3-1.png": {
        "prediction": "1. Pretraining BERT: The architecture of BERT in pretraining. Assume that in the lowercased example sequence consisting of the segment pair 'he starts to speak. the nervous crowd is watch-ing him.' the tokens 'speak' and 'nervous' were sampled to be masked. 'speak' is replaced by the '[MASK]' token and 'nervous' is replaced by the random token 'that'. The model's task is to predict the tokens 'speak' and 'nervous' from the representation vectors it learns at the positions of the input embeddings of '[MASK]' and 'that'. P(B follows A) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Figure 3: Training Data simulation steps. We estimated tissue parameters p from a large 12-contrast FLASH MRI acquisition containing multiple flip angles (FA) and echo time (TE) scanner parameters for 22 different ex vivo human brain specimens using a dictionary-based method [20]. Estimated tissue parameters were used as ground truth to simulate training data."
    },
    "2202.06076v1-Figure2-1.png": {
        "prediction": "1. Preprocessing: The input data is preprocessed to remove any irrelevant information and convert it into a format that can be used by the model. This includes tokenization, stemming, and lemmatization.",
        "reference": "Fig. 2: The overview of method. We extend a multi-layer transformer pre-trained on textual data with imaging input. The images are provided as features extracted from a ResNet50 network. The features are reshaped to 49 vectors of 2048 dimensions each and combined with two embeddings describing segment (image or text) and position of the token."
    },
    "2010.00150v1-Figure3-1.png": {
        "prediction": "1. The encoder takes in the input sequence and encodes it into a sequence of hidden states. 2. The attention layer attends to the hidden states of the encoder and generates a weighted sum of the hidden states. 3. The decoder takes in the attention weights and generates a sequence of hidden states. 4. The decoder generates the next hidden state by taking into account the attention weights and the hidden states of the encoder. 5. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 6. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 7. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 8. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 9. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 10. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 11. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 12. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 13. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 14. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 15. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 16. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 17. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 18. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 19. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 20. The decoder generates the next word in the sequence by taking into account the hidden state and the attention weights. 21. The decoder generates the next word in the sequence by taking into account the",
        "reference": "Figure 3: Attentional Encoder-Decoder architecture with each supervision method shown."
    },
    "2205.00186v2-Figure2-1.png": {
        "prediction": "1. The CNN model is trained on the augmented data using the softmax loss function. 2. The CNN model is trained on the augmented data using the MSE loss function. 3. The CNN model is trained on the augmented data using the CE loss function. 4. The CNN model is trained on the augmented data using the H-Avg loss function. 5. The CNN model is trained on the augmented data using the S&W Avg loss function. 6. The CNN model is trained on the augmented data using the S&W Avg loss function. 7. The CNN model is trained on the augmented data using the S&W Avg loss function. 8. The CNN model is trained on the augmented data using the S&W Avg loss function. 9. The CNN model is trained on the augmented data using the S&W Avg loss function. 10. The CNN model is trained on the augmented data using the S&W Avg loss function. 11. The CNN model is trained on the augmented data using the S&W Avg loss function. 12. The CNN model is trained on the augmented data using the S&W Avg loss function. 13. The CNN model is trained on the augmented data using the S&W Avg loss function. 14. The CNN model is trained on the augmented data using the S&W Avg loss function. 15. The CNN model is trained on the augmented data using the S&W Avg loss function. 16. The CNN model is trained on the augmented data using the S&W Avg loss function. 17. The CNN model is trained on the augmented data using the S&W Avg loss function. 18. The CNN model is trained on the augmented data using the S&W Avg loss function. 19. The CNN model is trained on the augmented data using the S&W Avg loss function. 20. The CNN model is trained on the augmented data using the S&W Avg loss function. 21. The CNN model is trained on the augmented data using the S&W Avg loss function. 22. The CNN model is trained on the augmented data using the S&W Avg loss function. 23. The CNN model is trained on the augmented data using the S&W Avg loss function. 24. The CNN model is trained on the augmented data using the S&W Avg loss function. 25. The CNN model is trained on the",
        "reference": "Figure 2: An overview of the proposed LC-Booster framework. We first warm up the model for a few iterations, and then fit a GMM to the loss distribution to separate clean or noisy sets. We then adopt H-Aug, which enforces consistency between weak-weak and weak-strong views. At the middle of training, we perform ReCo to revise the labels for all samples. The revised labels are used for GMM and CE Loss in the rest of training."
    },
    "211532678-Figure1-1.png": {
        "prediction": "1. The block diagram of the system is shown below.",
        "reference": "Figure 1. Adversarial training framework of image reconstruction network f , jointly with another network G, generating the additive perturbations"
    },
    "14194315-Figure2-1.png": {
        "prediction": "1. The input video is split into frames. 2. Each frame is processed by a CNN to extract features. 3. The features are concatenated and passed through a fully connected layer to produce a vector representation of the video. 4. The vector representation is used as input to a softmax layer to produce a probability distribution over the labels. 5. The predicted labels are used to generate the output video.",
        "reference": "Figure 2: Schematic representation of deep neural networks for supervised and unsupervised learning. We use pink boxes to denote convolutional layers, yellow boxes denote rectified linear unit layers and green boxes indicate normalization layers. Our technique combines unsupervised learning approaches (a) with labeled prediction (b) to predict gestures using massive amounts of unlabeled data and few labeled samples. Autoencoder (a) is used to initialize weights and then predictor (b) is fine tuned to predict labels."
    },
    "2207.06543v1-Figure3-1.png": {
        "prediction": "1. Memory stability over old tasks: The model learns to maintain the same behavior for similar tasks. This is achieved by storing the parameters of the model in memory and using them to predict the next token. 2. Discrepancy between task distributions: The model learns to predict the next token based on the current context and the previous tokens. This is achieved by minimizing the discrepancy between the task distributions. 3. Robust empirical loss (new): The model learns to predict the next token based on the current context and the previous tokens, while also considering the distribution of the task. This is achieved by using a robust empirical loss function that takes into account the distribution of the task. 4. Learning plasticity over new tasks: The model learns to adapt to new tasks by using a learning plasticity mechanism that adjusts the parameters of the model based on the distribution of the task. This is achieved by using a robust empirical loss function that takes into account the distribution of the task.",
        "reference": "Fig. 3. Illustration of simultaneously promoting learning plasticity and memory stability in continual learning, where arrows represent the tightening process."
    },
    "2012.10294v4-Figure1-1.png": {
        "prediction": "1. Normalized gray matter 3D volumes\n2. 20-fold cross-validation\n3. Convolutional network with age and sex as TV (with age + sex + TV)\n4. 3D convolution 5x3x3, ReLU\n5. Batch Normalization 2x2x2\n6. 3D convolution 3x3x3, ReLU\n7. Batch Normalization 2x2x2\n8. 3D convolution 3x3x3, ReLU\n9. Batch Normalization 2x2x2\n10. 3D convolution 3x3x3, ReLU\n11. Batch Normalization 2x2x2\n12. 3D convolution 3x3x3, ReLU\n13. Batch Normalization 2x2x2\n14. 3D convolution 3x3x3, ReLU\n15. Batch Normalization 2x2x2\n16. 3D convolution 3x3x3, ReLU\n17. Batch Normalization 2x2x2\n18. 3D convolution 3x3x3, ReLU\n19. Batch Normalization 2x2x2\n20. 3D convolution 3x3x3, ReLU\n21. Batch Normalization 2x2x2\n22. 3D convolution 3x3x3, ReLU\n23. Batch Normalization 2x2x2\n24. 3D convolution 3x3x3, ReLU\n25. Batch Normalization 2x2x2\n26. 3D convolution 3x3x3, ReLU\n27. Batch Normalization 2x2x2\n28. 3D convolution 3x3x3, ReLU\n29. Batch Normalization 2x2x2\n30. 3D convolution 3x3x3, ReLU\n31. Batch Normalization 2x2x2\n32. 3D convolution 3x3x3, ReLU\n33. Batch Normalization 2x2x2\n34. 3D convolution 3x3x3, ReLU",
        "reference": "Figure 1 Data flow chart and convolutional neural network structure."
    },
    "2103.12279v1-Figure2-1.png": {
        "prediction": "1. The model takes in a sentence as input and encodes it into a vector representation.",
        "reference": "Figure 2: Model Architecture: Our architecture comprises a base encoder that encodes the input and its relative non-terminals. GIL then uses MIPS to retrieve the most influential concepts that globally explain the sample, while LIL computes a relevance score for each ntj that quantifies its relevance to predict the label. The model interpretability is enforced through regularization (example parse tree inspired from Zanzotto et al. (2020))."
    },
    "1189033-Figure2-1.png": {
        "prediction": "1. The figure shows the overall architecture of the proposed method for online action tube generation. The input is a video frame, which is processed by a detection network to obtain the bounding boxes of people in the scene. The bounding boxes are then passed through a flow estimation network to estimate the optical flow between adjacent frames. The optical flow is then used to generate the action tube. The flow estimation network is trained using a combination of appearance and flow information. The flow information is obtained from the optical flow estimated by the detection network. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is trained using a combination of appearance and flow information. The flow estimation network is",
        "reference": "Figure 2. At test time, the input to the framework is a sequence of RGB video frames (a). A real-time optical flow (OF) algorithm (b) [16] takes the consecutive RGB frames as input to produce flow images (d). As an option, (c) a more accurate optical flow algorithm [1] can be used (although not in real time). (e) RGB and OF images are fed to two separate SSD detection [22] networks (\u00a7 3.2). (f) Each network outputs a set of detection boxes along with their class-specific confidence scores (\u00a7 3.2). (g) Appearance and flow detections are fused (\u00a7 3.3). Finally (h), multiple action tubes are built up in an online fashion by associating current detections with partial tubes (\u00a7 3.4)."
    },
    "2207.05544v1-Figure5-1.png": {
        "prediction": "1. User: The user interacts with the system by using the laser sensor and wheel sensors to gather data.",
        "reference": "Fig. 5: Robot navigation system overview"
    },
    "2204.12527v1-Figure1-1.png": {
        "prediction": "1. The generator generates fake interaction vectors based on the condition vector and the database. 2. The discriminator compares the real interaction vector with the fake interaction vector generated by the generator. 3. The discriminator outputs a probability value for the real interaction vector. 4. The generator adjusts the condition vector based on the discriminator's output to generate more realistic fake interaction vectors. 5. The generator continues to generate fake interaction vectors until the discriminator cannot distinguish between the real and fake interaction vectors.",
        "reference": "Fig. 1. Overview of our recommender system (CFWGAN-GP)"
    },
    "2204.00352v1-Figure1-1.png": {
        "prediction": "1. Self-supervised learning: The model learns to predict the next token in a sequence of tokens from the input sequence. This is done by masking a portion of the input sequence and predicting the masked tokens. The model is trained to minimize the difference between the predicted and actual masked tokens. This process is repeated for multiple masked portions of the input sequence. The model learns to capture the underlying patterns in the input sequence.",
        "reference": "Figure 1: The pipeline of our methods. First, we will decide whether to initial the encoder with the pre-trained self-supervised learning model and fix the parameters or not. Then, we will train our model using meta-learning algorithms. The white module blocks represent the randomly initialized parameters and the blue encoder block refer to the model pre-trained from SSL."
    },
    "2110.08607v1-Figure4-1.png": {
        "prediction": "1. The forward RNN takes the input sequence and passes it through the RNN layers. The output of the RNN is the hidden state h1.",
        "reference": "Figure 4: Inference neural network for PgDMM: The approximate inference model qi\u03c6(z i 1:T |x1:T ) for zi1:T , where i stands for either phy or NN, since the structure of both inference models is the same, is derived using bidirectional recurrent neural networks (BRNNs). The BRNN admits x1:T as inputs and - through a series of hidden layers of the neural network, denoted by blue arrows - forms two sequences of deterministic hidden states hft and h b t . These express the flow of information in the forward (from left to right) and backward (from right to left) sense, respectively. Then hft and h b t , together with the previous latent state z i t\u22121, are fed into a further neural network NN i\u03c6, denoted by the red arrows. Finally, the inference network outputs two vectors \u00b5i\u03c6(z i t\u22121,x) and \u03a3 i \u03c6(z i t\u22121,x), which parameterize the mean and diagonal covariance of qi\u03c6(z i t|zit\u22121,x), as shown in Eq.(14a)."
    },
    "2011.14771v1-Figure6-1.png": {
        "prediction": "1. The figure shows a ramp with a slope of \u03b8, height of h, and width of w. The ramp is divided into two parts, part A and part B. Part A is a triangle with base of length d and height of h. Part B is a rectangle with length of w and height of h. The ramp is tilted at an angle of \u03b8. The figure also shows a point O on the ramp, and a point P on the ramp at a distance s from O. The figure shows the direction of the ramp's slope as an arrow pointing up and to the right. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the left. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the right. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the left. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the right. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the left. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the right. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the left. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the right. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the left. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the right. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the left. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the right. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the left. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the right. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the left. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the right. The figure also shows the direction of the ramp's incline as an arrow pointing up and to the left. The figure also shows",
        "reference": "Fig. 6. Schematic illustration of the silicon master cross-section with \ud835\udeff = 29.5\u25e6 as the blaze angle and \ud835\udf03 \u2248 70.5\u25e6 defined by the crystal structure of silicon. At a groove spacing of \ud835\udc51 / 160 nm, the flat-top regions have widths \ud835\udc64 ' 30 nm as a result of the etch undercut while the groove depth is \u210e \u2248 65 \u2212 70 nm by Eq. (3). Indicated by the circle, the indented portion of the etched topography cannot be described with a functional form for diffraction-efficiency analysis."
    },
    "1023605-Figure10-1.png": {
        "prediction": "1. The input is passed through a convolutional layer with 32 filters, a ReLU activation, and a max pooling layer with a 2x2 window. The output of this layer is passed through a fully connected layer with 256 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this layer is passed through a fully connected layer with 32 neurons and a ReLU activation. The output of this",
        "reference": "Figure 10. The schema for 35 \u00d7 35 grid (Inception-ResNet-A) module of Inception-ResNet-v1 network."
    },
    "1011245-Figure6-1.png": {
        "prediction": "1. The input layer receives the input sequence of words. 2. The convolutional layer applies a 3x75 filter to each word in the sequence, resulting in a 3x75 matrix of feature vectors. 3. The max-pooling layer reduces the size of the feature vectors to 1x75, and the dropout layer randomly drops out some of the feature vectors. 4. The linear layer applies a 3x75 matrix of weights to the feature vectors, resulting in a 3x75 matrix of logits. 5. The output layer applies a softmax activation function to the logits, resulting in a 3x75 matrix of probabilities. 6. The cross-entropy loss is calculated between the predicted probabilities and the true labels. 7. The model is trained using backpropagation to minimize the cross-entropy loss. 8. The model is evaluated on a test set of sequences, and the predicted probabilities are used to calculate the accuracy.",
        "reference": "Figure 6: 2D object shape classification network architecture."
    },
    "2102.06872v1-Figure1-1.png": {
        "prediction": "1. The program and test suite are run. 2. The classifier generates new configurations. 3. The classifier determines if new trees are needed. 4. If new trees are needed, the post-processing is performed. 5. If new trees are not needed, the post-processing is skipped. 6. The final interactions are stored in the final interactions array.",
        "reference": "Fig. 1: GenTree overview"
    },
    "2107.01496v2-Figure2-1.png": {
        "prediction": "1. Repeated Negotiation Traces Dataset: This dataset contains repeated negotiation traces, where each trace is a sequence of negotiation steps. The negotiation steps are represented as tuples of (strategy, result) pairs, where strategy is a string representing the strategy used in the negotiation and result is a string representing the outcome of the negotiation. The dataset is used to train the negotiation trace recognition model.",
        "reference": "Fig. 2. The overall approach of strategy recognition."
    },
    "2110.14532v2-Figure2-1.png": {
        "prediction": "1. Fact checkers check the accuracy of the tweet or text. 2. Hoaxes from fact checkers are identified. 3. The tweet or text is sent to a natural language inference model, which uses a list of semantic similarities to determine if the tweet or text is similar to any of the hoaxes. 4. If the tweet or text is similar to a hoax, the model outputs a list of the top related hoaxes. 5. The model outputs a list of the top related hoaxes. 6. The model outputs a list of the top related hoaxes. 7. The model outputs a list of the top related hoaxes. 8. The model outputs a list of the top related hoaxes. 9. The model outputs a list of the top related hoaxes. 10. The model outputs a list of the top related hoaxes. 11. The model outputs a list of the top related hoaxes. 12. The model outputs a list of the top related hoaxes. 13. The model outputs a list of the top related hoaxes. 14. The model outputs a list of the top related hoaxes. 15. The model outputs a list of the top related hoaxes. 16. The model outputs a list of the top related hoaxes. 17. The model outputs a list of the top related hoaxes. 18. The model outputs a list of the top related hoaxes. 19. The model outputs a list of the top related hoaxes. 20. The model outputs a list of the top related hoaxes. 21. The model outputs a list of the top related hoaxes. 22. The model outputs a list of the top related hoaxes. 23. The model outputs a list of the top related hoaxes. 24. The model outputs a list of the top related hoaxes. 25. The model outputs a list of the top related hoaxes. 26. The model outputs a list of the top related hoaxes. 27. The model outputs a list of the top related hoaxes. 28. The model outputs a list of the top related hoaxes. 29. The model outputs a list of the top related hoaxes. 30. The model outputs a list of the top related ho",
        "reference": "Figure 2: Architecture for the evaluation of information pieces against hoaxes already identified by fact checkers. A first step allows to retrieve hoaxes that are semantically similar to the input text. In the second step, a Natural Language Inference model measures the degree of entailment against each hoax retrieved in step 1."
    },
    "2107.13826v1-Figure6-1.png": {
        "prediction": "1. A block diagram of the process flow in a typical methanol synthesis plant.",
        "reference": "Figure 6. Model overview: a) Flowchart of chlor-alkali process model, dashed controllers are removed from model and associated manipulated variables are used as input variables in sampling algorithm. b) Structure of used recurrent neural networks. Each output is modeled separately. Parameters N and O are determined in hyperparameter tuning."
    },
    "2110.11334v1-Figure3-1.png": {
        "prediction": "1. Density-based reconstruction-based classification-based anomaly detection & one-class novelty detection (DBSCAN, KPCA, OCC, ISOFST, DeepSVDD, GPND, DAGMM, ADGAN, CoRa, MemAE, OCCAN, GradAD, HighRes, RSAD, SCADN, LREN, KPCA, OCC, ISOFST, DeepSVDD, GPND, DAGMM, ADGAN, CoRa, MemAE, OCCAN, GradAD, HighRes, RSAD, SCADN, LREN, KPCA, OCC, ISOFST, DeepSVDD, GPND, DAGMM, ADGAN, CoRa, MemAE, OCCAN, GradAD, HighRes, RSAD, SCADN, LREN, KPCA, OCC, ISOFST, DeepSVDD, GPND, DAGMM, ADGAN, CoRa, MemAE, OCCAN, GradAD, HighRes, RSAD, SCADN, LREN, KPCA, OCC, ISOFST, DeepSVDD, GPND, DAGMM, ADGAN, CoRa, MemAE, OCCAN, GradAD, HighRes, RSAD, SCADN, LREN, KPCA, OCC, ISOFST, DeepSVDD, GPND, DAGMM, ADGAN, CoRa, MemAE, OCCAN, GradAD, HighRes, RSAD, SCADN, LREN, KPCA, OCC, ISOFST, DeepSVDD, GPND, DAGMM, ADGAN, CoRa, MemAE, OCCAN, GradAD, HighRes, RSAD, SCADN, LREN, KPCA, OCC, ISOFST, DeepSVDD, GPND, DAGMM, ADGAN, CoRa, MemAE, OCCAN, GradAD, HighRes, RSAD, SCADN, LREN, KPCA, OCC, ISOFST, DeepSVDD, GPND, DAGMM, ADGAN, CoRa, MemAE, OCCAN, GradAD, HighRes, RSAD, SCADN, LREN, KPCA, OCC, ISOFST, DeepSVDD, GPND, DAGMM, ADGAN, CoRa, MemAE, OCCAN, GradAD, HighRes, RSAD, SCADN, LREN, KPCA, OCC, ISOFST, DeepSV",
        "reference": "Fig. 3: Timeline for representative methodologies of (a) anomaly detection & one-class novelty detection, details in Section 3, (b) multi-class novelty detection & open set recognition, details in Section 4, and (c) OOD detection, details in Section 5, under generalized OOD detection framework. Different colors indicate different categories of methodologies. Each method has its corresponding reference (inconspicuous white) in the lower right corner. We do not list outlier detection methods in this figure due to their limited number of works on computer vision in deep learning era."
    },
    "822804-Figure1-1.png": {
        "prediction": "1. CNN: Convolutional Neural Network. 2. NTN: Non-linear Transformation Network. 3. Euclidean Cosine Similarity. 4. Element-wise Subtraction. 5. Element-wise Multiplication.",
        "reference": "Figure 1: The left hand side is an overview of the model. The right hand side shows the details about the different comparison functions. The rectangles in dark represent parameters to be learned. \u00d7 represents matrix multiplication."
    },
    "1576593-Figure4-1.png": {
        "prediction": "1. Data augmentation: Generate new training data by randomly replacing words in the abstract sentences with synonyms. This helps to increase the diversity of the training data and improve the model's generalization ability.",
        "reference": "Figure 4: An overview of our approach for utilizing abstract examples for data augmentation and model training."
    },
    "2011.02573v1-Figure8-1.png": {
        "prediction": "1. The figure shows a simple neural network architecture with two hidden layers. The input layer has two neurons, and the output layer has one neuron. The hidden layers have the same number of neurons as the input layer. The figure also shows the activation functions used in each layer.",
        "reference": "Fig. 8 A general appraisal-emotion network with k appraisal variables and l emotion types."
    },
    "2011.02573v1-Figure7-1.png": {
        "prediction": "1. Appraisal Variables: Desirability, Praiseworthiness, Unexpectedness, and Gratitude",
        "reference": "Fig. 7 An weighted appraisal-emotion network showing many-to-many relationship between appraisal variables and emotions"
    },
    "213980-Figure3-1.png": {
        "prediction": "1. The image is processed by the noisy image generator. 2. The noisy image is processed by the noisy image generator. 3. The noisy image is processed by the noisy image generator. 4. The noisy image is processed by the noisy image generator. 5. The noisy image is processed by the noisy image generator. 6. The noisy image is processed by the noisy image generator. 7. The noisy image is processed by the noisy image generator. 8. The noisy image is processed by the noisy image generator. 9. The noisy image is processed by the noisy image generator. 10. The noisy image is processed by the noisy image generator. 11. The noisy image is processed by the noisy image generator. 12. The noisy image is processed by the noisy image generator. 13. The noisy image is processed by the noisy image generator. 14. The noisy image is processed by the noisy image generator. 15. The noisy image is processed by the noisy image generator. 16. The noisy image is processed by the noisy image generator. 17. The noisy image is processed by the noisy image generator. 18. The noisy image is processed by the noisy image generator. 19. The noisy image is processed by the noisy image generator. 20. The noisy image is processed by the noisy image generator. 21. The noisy image is processed by the noisy image generator. 22. The noisy image is processed by the noisy image generator. 23. The noisy image is processed by the noisy image generator. 24. The noisy image is processed by the noisy image generator. 25. The noisy image is processed by the noisy image generator. 26. The noisy image is processed by the noisy image generator. 27. The noisy image is processed by the noisy image generator. 28. The noisy image is processed by the noisy image generator. 29. The noisy image is processed by the noisy image generator. 30. The noisy image is processed by the noisy image generator. 31. The noisy image is processed by the noisy image generator. 32. The noisy image is processed by the noisy image generator. 33. The noisy image is processed by the noisy image generator. 34. The noisy image is processed by the noisy image generator. ",
        "reference": "Figure 3. Ladder network architectures Rasmus et al. (2015). :"
    },
    "2102.05963v1-Figure4-1.png": {
        "prediction": "1. Z-NBRDF: A Z-NBRDF model is used to generate the normal map. The normal map is then converted to a normal vector using the Z-NBRDF model parameters.",
        "reference": "Figure 4: Scheme for quick computation of inverse CDF from an NBRDF: we train a network to map from latent NBRDF embeddings to importance sampling parameters of a chosen analytic BRDF model."
    },
    "2011.05452v1-Figure2-1.png": {
        "prediction": "1. The figure shows the difference between the contiguous and non-contiguous cases of the Hopf bifurcation. In the contiguous case, the Hopf bifurcation occurs at a single point, while in the non-contiguous case, the Hopf bifurcation occurs at two points.",
        "reference": "Figure 2. Schematic of relation between parts of the AKLT state and the corresponding entanglement Hamiltonian. The entanglement Hamiltonian for a part of the state is similar to the Heisenberg Hamiltonian between the effective spin-1/2 particles at the boundaries of the blocks. The green line in Hint represents couplings between interblock effective spin-1/2 particles (J(2)E ), while the red lines represent the coupling between intra-block spins (J(1)E )."
    },
    "909872-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of a convolutional neural network (CNN) with residual learning. The input image is passed through several convolutional layers, which extract features from the image. The output of each convolutional layer is passed through a ReLU activation function, which prevents the network from getting stuck in local minima. The output of the convolutional layers is then passed through a series of fully connected layers, which are connected to the output layer. The output of the network is the predicted class of the input image.",
        "reference": "Fig. 2. (a) The flowchart of the sub-band reconstruction for image super-resolution. (b) A relaxed version of (a). Gi is set as the element-wise summation function. In this framework, only the MSE loss is used to constrain the recovery. (c) The deep network designed with the intuition of (b). Gi is the element-wise summation function and Fi is modeled by two layer convolutions."
    },
    "126819-Figure2-1.png": {
        "prediction": "1. DeepSurvey architecture",
        "reference": "Fig. 2 DeepSurvey architecture: ( ) shows the actual number of papers and the number of ideas and implementations."
    },
    "211678216-Figure8-1.png": {
        "prediction": "1. Input: X-ray image of a dog's chest. 2. Preprocessing: Normalization, resizing, and cropping. 3. Convolutional layers: 7x7, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3, 3x3,",
        "reference": "Fig. 8. Structural design for the model of vanilla CNN"
    },
    "2012.00020v3-Figure1-1.png": {
        "prediction": "1. The figure shows the time evolution of a system undergoing a measurement process. The initial state is represented by the point (A), and the measurement process is represented by the point (C). The time evolution is described by the equations for turn-on interactions, interactions, and measurement (C). The initial state is prepared by the point (A).",
        "reference": "FIG. 1. Overview of the general algorithm to quantum compute high energy scattering cross-sections, including the values of the bare couplings \u03bb and m for simulation time t. Initial state preparation is discussed in Section IV A, time evolution in Section IV B, and measurement of particle cross-sections in Section IV C. The choice of (renormalized) couplings \u03bb(t), m(t) is discussed in Section IV D."
    },
    "2012.00020v3-Figure3-1.png": {
        "prediction": "1. The input sequence is divided into overlapping windows of size L. Each window is represented by a vector of length L. The vector represents the sequence of values in the window. The vector is called a window vector.",
        "reference": "FIG. 3. Overview of the time evolution scheme for one Trotter-Suzuki step \u03b4. Here S (discussed in Section IV B 2) and qFT (discussed in Section IV B 3) denote the squeezing operation and the quantum Fourier transformation, respectively."
    },
    "2011.06125v3-Figure2-1.png": {
        "prediction": "1. Feature Extraction: Extract features from the raw data using a feature extraction module. 2. Concatenation: Concatenate the extracted features with the statistical data. 3. Forecasting: Use a forecasting model to predict the intensity or track forecast of the storm.",
        "reference": "Fig. 2. Representation of our multimodal machine learning framework using the two data sources: statistical and reanalysis maps. During Step 1, we extract embeddings from the reanalysis maps. In particular, we use encoder-decoder architectures or tensor decomposition to obtain a one-dimensional representation. During Step 2, we concatenate the statistical data with the features extracted from the reanalysis maps. During Step 3, we train one XGBoost model for each of the prediction tasks: intensity in 24 h, latitude displacement in 24 h, and longitude displacement in 24 h."
    },
    "2202.10332v1-Figure2-1.png": {
        "prediction": "1. Data source: Consuming SQL databases and Microsoft .NET Portals. 2. Azure Blob Storage / ML Storage Account: Stores data in the form of blobs. 3. Azure ML Service: Provides a REST API for data ingestion and model training. 4. Apigee API Gateway: Provides a REST API for data ingestion and model training. 5. Azure Kubernetes Service: Provides a containerized environment for deploying and managing the model.",
        "reference": "Figure 2: Azure ML Deployment Solution Architecture"
    },
    "2202.09110v1-Figure1-1.png": {
        "prediction": "1. User input: User provides the input to the model.",
        "reference": "Fig. 1. Flowchart outlining the developed framework."
    },
    "2101.08122v1-Figure1-1.png": {
        "prediction": "1. Self-supervised pretraining: The model is trained on a set of patches from the image. The patches are selected from the image using a CNN layer. The model learns to predict the next patch in the image based on the previous patches. The model is trained using a pretext task output. 2. Change detection: The model is trained on a set of images. The model is trained to predict the change map between two images. The change map is a representation of the changes in the image. The model is trained using a classifier.",
        "reference": "Fig. 1: Overview of the methodology."
    },
    "2011.06236v3-Figure4-1.png": {
        "prediction": "1. Figure 1: Closed-loop control system with a feedback loop.",
        "reference": "Fig. 4: Block diagram of the proposed adaptive force-based controller."
    },
    "2110.07525v2-Figure1-1.png": {
        "prediction": "1. Edge Intelligence: The edge intelligence is the processing and analysis of data that is collected at the edge of the network, such as in the base stations or in the mobile devices. This allows for faster and more efficient processing of data, as it is not sent to the central data center for processing.",
        "reference": "Fig. 1. ORAN architecture with distributed controllers located at CU and DU/RU, and intelligence controller RIC"
    },
    "2102.03357v1-Figure16-1.png": {
        "prediction": "1. The user provides the logic library, synthesis configurations, and synthesis features. 2. The system filters the circuit features and the FUB library. 3. The system filters the FUB library. 4. The system proposes a flow based on the baseline and the proposed flow.",
        "reference": "Fig. 16. The flow of proposed method in [97]."
    },
    "2204.07810v1-Figure6-1.png": {
        "prediction": "1. Model Training: The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS. The model is trained using the data from the DNS.",
        "reference": "Fig. 6 Model training and prediction flow chart"
    },
    "2204.07810v1-Figure3-1.png": {
        "prediction": "1. The flowchart shows the process of using the RANS model to solve the Navier-Stokes equations. The RANS model is a simplified version of the Navier-Stokes equations that is used to predict the flow field in a fluid. The process starts with the baseline RANS model, which is a simplified model that is used to predict the flow field in a fluid. The next step is to converge the mean flow, which is the average flow field in the fluid. The mean flow is then used to predict the stress in the fluid. The final stress is then used to predict the final flow field in the fluid. The process is then repeated to converge the mean flow and predict the stress in the fluid. The final flow field is then predicted using the converged mean flow and stress.",
        "reference": "Fig. 3 CFD solution process of the iterative framework"
    },
    "2110.03183v1-Figure1-1.png": {
        "prediction": "1. The input to the network is a sequence of audio frames. 2. The network extracts features from each frame, such as the frequency and amplitude of the sound. 3. The network then uses these features to predict the next frame in the sequence. 4. The network can also be trained to predict the next frame in a sequence of audio frames, which can be used for speech recognition or other audio processing tasks.",
        "reference": "Fig. 1. Figure capturing our proposed method and learning different codes for different representations of mel-spectogram"
    },
    "2202.12108v1-Figure2-1.png": {
        "prediction": "1. Preprocessing: The image is resized to 224x224 and converted to grayscale.",
        "reference": "Fig. 2: Overview of the proposed fusion method."
    },
    "2101.04861v1-Figure1-1.png": {
        "prediction": "1. A schematic of the module structure of the proposed thermoelectric generator. 2. A schematic of the fundamental element of the thermoelectric generator. 3. A schematic of the magnetic TI layer. 4. A schematic of the module structure of the proposed thermoelectric generator.",
        "reference": "FIG. 1. (a) Schematic illustration (top view) of the TE module made of a film of magnetic TI. A periodic array of small square hollows filled with two different dielectrics is introduced into the magnetic TI. Each hollow harbors gap-opened metallic surface states (yellow lines) and is connected in series by leads (black solid and dashed lines). Refrigeration mode is shown here. (b) Schematic geometry of the fundamental element (topological thermocouple) consisting of two connected hollows with different dielectrics (1 and 2), possessing the p- and n-types metallic surface states. d is the distance between the two adjoined surfaces. (c) Corresponding k-dependent surface band dispersions around the \u0393 point are depicted by blue lines in which \u00b5 denotes the chemical potential at equilibrium and USIA describes the structure inversion asymmetry (SIA) between the adjoined two surfaces due to band bending induced by the dielectrics."
    },
    "2107.12100v2-Figure3-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed network model. The model consists of three main components: a) path data, b) ground truth ranking, and c) AUC-based evaluation. The path data consists of training paths and test paths. The ground truth ranking is used to predict the top 10% of node sequences in the ground truth. The AUC-based evaluation is used to evaluate the performance of the model. The model is trained using the training paths and the ground truth ranking. The predicted node sequences are used to evaluate the model's performance using the AUC-based evaluation.",
        "reference": "Figure 3: Overview of our approach to predict in uential nodes and node sequences based on path data. We start from path data which we split into training and test sets. We learn three di erent models bases on the training data: (i) a network model containing all transitions from the training data, (ii) a multi-order generative model containing observed higher-order transitions up to a maximum order of K , which is determined by model selection, and (iii) a path model containing the full paths in the training set. Based on these models, we predict the in uence of node or node sequences according to a broad range of centrality measures. We compare the ranking of node sequences to the ground truth rankings obtained from the test paths using AUC-based evaluation."
    },
    "209217-Figure2-1.png": {
        "prediction": "1. Embedding: The image is embedded into a vector representation. 2. LSTM: A long short-term memory network is used to process the embedded representation. 3. CNN: A convolutional neural network is used to extract features from the image. 4. Softmax: A softmax layer is used to classify the image as one of the four categories. 5. Fusing: The output of the CNN and LSTM is fused using a weighted sum. 6. Intermediate: The fused representation is passed through an intermediate layer. 7. Softmax: The intermediate representation is passed through a softmax layer to classify the image as one of the four categories. 8. Output: The final output of the model is the probability distribution over the four categories.",
        "reference": "Figure 2: Illustration of the mQA model architecture. We input an image and a question about the image (i.e. \u201cWhat is the cat doing?\u201d) to the model. The model is trained to generate the answer to the question (i.e. \u201cSitting on the umbrella\u201d). The weight matrix in the word embedding layers of the two LSTMs (one for the question and one for the answer) are shared. In addition, as in [25], this weight matrix is also shared, in a transposed manner, with the weight matrix in the Softmax layer. Different colors in the figure represent different components of the model. (Best viewed in color.)"
    },
    "1504097-Figure2-1.png": {
        "prediction": "1. Argumentative Summarizer: This module takes the input text document and pre-processes it to extract the main arguments and their supporting evidence. It also generates constraints that will be used to generate the summary document.",
        "reference": "Fig. 2. ASDS Architecture"
    },
    "2010.15560v2-Figure9-1.png": {
        "prediction": "1. The input sequence is fed into the first convolutional layer, which outputs a sequence of feature maps. 2. The sequence of feature maps is then passed through a series of convolutional layers, each with a different number of filters and kernel sizes. 3. The output of each convolutional layer is then passed through a series of max pooling layers, which reduce the spatial dimensions of the feature maps. 4. The output of the max pooling layers is then passed through a series of fully connected layers, which output the final classification probabilities.",
        "reference": "Fig. 9. The first best architecture."
    },
    "2202.01897v1-Figure2-1.png": {
        "prediction": "1. The input image is resized to 160x160 pixels.",
        "reference": "Figure 2: The AtmoDist network used for learning the pretext task. Numbers after layer names indicate the number of filters / feature maps of an operation. The comparison network is only required during training and can be discarded afterwards."
    },
    "1045792-Figure3-1.png": {
        "prediction": "1. The figure shows a recurrent neural network (RNN) with a long short-term memory (LSTM) cell. The input sequence X is fed into the LSTM cell, which updates its internal state Ht-1 and generates the output sequence Ht. The LSTM cell has three gates: the forget gate f, the input gate i, and the output gate o. The forget gate controls the amount of information that is forgotten from the previous state, the input gate controls the amount of information that is added to the previous state, and the output gate controls the amount of information that is outputted from the current state. The LSTM cell also has a cell state ct, which is updated by the input sequence X and the previous cell state Ht-1. The cell state ct is used to compute the output sequence Ht.",
        "reference": "Figure 3: Block diagram of the recurrent module of an LSTM network."
    },
    "2202.05262v2-Figure5-1.png": {
        "prediction": "1. The figure shows the architecture of a BERT model for the task of named entity recognition. The input text is first tokenized and each token is represented as a vector. The vectors are then passed through the BERT model, which learns a representation for each token. The output of the model is a sequence of vectors, where each vector represents a token in the input text. The model is trained to predict the labels for each token in the input text.",
        "reference": "Figure 5. The ROME method. To insert a fact (s, r, o\u2217) relating a subject s to an object o\u2217, a rank-one update \u039b(C\u22121k\u2217)T is applied to a single midlayer MLP projection, where (a) k\u2217 is chosen to select the last token of the subject name s, and (b) \u039b \u2208 RH is computed from v\u2217 that causes the prediction o\u2217 after text for (s, r)."
    },
    "2205.13038v2-Figure1-1.png": {
        "prediction": "1. The original graph is given as input. 2. Subgraph perturbation is applied to the original graph to generate perturbed graphs. 3. Subgraph embedding extraction is applied to the perturbed graphs to extract subgraph embeddings. 4. Pooling is applied to the subgraph embeddings to generate the final graph embedding.",
        "reference": "Figure 1. Overview of our proposed subgraph augmentation approach. The two subgraphs in the original graph are colored in gree and orange. We first generate multi-subgraph views via stochastic augmentation. Following that we connect the augmented subgraph to the remaining part of the original graph, by adding edges that link the augmented subgraph and the whole graph. After feeding forward the whole graph into subgraph-specific GNNs, we extract the subgraph embeddings of different views, respectively (triangles and squares). Ultimately, we fuse the embeddings of different views by a pooling function and obtain the augmented subgraph embeddings (diamonds)."
    },
    "1872130-Figure1-1.png": {
        "prediction": "1. The CNNs are trained on the input images and the class labels. 2. The CNNs are used to generate the saliency maps. 3. The saliency maps are used to revise the images using a gradient-based method. 4. The revised images are fed back to the CNNs to generate new saliency maps. 5. The CNNs are used to generate the final saliency maps. 6. The final saliency maps are used to calculate the different errors for the CNNs.",
        "reference": "Figure 1. The proposed method to generate the object-specific saliency maps directly from DCNNs."
    },
    "2204.06981v1-Figure8-1.png": {
        "prediction": "1. The figure shows a schematic of a rectangular box with a hole in the center. The hole is represented by a dashed line. The box is oriented so that the hole is at the top of the box. The box is also oriented so that the x-axis is parallel to the side of the box that is not the hole. The y-axis is perpendicular to the side of the box that is not the hole. The hole is represented by a dashed line. The hole is oriented so that the x-axis is parallel to the side of the box that is not the hole. The y-axis is perpendicular to the side of the box that is not the hole. The hole is represented by a dashed line. The hole is oriented so that the x-axis is parallel to the side of the box that is not the hole. The y-axis is perpendicular to the side of the box that is not the hole. The hole is represented by a dashed line. The hole is oriented so that the x-axis is parallel to the side of the box that is not the hole. The y-axis is perpendicular to the side of the box that is not the hole. The hole is represented by a dashed line. The hole is oriented so that the x-axis is parallel to the side of the box that is not the hole. The y-axis is perpendicular to the side of the box that is not the hole. The hole is represented by a dashed line. The hole is oriented so that the x-axis is parallel to the side of the box that is not the hole. The y-axis is perpendicular to the side of the box that is not the hole. The hole is represented by a dashed line. The hole is oriented so that the x-axis is parallel to the side of the box that is not the hole. The y-axis is perpendicular to the side of the box that is not the hole. The hole is represented by a dashed line. The hole is oriented so that the x-axis is parallel to the side of the box that is not the hole. The y-axis is perpendicular to the side of the box that is not the hole. The hole is represented by a dashed line. The hole is oriented so that the x-axis is parallel to the side of the box that is not the hole. The y-axis is perpendicular to the side of the box that is not the hole. The hole is represented by a dashed line. The hole is oriented so that the x-axis is parallel to the side of",
        "reference": "Figure 8: A 2D diagram of the integration region. The compact binary is the wiggly curve between (0, \u03b70) and (0, \u03b7coal), the merger takes place at (0, \u03b7coal), the merger pulse arrives at the detector at (r, \u03b7f ), GW emitted by the binary moves along the light cone and emits at (r \u2032, \u03b7\u2032) a subluminal tail pulse which arrives at the detector at r at the time \u03b7 > \u03b7f after the merger pulse. GW in the two shaded regions can also source a tail pulse to (r, \u03b7). Similar post-merger first order tail radiation (emission of dashed line from the wiggly binary curve) would also be possible but is canceled by the dynamics of the process (subsection 3.5)."
    },
    "2012.03418v1-Figure1-1.png": {
        "prediction": "1. The input sentence is tokenized and converted to a sequence of word embeddings. 2. A sequence of word embeddings is passed through a bidirectional LSTM to obtain a sequence of hidden states. 3. The hidden states are used as input to a softmax layer to obtain a distribution over the possible NER labels. 4. The predicted NER labels are used to generate the output sentence.",
        "reference": "Fig. 1. An example of a hypernym-hyponym pair in a definition: \u201csql is a language for querying databases\u201d. The definition is split into units (words and the corresponding PoS) for analysis. The word \u201clanguage\u201d and \u201cdatabases\u201d are two hypernym candidates. The PoS elements surround \u201clanguage\u201d and \u201cdatabases\u201d are different. Our model learns such features and identifies \u201clanguage\u201d as the hypernym of \u201csql\u201d."
    },
    "2107.10658v1-Figure2-1.png": {
        "prediction": "1. The user sends a request to the Einstein Web app.",
        "reference": "Figure 2: Cloud architecture for Synchronous TTS."
    },
    "2102.11262v5-Figure2-1.png": {
        "prediction": "1. Input image is fed into a ResNet model to extract early features. 2. The early features are then fused with late features using a shape discriminator. 3. The fused features are then passed through a shape regularizer to produce a prediction map. 4. The prediction map is then segmented forward using a shape discriminator to produce a label map.",
        "reference": "Fig. 2: Architecture of the proposed Adversarial Shape Learning Network (ASLNet) for building extraction. We designed an explicit shape regularizer to model the shape features, and a shape discriminator to guide the segmentation network."
    },
    "2107.12979v3-Figure2-1.png": {
        "prediction": "1. The subcortical pathway is a direct pathway from the thalamus to the cortex.",
        "reference": "Figure 2: The canonical microcircuit proposed by Bastos et al mapped onto the laminar connectivity of a cortical region (which comprises 6 layers). Here, for simplicity, we group layers L2 and L3 together into a broad \u2018superficial\u2019 layer and L5 and L6 together into a \u2018deep\u2019 layer. We ignore L1 entirely since there are few neurons there and they are not involved in the Bastos microcircuit. Bold lines are included in the canonoical microcircuit of Bastos et al. Dashed lines are connections which are known to exist in the cortex which are not explained by the model. Red text denotes the values which are computed in each part of the canonical microcircuit"
    },
    "2102.08893v1-Figure4-1.png": {
        "prediction": "1. Encoder CNN: This layer takes the input image and encodes it into a representation vector. 2. Decoder CNN: This layer takes the representation vector and decodes it back into an image. 3. Sum: This layer takes the output of the decoder CNN and adds it to the input image. 4. Sigmoid: This layer takes the output of the sum layer and applies a sigmoid activation function to it. 5. Gaussian: This layer takes the output of the sigmoid layer and applies a Gaussian distribution to it. 6. Loss: This layer calculates the mean squared error between the output of the decoder CNN and the input image. 7. MSELoss: This layer calculates the mean squared error between the output of the decoder CNN and the input image. 8. N^2: This layer calculates the square of the number of pixels in the image. 9. Sum: This layer takes the output of the MSELoss layer and adds it to the input image. 10. Loss: This layer calculates the mean squared error between the output of the decoder CNN and the input image. 11. MSELoss: This layer calculates the mean squared error between the output of the decoder CNN and the input image. 12. N^2: This layer calculates the square of the number of pixels in the image. 13. Sum: This layer takes the output of the MSELoss layer and adds it to the input image. 14. Loss: This layer calculates the mean squared error between the output of the decoder CNN and the input image. 15. MSELoss: This layer calculates the mean squared error between the output of the decoder CNN and the input image. 16. N^2: This layer calculates the square of the number of pixels in the image. 17. Sum: This layer takes the output of the MSELoss layer and adds it to the input image. 18. Loss: This layer calculates the mean squared error between the output of the decoder CNN and the input image. 19. MSELoss: This layer calculates the mean squared error between the output of the decoder CNN and the input image. 20. N^2: This layer calculates the square of the number of pixels in the image. 21. Sum: This layer takes the output of the MSELoss layer and adds it to the input image",
        "reference": "Fig. 4. Architecture of the training phase [2]"
    },
    "2202.13313v3-Figure2-1.png": {
        "prediction": "1. Search Space: A set of possible actions that the agent can take. 2. Training & Evaluation: The process of training the agent to learn the optimal actions based on the reward signal. 3. Controller Sampling: A process of selecting a subset of actions from the search space to train the controller. 4. Controller RNN (LSTM): A recurrent neural network that learns to predict the reward for each action. 5. Reward = Paras. & Acc.: The reward signal that the agent receives for each action. 6. ReLU: A non-linear activation function used in the controller RNN. 7. ELU: Another non-linear activation function used in the controller RNN. 8. Swish: A non-linear activation function used in the controller RNN. 9. Swish: A non-linear activation function used in the controller RNN. 10. Swish: A non-linear activation function used in the controller RNN. 11. Swish: A non-linear activation function used in the controller RNN. 12. Swish: A non-linear activation function used in the controller RNN. 13. Swish: A non-linear activation function used in the controller RNN. 14. Swish: A non-linear activation function used in the controller RNN. 15. Swish: A non-linear activation function used in the controller RNN. 16. Swish: A non-linear activation function used in the controller RNN. 17. Swish: A non-linear activation function used in the controller RNN. 18. Swish: A non-linear activation function used in the controller RNN. 19. Swish: A non-linear activation function used in the controller RNN. 20. Swish: A non-linear activation function used in the controller RNN. 21. Swish: A non-linear activation function used in the controller RNN. 22. Swish: A non-linear activation function used in the controller RNN. 23. Swish: A non-linear activation function used in the controller RNN. 24. Swish: A non-linear activation function used in the controller RNN. 25. Swish: A non-linear activation function used in the controller RNN. 26. Swish: A non-linear activation function used in the controller",
        "reference": "Fig. 2. Searching process of neural architecture."
    },
    "207468-Figure3-1.png": {
        "prediction": "1. The input to the decoder is the encoded sentence representation, which is the concatenation of the word embeddings and the sentence representation. The decoder generates a word embedding for each time step, which is then added to the sentence representation. The decoder also generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to the sentence representation. The decoder generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to the sentence representation. The decoder generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to the sentence representation. The decoder generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to the sentence representation. The decoder generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to the sentence representation. The decoder generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to the sentence representation. The decoder generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to the sentence representation. The decoder generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to the sentence representation. The decoder generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to the sentence representation. The decoder generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to the sentence representation. The decoder generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to the sentence representation. The decoder generates a sentence representation for each time step, which is added to the sentence representation. The decoder generates a word embedding for each time step, which is added to",
        "reference": "Figure 3: Hierarchical Sequence to Sequence Model with Attention."
    },
    "2102.12816v3-Figure1-1.png": {
        "prediction": "1. The figure shows the time evolution of a system with a delay. The system is represented by a set of points on a line, with the first point labeled as t = 0 and the last point labeled as t = T. The points are connected by a line, with the points at t = m and t = m + 1 connected by a horizontal line. The points at t = m and t = m + 1 are labeled as t(m) and t(m + 1), respectively. The points at t = m and t = m + 1 are connected by a line, with the points at t = m and t = m + 1 labeled as t(m) and t(m + 1), respectively. The points at t = m and t = m + 1 are connected by a line, with the points at t = m and t = m + 1 labeled as t(m) and t(m + 1), respectively. The points at t = m and t = m + 1 are connected by a line, with the points at t = m and t = m + 1 labeled as t(m) and t(m + 1), respectively. The points at t = m and t = m + 1 are connected by a line, with the points at t = m and t = m + 1 labeled as t(m) and t(m + 1), respectively. The points at t = m and t = m + 1 are connected by a line, with the points at t = m and t = m + 1 labeled as t(m) and t(m + 1), respectively. The points at t = m and t = m + 1 are connected by a line, with the points at t = m and t = m + 1 labeled as t(m) and t(m + 1), respectively. The points at t = m and t = m + 1 are connected by a line, with the points at t = m and t = m + 1 labeled as t(m) and t(m + 1), respectively. The points at t = m and t = m + 1 are connected by a line, with the points at t = m and t = m + 1 labeled as t(m) and t(m + 1), respectively. The points at t = m and t = m + 1 are connected by a line, with",
        "reference": "Fig. 1. An illustration of the relationship among T , Ti and delay in slot m. In this example, Pi,m \u2212 ni,m = 2 and tm+1 \u2212 \u03c4(tm+1) = 4, which satisfies (12)."
    },
    "90264-Figure5-1.png": {
        "prediction": "1. Input video is processed to extract appearance and motion information. 2. Appearance information is processed by a convolutional neural network (FCa1) to generate a feature map. 3. Motion information is processed by a convolutional neural network (FCb1) to generate a feature map. 4. Appearance and motion information are fused using a convolutional neural network (FCb2). 5. The fused feature map is passed through a fully connected layer to generate a classification score.",
        "reference": "Fig. 5: Illustration of late-fusion. We encode, temporally pool the appearance and motion representation separately and fuse the two probability distributions."
    },
    "2102.10461v1-Figure4-1.png": {
        "prediction": "64x64x3 Upsqueeze",
        "reference": "Figure 4: CelebA architecture for the injective portion g of TRUMPET. The input size to each layer is written below it."
    },
    "2110.06804v3-Figure4-1.png": {
        "prediction": "1. The input layer is the first layer of the neural network. It receives the input data and passes it to the hidden layers. 2. The hidden layers are the middle layers of the neural network. They process the input data and generate intermediate results. 3. The output layer is the last layer of the neural network. It receives the output from the hidden layers and generates the final output. 4. The weights and biases are the parameters of the neural network that are learned during training. They are used to adjust the output of the network based on the input data. 5. The activation function is a mathematical function that is applied to the output of the hidden layers to generate the final output. It is used to smooth the output of the network and prevent it from exploding or shrinking. 6. The loss function is a mathematical function that measures the difference between the predicted output of the network and the actual output. It is used to evaluate the performance of the network and guide the training process. 7. The optimizer is a mathematical function that updates the weights and biases of the network based on the loss function. It is used to improve the accuracy of the network during training. 8. The training process is the process of adjusting the weights and biases of the network based on the loss function. It is used to minimize the loss function and improve the accuracy of the network. 9. The validation process is the process of evaluating the performance of the network on a separate dataset. It is used to check the stability and accuracy of the network during training. 10. The testing process is the process of evaluating the performance of the network on a separate dataset. It is used to check the accuracy of the network after training. 11. The neural network is a type of machine learning model that is inspired by the structure and function of the human brain. It is composed of multiple layers of interconnected nodes that process input data and generate output. 12. The input layer is the first layer of the neural network. It receives the input data and passes it to the hidden layers. 13. The hidden layers are the middle layers of the neural network. They process the input data and generate intermediate results. 14. The output layer is the last layer of the neural network. It receives the output from the hidden layers and generates the final output. 15. The weights and biases are the parameters of the neural network that are learned during training",
        "reference": "Figure 4. An artificial neural network"
    },
    "1339502-Figure4-1.png": {
        "prediction": "1. Input: The input to the model is the sequence of words in the sentence.",
        "reference": "Figure 4. Structure of the network. Left: Detailed components of the convolutional feature extraction and multi-level feature fusion. The \u201cConvUnit(w, h, n)\u201d represents a convolutional layer of n w\u00d7 h kernels, connected by a batch normalization layer and a ReLU layer. The \u201cUpSample(n)\u201d represents a deconvolution layer of n\u00d7 n kernels with stride n. Right: The design of multi-task module. \u201cScale&Shift\u201d is used to stretch and translate the values."
    },
    "1639981-Figure3-1.png": {
        "prediction": "1. The figure shows the architecture of a convolutional neural network (CNN) with multiple convolutional layers. The input image is passed through the first convolutional layer, which applies a set of filters to the image. The output of the first convolutional layer is then passed through a set of pooling layers, which reduce the spatial dimensions of the image. The output of the pooling layers is then passed through a set of fully connected layers, which apply a set of non-linear functions to the image. Finally, the output of the fully connected layers is passed through a softmax layer, which outputs a probability distribution over the classes of the image.",
        "reference": "Figure 3: Left: Columnar architecture in a fully connected network, with the path through one column highlighted. Each column corresponds to a different \u03b1j . Right: Columnar architecture in a convolutional network. In this setting the w\u03b1\u2019s take linear combinations of the feature maps obtained by convolving the input with the dictionary. We make the same abuse of notation here as in the main text\u2014the vectorized filter banks must be reshaped before the convolution takes place."
    },
    "2204.05103v1-Figure1-1.png": {
        "prediction": "1. Fully-connected transformer encoder: This layer is responsible for aggregating contextualized representations of masked tokens. It consists of a fully-connected layer followed by a transformer encoder. The transformer encoder is a two-layered architecture with a self-attention mechanism. The output of the transformer encoder is passed through a fully-connected layer to obtain the contextualized representations of masked tokens.",
        "reference": "Figure 1: Our approach with self supervised learning based on a Transformer (a) and fine-tuning strategy for learning the final emotion predictor (b)."
    },
    "2012.03152v1-Figure2-1.png": {
        "prediction": "1. Large Tree point cloud data is collected. 2. Feature construction is performed, including the change of curvature, the average distance of neighborhood, and the SVM classification. 3. Building standard result is obtained. 4. Classification is performed using the SVM classification, and the accuracy analysis is performed. 5. Results visualization is performed, including the correct rate of classification and the Kappa coefficient.",
        "reference": "Fig. 2. Flowchart of experiment."
    },
    "2103.11568v4-Figure2-1.png": {
        "prediction": "1. The query images are fed into the encoder to obtain their representations. 2. The representations are then clustered using a clustering algorithm. 3. Pseudo labels are assigned to the clusters based on the cluster centroids. 4. The encoder is updated using the pseudo labels. 5. The encoder is then used to generate representations for the query images. 6. The representations are then clustered again, and the process is repeated until convergence.",
        "reference": "Fig. 2: The system pipeline of our unsupervised person re-ID method. The upper branch depicts the memory initialization stage. The training data features are assigned pseudo labels by clustering, where features of the same color belong to the same cluster. The lower branch represents the model training stage. Query features in iterative mini-batch are used to update the memory cluster representations with a momentum. The ClusterNCE loss computes the contrastive loss between query features and all cluster representations."
    },
    "2103.10685v2-Figure3-1.png": {
        "prediction": "1. The figure shows the architecture of the BERT model, which is pre-trained on a large corpus of text data. The model is then fine-tuned on a specific task, such as sentiment analysis or question-answering. The pre-training process involves training the model to predict the next word in a sequence of words, while the fine-tuning process involves training the model to predict the output of a specific task. The figure also shows how the BERT model is used in the task of predicting the next word in a sequence of words based on the previous word and the context of the sentence.",
        "reference": "Fig. 3. Language model generation and language model inverse prompting scoring for generating a poem sentence."
    },
    "2012.07082v1-Figure3-1.png": {
        "prediction": "1. The figure shows the process of backtracking in a game tree. The figure starts with the root node of the game tree, which is labeled as \"Game 0\". The figure then shows the process of backtracking from the root node to the leaf node, which is labeled as \"Game 3\". The figure shows the backtracking process for each node in the game tree, with the backtracking process starting from the root node and ending at the leaf node. The figure also shows the process of backtracking for each game, with the backtracking process starting from the root node of the game tree and ending at the leaf node of the game tree.",
        "reference": "Figure 3: Illustration of the sampled games generated by modified SGM during its execution."
    },
    "2107.06179v2-Figure1-1.png": {
        "prediction": "1. Coronary arteries: The heart's blood supply comes from the coronary arteries, which are responsible for delivering oxygen-rich blood to the heart muscle.",
        "reference": "Fig. 1. Illustration of myocardial infarct caused by acute thrombus in the culprit coronary artery (see text)."
    },
    "2102.03347v1-Figure2-1.png": {
        "prediction": "1. Off-chain: The attacker sends a message to the bot contract, which then sends a message to the attacker EOAs. The attacker EOAs can then send messages to the attacker's bot.",
        "reference": "Figure 2: Attacker model with on-chain and off-chain parts."
    },
    "2102.06702v1-Figure3-1.png": {
        "prediction": "1. Figure 1: Schematic of a two-dimensional electron gas (2DEG) in a GaAs/AlGaAs heterostructure. The 2DEG is formed by the depletion of the GaAs layer due to the presence of an n-doped AlGaAs barrier. The 2DEG is confined to a two-dimensional plane by the presence of a gate voltage applied to the top gate. The 2DEG is also confined in the lateral direction by the presence of a second gate voltage applied to the bottom gate.",
        "reference": "Fig. 3. Schematic illustration of the PCD. The gray box indicates the domain of the unit-cell model. Note that, in practice, the size of the THz antenna attached to the electrodes is much larger than the gap distance between the electrodes."
    },
    "2101.00048v1-Figure1-1.png": {
        "prediction": "1. The image shows the optical setup of the microscope.",
        "reference": "Figure 1. Schematic diagram of the Slimfield microscope. a) Laser light vertically polarized at source (blue) passes through a shutter (S) and is expanded 3x by the lens pair L1 and L2, before passing through L3 which forms a telescope with the objective lens to ensure light at the sample is collimated. Finally, in the excitation path the laser light passes through a wave plate (WP) on a rotational mount, either \u03bb/2 for linear polarization or \u03bb/4 for circular polarization. The light is directed to the sample with a dichroic mirror which allows the collected fluorescence (green) to pass through it. The emission then encounters the lens L4 and is focused at the side port of the"
    },
    "2010.09890v1-Figure16-1.png": {
        "prediction": "1. The state encoder takes as input the visible objects and their corresponding state, and outputs a sequence of hidden states representing the state of the objects. 2. The transformer module is used to encode the visible objects and their corresponding state. 3. The LSTM module is used to encode the sequence of hidden states. 4. The MLP module is used to combine the hidden states and the visible objects to generate the final representation of the state. 5. The MLP module is used to generate the final representation of the state.",
        "reference": "Figure 16: Network architecture of the goal inference model, which encodes the symbolic state sequence in demonstrations and infers the count for each predicate."
    },
    "2110.07150v3-Figure1-1.png": {
        "prediction": "1. Question in Language U: Translate to L Languages (e.g. English, Bengali, Russian). 2. Monolingual Document Retriever: Monolingual Document Retriever. 3. Monolingual Answer Selection: Monolingual Answer Selection. 4. Aggregate Candidates: Aggregate Candidates. 5. Cross-Lingual GenQA: Cross-Lingual GenQA. 6. Answer in Language U: Answer in Language U.",
        "reference": "Figure 1: Illustration of our proposed Cross-Lingual, Retrieval-based GENQA pipeline."
    },
    "2101.11878v3-Figure1-1.png": {
        "prediction": "1. Knowledge Base: A collection of images and their corresponding descriptions. 2. Spatial Location: The location of the image in the knowledge base. 3. Components: The individual parts of the image, such as a cow, a field, and a truck. 4. Component Dictionary: A dictionary that maps each component to a unique identifier. 5. Map Dictionary: A dictionary that maps each image to a unique identifier. 6. Spatial Location: The location of the image in the knowledge base. 7. Components: The individual parts of the image, such as a cow, a field, and a truck. 8. Map Dictionary: A dictionary that maps each image to a unique identifier. 9. Spatial Location: The location of the image in the knowledge base. 10. Components: The individual parts of the image, such as a cow, a field, and a truck. 11. Map Dictionary: A dictionary that maps each image to a unique identifier. 12. Spatial Location: The location of the image in the knowledge base. 13. Components: The individual parts of the image, such as a cow, a field, and a truck. 14. Map Dictionary: A dictionary that maps each image to a unique identifier. 15. Spatial Location: The location of the image in the knowledge base. 16. Components: The individual parts of the image, such as a cow, a field, and a truck. 17. Map Dictionary: A dictionary that maps each image to a unique identifier. 18. Spatial Location: The location of the image in the knowledge base. 19. Components: The individual parts of the image, such as a cow, a field, and a truck. 20. Map Dictionary: A dictionary that maps each image to a unique identifier. 21. Spatial Location: The location of the image in the knowledge base. 22. Components: The individual parts of the image, such as a cow, a field, and a truck. 23. Map Dictionary: A dictionary that maps each image to a unique identifier. 24. Spatial Location: The location of the image in the knowledge base. 25. Components: The individual parts of the image, such as a cow, a field, and a truck. 26. Map Dictionary: A dictionary that maps each image to",
        "reference": "Figure 1: Intuitive illustration of how our model acquires knowledge during meta-learning. In particular, it learns a dictionary of component representations, which resemble individual object components. Some of these can be shared across different classes, e.g., car tires. In addition, it learns a map dictionary that contains common spatial activation patterns of components. During meta-testing, the knowledge base facilitates the learning of novel classes by re-using the already learned components and spatial activation patterns."
    },
    "2102.04335v1-Figure1-1.png": {
        "prediction": "1. The COVID-20 dataset is the source dataset.",
        "reference": "Figure 1. Research empirical model"
    },
    "2011.08706v1-Figure2-1.png": {
        "prediction": "1. Feature extraction: The first convolutional layer extracts features from the input image. 2. Pooling: The output of the first convolutional layer is downsampled using max pooling to reduce the spatial dimensions. 3. Fully connected layers: The output of the pooling layer is fed into a series of fully connected layers, which are used to learn the complex relationships between the features and the labels. 4. Classification: The output of the fully connected layers is fed into a final fully connected layer, which outputs the predicted label for the image.",
        "reference": "Fig. 2. The framework of our proposed FPAENet method. ResNet-50 as the backbone to extract features. Two top-down channels are added in the FPN, and feature enhancement with attention is placed on the horizontal connection to enhance the effective information. Next, two parallel Fully Convolutional Networks to classify whether the candidate area is a lesion and locate the lesion."
    },
    "2012.13965v1-Figure1-1.png": {
        "prediction": "1. The block diagram of the proposed method.",
        "reference": "Fig. 1: Pipeline of our method for computing IK on general soft robots by using three learned networks \u2013 1) forward kinematics Nfk, 2) Jacobian NJ and 3) sim-to-real mapping Ns2r . According to the current configurations of robot\u2019s actuation ci, the actuation ci+1 for reaching a target position (or shape) pi+1 is computed by the Jacobian-based iteration."
    },
    "1165810-Figure3-1.png": {
        "prediction": "1. Inverted pyramid sampling: The image is sampled at different scales using an inverted pyramid. 2. Image sampled at different scales: The image is sampled at different scales using an inverted pyramid. 3. Downsampling: The image is downsampled using a filter. 4. Input to model: The downsampled image is input to the model.",
        "reference": "Figure 3: Eccentricity-dependent model: Inverted pyramid with sampling points. Each circle represents a filter with its respective receptive field. For simplicity, the model is shown with 3 scales."
    },
    "2011.07995v3-Figure1-1.png": {
        "prediction": "1. Tomography exams with at least one reconstruction view available in DuKe Health System performed between August 26, 2014 and January 29, 2018 (n=16,802 studies from 13,954 patients",
        "reference": "Figure 1: Patient flowchart. BI-RADS = Breast Imaging-Reporting and Data System; AD = architectural distortion; LCC = left craniocaudal; RCC = right craniocaudal; LMLO = left mediolateral oblique; RMLO = right mediolateral oblique."
    },
    "2103.09966v1-Figure3-1.png": {
        "prediction": "1. The circuit diagram shows a single-phase, three-wire system with a generator, a load, and a capacitor. The generator is connected to the line through a capacitor, which is used to regulate the voltage. The load is connected to the line through a capacitor, which is used to regulate the current. The capacitor is also used to filter the line current and improve the power factor. The capacitor is connected to the line through a switch, which allows the capacitor to be disconnected from the line when it is not needed. The capacitor is also connected to a load capacitor bank, which is used to regulate the voltage and current of the load. The load capacitor bank is connected to the load through a switch, which allows the load capacitor bank to be disconnected from the load when it is not needed. The load capacitor bank is also connected to a capacitor bank, which is used to regulate the voltage and current of the load. The capacitor bank is connected to the load through a switch, which allows the capacitor bank to be disconnected from the load when it is not needed. The capacitor bank is also connected to a load capacitor bank, which is used to regulate the voltage and current of the load. The load capacitor bank is connected to the load through a switch, which allows the load capacitor bank to be disconnected from the load when it is not needed. The load capacitor bank is also connected to a capacitor bank, which is used to regulate the voltage and current of the load. The capacitor bank is connected to the load through a switch, which allows the capacitor bank to be disconnected from the load when it is not needed. The capacitor bank is also connected to a load capacitor bank, which is used to regulate the voltage and current of the load. The load capacitor bank is connected to the load through a switch, which allows the load capacitor bank to be disconnected from the load when it is not needed. The load capacitor bank is also connected to a capacitor bank, which is used to regulate the voltage and current of the load. The capacitor bank is connected to the load through a switch, which allows the capacitor bank to be disconnected from the load when it is not needed. The load capacitor bank is also connected to a capacitor bank, which is used to regulate the voltage and current of the load. The capacitor bank is connected to the load through a switch, which allows the capacitor bank to be disconnected from the load when it is not needed. The load capacitor bank is also connected to a capacitor bank",
        "reference": "Fig. 3: Single-line diagram of the test system."
    },
    "2102.10535v1-Figure4-1.png": {
        "prediction": "1. Figure 1: Overview of the BERT architecture. BERT is a bidirectional encoder representation learning (BERT) model for language understanding. It takes a sequence of words as input and outputs a sequence of vectors, each representing a word in the sequence. The model is trained to predict the next word in the sequence based on the context of the previous words. The BERT model is pre-trained on a large corpus of text and then fine-tuned on a specific task, such as question-answering or text classification.",
        "reference": "Figure 4. General CodeSearchNet architecture for all of our baselines. Each language is processed through different encoder mechanisms. The query encoder is shared (an NLP encoder), and the purpose of the CodeSearchNet tasks is to retrieve the most relevant code snippets subject to the natural language query."
    },
    "1087044-Figure1-1.png": {
        "prediction": "1. Geometric Labeling: The first step is to label the 3D point cloud with geometric labels such as walls, floors, and ceilings. This is done by using a 3D segmentation algorithm to identify the different parts of the building. 2. Relation Prediction: The next step is to predict the relations between the different parts of the building. This is done by using a relation prediction model that takes as input the geometric labels and predicts the relations between them. 3. Layering: The third step is to layer the building. This is done by using a layering model that takes as input the geometric labels and predicts the layering of the building. 4. Supporting: The fourth step is to support the layers with supporting elements such as columns and beams. This is done by using a supporting model that takes as input the layering and predicts the supporting elements. 5. Siding: The fifth step is to add siding to the building. This is done by using a siding model that takes as input the layering and predicts the siding material. 6. Layering: The sixth step is to layer the siding with a layering model. 7. 3D Reconstruction: The final step is to reconstruct the 3D model of the building. This is done by using a 3D reconstruction model that takes as input the layering and predicts the 3D model of the building.",
        "reference": "Figure 1: An illustration of our geometric scene parsing. Our task aims to predict the pixel-wise geometric surface labeling (first column) and the interaction relations between main regions (second column). Then the parsing result is applied to reconstruct a 3D model (third column)."
    },
    "2210.00705v2-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed model for speech-based image retrieval. The model consists of two main components: a parallel speech-CLIP model and a cascaded speech-CLIP model. The parallel speech-CLIP model is trained to predict the CLIP features of the speech signal and the corresponding image features. The cascaded speech-CLIP model is trained to predict the CLIP features of the speech signal and the corresponding image features, as well as the audio features of the speech signal. The model is trained using contrastive loss and cross-entropy loss.",
        "reference": "Fig. 2: An illustration of SpeechCLIP models. (a) A pre-trained HuBERT [12] extracts audio features. The features are concatenated with a learnable CLS token and fed into a transformer encoder layer to obtain a single vector representing the information of the entire sequence. The vector is then used to compute contrastive loss with the CLIP image encoder\u2019s output [23]. (b) Cascaded SpeechCLIP uses K CLS tokens to capture a small sequence of keywords from the audio signal. The keywords are batch-normalized and vector-quantized before passing to the CLIP text encoder. BN and VQ respectively denote batch normalization and vector quantization."
    },
    "2107.12847v1-Figure3-1.png": {
        "prediction": "1. The model takes in the input image and predicts the corresponding pose and shape of the human body.",
        "reference": "Figure 3: The proposed local recurrent modeling approach to human mesh recovery."
    },
    "2101.06808v1-Figure1-1.png": {
        "prediction": "1. Start from initial solution (x0). 2. Global phase over \u03a9: Update the DoE. 3. Local phase over \u03a9: Update the DoE. 4. Failure: Increment k. 5. Success: Stop and return current iterate as solution. 6. Failure: Stop and return current iterate as solution. 7. Increment k: A stopping condition is satisfied. 8. Stop and return current iterate as solution.",
        "reference": "Figure 1: An overview of the TREGO framework (detailed in Algorithm 1)."
    },
    "2202.00528v2-Figure1-1.png": {
        "prediction": "1. The model takes in the masked sentence and the corresponding context sentence as input. The masked sentence is represented by the token embeddings and the position embeddings. The context sentence is represented by the token embeddings and the position embeddings. The token embeddings and the position embeddings are concatenated and fed into the feed-forward sublayer. The output of the feed-forward sublayer is then passed through the self-attention layer. The output of the self-attention layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the attention layer. The output of the attention layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the decoder layer. The output of the decoder layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the attention layer. The output of the attention layer is then passed through the decoder layer. The output of the decoder layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the decoder layer. The output of the decoder layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the decoder layer. The output of the decoder layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the decoder layer. The output of the decoder layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the decoder layer. The output of the decoder layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the decoder layer. The output of the decoder layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the decoder layer. The output of the decoder layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the decoder layer. The output of the decoder layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the decoder layer. The output of the decoder layer is then passed through the layer-wise encoding layer. The output of the layer-wise encoding layer is then passed through the decoder layer. The output of the decoder layer is then passed through the layer-wise encoding layer. The output of",
        "reference": "Figure 1: Illustration for translation-oriented language models. X and Y denote source and target input, respectively. To enable translation, we adapt the LM self-attention mask to either the PrefixLM mask or CausalLM mask (top right), where filled black circles indicate disallowed attention. We also explore top-only encoding (Top Encoding) for PrefixLM which feeds the final-layer source encodings to generation similar to EncDec, rather than layer-wise coordinated encodings (He et al., 2018). Masks of EncDec are shown in the bottom right for comparison."
    },
    "2107.05802v2-Figure4-1.png": {
        "prediction": "1. The figure shows the projection of a 3D object onto a 2D plane. The object is represented by a green blob, and the plane is represented by a blue rectangle. The projection of the object onto the plane is represented by a red line. The projection is a 2D representation of the 3D object.",
        "reference": "Figure 4: Left panel: An illustration of measuring the width of a set S (in green) in a direction g\u0302 by identifying x,y \u2208 S in maxx,y\u2208S g\u0302 \u00b7 (y \u2212 x). The expectation of this width using random vectors g \u223c N (0, ID\u00d7D) instead of g\u0302 is twice the Gaussian width w(S). Intuitively, it is the characteristic extent of the set T over all directions rescaled by a factor between D/ \u221a D + 1 and \u221a D. Right panel: Illustration of projecting manifolds on the unit sphere and Gordon\u2019s escape theorem. The same manifold far from the sphere will have a smaller projection to it than the one that is close, and therefore it will be harder to intersect with an affine subspace."
    },
    "2011.09361v1-Figure3-1.png": {
        "prediction": "1. The training group is used to train the model, while the validation group is used to validate the model's performance.",
        "reference": "Fig. 3: The procedure followed for splitting the datasets into training, validation and testing sets used by KD-OP."
    },
    "2010.04560v3-Figure9-1.png": {
        "prediction": "1. Select a recommendation fact type",
        "reference": "Figure 9: Flowchart of an explainable energy recommender system."
    },
    "2110.03143v1-Figure2-1.png": {
        "prediction": "1. The model is trained on a source domain (e.g., labeled images of cars) and a target domain (e.g., unlabeled images of cars). 2. The model is trained to predict the target domain labels. 3. The model is trained to predict the source domain labels. 4. The model is trained to predict the target domain labels. 5. The model is trained to predict the source domain labels. 6. The model is trained to predict the target domain labels. 7. The model is trained to predict the source domain labels. 8. The model is trained to predict the target domain labels. 9. The model is trained to predict the source domain labels. 10. The model is trained to predict the target domain labels. 11. The model is trained to predict the source domain labels. 12. The model is trained to predict the target domain labels. 13. The model is trained to predict the source domain labels. 14. The model is trained to predict the target domain labels. 15. The model is trained to predict the source domain labels. 16. The model is trained to predict the target domain labels. 17. The model is trained to predict the source domain labels. 18. The model is trained to predict the target domain labels. 19. The model is trained to predict the source domain labels. 20. The model is trained to predict the target domain labels. 21. The model is trained to predict the source domain labels. 22. The model is trained to predict the target domain labels. 23. The model is trained to predict the source domain labels. 24. The model is trained to predict the target domain labels. 25. The model is trained to predict the source domain labels. 26. The model is trained to predict the target domain labels. 27. The model is trained to predict the source domain labels. 28. The model is trained to predict the target domain labels. 29. The model is trained to predict the source domain labels. 30. The model is trained to predict the target domain labels. 31. The model is trained to predict the source domain labels. 32. The model is trained to predict the target domain labels. 33. The model is trained to predict",
        "reference": "Figure 2. Overview of meta-learning strategy is presented on the left side and adaptation framework is presented on the right side. The meta-learning performs bi-level optimization, where the inner loop performs domain adaptation and the outer loop performs meta-update with respect to meta-loss. The updated meta-parameters are loaded as the initial condition of Faster-RCNN and this process is repeated. The adaptation framework performs adversarial feature alignment between source and target domain at the image and instance level."
    },
    "1292676-Figure4-1.png": {
        "prediction": "1. Train DRL Network: Use the DRL network to learn the policy for generating features. The DRL network takes as input the current state of the system and outputs the next state of the system. The DRL network is trained using the backpropagation algorithm to minimize the difference between the predicted and actual next states.",
        "reference": "Figure 4: An overview of the LS-DQN algorithm."
    },
    "1321977-Figure3-1.png": {
        "prediction": "1. Preprocessing: The input MIDI training data is preprocessed to remove any noise or errors. This could involve cleaning the data, removing duplicates, or normalizing the data. 2. Extracting features: The preprocessed data is then used to extract features, such as pitch, rhythm, and tempo information. 3. Training the model: The extracted features are used to train a machine learning model, such as a neural network, to learn the relationship between the features and the target output, which is the generated MIDI file. 4. Generating the MIDI file: Once the model has been trained, it can be used to generate the MIDI file based on the input features. 5. Instrumentation: The generated MIDI file can be used to generate a musical instrument, such as a piano or drum machine, to play the generated music.",
        "reference": "Fig. 3. The architecture of JamBot. Chords and piano roll representations are extracted from the MIDI files in the training data (in black). The extracted chords and piano rolls are then used to train the chord and polyphonic LSTMs (in red). During music generation (in blue), the chord LSTM generates a chord progression that is used as input to the polyphonic LSTM which generates new music in MIDI format. When listening to the music, one can freely vary tempo and instrumentation."
    },
    "2010.14795v2-Figure3-1.png": {
        "prediction": "1. Randomness-Utilizing Process: The process of generating random numbers from a given distribution.",
        "reference": "FIG. 3: A generalized randomness-utilizing process \u03a6. If one intends to implement a certain quantum map \u03a8 utilizing a randomness source \u03c3 which has large enough min-entropy but is not a uniform random state, it could be implemented if one employs a broader notion of catalycity and allows the uncorrelated leftover randomness in the output state."
    },
    "2011.13183v1-Figure1-1.png": {
        "prediction": "1. Feature extractor: This block takes the input image and extracts features from it. It is typically a convolutional neural network (CNN) that uses multiple convolutional layers to extract features from the image. The output of this block is a set of feature maps that represent the image.",
        "reference": "Figure 1: The model architecture of TinaFace. (a) Feature Extractor: ResNet-50 [11] and 6 level Feature Pyramid Network [18] to extract the multi-scale features of input image. (b) Inception block to enhance receptive field. (c) Classification Head: 5 layers FCN for classification of anchors. (d) Regression Head: 5 layers FCN for regression of anchors to ground-truth objects boxes. (e) IoU Aware Head: a single convolutional layer for IoU prediction."
    },
    "2103.00815v1-Figure1-1.png": {
        "prediction": "1. Input Layer: The input layer is the first layer of the neural network and it receives the input data. It is connected to the previous layer by a set of weights and biases. The input layer is also known as the hidden layer.",
        "reference": "Figure 1: The graph associated to a deep neural network with input dimension 3 and 5 layers"
    },
    "2202.06743v1-Figure4-1.png": {
        "prediction": "1. The deformation of a grid in the parametric domain (P) is represented by the deformation configuration (S).",
        "reference": "Figure 4: Schematic representing the finite element discretization used for the numerical implementation of the proposed continuum formulation."
    },
    "2206.12772v1-Figure2-1.png": {
        "prediction": "1. Visual-Sound Localiser: This module takes as input the image and audio features and outputs the visual and audio embeddings. The visual and audio embeddings are then concatenated and passed through a Slamease Network to obtain the visual-audio embedding space.",
        "reference": "Figure 2: Framework Overview. We exploit a Siamese network, with two identical branches, each branch consists of an image encoder and an audio encoder. For the one branch, we perform transformations T 1vis + T 1 aud, while for the other branch, we use transformations T 2vis + T 2 aud. In this figure, T 1 vis only includes appearance transformation Tapp, while T 2 vis includes both appearance and geometric transformations Tapp + Tgeo. Both audio transformations are T\ud835\udc4e\ud835\udc62\ud835\udc51 . The framework is optimised by encouraging the audio-visual representation to be invariant to T\ud835\udc4e\ud835\udc5d\ud835\udc5d and T\ud835\udc54\ud835\udc52\ud835\udc5c , while being equivalent to T\ud835\udc54\ud835\udc52\ud835\udc5c ."
    },
    "2109.09113v1-Figure2-1.png": {
        "prediction": "1. Data Preprocessing: This step involves cleaning and transforming the data to prepare it for analysis. This can include tasks such as data cleaning, data normalization, and data transformation.",
        "reference": "Figure 2: The HPTQ framework. Dashed lines represent statistical information passing, which include also their updates, dotted lines represent data passing and solid lines represent an updated network."
    },
    "2011.13733v1-Figure3-1.png": {
        "prediction": "1. Relu activation: The input is multiplied by the ReLU function, which outputs the input if it is greater than 0, and 0 otherwise. The output of the ReLU activation is the input with the ReLU function applied to it.",
        "reference": "FIG. 3: The overall schema of the Mini Inception-Resnet network. For the detailed modules, please refer to Figs. 4, 5 and 6."
    },
    "2101.10804v2-Figure1-1.png": {
        "prediction": "1. Preprocessing: The input image is resized to a fixed size and converted to a tensor. 2. Encoding: The image is encoded using a convolutional neural network (CNN) to extract features. 3. Patch Embedding: The image is divided into patches, and each patch is encoded separately using a CNN. 4. Position Embedding: The position of each patch in the image is encoded using a learned position embedding. 5. Self-Attention: The patches are passed through a self-attention layer to capture the relationship between them. 6. Add & Layer Norm: The self-attention layer is followed by an add layer and a layer normalization layer. 7. Cross-Attention: The patches are passed through a cross-attention layer to capture the relationship between different patches. 8. Add & Layer Norm: The cross-attention layer is followed by an add layer and a layer normalization layer. 9. Position Embedding: The patches are passed through a position embedding layer to capture the position of each patch in the image. 10. Masked Self-Attention: The patches are passed through a masked self-attention layer to capture the relationship between the masked patches. 11. Add & Layer Norm: The masked self-attention layer is followed by an add layer and a layer normalization layer. 12. Word Embedding: The patches are passed through a word embedding layer to convert the patch embeddings to word embeddings. 13. Masked Sequence Embedding: The word embeddings are passed through a masked sequence embedding layer to convert the word embeddings to masked sequence embeddings. 14. Linear & Softmax: The masked sequence embeddings are passed through a linear layer and a softmax layer to generate the probability distribution over the masked sequence. 15. Output Layer: The output layer generates the final prediction.",
        "reference": "Fig. 1. The overall architecture of proposed CPTR model."
    },
    "2205.14647v1-Figure2-1.png": {
        "prediction": "1. Efficient MAJ/NOT implementation of desired operation (SIMDRAM-enabled application",
        "reference": "Figure 2: Overview of the SIMDRAM framework."
    },
    "2101.02550v1-Figure2-1.png": {
        "prediction": "1. SE module: This module consists of a self-attention layer and a feedforward neural network. The self-attention layer is used to compute the attention weights for each token in the input sequence. The feedforward neural network is used to compute the output of the module. The output of the module is a weighted sum of the input sequence.",
        "reference": "Fig. 2. The architecture of the first proposed ATM model, which is denoted as \u201cATMbef \u201d. From the figure, the output of the L-th LSTM layer is used to perform \u03c9, which is then used to extract the representative features at (L\u2212 1)-th LSTM layer in the SE model."
    },
    "2206.09770v1-Figure2-1.png": {
        "prediction": "1. Object detection: The camera captures an image and detects objects in the image.",
        "reference": "Fig. 2: An overview of the proposed framework for roadside vision-based traffic scene perception."
    },
    "1068967-Figure7-1.png": {
        "prediction": "1. The first convolutional layer has 32 filters, a kernel size of 3x3, and a stride of 1. The output of this layer is passed through a max pooling layer with a pool size of 2x2 and a stride of 2. The output of this layer is then passed through a fully connected layer with 128 neurons.",
        "reference": "Fig. 7: The architecture of face classifier on the shape-indexed local patches. \u201cConv\u201d means convolution, \u201cMP\u201d means max pooling, and N is the landmark number. The step size in convolution and pooling is 1 and 2, respectively."
    },
    "2012.14142v1-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed model for the task of image denoising. The model consists of an input layer, an output layer, and two convolutional layers. The input layer is a 64x64 image, and the output layer is a 16x16 image. The convolutional layers are used to learn the denoising process. The first convolutional layer has 128 filters, and the second convolutional layer has 512 filters. The ReLU activation function is used after each convolutional layer. The model is trained using the Adam optimizer with a learning rate of 0.001. The model is trained for 100 epochs with a batch size of 16. The model is evaluated on the test set using the mean squared error (MSE) metric.",
        "reference": "Fig. 2: The detailed structure of our HR-to-LR ultrasound image generation network."
    },
    "2109.00038v1-Figure1-1.png": {
        "prediction": "1. The input sequence is encoded by the encoder network, which produces a sequence of hidden states. 2. The hidden states are passed through a feedforward network, which produces the output sequence. 3. The output sequence is then decoded by the decoder network, which produces a sequence of hidden states. 4. The hidden states are passed through a feedforward network, which produces the final output sequence.",
        "reference": "Figure 1. Deriving the severity measure m/r from the epidemics compartmental model. SPEIRD model is schematically shown. Transitions between the compartments are denoted by solid arrows, with the transition rates indicated above arrows. The dashed arrow from I to S indicates the interaction of I and S (infections) leading to the transition to E. The dashed arrow from P to S indicates the potential (reverse) transition from P to S due to the easing of measures. The dashed rectangles indicate parts of the model corresponding to the disease transmission (the left rectangle) and the disease outcome for the detected cases (the right rectangle). The single arrows indicate parts of the model from which the reproduction number R(t) and the severity measure (m/r) are, respectively, inferred. The total number of detected cases (D) corresponds to the sum of A, H and F and is denoted by a double arrow. Compartments are S \u2013 susceptible, P \u2013protected, E \u2013 exposed, I \u2013infected, R \u2013 recovered, A \u2013 active, H \u2013 healed, F \u2013 fatalities, D \u2013 total number of detected cases. r and m represent recovery and mortality rates of active (detected) cases."
    },
    "2202.07728v2-Figure3-1.png": {
        "prediction": "1) Sampling N random perturbations. 2) Computing intermediate activations for the perturbed inputs. 3) Propagate the bounds using verified perturbation analysis.",
        "reference": "Figure 3. Scaling strategy. In order to scale to very large models, we propose to estimate the bounds of an intermediate layer\u2019s activations empirically by (1) Sampling N input perturbations and (2) calculating empirical bounds on the resulting activations for the layer h(\u00b7). We can then form the set PxB which is a subset of the true boundsHxB since the sampling is never exhaustive. We can then plug this set into a verified perturbation analysis method (3) and continue the forward propagation of the inputs through the rest of the network."
    },
    "2204.11695v1-Figure2-1.png": {
        "prediction": "1. Feature Pyramid: The feature pyramid is a hierarchical representation of the input image. It consists of multiple feature maps at different scales, each of which captures different aspects of the image. The feature pyramid is used to extract features from the image and to generate a set of feature maps at different scales. 2. Quality: The quality of the feature pyramid is measured by the difference between the feature maps at different scales. 3. Refinement quality: The refinement quality is measured by the difference between the feature maps at different scales. 4. Alignment: The alignment is measured by the difference between the feature maps at different scales. 5. Offset: The offset is measured by the difference between the feature maps at different scales. 6. Boundary: The boundary is measured by the difference between the feature maps at different scales. 7. Temporal dimension: The temporal dimension is measured by the difference between the feature maps at different scales. 8. Temporal dimension: The temporal dimension is measured by the difference between the feature maps at different scales. 9. Multi-scale anchor feature map: The multi-scale anchor feature map is a set of feature maps at different scales, each of which captures different aspects of the image. 10. Multi-scale boundary quality: The multi-scale boundary quality is measured by the difference between the feature maps at different scales. 11. Multi-scale anchor feature map: The multi-scale anchor feature map is a set of feature maps at different scales, each of which captures different aspects of the image. 12. Multi-scale boundary quality: The multi-scale boundary quality is measured by the difference between the feature maps at different scales. 13. Multi-scale anchor feature map: The multi-scale anchor feature map is a set of feature maps at different scales, each of which captures different aspects of the image. 14. Multi-scale boundary quality: The multi-scale boundary quality is measured by the difference between the feature maps at different scales. 15. Multi-scale anchor feature map: The multi-scale anchor feature map is a set of feature maps at different scales, each of which captures different aspects of the image. 16. Multi-scale boundary quality: The multi-scale boundary quality is measured by the difference between the feature maps at different scales. 17. Multi-scale anchor feature map: The multi-scale anchor feature map is a set of feature maps at different scales, each of which captures different aspects of the image.",
        "reference": "Figure 2: Illustration of the proposed BREM. Untrimmed videos are first fed into the backbone to generate the 1D temporal feature, which is used to construct the feature pyramid and frame-level feature. REM adopts each pyramid feature as input and generates coarse proposals and scores. Then the aligned feature is used for refinement of action location and scores. In parallel, BEM acquires the frame-level feature as input and produces the multi-scale boundary quality map for localization quality prediction."
    },
    "2207.11906v2-Figure1-1.png": {
        "prediction": "1. The model is split into two parts: the encoder and the decoder. The encoder takes the input sequence and produces a sequence of hidden states. The decoder takes the hidden states and produces the output sequence. The output sequence is a sequence of logits, which are the probabilities of each word in the vocabulary.",
        "reference": "Fig. 1. Illustration of the proposed dual-mode ASR supernet training. When the encoder operates in the streaming mode, it is pruned by the binary mask (marked in purple). The predictor is pruned during streaming mode in the similar way, while intact during the nonstreaming mode."
    },
    "2010.06256v1-Figure3-1.png": {
        "prediction": "1. The tree structure of the AST.",
        "reference": "Figure 3. A meta-model for BehaviorTree.CPP (reverseengineered from its XML format)"
    },
    "2206.00770v2-Figure2-1.png": {
        "prediction": "1. Figure 1: Overview of the architecture of a self-driving car.",
        "reference": "Fig. 2. Overview of the Autoware.Auto software stack components"
    },
    "2012.06782v1-Figure3-1.png": {
        "prediction": "1. The input vector is converted to a sequence of 5000 vectors, each representing a frame of audio. 2. The sequence is fed into a convolutional layer, which extracts features from each frame. 3. The convolutional layer is followed by a ReLU activation function, which increases the range of the features to [0, 1]. 4. The sequence is then fed into a maxpooling layer, which reduces the dimensionality of the features to 4996. 5. The sequence is then fed into a convolutional layer, which extracts more features from each frame. 6. The convolutional layer is followed by a ReLU activation function, which increases the range of the features to [0, 1]. 7. The sequence is then fed into a maxpooling layer, which reduces the dimensionality of the features to 4992. 8. The sequence is then fed into a convolutional layer, which extracts more features from each frame. 9. The convolutional layer is followed by a ReLU activation function, which increases the range of the features to [0, 1]. 10. The sequence is then fed into a maxpooling layer, which reduces the dimensionality of the features to 4992. 11. The sequence is then fed into a convolutional layer, which extracts more features from each frame. 12. The convolutional layer is followed by a ReLU activation function, which increases the range of the features to [0, 1]. 13. The sequence is then fed into a maxpooling layer, which reduces the dimensionality of the features to 4992. 14. The sequence is then fed into a convolutional layer, which extracts more features from each frame. 15. The convolutional layer is followed by a ReLU activation function, which increases the range of the features to [0, 1]. 16. The sequence is then fed into a maxpooling layer, which reduces the dimensionality of the features to 4992. 17. The sequence is then fed into a convolutional layer, which extracts more features from each frame. 18. The convolutional layer is followed by a ReLU activation function, which increases the range of the features to [0, 1]. 19. The sequence",
        "reference": "Fig. 3: The proposed CNN architecture for classification of different mental tasks. Note: M denotes the number of classes."
    },
    "1026139-Figure2-1.png": {
        "prediction": "1. The input video is processed by a C3D module to extract features, which are then passed to a video features module. The video features module extracts features from the video frames and passes them to the proposal module. The proposal module generates proposals for the captions, which are then passed to the captioning module. The captioning module generates captions for the proposals, which are then passed to the output captions module. The output captions module generates output captions for the captions.",
        "reference": "Figure 2: Complete pipeline for dense-captioning events in videos with descriptions. We first extract C3D features from the input video. These features are fed into our proposal module at varying stride to predict both short as well as long events. Each proposal, which consists of a unique start and end time and a hidden representation, is then used as input into the captioning module. Finally, this captioning model leverages context from neighboring events to generate each event description."
    },
    "2207.06011v1-Figure1-1.png": {
        "prediction": "1. The reference waveform is extracted from the spectrogram. 2. The noise is added to the reference waveform. 3. The phoneme sequence is extracted from the reference waveform. 4. The phoneme sequence is projected to the decoder. 5. The decoder predicts the phoneme sequence duration. 6. The predicted phoneme sequence duration is projected to the decoder. 7. The decoder predicts the phoneme sequence. 8. The predicted phoneme sequence is projected to the decoder. 9. The decoder predicts the phoneme sequence duration. 10. The predicted phoneme sequence duration is projected to the decoder. 11. The decoder predicts the phoneme sequence. 12. The predicted phoneme sequence is projected to the decoder. 13. The decoder predicts the phoneme sequence duration. 14. The predicted phoneme sequence duration is projected to the decoder. 15. The decoder predicts the phoneme sequence. 16. The predicted phoneme sequence is projected to the decoder. 17. The decoder predicts the phoneme sequence duration. 18. The predicted phoneme sequence duration is projected to the decoder. 19. The decoder predicts the phoneme sequence. 20. The predicted phoneme sequence is projected to the decoder. 21. The decoder predicts the phoneme sequence duration. 22. The predicted phoneme sequence duration is projected to the decoder. 23. The decoder predicts the phoneme sequence. 24. The predicted phoneme sequence is projected to the decoder. 25. The decoder predicts the phoneme sequence duration. 26. The predicted phoneme sequence duration is projected to the decoder. 27. The decoder predicts the phoneme sequence. 28. The predicted phoneme sequence is projected to the decoder. 29. The decoder predicts the phoneme sequence duration. 30. The predicted phoneme sequence duration is projected to the decoder. 31. The decoder predicts the phoneme sequence. 32. The predicted phoneme sequence is projected to the decoder. 33. The decoder predicts the phoneme sequence duration. 34. The predicted phoneme sequence duration is projected to the decoder. 35. The decoder predicts the phoneme sequence. 36. The predicted phoneme sequence is projected to the decoder. 37. The decoder",
        "reference": "Figure 1: System overview of SATTS during training and inference."
    },
    "2206.04962v1-Figure1-1.png": {
        "prediction": "1. Masking Module: Speech Latent Representation",
        "reference": "Fig. 1. The overall architecture of the proposed method. The clean speech S and interference I are fed into the ECAE . The interference consists of background noises, reverberation of both speech and noise signals. After the feature combination is extracted, as the first pre-task, the latent representation of the clean speech signal is learned via ECAE . As the second pre-task, the DM and ERM are estimated in the masking module. Besides, the proposed method utilizes the speech reconstruction losses of each pre-task to train the other pre-task. After the feature maps are recovered in the decoder, the reconstructed clean spectra are obtained as the output by using DCAE . By using the learned speech representation into the mixture representation, the estimated mixtures are produced from the mixture autoencoder (MAE) with unpaired and unseen training mixture spectra Y."
    },
    "2110.05666v1-Figure3-1.png": {
        "prediction": "1. The figure shows the setup of the experiment. The incident X-ray beam is incident on the HOPG sample, which is placed in a heat bath. The transmitted beam is measured by a detector. The heating area is the region where the incident X-ray beam is incident on the HOPG sample. The transmitted beam is measured by a detector. The heat bath is a copper plate that is fixed to the HOPG sample. The HOPG sample is 50 \u00b5m thick and 1.0 mm in diameter. The incident X-ray beam is 0.3 mm in diameter.",
        "reference": "Fig. 3 (Color online) Thermal model for estimation of temperature rise at at the graphite surface. See the text for details."
    },
    "2207.00422v2-Figure5-1.png": {
        "prediction": "1. The user provides historical images and reviews to the model. 2. The model uses a multi-modal encoder to extract features from the images and reviews. 3. The model uses a selection model to select the most relevant features from the images and reviews. 4. The model uses a multi-modal decoder to generate personalized images based on the selected features. 5. The model uses a contrastive learning algorithm to improve the quality of the generated images. 6. The model uses a cross-modal encoder to generate a final image that combines the selected features from the images and reviews. 7. The model uses a multi-modal decoder to generate a final image that combines the selected features from the images and reviews. 8. The model uses a contrastive learning algorithm to improve the quality of the generated images. 9. The model uses a cross-modal encoder to generate a final image that combines the selected features from the images and reviews. 10. The model uses a multi-modal decoder to generate a final image that combines the selected features from the images and reviews. 11. The model uses a contrastive learning algorithm to improve the quality of the generated images. 12. The model uses a cross-modal encoder to generate a final image that combines the selected features from the images and reviews. 13. The model uses a multi-modal decoder to generate a final image that combines the selected features from the images and reviews. 14. The model uses a contrastive learning algorithm to improve the quality of the generated images. 15. The model uses a cross-modal encoder to generate a final image that combines the selected features from the images and reviews. 16. The model uses a multi-modal decoder to generate a final image that combines the selected features from the images and reviews. 17. The model uses a contrastive learning algorithm to improve the quality of the generated images. 18. The model uses a cross-modal encoder to generate a final image that combines the selected features from the images and reviews. 19. The model uses a multi-modal decoder to generate a final image that combines the selected features from the images and reviews. 20. The model uses a contrastive learning algorithm to improve the quality of the generated images. 21. The model uses a cross-modal encoder to generate a final image that combines the selected features from the images and reviews. 22. The model uses a multi",
        "reference": "Figure 5: Illustration of our personalized showcases framework for the given business. We take user historical images and textual reviews as inputs. First, we select an image set that is most relevant to a user\u2019s interest. Thenwe generate natural language explanations accordingly with a multi-modal decoder. A cross-modal contrastive loss and a personalized contrastive loss are applied between each input modality and the explanations. Last, the selected images and generated textual explanations will be organized as multi-modal explanations to users."
    },
    "2103.11913v1-Figure8-1.png": {
        "prediction": "1. The figure shows a square grid with a small square in the center. The small square is surrounded by a larger square with a smaller square in the center. The figure also shows a line that represents the boundary of the larger square. The figure is labeled as \"d(x) = d\" and \"k\" is labeled as the center of the small square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the larger square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the small square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the larger square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the small square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the larger square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the small square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the larger square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the small square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the larger square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the small square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the larger square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the small square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the larger square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the small square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the larger square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the small square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the larger square. The figure is labeled as \"d(x)\" and \"d\" is labeled as the center of the small square. The figure is labeled as \"d(x)\" and \"d",
        "reference": "Figure 8: Illustration of the stencil that refers to the divergence matrix Dn,n+1."
    },
    "2210.07587v2-Figure1-1.png": {
        "prediction": "1. Query: Premise: We had a great breakfast at the waffle shop. Hypothesis: I bought this for myself a short time ago and I love it. An excellent piece for my movie collection. [SEP] Happy. Similarity: 0.9. Label: Positive.",
        "reference": "Figure 1: The overview of the CONENTAIL framework. By casting the classification as a nested entailment task, the model performs classification by telling if a query sentence q entails [premise example p entails hypothesis label h]. In a few-shot setting, the premise is an example sentence; in a zero-shot setting, the premise is a \u201cNULL\u201d placeholder."
    },
    "2204.00172v3-Figure2-1.png": {
        "prediction": "1. The input image is first preprocessed to remove any noise or artifacts. 2. The image is then divided into small patches, each of which is processed separately. 3. The patches are then combined to form a single image, which is then passed through a convolutional neural network to extract features. 4. The features are then passed through a fully connected layer to generate the final output.",
        "reference": "Fig. 2: An overview of our unified framework comprising a supervised branch that learns from source domain data with corresponding annotation, as well as an unsupervised branch that learns from unlabeled target domain data. We perform domain alignment both in the input-level via style-transfer with style references from the opposite domain, and the output-level of the model that guides the training on the target domain with more reliable pseudo-labels. The student model is trained by the combination of two losses, while the teacher model is updated with the exponential moving average weights of the student"
    },
    "2012.05858v3-Figure3-1.png": {
        "prediction": "1. The input image is passed through a WarpingNet, which estimates the motion of the image. 2. The estimated motion is then passed through a ShadingNet, which estimates the shading of the image. 3. The estimated shading is then used to generate a loss function, which is used to train the WarpingNet and ShadingNet. 4. The WarpingNet and ShadingNet are then used to generate a warped image, which is the final output of the system.",
        "reference": "Figure 3: PCNet \u03c0\u0302 architecture and training. PCNet approximates the real project-and-capture process \u03c0 using a deep neural network (WarpingNet + ShadingNet). The inputs are a projector input image x, a camera-captured scene image (under normal light) Is, and a projector direct light mask Im. The output I\u0302x is an inferred camera-captured scene (under superimposed projection). WarpingNet consists of a learnable affine matrix \u03b8aff, thin-plate-spline (TPS) parameters \u03b8TPS and a grid refinement networkW\u03b8r . This coarse-to-fine pipeline allows WarpingNet to learn a fine-grained image sampling grid \u2126 to warp the projector input image x to the camera\u2019s canonical frontal view by \u03c6(x,\u2126), where \u03c6(\u00b7; \u00b7) is a differentiable image interpolator [16] denoted as \u2297. Then, we use the input projector direct light mask Im to exclude occluded pixels by \u03c6(x,\u2126) Im, where is element-wise multiplication. Afterwards, this warped projector image is further used to compute an intermediate rough shading image \u03c6(x,\u2126) Im Is to enforce the occlusion constraint. ShadingNet has a two-branch encoder-decoder structure to capture complex photometric transformations. In particular, it concatenates Is and \u03c6(x,\u2126) Im Is and feeds them to the middle encoder branch. Similarly, \u03c6(x,\u2126) Im is fed to the backbone encoder branch. The skip connections between the two branches model photometric interactions between the three inputs at different levels. In addition, we pass Is to the output layer through three convolutional layers. Finally, the feature maps are fused into one inferred camera-captured scene (under superimposed projection) I\u0302x by the backbone decoder."
    },
    "2102.09652v2-Figure1-1.png": {
        "prediction": "1. The figure shows a grid of dots with a square in the center. The dots are colored red and blue, and the squares are colored red and blue. The figure is labeled \"Kvir\" and \"Kocc\".",
        "reference": "Figure 1: Illustration of the staggered meshes Kocc and Kvir for a quasi-2D system."
    },
    "2207.14568v1-Figure1-1.png": {
        "prediction": "1. Training Stage: The model is trained on the real phone sequences and the generated phone sequences. The discriminator is used to distinguish between the real and generated phone sequences. The generator is trained to generate phone sequences that are similar to the real phone sequences. The model is trained in a self-re-training mode, where the generator is trained in an evaluation mode to generate phone sequences that are similar to the real phone sequences. The model is trained in a GAN training mode, where the discriminator is trained to distinguish between the real and generated phone sequences. The model is trained in a pseudo transcription mode, where the generator is trained to generate phone sequences that are similar to the real phone sequences. The model is trained in a self-re-training mode, where the generator is trained in an evaluation mode to generate phone sequences that are similar to the real phone sequences. The model is trained in a GAN training mode, where the discriminator is trained to distinguish between the real and generated phone sequences. The model is trained in a pseudo transcription mode, where the generator is trained to generate phone sequences that are similar to the real phone sequences. The model is trained in a self-re-training mode, where the generator is trained in an evaluation mode to generate phone sequences that are similar to the real phone sequences. The model is trained in a GAN training mode, where the discriminator is trained to distinguish between the real and generated phone sequences. The model is trained in a pseudo transcription mode, where the generator is trained to generate phone sequences that are similar to the real phone sequences. The model is trained in a self-re-training mode, where the generator is trained in an evaluation mode to generate phone sequences that are similar to the real phone sequences. The model is trained in a GAN training mode, where the discriminator is trained to distinguish between the real and generated phone sequences. The model is trained in a pseudo transcription mode, where the generator is trained to generate phone sequences that are similar to the real phone sequences. The model is trained in a self-re-training mode, where the generator is trained in an evaluation mode to generate phone sequences that are similar to the real phone sequences. The model is trained in a GAN training mode, where the discriminator is trained to distinguish between the real and generated phone sequences. The model is trained in a pseudo transcription mode, where the generator is trained to generate phone sequences that are similar to the real phone sequences. The model is trained in a self-re-training mode, where the generator",
        "reference": "Fig. 1: Framework overview: blue lines denote GAN training; yellow lines denote self re-training; black lines denote the paths which are not trainable."
    },
    "2107.02271v1-Figure7-1.png": {
        "prediction": "1. Model-based data communication: The model is trained on a dataset of network traffic patterns and is used to predict the next packet to be transmitted. The prediction is then used to determine the next packet to be transmitted.",
        "reference": "FIGURE 7: Overview of the model-based data communication."
    },
    "2107.05792v4-Figure3-1.png": {
        "prediction": "1. Example 1 (MLR6): \"Smell correction is done mostly using refactoring. [35] suggested that, to correct eager tests (tests with too many assertions), it is a good idea to group assertions in a private method.\" Code: Suggestion",
        "reference": "Figure 3: Example of classification process used to analyze the contributions by GL use."
    },
    "2010.07804v1-Figure2-1.png": {
        "prediction": "1. Generation of Semantic Information: The input image is transformed into a set of hash codes, which are then used to generate semantic information. This information is then used to generate a similarity matrix, which is used to generate a hash code for the transformed image. The hash code is then used to generate a confidence matrix, which is used to generate a semantic representation of the image.",
        "reference": "Figure 2: Overview of CIMON. (a) CIMON generates semantic information including refined pseudo-graph and confidence matrix for reliable guidance. (b) With data augmentation, CIMON generates two groups of semantic information. The parallel and cross semantic consistency is constructed between features and hash codes under the same group and crossing the different groups respectively. The contrastive consistency is based on two groups of hash codes."
    },
    "215238689-Figure3-1.png": {
        "prediction": "1. Context and Schema Fusion Networks (CSFN) architecture",
        "reference": "Figure 3: The overview of the proposed CSFN-DST. It takes the current dialogue utterance, the previous dialogue state and the schema graph as the input and predicts the current dialogue state. It consists of an embedding layer, context and schema fusion networks, a slot-gate classifier and an RNN-based value decoder."
    },
    "1016639-Figure2-1.png": {
        "prediction": "1. Input data: A video of a woman applying lipstick. 2. CNN: A neural network that takes the input data and outputs a representation of the image. 3. Search: A process that takes the representation output by the CNN and searches for similar images in a database. 4. Output: The result of the search, which is a list of similar images. 5. Apply Lipstick: The process of applying lipstick to the woman's face. 6. \"A woman makeup her face\": The description of the result of the search, which is a list of similar images of women applying lipstick.",
        "reference": "Figure 2. Our multi-task convolutional neural network (CNN). The model is based on CNN with two task branches for classification and caption embedding respectively."
    },
    "2010.07860v3-Figure1-1.png": {
        "prediction": "1. The model is trained on a set of labeled data.",
        "reference": "Figure 1. Architecture of a deep conditional transformation model. Both the shift and interaction predictor can potentially be defined by a structured network including linear terms, (penalized) splines or other structured additive regression terms and deep neural network defined by an arbitrary network structure. While the shift predictor (C\u03a8) is a sum of both subnetwork predictions, the interaction predictor (A B) is only multiplied with a final 1-hidden unit fully-connected layer (network head, vec(\u0393)) after the structured predictors and latent features of the deep neural network are combined with the basis evaluated outcome. The shift and interaction network part together define the transformation function, which transforms the error distribution and yields the final likelihood used as loss function."
    },
    "2011.08010v1-Figure1-1.png": {
        "prediction": "1. The Sentinel-2 image is fed into the trained collector model, which extracts features from the image. 2. The extracted features are then fed into the refiner network, which refines the features and produces the final output. 3. The final output is then used to generate the model output, which is the refined Sentinel-2 image.",
        "reference": "Figure 1: The inference pipeline of our model. The two-stage model first generates a segmentation mask from Sentinel-2 imagery in Stage 1, and then combines Sentinel-2 imagery, initial coarse output, and crowdsourced points in Stage 2 to generate the final segmentation mask. Points collected from either a Trained Collector or Social Media model can be used interchangeably in this model."
    },
    "2210.00486v1-Figure2-1.png": {
        "prediction": "1. Figure 1: A joint model for multi-party collaboration.",
        "reference": "Figure 2: Overview of pMPL"
    },
    "2107.10538v1-Figure4-1.png": {
        "prediction": "1. MARS (Multi-Agent Speech Recognition System) architecture",
        "reference": "Fig. 4: MAGS model of MCCOMP+DIV."
    },
    "2202.05132v2-Figure1-1.png": {
        "prediction": "1. The figure shows a block diagram of a system with two blocks, A and B, connected by a single link. The link is labeled \"u\" and has a direction of flow indicated by an arrow. The figure also shows a block diagram of the same system with the same link labeled \"u\" but with a direction of flow indicated by an arrow. The figure also shows a block diagram of the same system with the same link labeled \"u\" but with a direction of flow indicated by an arrow.",
        "reference": "FIG. 1. (a) Representation of the operator state \u03c1op(t) [Eq. (1)]. Each qubit in Qout is prepared in a maximally entangled state (black dots) with the corresponding qubit Qin, before being time evolved under the channel Nt. (b) Illustration of the Hayden-Preskill protocol [5]. An unknown quantum state |\u03c8\u3009 is used as an input to a small subregion A, while the remaining qubits (B) are prepared in a maximally entangled state with a set of ancillas B\u2032 (circled). If the channel is perfectly scrambling then |\u03c8\u3009 can be reconstructed using the ancillas combined with a subset of output qubits C of the same size as A, regardless of which qubits are in C (qubits in D are discarded). Formally, the final state of the ancillas combined with the outputs C depends on the input state to A through the channel NA\u2192B \u2032C"
    },
    "2202.06453v1-Figure1-1.png": {
        "prediction": "1. The circuit of interest is modeled as a block diagram, with the load and the external input signal uext.",
        "reference": "Figure 1: Simulation diagram for an interconnection between a circuit of interest and a load."
    },
    "2010.09602v1-Figure5-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed model for acoustic speech recognition. The model consists of an encoder, a decoder, and a loss function. The encoder takes as input the acoustic features and the corresponding text tokens, and produces a sequence of latent representations. The decoder takes as input the latent representations and the text tokens, and produces the corresponding acoustic features. The loss function is used to train the model to minimize the difference between the predicted and the actual acoustic features.",
        "reference": "Fig. 5: Detailed architecture of proposed TTS model. Dashed line denotes feedback loop. Number in bracket denotes neural layer size. FC denotes a fully connected layer. During inference, only prior and decoder are used."
    },
    "2204.10523v1-Figure1-1.png": {
        "prediction": "1. The input to the model is a sequence of words, which are represented as vectors in the embedding layer.",
        "reference": "Figure 1: The probabilistic graphical model of two-covariance PLDA"
    },
    "2011.12854v6-Figure3-1.png": {
        "prediction": "1. Concept Embedding Layer: This layer takes the input image and embeds it into a vector representation. The vector representation is used to represent the concept of the image.",
        "reference": "Figure 3: Neuro-Symbolic XIL for improved explanations and interaction. (Top) Neuro-Symbolic Concept Learner with Slot-Attention and Set Transformer. (Bottom) Neuro-Symbolic revision pipeline with explanations of the model before (gray) and after applying the feedback (orange)."
    },
    "2207.04317v1-Figure1-1.png": {
        "prediction": "1. The movie \"Goodfellas\" is a crime drama film directed by Martin Scorsese and released in 1990. It is based on the book \"Wiseguy\" by Nicholas Pileggi, which tells the story of mobster Henry Hill. The movie stars Ray Liotta, Robert De Niro, and Joe Pesci. The movie is about the rise and fall of a mobster and his friends. The movie is known for its strong performances, particularly by De Niro and Liotta. The movie is a classic and has been widely praised by critics.",
        "reference": "Figure 1: Illustration of counterfactual explanation in neural recommender systems."
    },
    "2202.02646v1-Figure1-1.png": {
        "prediction": "1. Abstract Retrieval: Retrieves the most similar documents to the query document based on their abstracts.\n2. Similarity Ranker: Computes the similarity between the query document and each retrieved document based on their abstracts.\n3. Claim C: Classifies the query document as Claim C based on the similarity with the retrieved documents.\n4. TF-IDF Vectorizer: Converts the abstracts of the retrieved documents into TF-IDF vectors.\n5. Rationale Selection: Selects the most relevant sentences from the abstracts of the retrieved documents based on the similarity with the query document.\n6. Decision: Makes a decision about the claim based on the selected sentences and the similarity with the query document.\n7. Label Prediction: Predicts the label of the query document based on the selected sentences and the similarity with the query document.",
        "reference": "Figure 1: Architectural illustration of the RERRFACT scientific claim verification pipeline."
    },
    "117714-Figure2-1.png": {
        "prediction": "1. The model takes in the sequence of actions and the corresponding intention outputs as input. 2. The model predicts the next intention output based on the previous intention output and the corresponding action. 3. The model predicts the next action based on the previous action and the corresponding intention output. 4. The model predicts the next intention output based on the previous intention output and the corresponding action. 5. The model predicts the next action based on the previous action and the corresponding intention output. 6. The model predicts the next intention output based on the previous intention output and the corresponding action. 7. The model predicts the next action based on the previous action and the corresponding intention output. 8. The model predicts the next intention output based on the previous intention output and the corresponding action. 9. The model predicts the next action based on the previous action and the corresponding intention output. 10. The model predicts the next intention output based on the previous intention output and the corresponding action. 11. The model predicts the next action based on the previous action and the corresponding intention output. 12. The model predicts the next intention output based on the previous intention output and the corresponding action. 13. The model predicts the next action based on the previous action and the corresponding intention output. 14. The model predicts the next intention output based on the previous intention output and the corresponding action. 15. The model predicts the next action based on the previous action and the corresponding intention output. 16. The model predicts the next intention output based on the previous intention output and the corresponding action. 17. The model predicts the next action based on the previous action and the corresponding intention output. 18. The model predicts the next intention output based on the previous intention output and the corresponding action. 19. The model predicts the next action based on the previous action and the corresponding intention output. 20. The model predicts the next intention output based on the previous intention output and the corresponding action. 21. The model predicts the next action based on the previous action and the corresponding intention output. 22. The model predicts the next intention output based on the previous intention output and the corresponding action. 23. The model predicts the next action based on the previous action and the corresponding intention output. 24. The model predicts the next intention output based on the previous intention output",
        "reference": "Figure 2. Visualization of our motion-triggered model. Our model consists of an RNN with LSTM cell encoder (blue block) and a Policy Network (yellow block). At each frame, RNN will generate an anticipated intention according to a new embedded representation g and the previous hidden state h of the RNN. The policy will generate the motion-trigger decision a for next frame, based on motion representation fm and the hidden state h of the RNN. The orange circle represents the fusion operation (details in Sec. 3.2). The red and black circles represent a trigger and non-trigger decision of policy network, respectively (details in Sec. 3.3). When a = 0, fo is empty since it is not processed."
    },
    "2107.02290v1-Figure8-1.png": {
        "prediction": "1. The figure shows a cylindrical object with a circular cross-section. The object is placed in a uniform magnetic field, and the magnetic field is perpendicular to the axis of the cylinder. The object is also placed in a uniform electric field, and the electric field is parallel to the axis of the cylinder. The figure also shows the directions of the magnetic and electric fields, as well as the directions of the forces acting on the object due to the magnetic and electric fields.",
        "reference": "FIGURE 8 Coaxial cable model and its cross section. The inner, black region \u03a90 models the copper wire, the white region \u03a91 the air insulator and the outer, gray region \u03a92 the conducting shield35."
    },
    "2206.13082v1-Figure2-1.png": {
        "prediction": "1. The figure shows the architecture of the proposed method for the task of semantic segmentation. The input image is first processed by a pre-trained VGG-16 network to extract features. These features are then passed through a fully connected layer to obtain the initial segmentation map. The initial segmentation map is then refined using a dynamic voxel feature encoder, which is composed of a skip connection, a fully connected layer, and a dynamic voxel self-attention module. The refined segmentation map is then passed through a fully connected layer to obtain the final segmentation map. The figure also shows the network architecture of the dynamic voxel feature encoder.",
        "reference": "Fig. 2. Illustration of the PST architecture. (a) is the main component of PST, which follows an encoder-decoder pipeline. (b) is a demonstration of dynamic voxelization used in the encoding part."
    },
    "2107.08402v1-Figure2-1.png": {
        "prediction": "1. The server receives the local model updates from the clients. 2. The server calculates the truth inference score and the temporal data reliability score. 3. The server removes outlier data based on the truth inference score and the temporal data reliability score. 4. The server removes outlier data based on the truth inference score and the temporal data reliability score. 5. The server downloads the global model.",
        "reference": "Fig. 2: Overview of Proposed Methods"
    },
    "2102.04593v1-Figure3-1.png": {
        "prediction": "1. The input image is passed through a CNN to extract features. 2. The features are passed through a real image discriminator to generate a real image representation. 3. The real image representation is passed through a fake image discriminator to generate a fake image representation. 4. The fake image representation is passed through a real image discriminator to generate a real image representation. 5. The real image representation is passed through a fake image discriminator to generate a fake image representation. 6. The fake image representation is passed through a real image discriminator to generate a real image representation. 7. The real image representation is passed through a fake image discriminator to generate a fake image representation. 8. The fake image representation is passed through a real image discriminator to generate a real image representation. 9. The real image representation is passed through a fake image discriminator to generate a fake image representation. 10. The fake image representation is passed through a real image discriminator to generate a real image representation. 11. The real image representation is passed through a fake image discriminator to generate a fake image representation. 12. The fake image representation is passed through a real image discriminator to generate a real image representation. 13. The real image representation is passed through a fake image discriminator to generate a fake image representation. 14. The fake image representation is passed through a real image discriminator to generate a real image representation. 15. The real image representation is passed through a fake image discriminator to generate a fake image representation. 16. The fake image representation is passed through a real image discriminator to generate a real image representation. 17. The real image representation is passed through a fake image discriminator to generate a fake image representation. 18. The fake image representation is passed through a real image discriminator to generate a real image representation. 19. The real image representation is passed through a fake image discriminator to generate a fake image representation. 20. The fake image representation is passed through a real image discriminator to generate a real image representation. 21. The real image representation is passed through a fake image discriminator to generate a fake image representation. 22. The fake image representation is passed through a real image discriminator to generate a real image representation. 23. The real image representation is passed through a fake image discriminator to generate a fake image representation. 24. The fake image representation is passed through a",
        "reference": "Figure 3. DCGAN architecture"
    },
    "2012.09727v2-Figure1-1.png": {
        "prediction": "1. The input stream is split into segments using a STFT module. 2. The segments are passed through a speaker profile selection module to select the speaker embedding. 3. The selected speaker embedding is passed through a mixture module to generate the speaker embedding. 4. The speaker embedding is passed through a speaker inventory construction module to generate the speaker inventory. 5. The speaker inventory is passed through a feature layer to generate the speaker feature. 6. The speaker feature is passed through a mixture module to generate the speaker feature. 7. The speaker feature is passed through a feature layer to generate the speaker feature. 8. The speaker feature is passed through a mixture module to generate the speaker feature. 9. The speaker feature is passed through a feature layer to generate the speaker feature. 10. The speaker feature is passed through a mixture module to generate the speaker feature. 11. The speaker feature is passed through a feature layer to generate the speaker feature. 12. The speaker feature is passed through a mixture module to generate the speaker feature. 13. The speaker feature is passed through a feature layer to generate the speaker feature. 14. The speaker feature is passed through a mixture module to generate the speaker feature. 15. The speaker feature is passed through a feature layer to generate the speaker feature. 16. The speaker feature is passed through a mixture module to generate the speaker feature. 17. The speaker feature is passed through a feature layer to generate the speaker feature. 18. The speaker feature is passed through a mixture module to generate the speaker feature. 19. The speaker feature is passed through a feature layer to generate the speaker feature. 20. The speaker feature is passed through a mixture module to generate the speaker feature. 21. The speaker feature is passed through a feature layer to generate the speaker feature. 22. The speaker feature is passed through a mixture module to generate the speaker feature. 23. The speaker feature is passed through a feature layer to generate the speaker feature. 24. The speaker feature is passed through a mixture module to generate the speaker feature. 25. The speaker feature is passed through a feature layer to generate the speaker feature. 26. The speaker feature is passed through a mixture module to generate the speaker feature. 27. The speaker feature is passed through a feature layer to",
        "reference": "Fig. 1. (A) The architecture of the proposed continuous speech separation using speaker inventory. The Speaker inventory construction module forms the speaker inventory from the long mixture by using Kmeans clustering; the long mixture is split into small segments, and the speaker profile selection module selects two relevant profiles from the inventory for each segment; the speech separation module fuses the selected speaker profiles into the system for source separation. (B) Multiplicative adaptation of the selected profiles ep1 and ep2 . (C) Stitching procedure of adjacent segment outputs in a long recording."
    },
    "2101.05804v3-Figure1-1.png": {
        "prediction": "1. The figure shows the three different phases of the system: (a) the superfluid phase, (b) the normal phase, and (c) the Mott insulator phase. The system is in the superfluid phase when the temperature is higher than the critical temperature, and the system is in the normal phase when the temperature is lower than the critical temperature. The Mott insulator phase is the phase where the system is in between the superfluid and normal phases.",
        "reference": "Figure 1: Illustration of the three relevant physical regimes for a system of massive fermions [purple dots] with a long range interaction mediated by a scalar field [blue]. T and n are the characteristic fermion kinetic energy and number density, respectively. For ultrarelativistic fermions, in panel (a); and interparticle distances above the interaction range \u223c M\u22121\u03c6 , in panel (c); there are no long range effects."
    },
    "2107.02955v1-Figure2-1.png": {
        "prediction": "1. Prismatic Joint",
        "reference": "Fig. 2. Our model for the flat elastic terrain. Each tile is connected to the floor by a spring-loaded prismatic joint."
    },
    "2207.04913v1-Figure1-1.png": {
        "prediction": "1. The target samples are fed into the Barycenter sampler to obtain the optimal samples. 2. The optimal samples are fed into the LFDs P to obtain the coupling matrix y. 3. The coupling matrix y is used to predict the class likelihood of the target samples. 4. The adaptive inference is performed using optimal transport.",
        "reference": "Fig. 1. An overview of our WDRDG framework, consisting of three components: (a) Wasserstein uncertainty set construction for each class based on the empirical Wasserstein barycenters and radius obtained from given source domains. One constraint is added to control the discriminability of LFDs; (b) distributionally robust optimization to solve for the least favorable distributions; (c) adaptive inference for target testing samples based on probability mass on LFDs and coupling matrix from optimal transportation between barycenter samples and target samples."
    },
    "2202.13626v1-Figure3-1.png": {
        "prediction": "1. The user connects to the home network and activates the IoT devices.",
        "reference": "Figure 3: Remote control (IFTTT) of home IoT service with the centralized learning model."
    },
    "19488885-Figure4-1.png": {
        "prediction": "1. Parser state: The parser state is a representation of the current parsing state, which includes the current word, the previous word, and the current dependency relation. The parser state is updated at each step of the parsing process.",
        "reference": "Figure 4: MTL model. Token representations are computed both by a task-specific and a shared BiLSTM. Their outputs are concatenated with the parser state embedding, identical to Figure 2, and fed into the task-specific MLP for selecting the next transition. Shared parameters are shown in blue."
    },
    "2011.10683v1-Figure5-1.png": {
        "prediction": "1. strawberries 2. spinach 3. knife 4. ...",
        "reference": "Figure 5: Illustration of response composition in Flow-RG."
    },
    "2103.15997v2-Figure1-1.png": {
        "prediction": "1. Criss-cross Attention block",
        "reference": "Fig. 1. Diagram of the criss-cross attention module. For each position in the input feature map, the attention module generates a sparse attention map by aggregating information in the horizontal and vertical axes. After the second operation, each pixel has extracted context from all the pixels."
    },
    "2102.03921v1-Figure3-1.png": {
        "prediction": "1. The architecture of the proposed model for learning to respond to visual stimuli in an environment with differentiable and non-differentiable states.",
        "reference": "Figure 3: Least action classifier network architecture. Blocks of solid lines are differentiable. Sample images are from dataset CIFAR-10, which is collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton[MIT License](https://www.cs.toronto.edu/ kriz/cifar.html)"
    },
    "2102.03921v1-Figure2-1.png": {
        "prediction": "1. The figure shows the basic architecture of a convolutional neural network (CNN) for image classification. The input image is passed through several convolutional layers, which extract features from the image. These features are then passed through a fully connected layer, which outputs the predicted class of the image. The figure also shows the flow of information through the network, with the input image at the top and the predicted class at the bottom.",
        "reference": "Figure 2: Markov decision process diagram [Malashin, 2019]"
    },
    "2210.05232v1-Figure4-1.png": {
        "prediction": "1. The input image is fed into the network, which outputs a set of keypoints and a confidence map.",
        "reference": "Fig. 4. An illustration of the iterative confidence-based pose estimation network."
    },
    "2107.00679v1-Figure1-1.png": {
        "prediction": "1. Layer 1: Validators (blockchain network) 2. Layer 2: Non-validators (normal network communication) 3. Layer 3: Devices (blockchain communication)",
        "reference": "Fig. 1: Three-layer system architecture [7]"
    },
    "2110.00808v2-Figure1-1.png": {
        "prediction": "1. Domain A: Image of a field with green grass and brown dirt. 2. Domain B: Image of a field with green grass and brown dirt. 3. Transition Model: A model that takes an image from Domain A and maps it to an image from Domain B. 4. Cyclic Loss: A loss function that measures the similarity between the output of the transition model and the input image from Domain B. 5. Latent Codes: A set of parameters that represent the transition model. 6. Regularization Loss: A loss function that penalizes the model for having too many parameters. 7. Adversarial Loss: A loss function that measures the similarity between the output of the transition model and the input image from Domain A.",
        "reference": "Figure 1: Cycle-consistent world model. In the pictured situation, a sequence of top camera images is used a the input. The images are encoded frame-wise into latent states and forward predicted by the transition model. From these latent codes, reconstructed top camera images and images translated to semantic top camera images are calculated. From the translated images, cyclic latent codes are calculated. Finally, the four losses can be calculated, which enforce equations (2) and (3)."
    },
    "1245438-Figure1-1.png": {
        "prediction": "1. Convolutional layers: These layers are responsible for extracting features from the input image. They are followed by an activation function (e.g., ReLU) and a batch normalization layer. The output of each convolutional layer is passed through a max pooling layer to reduce the spatial dimensions of the feature maps.",
        "reference": "Fig. 1: The network architecture with pipe-lined components."
    },
    "407011-Figure3-1.png": {
        "prediction": "1. Convolutional layers: These layers are responsible for extracting features from the input image. They are used to extract features such as edges, corners, and textures from the image. These features are then used to classify the image as a positive or negative example.",
        "reference": "Fig. 3. Illustration of a triplet network by incorporating intra-class variance into triplet embedding, in which the joint learning objective is to minimize the combination of softmax loss and triplet loss (consisting of inter-class and intra-class triplet loss)."
    },
    "2204.04591v2-Figure3-1.png": {
        "prediction": "1. The user sends a request to the publisher for a decryption key over the NDN network. 2. The publisher generates a CP-ABE key pair and sends the public key to the user. 3. The user generates a CP-ABE key pair and sends the public key to the publisher. 4. The publisher encrypts the data with the public key and sends it to the user. 5. The user decrypts the data with the CP-ABE key pair. 6. The user retrieves the data over the NDN network. 7. The user sends the data over the NDN network to the publisher. 8. The publisher generates and sends the CP-ABE key pair to the user. 9. The user generates and sends the CP-ABE key pair to the publisher. 10. The publisher generates and sends the CP-ABE key pair to the user. 11. The user generates and sends the CP-ABE key pair to the publisher. 12. The publisher generates and sends the CP-ABE key pair to the user. 13. The user generates and sends the CP-ABE key pair to the publisher. 14. The publisher generates and sends the CP-ABE key pair to the user. 15. The user generates and sends the CP-ABE key pair to the publisher. 16. The publisher generates and sends the CP-ABE key pair to the user. 17. The user generates and sends the CP-ABE key pair to the publisher. 18. The publisher generates and sends the CP-ABE key pair to the user. 19. The user generates and sends the CP-ABE key pair to the publisher. 20. The publisher generates and sends the CP-ABE key pair to the user. 21. The user generates and sends the CP-ABE key pair to the publisher. 22. The publisher generates and sends the CP-ABE key pair to the user. 23. The user generates and sends the CP-ABE key pair to the publisher. 24. The publisher generates and sends the CP-ABE key pair to the user. 25. The user generates and sends the CP-ABE key pair to the publisher. 26. The publisher generates and sends",
        "reference": "Fig. 3. Encryption and Decryption Model"
    },
    "2107.04229v1-Figure3-1.png": {
        "prediction": "1. Tracheal Sound Files",
        "reference": "Fig. 3. Flowchart of ground-truth labels establishment."
    },
    "2107.04229v1-Figure4-1.png": {
        "prediction": "1. The figure shows the architecture of the BERT model, which is a pre-trained language model. It consists of two main components: the encoder and the decoder. The encoder takes in a sequence of words and outputs a sequence of hidden states, while the decoder takes in a sequence of hidden states and outputs a sequence of words. The encoder and decoder are connected through a bidirectional attention mechanism, which allows the model to attend to both the past and future context in the sequence. The model is trained using a combination of masked language modeling and next sentence prediction tasks.",
        "reference": "Fig. 4. Architecture of the CNN-BiGRU model."
    },
    "2101.10102v2-Figure3-1.png": {
        "prediction": "1. The input is fed into the DNN model, which outputs a score for each possible output. 2. The score is compared to the score difference threshold, which is determined by the output and the score difference model. 3. If the score difference is greater than the threshold, the model is considered robust. 4. If the score difference is less than the threshold, the model is considered non-robust. 5. The model is then analysed using the PAC model, which outputs the maximal values of the PAC model robustness.",
        "reference": "Figure 3: Framework of PAC-model robustness analysis base on model learning"
    },
    "2010.13007v1-Figure1-1.png": {
        "prediction": "1. Pre-trained encoder: This is a neural network that has been trained on a large amount of data to learn the acoustic representations of speech. It is used to encode the input audio signals into a fixed-length vector representation.",
        "reference": "Fig. 1. An illustration of the model architecture used in probing experiments."
    },
    "2012.05892v2-Figure2-1.png": {
        "prediction": "1. A: Infrared detector. B: Blackbody. C: Sapphire lens. D: Doped-silica wafer. E: Fused-silica wafer. F: Actual measured response. G: Provided measured response. H: Temperature stage. I: Blackbody. J: Sapphire wafer. K: Fused-silica wafer. L: Fused-silica wafer. M: Sapphire wafer. N: Fused-silica wafer. O: Sapphire wafer. P: Fused-silica wafer. Q: Sapphire wafer. R: Fused-silica wafer. S: Sapphire wafer. T: Fused-silica wafer. U: Sapphire wafer. V: Fused-silica wafer. W: Sapphire wafer. X: Fused-silica wafer. Y: Sapphire wafer. Z: Fused-silica wafer. AA: Infrared detector. BB: Blackbody. CC: Sapphire lens. DD: Doped-silica wafer. EE: Fused-silica wafer. FF: Actual measured response. GG: Provided measured response. HH: Temperature stage. II: Blackbody. JJ: Sapphire wafer. KK: Fused-silica wafer. LL: Fused-silica wafer. MM: Sapphire wafer. NN: Fused-silica wafer. OO: Sapphire wafer. PP: Fused-silica wafer. QQ: Sapphire wafer. RR: Fused-silica wafer. SS: Sapphire wafer. TT: Fused-silica wafer. UU: Sapphire wafer. VV: Fused-silica wafer. WW: Sapphire wafer. XX: Fused-silica wafer. YY: Sapphire wafer. ZZ: Fused-silica wafer. AA: Infrared detector. BB: Blackbody. CC: Sapphire lens. DD: Doped-silica wafer. EE: Fused-silica wafer. FF: Actual measured response. GG: Provided measured response. HH: Temperature stage. II: Blackbody. JJ: Sapphire wafer. KK: Fused-silica wafer. LL: Fused-silica wafer. MM: Sapphire wafer. NN: Fused-silica wafer. OO: Sapphire wa",
        "reference": "Fig. 2. Experimental demonstration of Planck spectroscopy. (A) Schematic of the experimental setup. Thermal emission from samples on a temperature stage was collected using a lens and focused onto an infrared detector. (B) Normalized voltage vs. temperature for five different samples: a laboratory blackbody, a sapphire wafer, a fused-silica wafer, and two n-doped silicon wafers with doping levels of 1020 and 1021 cm-3. (C) The extracted response function of the measurement setup, obtained by using the blackbody reference (dotted circle), is in good agreement with the expected response function based on vendor-provided properties of the detector, heater window, and lens. (D-G) Measured emissivity of the samples using Planck spectroscopy (circles), and the corresponding spectra measured using a Fouriertransform spectrometer (solid line)."
    },
    "1446589-Figure1-1.png": {
        "prediction": "1. Image Feature Space: This space contains the features of the images, such as the color, shape, and texture of the objects in the image. These features are extracted using image processing techniques. 2. Label Embedding Space: This space contains the embeddings of the labels, which are the categories or labels of the objects in the image. These embeddings are learned using a machine learning algorithm. 3. Label Space: This space contains the labels of the objects in the image. The labels are mapped to the label embeddings in the label embedding space. 4. Data Distribution: This refers to the distribution of the data in the dataset, such as the number of images in each category or the distribution of the labels. 5. Structure Transfer: This refers to the process of transferring the structure of the image features to the label embeddings. This is done using a machine learning algorithm, such as a neural network. 6. Optimization: This refers to the process of optimizing the parameters of the machine learning algorithm to improve the performance of the model.",
        "reference": "Figure 1: Illustration of the proposed method. The manifold structure (the straight lines) in the label embedding space is transferred to the image feature space for synthesizing the virtual signature (the purple star) of an unseen class. The purple arrow points to a refined signature, which demonstrates that the synthesized virtual signature is optimized after running the Expectation-Maximization algorithm so that unseen data are assigned to labels according to the data distribution."
    },
    "2205.03991v2-Figure4.1-1.png": {
        "prediction": "1. Generalized G-PDE",
        "reference": "FIGURE 4.1. Overview of nonlocal diffusion processes proposed in related work [ETT15, GO09, AVMRTM10] and their interrelations to the nonlocal G-PDE (4.7). The approaches highlighted by the blue region only model the image labeling problem. Edge labels refer to the corresponding sections of the analytical ablation study."
    },
    "2012.12554v1-Figure1-1.png": {
        "prediction": "1. The model takes in an image and predicts the next frame to annotate.",
        "reference": "Figure 1: Overview of our video annotation process. A human annotator draws a box on the first frame of the video; then our guiding frame selection mechanism predicts the next frame to annotate and the process iterates. Our method automatically and accurately interpolates bounding boxes for all frames that were not directly annotated by the human. Hence, at the end of the process object annotations are generated for all frames."
    },
    "216738-Figure2-1.png": {
        "prediction": "1. The convolutional layers are stacked to form the convolutional sequence.",
        "reference": "Figure 2. Architecture of proposed image fusion CNN illustrated for input exposure stack with images of size h\u00d7w. The pre-fusion layers C1 and C2 that share same weights, extract low-level features from input images. The feature pairs of input images are fused into a single feature by merge layer. The fused features are input to reconstruction layers to generate fused image Yfused."
    },
    "2210.01442v1-Figure3-1.png": {
        "prediction": "1. Experiment control system",
        "reference": "Figure 3: Schematic overview of the experiment control system."
    },
    "2103.04138v2-Figure2-1.png": {
        "prediction": "1. The figure shows a schematic of a pipe with a funnel at one end and a nozzle at the other. The pipe is labeled with the variables L, u, and q. The funnel is labeled with the variable Bq, and the nozzle is labeled with the variable Dq. The arrows indicate the flow of fluid through the pipe.",
        "reference": "Figure 2. Leading-order Feynman diagram contributing to the hard kernels Tij(u), where the local four-quark operators are represented by the black square."
    }
}