{
    "2204.00424v1-Figure6-1.png": {
        "prediction": "The image shows a flow diagram illustrating an architecture that uses cloudy and cloud-free input images S1 and S2. The cloudy S1 and S2 images are passed through an encoder E to generate a shared representation. This representation is then passed through a decoder D to generate the cloud-free image S2'. The cloud-free S2' output is compared against a loss/metrics block to train the model.",
        "reference": "Figure 6. MSOPunet network. (S1t\u22121, S2t\u22121), (S1t, S2t) and (S1t+1, S2t+1) denotes the input pairs of optical and SAR images. S2t\u2032 denotes the reference image used to compute the loss (during training) and the metrics (at inference time from test data) from the reconstructed optical image S\u03022t."
    },
    "2202.03651v2-Figure1-1.png": {
        "prediction": "The image shows a perception model being used to discover failures in a traffic scene. The perception model learns a scene graph representation from the input traffic images. By analyzing the learned scene graphs, the model can detect \"challenging groups\" of consistent failure causes, which are discovered at the intervention level. This allows the discovery of failures in the perception model's understanding of the traffic scene.",
        "reference": "Figure 1: Instead of retrospectively discovering individual failure cases for perception, we actively search for causal interventions (edits) to existing scenes that consistently result in perception failures. The middle shows an example of a single intervention causing perception failure, which we attribute to the intervention, as opposed to the left where a combinatorial set of factors could explain the error. Consistent failures through this type of intervention constitute a challenging group for the perception model as seen on the right."
    },
    "2011.15102v2-Figure1-1.png": {
        "prediction": "The image shows a learner being given harder tests by a tester. This leads to the learner developing better solutions, which are stored in a \"Test Bank\" of various test cases, represented by different colored books on a shelf.",
        "reference": "Figure 1: Learning by passing tests. A tester model creates tests with increasing levels of difficulty from a test bank to evaluate a learner model. The learner continuously improves its learning ability to deliver better solutions for passing those difficult tests."
    },
    "14078997-Figure2-1.png": {
        "prediction": "The image depicts the flowchart of a Convolutional Neural Network (CNN) for video frame prediction and reconstruction.\n\nInput frames from times t-3 to t are passed through a stack of affine transforms (At-1 to At-3) to extract input affine transforms. These are passed into the CNN module.\n\nThe CNN module predicts the affine transforms for the next timestep, At+1 and At+2. It also outputs a reconstruction of the input frame at time t+1.\n\nThe predicted affine transforms At+1 are used to transform the ground truth frame at t+2 to calculate the mean squared error (MSE) loss, which is used to train the CNN.",
        "reference": "Figure 2: Outline of the transformation-based model. The model is a CNN that takes as input a sequence of consecutive affine transforms between pairs of adjacent video frames. It predicts the affine transform between the last input frame and the next one in the sequence. We compute affine transforms (6 parameters per patch) for overlapping patches of size 8 \u00d7 8 in each video frame. Learning operates in the space of transformations as shown inside the dashed box. The front-end on the left is a module that estimates the affine transforms between pairs of consecutive input frames. The post-processor on the right reconstructs a frame from the predicted set of affine transforms and it is only used at test time."
    },
    "14078997-Figure3-1.png": {
        "prediction": "The image illustrates the architecture of a Convolutional Neural Network (CNN). It shows multiple inputs A1 through A7 being fed into a shared CNN module. The CNN processes each input independently to generate corresponding outputs A1 through A7 on the right side. This architecture allows the same CNN to be applied to multiple inputs in parallel to efficiently extract features or generate predictions from each input using the shared convolutional layers and weights of the CNN module.",
        "reference": "Figure 3: Outline of the system predicting 4 frames ahead in time. Only affine transforms A1, A2 and A3 are provided, and the model predicts A\u03034, A\u03035, A\u03036 and A\u03037, which are used to reconstruct the next 4 frames. Since affine parameters are continuous values and the whole chain of CNNs is differentiable, the whole unrolled system can be trained by back-propagation of the error. Note that CNNs all share the same parameters"
    },
    "2204.10509v1-Figure2-1.png": {
        "prediction": "The image shows the architecture of the PEGE (Positive-Emotion-Guided Empathetic) Model. It consists of a Transformer module that takes in the context utterances u1 to ut-1 and generates the response yt-1. This response, along with the context, is fed into two softmax layers to calculate the NLL (negative log-likelihood) loss.\n\nThe transformer hidden state ht is passed through a VAD estimator to predict the Valence, Arousal and Dominance (VAD) of the response. The predicted emotion f(|C|) and ground truth emotion st are used to calculate an emotional loss, which is split into a PEG Term for positive emotions and an NER Term for negative emotions.\n\nFinally, an Emotion Classifier takes the context and response representations to predict p(pos|u1) and p(neg|u1), the probabilities of the response being positive or negative. The overall model is trained to minimize the NLL loss and emotional loss terms.",
        "reference": "Figure 2: Illustration of our PEGE Model"
    },
    "2202.05735v2-Figure1-1.png": {
        "prediction": "The image shows a process for analyzing waveform data from photoplethysmography (PPG) to extract useful information.\n\nThe main steps are:\n\n1. The raw PPG waveform is processed to derive the instantaneous pulse rate (IPR) time series. This shows how the pulse rate varies over time.\n\n2. Features are engineered from the IPR time series, including calculating the pulse rate variability (PRV) and extracting morphological measures (MOR) from the pulse waveform shape.\n\n3. The engineered features are used as inputs to machine learning models:\n- The PRV features are input to a SleepPPG-Net deep learning model for sleep staging \n- The MOR features are input to a BM-FE neural network model\n\nSo in summary, the raw PPG signal is processed in a pipeline to ultimately allow sleep staging and extraction of cardiovascular information using deep learning on the derived time series and waveform features. The two key ML models used are SleepPPG-Net and BM-FE.",
        "reference": "Fig. 1: Three approaches to ML for sleep staging from PPG. Our new model, SleepPPG-Net takes as input the PPG waveform (WAV).The derived time series (DTS) and feature engineering (FE) approaches allow comparison with SOTA algorithms described in the literature."
    },
    "2101.02429v1-Figure6-1.png": {
        "prediction": "The image shows a 3D plot comparing the eigenvectors (in blue) versus the neural decomposition (in red) of some data points. The eigenvectors form a rectangular grid, while the neural decomposition vectors are slightly curved, originating from near the origin. The plot is bounded between -1 and 1 on all three axes labeled x, y, and z.",
        "reference": "Figure 6: Illustration of the neural decomposition procedure for d = 2 (i.e. Z\u2217 \u2208 R3\u00d73). The dashed red arrows correspond to the eigenvectors of Z\u2217 (q1, q2, q3) and the solid blue arrows show the decomposed vectors p1 and p2. In this example, the rank of Z \u2217 is 2 where q1 and q2 are its two principal eigenvectors. The eigenvalue corresponding to the eigenvector q1 is zero. The light blue colored surface shows the Lorentz cones z = \u221a x2 + y2 and z = \u2212 \u221a x2 + y2. We observe that the decomposed vectors p1 and p2 lie on the boundary of Lorentz cones."
    },
    "1246860-Figure1-1.png": {
        "prediction": "The diagram shows the overall architecture of the sentiment analysis model. The model consists of several components:\n\nDocument Sentiment module: Takes the full document as input and produces a document-level sentiment representation.\n\nSentence Sentiment module: Takes each individual sentence as input and produces sentence-level sentiment representations. There are two sentences shown in the example document.\n\nMIL (Multiple Instance Learning) Transfer module: Transfers information between the document and sentence levels in both directions. The document sentiment is used to guide the sentence sentiment predictions, and the aggregated sentence sentiments inform the final document-level sentiment.\n\nWord Sentiment module: Although not the focus, the diagram also shows a word-level sentiment component that feeds into the sentence representations.\n\nAt the top, a logistic classifier makes the final document sentiment prediction (positive or negative), informed by the document representation, aggregated sentence representations, and a direct word embedding of the document.\n\nThe key aspects are the hierarchical document modeling at document, sentence and word levels, and the MIL-based transfer of information across the levels to improve the document-level sentiment classification.",
        "reference": "Figure 1: Deep multi-instance transfer learning approach for review data."
    },
    "2102.10828v2-Figure7-1.png": {
        "prediction": "The image shows a triangular lattice structure where each node is labeled with its degree deg(v). The top node has degree 1, the middle row nodes have degree deg(v0) - 1, a_{l-1}, and |w_0| = 1, and the bottom row nodes have degrees n-1, m, m+1, and a_0. The nodes are connected by edges to form triangular shapes.",
        "reference": "Figure 7. Illustration of Equation (4.1)."
    },
    "1022460-Figure1-1.png": {
        "prediction": "The image shows the progression of object detection in a curriculum learning setup for autonomous vehicles over time. It starts with detecting simple objects like a bike, then moves to detecting wheeled vehicles like cars, and finally detects more complex objects like road bikes and tandem bikes.\n\nThe curriculum is represented at the bottom, showing the objective functions becoming more complex over the 4 stages, with the shapes of the functions becoming more curvy and detailed.\n\nThis allows the autonomous vehicle perception system to gradually learn to detect increasingly challenging objects in a structured manner over time, rather than trying to detect everything at once from the beginning.",
        "reference": "Figure 1. A feedback based learning model. The basic idea is to make predictions in an iterative manner based on a notion of the thus-far outcome. This provides several core advantages: I. enabling early predictions (given total inference time T , early predictions are made in fractions of T ); II. naturally conforming to a taxonomy in the output space; and III. better grounds for curriculum learning."
    },
    "2110.03031v3-Figure1-1.png": {
        "prediction": "The image shows a probabilistic graphical model represented as a diagram. There are 4 vertical bars or rectangles on the left representing the latent variable Z. Arrows point from Z to two observed variables represented by horizontal rectangles, labeled \u03b1 and g(\u22c5). This depicts that the observed variables \u03b1 and g(\u22c5) are conditionally dependent on the latent variable Z.",
        "reference": "Figure 1. RieszNet architecture."
    },
    "2101.11189v1-Figure2-1.png": {
        "prediction": "The image illustrates the high-level architecture of a 3D object detection system. The input is an RGB image and the backbone convolutional neural network generates feature maps. These feature maps are then used to predict the center position, center offset, object size, and head point regression for detected objects. The head point estimation is used to derive the head offset orientation vector (xh, yh) for each object.",
        "reference": "Fig. 2: The overall framework of our arbitrary-oriented ship detection method. Feature maps are first generated by using a fully convolutional network. Afterward, the peaks of the center feature map are selected as center points. Then, the center points offsets, object sizes and head regression locations are regressed on the corresponding feature maps on the position of each center point. The potential head points are collected by extracting peaks with confidence scores larger than 0.1 on the head feature map. The final head location is obtained by assigning each regressed location to its nearest potential head points."
    },
    "2204.01715v1-Figure7-1.png": {
        "prediction": "Based on the flowchart in the image, data from various sources like parquet files, images, CSV files, JSON files etc. is ingested using Tensorflow Dataset and PyTorch DataLoader libraries. This data is then transformed using arbitrary Python libraries like pandas, numpy etc.\n\nFor the learning component, the flowchart shows Estimator, AutoEstimator libraries being used. The models and applications listed include Tensorflow, PyTorch, MXNet, BigDL and Ray.\n\nOn the runtime, Apache Spark is used. And the underlying infrastructure consists of Kubernetes clusters, Hadoop/Yarn clusters, Ray clusters, AWS and so on.",
        "reference": "Figure 7. BigDL-Orca architecture"
    },
    "2202.07508v3-Figure3-1.png": {
        "prediction": "The image shows a flowchart of a convolving filters process using ResBlocks. The input image of size 21x21 is passed through a series of ResBlocks h1 through h4. The outputs of each ResBlock go through a reshape operation, followed by a global pooling layer. The final output goes through a fully connected (FC) layer to produce the classification result lk.",
        "reference": "Figure 3. Architecture of the dynamic deep linear kernel."
    },
    "2202.11824v1-Figure1-1.png": {
        "prediction": "The flowchart depicts a DNA-Sequencing Service that can be exploited by an attacker. Here are the main steps:\n\n1. The attacker sends a BioCybernetic Hacking Alert to the hospital.\n\n2. The hospital transfers this alert to the DNA Sequencing Service. \n\n3. The DNA Assembler component performs shotgun sequencing on the input data.\n\n4. The sequenced data is passed to an Input Control Convolutional Neural Network (CNN).\n\n5. The CNN processes the input and sends an activation signal.\n\n6. The activation signal triggers the Trojan Software Interrupt (SWI) that was implanted by the attacker. \n\n7. Activating the Trojan SWI allows the attacker to take control.\n\n8. The attacker can now access the system and potentially cause damage or steal sensitive data.\n\nIn summary, it shows how an attacker could compromise a DNA sequencing service by sending a malicious input that triggers a hidden malware (the Trojan SWI) to gain unauthorized access to the system.",
        "reference": "Fig. 1: Hybrid Trojan Bio-Cyber Hacking Attack. Steps 1 - 3 indicate a typical genetic sequencing operation for patients. Steps 4 - 6 indicate a situation where a hacker has embedded their IP address and Port number into a DNA that will trigger a remote connection from a Trojan-horse infected software tool leading to a connection to the attacker in Step 8. Our proposed approach utilizes Deep-Learning to detect Trojan payload in digital data using encoded into DNA strands that can prevent the attack."
    },
    "2011.04105v1-Figure3.5-1.png": {
        "prediction": "The diagram depicts the Network-Wide Views of a system, which consists of three main planes connected by arrows:\n\n1. The Data Plane, which feeds into the\n2. Dissemination Plane, which in turn feeds into the  \n3. Decision Plane\n\nThe Decision Plane is shown providing network-level objectives back to the overall system.\n\nAn arrow from the Discovery Plane also feeds into the Dissemination Plane.\n\nThe entire system operates under Direct Control, as indicated by the arrow at the bottom spanning the width of the diagram.",
        "reference": "Figure 3.5: Network-level objectives."
    },
    "2202.01326v1-Figure5-1.png": {
        "prediction": "The flowchart outlines a decision process for determining if triplets seen in an experiment are from singlet fission (SF) that enhances a photovoltaic (PV) device.\n\nIt starts by asking if triplets from SF are enhancing the PV device. If yes, an IQE (internal quantum efficiency) measurement is performed. \n\nIf the IQE is greater than 100%, this indicates the SF material is absorbing and the triplet effect is seen, confirming triplets from SF are playing a role.\n\nIf IQE is not over 100%, MPC (magnetic field effect on photocurrent) and MPL (magnetic field effect on photoluminescence) measurements from the SF material are done. If a triplet effect is seen in both MPC and MPL with the same sign, it confirms triplets from SF are present, even if they are not enhancing the PV device.\n\nIf no triplet effect is seen in MPC and MPL, or there is no evidence of an effect to begin with, then the triplets seen are likely from SF in rapid exothermic systems but are not playing a role in the device.",
        "reference": "Figure 5: Flowchart describing the process for determining if SF is improving photocurrent in a device."
    },
    "2204.08696v2-Figure1-1.png": {
        "prediction": "The image shows an encoder-decoder architecture for facial image generation. The encoder downsamples the input image through a series of convolution, LeakyReLU, and downsampling layers to extract a latent representation. This goes through 4 Feature Refinement Modules (FRMs) before being upsampled by the decoder through LGCM (Local-Global Feature Cooperation Modules) and MFFU (Multi-scale Feature Fusion Units) to generate a 3xHxW output face image at the original resolution. The key components are the downsampling and upsampling paths in the encoder and decoder, the FRMs for refining the latent features, and the LGCM and MFFU modules for fusing multi-scale features during upsampling in the decoder to reconstruct the high-resolution output.",
        "reference": "Fig. 1: The complete structure of the proposed CNN-Transformer Cooperation Network (CTCNet). CTCNet is a U-shaped symmetrical hierarchical network with three stages: encoding stag, bottleneck stage, and decoding stage. Among them, the encoding stage is designed to extract local and global features with different scales, and the decoding stage is designed for feature fusion and image reconstruction."
    },
    "2101.08366v2-Figure2-1.png": {
        "prediction": "The image shows a Quadratic Residual Network (QRes) layer. It takes input x and passes it through two parallel paths: one directly to the addition operation, and the other through two weight matrices W1 and W2 with a nonlinearity (represented by the crossed circle) in between. The result of the addition then goes through a final nonlinearity (the crossed square) to produce the output y. The parameter b represents a bias term added in the second path.",
        "reference": "Figure 2: Overview of our proposed Quadratic Residual Network (QRes) layer in comparison with plain DNN layer. Blue rectangular boxes represent trainable parameters and round boxes represent operations (purple \u201c\u00d7\u201d: multiplication, orange \u201c+\u201d: addition, green \u201d\u00b7\u201d: Hadamard product, and cyan \u201d\u03c3\u201d: activation operator)."
    },
    "2205.09510v2-Figure6.4-1.png": {
        "prediction": "The image shows a flowchart depicting a quantum machine learning pipeline. The steps are:\n\n1. Taking an average over quantum data\n2. Passing the average through a parameterized quantum circuit denoted by \u03b8\n3. Feeding the output of the quantum circuit into a classical optimizer\n\nSo in summary, it illustrates using a quantum computer to process quantum data, with the results then optimized using classical techniques. This hybrid quantum-classical approach is common in quantum machine learning algorithms.",
        "reference": "Figure 6.4: An illustration of the \u201cQC\u201d setting of quantum machine learning, in which data are quantum and processing is classical."
    },
    "2205.09510v2-Figure6.11-1.png": {
        "prediction": "The image shows a diagram depicting a classical optimizer. It takes in an input function U(\u03b8), averages over that function using \u03c8(\u03b8), and outputs the optimized result denoted as \u3008\u03c8(\u03b8)|F|\u03c8(\u03b8)\u3009.",
        "reference": "Figure 6.11: Illustration of the operation of a VQE. The classical optimizer aims at minimizing the expected value \u3008F \u3009\u03c8(\u03b8)\u3009 = \u3008\u03c8(\u03b8)|F |\u03c8(\u03b8)\u3009 of the observable F ."
    },
    "2206.05240v1-Figure2-1.png": {
        "prediction": "The diagram illustrates the curriculum learning process for posterior sampling in a reinforcement learning setup.\n\nThe agent samples from its belief distribution over models \u03c0(at|ot, z) given the current observation ot and latent variable z ~ q(\u00b7|\u03c4t-1) inferred from the history \u03c4t-1. This results in an action that is passed to the bidder module, which interacts with the auction environment to generate a reward. The reward, action, and new observation are added to the history to update the dynamics model M\u03c9_{t+1}.\n\nThe curriculum specifies a schedule for the ROI (return on investment) and CT (likely computational cost) constraints over a series of T stages, with the constraints becoming tighter over time to guide the learning process.",
        "reference": "Figure 2: Model Overview. We propose CBRL for RCB. Guided by a curriculum sequence, the agent learns to adaptively trade-off constraints and the objective in non-stationarymarkets. During deployment, the agent updates its belief over the market dynamics based on its past experience, and acts through posterior sampling."
    },
    "2210.02172v1-Figure2-1.png": {
        "prediction": "The image shows an overview of a neural network-based reinforcement learning system. The key components are:\n\n- A target Q-value that the network is trying to approximate, evaluated based on the network's current outputs\n- An evolution neural network that gets updated via mini-batches to minimize the approximation error between the target and evaluated Q-values\n- A target neural network that provides the target Q-values and is periodically copied from the evolution network\n- An experience memory that stores past states, actions and rewards to sample mini-batches from for training\n- The environment, which contains macro-cell and small-cell base stations that the reinforcement learning agent interacts with to take actions and receive rewards\n- An optimal policy is learned over time to maximize the long-term discounted reward by mapping states to optimal actions\n\nThe diagram illustrates how the neural networks, replay memory, and interaction with the wireless environment enable the RL agent to iteratively improve its policy for optimizing the cellular network.",
        "reference": "Fig. 2: An illustration of deep Q-learning model for the ISRE-based 6G vehicular aided HetNet environment. The model uses a deep neural network to find the approximate Q-values. The model includes experience replay mechanism (to remove the correlation between different observations), a feature set (given as input to the deep neural network), a target Q-network for updating the primary Q-network and the simulation environment for extracting different parameters."
    },
    "14844989-Figure2-1.png": {
        "prediction": "The image illustrates two different architectures for performing multiple tasks using deep learning:\n\nMulti-task Network:\n- Takes input images and passes them through a shared network (3x224x224 to 256x13x13)\n- The shared representation is then fed into a 512-dimensional layer\n- From there, it branches into a ranking task output and a classification task, which takes the 512-dim representation and passes it through \"Pos Pair\", \"Neg Pair\", and pairwise feature map layers before the final classification output\n\nCross-domain Architecture:\n- Takes input images from the main task and an auxiliary dataset\n- Each set of images goes through its own network (3x224x224 to 256x13x13)\n- The main task representation goes to a \"Pair\" layer (512x13x13)\n- The auxiliary representation goes to \"Joint feature maps\" (256x13x13)\n- These are combined for the final cross-domain task output\n\nThe bottom shows the different types of loss functions used during training: convolutional layer, fully connected layer, softmax loss, triplet loss, and contrastive loss.",
        "reference": "Figure 2: The framework of the proposed multi-task deep network and the cross-domain architecture. The crossdomain architecture is only used when an auxiliary dataset is needed for training."
    },
    "2205.03661v1-Figure1-1.png": {
        "prediction": "The flowchart illustrates the process of designing and binarizing baseline deep networks to maintain accuracy while enhancing computing speed and reducing memory overhead.\n\nThe main steps are:\n\n1. Design of Baseline Deep Networks\n- Design basic blocks\n- Determine network depth \n- Determine the number of basic blocks\n\n2. Methods of Binarizing Baseline Networks\n- Sign(x): Replace Sign(x) with the piecewise function F(x)\n- Introduce the learnable parameter \u03b1\n\n3. Three Goals\n- Maintain the accuracy of net\n- Enhancing computing speed\n- Reduce memory overhead\n\nThe flowchart shows how the baseline deep network is preprocessed and binarized using specific methods in order to achieve the three stated goals of maintaining accuracy while improving computing speed and memory efficiency.",
        "reference": "Fig. 1. Overview of methods adopted"
    },
    "162169-Figure3-1.png": {
        "prediction": "The image shows an ontology for event modeling. The top level EVENT concept has attributes Event, Place, and Time. An event can have an OBJECTIVE_INFLUENCE on Place, Time, Influencing_entity, Influencing_situation, and Dependent_entity.\n\nAn event leads to a TRANSITIVE_ACTION which has attributes Event, Place, Time, Agent, Cause, and Patient. This transitive action can CAUSE_TO_MAKE_NOISE for a given Purpose, Place, Time, Agent, Cause, and Sound_maker. \n\nMaking noise (MAKE_NOISE) involves a Sound emitted at a certain Place and Time by a Noisy_event with a particular Sound_source. Causal and inheritance relations are shown between the various concepts.\n\nIn summary, the ontology captures the key entities, attributes, and relationships involved in modeling events, the actions they cause, and the resulting emissions of sound. It provides a structured representation for reasoning about events and their auditory effects.",
        "reference": "Figure 3 Partial illustration of frames, roles, and lexical units related to the CAUSE TO MAKE NOISE frame, from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are unfilled bars. No particular significance is ascribed to the ordering of a frame\u2019s roles in its lexicon entry (the selection and ordering of roles above is for illustrative convenience). CAUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here."
    },
    "2202.03344v2-Figure2-1.png": {
        "prediction": "The flowchart depicts an algorithm that selects the optimal hyperparameters for a model through an iterative process:\n\n1. Initialize the latent variables iz, ip, iq to 1.\n2. Set the latent variable Z to a truncated set based on candidate distributions D of degrees r and a q-norm adaptivity criterion.\n3. Apply a hybrid LAR approach to select the basis An for the mean function.\n4. Initialize the coefficients \u03b2^.\n5. Check if iz > Nz. If no, increment iz, ip, iq and repeat from step 2. If yes, proceed.\n6. Check if ip > Np or the current degree does not improve CV score. If no, increment ip, iq and repeat from step 5. If yes, proceed. \n7. Check if iq > Nq or two consecutive increases of q-norm do not improve CV score. If no, increment iq and repeat from step 6. If yes, proceed.\n8. Estimate coefficients \u03b1/d for each cross-validation training set.\n9. Evaluate the out-of-sample performance using the CV score.\n10. Check if \u03b1^ has converged. If no, update \u03b1^ and repeat from step 8. If yes, output the model with the best CV score.",
        "reference": "Figure 2: Flow chart of the procedure to adaptively build a stochastic PCE"
    },
    "2207.12764v1-Figure1-1.png": {
        "prediction": "The image shows an OCEL extraction process which involves several steps:\n\n1. Enrich OCEL (Object-Centric Event Log) with object and graph related attributes. \n2. Select the desired object type for clustering.\n3. Perform clustering using either agglomerative or k-means clustering algorithms.\n4. Assign events to the generated clusters. \n\nThe key points mentioned are:\n- Existence: The cluster contains at least one object that belongs to the event.\n- All: The cluster contains all the objects that belong to the event.\n\nSo in summary, it depicts an approach to group/cluster objects and their related events from an object-centric event log into meaningful clusters using attribute enrichment and clustering techniques.",
        "reference": "Fig. 1: Overview of the proposed framework."
    },
    "2103.13446v3-Figure2-1.png": {
        "prediction": "The flowchart shows the process for a node receiving input observations f(i)_input, sharing them with its neighbors j in N_i, receiving f(l)_com communications from those neighbors at timesteps k=0 through k=3, concatenating the received neighbor communications, and using the result as the node input for the next timestep.",
        "reference": "Fig. 2: ModGNN\u2019s message aggregation module. In this diagram, the system is shown from a centralized point of view. First, the raw observation or output from the last layer is transformed by finput. Then, for each transmission up to K hops, the data from the neighboring agents is passed through fcom and then aggregated. The output is the set of the data from each k-hop neighborhood up to K."
    },
    "2107.06007v1-Figure14-1.png": {
        "prediction": "The image shows the system architecture diagram for the Waste Isolation Pilot Plant in Carlsbad, NM on the left, and the SLAC National Accelerator Lab on the right.\n\nFor the Waste Isolation Pilot Plant, there are two SQL Server instances that connect to Transfer PC 1 and Transfer PC 2. These transfer PCs then connect to PLCs (Programmable Logic Controllers) and on-site control PCs. Off-site control PCs are also shown at the bottom.\n\nThe SLAC National Accelerator Lab has a simpler architecture, with two SQL Server instances connecting directly to worldwide user computers, without any intermediate components.\n\nIn both diagrams, control parameters flow into the systems, while system data output is generated.",
        "reference": "Figure 14. The EXO-200 slow control architecture consisted of three major parts- the MySQL backbone, the embedded system, and the distributed system."
    },
    "2107.12859v2-Figure3-1.png": {
        "prediction": "The image shows an architecture diagram for Graph Message Passing Pose Estimation. It consists of three main parts:\n\n(a) Part Feature Extraction - PointNet networks extract features v(0) from each input part P1 through P6. These part features are shared weights.\n\n(b) Forward Sequential Encoding - The part features v(0) are passed through a sequence of Gated Recurrent Units (GRUs) to encode information in a forward direction, producing hidden states h(t) at each step t.\n\n(c) Reverse Sequential Encoding - The final hidden state from the forward encoding is passed through another sequence of GRUs in the reverse direction. The outputs g(t) from both directions are concatenated and passed through a fully connected layer to predict the final pose a(t) for each part.\n\nThe graph message passing allows information to propagate between the connected parts in both forward and reverse sequential order to refine the pose estimates.",
        "reference": "Figure 3: One iteration of our Recurrent Graph Learning framework. (a) We process part features and compute a graph message. (b) The message is encoded sequentially in our bidirectional GRU framework. (c) The features generated by the forward and reverse GRU are used to regress part-pose. We use three such iterations in our framework."
    },
    "2012.00248v1-Figure1-1.png": {
        "prediction": "The image depicts the relationship between objects, a Fourier lens, and a hologram plane in the context of holography. Light rays originating from points A on an object pass through a Fourier lens, which focuses them to corresponding points A' on the hologram plane. The distances d0, d1 and d2 represent the separations between the object plane, Fourier lens, and hologram plane along the optical axis (z-axis). The focal length of the Fourier lens is denoted by f. This diagram illustrates the basic optical setup for recording a Fourier hologram.",
        "reference": "Fig. 1. Schematic diagram for synthesizing the digital Fourier hologram. The letter objects are placed at a distance d in front of the Fourier lens in the direction of propagation of light. The real-valued hologram are drawn in the logarithmic scale."
    },
    "2010.06310v1-Figure2-1.png": {
        "prediction": "The diagram illustrates the cross-supervised mechanism for joint entity and trigger extraction. It consists of the following key components:\n\n1. Ground-truth joint annotation (entity + trigger) sequences are used for sequence-to-sequence labeling to produce an annotated tag (entity + trigger) distribution.\n\n2. A SoftMax layer is applied to the annotated tag distribution.\n\n3. An RNN and embedding layer are used to encode the sentences. \n\n4. Entity extraction is performed using:\n- The ground-truth entity distribution Fe\n- An annotated trigger distribution Meta-path based adjacency matrix M' \nThis produces a predicted entity distribution Fe.\n\n5. KL divergence is computed between the ground-truth and predicted entity distributions as part of the entity extraction loss.\n\n6. Similarly, trigger extraction uses:\n- The ground-truth trigger distribution Ft\n- An \"entity-trigger\" HING adjacency matrix M'\nThis produces a predicted trigger distribution Ft.\n\n7. KL divergence is computed between the ground-truth and predicted trigger distributions as the trigger extraction loss.\n\n8. The final loss is a combination of the sequence-to-sequence labeling cross-entropy loss and the cross-supervised entity and trigger extraction losses.\n\nThe key aspect is that indirect \"entity-trigger\" co-occurrence relationships are encoded in M' by the neural cross-supervising layer to enable the cross-supervised mechanism.",
        "reference": "Fig. 2: The framework of the joint-event-extraction model with our proposed cross-supervised mechanism."
    },
    "2202.05336v1-Figure1-1.png": {
        "prediction": "The flowchart illustrates the video object segmentation process. It includes the following steps:\n\n1. Motion estimation is performed on the fixed image If and input image Ii to obtain the motion Mi between the images. \n\n2. Mi is used to warp the input images, generating the warped images warp(Ii, Mi).\n\n3. Background subtraction is applied between the warped images and fixed image, producing the background subtracted result Bi.\n\n4. Bi undergoes background generation to create the background image.\n\n5. The background image is subtracted from Ii in an inverse warping step to obtain the initial binary segmentation Si_init.\n\n6. Dynamic entropy map computation is performed on Si_init to get the entropy map C(x).\n\n7. Pixel-wise distance threshold computation is done using C(x) to obtain the threshold map R(x). \n\n8. The final binary segmentation Si is produced by applying post-processing to the initial segmentation Si_init using the computed maps.\n\nSo in summary, it depicts a multi-step segmentation pipeline leveraging motion estimation, background subtraction, entropy calculation and post-processing to extract the moving foreground objects from a video sequence.",
        "reference": "Fig. 1. Block diagram of DBSGen"
    },
    "2107.03026v1-Figure7-1.png": {
        "prediction": "The image shows that Ecological Systems and Social Economic Systems mutually influence each other, illustrating the interconnectedness and feedback between natural and human systems.",
        "reference": "Figure 7: Influence matrix schematic graph, based on [5, Figure 5]"
    },
    "1381187-Figure1-1.png": {
        "prediction": "The image shows the high-level architecture of an encoder-decoder model. The encoder consists of convolutional layers (conv), batch normalization (BN), rectified linear unit activation (ReLU), and pooling. The decoder performs upsampling followed by conv, BN, and ReLU layers to generate the output prediction/segmentation map.",
        "reference": "Fig. 1: Illustration of the SegNet architecture applied to EO data."
    },
    "13567899-Figure3-1.png": {
        "prediction": "The image depicts a convolutional neural network architecture diagram. It shows several convolutional layers (Conv1a, Conv2a, Conv3a, Conv3b, Conv4a, Conv4b, Conv5a, Conv5b) with different filter sizes and numbers of feature maps. There are also pooling layers (Pool1, Pool2, Pool3, Pool4) between some of the convolutional layers to downsample the spatial dimensions.\n\nThe convolutional layers feed into a series of fully connected layers (Conv3c, Deconv3, Conv-pre, Deconv4, Conv4c, Deconv5) that ultimately output a 4x14x14 sized tensor. This final tensor is concatenated with the feature maps from earlier layers (Conv1a, Conv5a, Conv5b) to produce a final 16x112x112 output, likely for a dense prediction task like semantic segmentation. The Voxel wise loss is computed at this final output resolution.\n\nSo in summary, it is a multi-scale convolutional neural network that combines features at different resolutions through skip connections and upsampling in order to make dense, pixelwise predictions on an input image while leveraging both low-level and high-level features. The specific connectivity and filter dimensionalities are detailed in the diagram.",
        "reference": "Figure 3. V2V Architecture for Voxel Prediction. The lower part (below dashed line) consists of layers from C3D [28]. Connected to these layers we have three 3D convolution layers: Conv3c,Conv4c,Conv-pre use filters of size 3\u00d7 3\u00d7 3 with stride 1\u00d7 1\u00d7 1. Both Deconv5 and Deconv4 are deconvolutional layers employing kernels of size 4 \u00d7 4 \u00d7 4 with output stride of 2 \u00d7 2 \u00d7 2. Deconv3 has kernel size 8 \u00d7 4 \u00d7 4 and output stride of 4 \u00d7 2 \u00d7 2. The numbers inside the boxes represent the number of learning filters in that layer, while the numbers near the boxes (above or below) represent the size of output signals produced by that layer. The part inside the thick-dashed box is application-dependent."
    },
    "2204.10374v1-Figure1-1.png": {
        "prediction": "The flowchart depicts a hierarchical reinforcement learning (RL) setup for gesture recognition on an Android device. The key components are:\n\n- RL agent 0 directly interacts with the Android device to perform gestures. It receives the gesture GVF (Generalized Value Function), last touch position as state, and pixel observations + reward from the device.\n\n- RL agent 1 selects the gesture GVF to be performed by agent 0. It receives the GVF parameters from agent 2 and provides the selected GVF to agent 0.  \n\n- RL agent 2 selects the gesture class to be performed. It receives the gesture class as state and a reward, then provides the GVF parameters for that class to agent 1.\n\nSo in summary, agent 2 selects the high-level gesture class, agent 1 selects the specific GVF within that class, and agent 0 executes the low-level touch actions on the device to perform the selected gesture. The rewards propagate up the hierarchy to enable end-to-end learning of the gesture recognition task.",
        "reference": "Figure 1 | Gesture Hierarchy. The architecture used for the Android applications is based on a 3-layer hierarchy: (1) The lowest level operates over GVFs corresponding to all supported gestures; (2) The middle layer selects a gesture GVF given the latest pixel image in AndroidEnv and its agent is trained to maximize the return associated with the task that the agent is trained on; and (3) The top layer selects a single gesture class for the task and the agent is trained to maximize the average per step reward. All levels are operated by distributed DQN agents."
    },
    "2010.00041v1-Figure3-1.png": {
        "prediction": "The flowchart depicts a genetic algorithm process that starts with the random creation of an initial population. The fitness of this population is evaluated, and if the stopping criteria are not satisfied, the algorithm proceeds with passing elites to the next population and performing roulette selection of operators based on fitness. The selected operators are cross-over and mutation, which produce a new population. This process iterates until the stopping criteria are met, at which point the algorithm returns the best individual found.",
        "reference": "Fig. 3. The optimization flowchart for the inverse material design."
    },
    "2103.14005v2-Figure9-1.png": {
        "prediction": "The image shows the architecture of ResNet50, which consists of:\n\n1. A Conv layer with dimensions n x 64 x h/2 x w/2\n2. Block 1 with dimensions n x 256 x h/4 x w/4  \n3. Block 2 with dimensions n x 512 x h/8 x w/8\n4. Block 3 with dimensions n x 1024 x h/16 x w/16\n5. Block 4 with dimensions n x 2048 x h/32 x w/32\n6. A Global Average Pooling layer that produces an Embedding of dimensions n x 2048 x 1 x 1\n7. A Fusion Layer\n8. A final layer for Class Prediction with dimensions n x c\n\nThe bottom part shows the same architecture for a second ResNet50 model.",
        "reference": "Figure 9. The Multi Input Fusion Classifier end task architecture. The orange box shows the frozen encoder."
    },
    "2110.14764v2-Figure1-1.png": {
        "prediction": "The image shows a pipeline for document classification with the following stages:\n\n1. Raw Documents - Represented by stacks of documents for different languages (Chinese, English, French).\n\n2. First-tier Classifiers - Takes the raw document vectors as input and produces vectors of calibrated posterior probabilities for each class.\n\n3. Vectors of Calibrated Posterior Probabilities - The output from the first-tier classifiers, shown as colored rectangular boxes.\n\n4. Stacking - Takes the posterior probability vectors and concatenates them into a stacked vector representation.\n\n5. Meta-Classifier - Takes the stacked probability vector as input and produces the final classification output.\n\nThe key aspects are using multiple first-tier classifiers, calibrating their outputs into class probabilities, stacking the probabilities, and using a meta-classifier to make the ultimate prediction from the stacked probabilities.",
        "reference": "Fig. 1. The Fun architecture, exemplified with |L|=3 languages (Chinese, Italian, English). Note that the different term-document matrices in the 1st-tier may contain different numbers of documents and/or different numbers of terms. The three grey diamonds on the left represent calibrated classifiers that map the original vectors (e.g., TFIDF vectors) into |Y|-dimensional spaces. The resulting vectors are thus aligned and can all be used for training the meta-classifier, which is represented by the grey diamond on the right."
    },
    "1422748-Figure1-1.png": {
        "prediction": "The image shows a text preprocessing pipeline consisting of the following steps in order:\n\n1. Language Detection\n2. Text Normalization \n3. Sentence Breaking\n4. Tokenization\n5. Entity Extraction\n6. Entity Disambiguation and Linking\n\nThe input to the pipeline is a document text, and the output after going through these preprocessing steps is entities that have been disambiguated and linked.",
        "reference": "Figure 1: Overview of the Lithium NLP pipeline"
    },
    "2101.07004v1-Figure2-1.png": {
        "prediction": "The image shows a diagram of downlink data transmission between a transmitter and receiver. The transmitter has a channel state information (CSI) timer \u03c4csi and a processing timer \u03c4pro. During the uplink pilot phase, the CSI is transmitted from the receiver to the transmitter. Then during the downlink data transmission phase, the transmitter processes the data and sends it to the receiver.",
        "reference": "Figure 2. Block diagram of one transmission block."
    },
    "1061647-Figure5-1.png": {
        "prediction": "The image shows the process of deconvolution to generate a 60x60 nose heat map from a 256x256 input image. The input image is passed through a series of deconvolution layers, which upsample and transform the features to produce the final 60x60 nose heat map output. A 1x1 convolution layer is then applied to the nose heat map to generate the final result.",
        "reference": "Fig. 5. Architecture of the pose estimation network. First 5 layers copied from VGGM, followed by a deconvolution layer. A 1x1 convolution layer is then used to predict each output heat map."
    },
    "2103.06081v1-Figure2-1.png": {
        "prediction": "The image shows several diagrams related to polarization and optics:\n\n(a) A sample plane with x and y axes, and two cones representing the polarization of light along the z axis.\n\n(b) Illustration of circularly polarized light, showing the electric field vector rotating either clockwise (right-handed, red arrows) or counterclockwise (left-handed, black arrows) when looking along the direction of propagation.\n\n(c) Intensity profiles for right-handed (-C0) and left-handed (C0) circularly polarized light.\n\n(d) Intensity profile for linearly polarized light (0). \n\n(e) Representation of elliptically polarized light (Ue0).\n\n(f) The C1 Stokes parameter as a function of x, showing a sinusoidal variation.\n\n(g) Intensity profiles Ue for elliptically polarized light as a function of x, showing a periodic envelope modulation.\n\nThe bottom part shows a representation of the back focal plane (BFP) of an objective lens, with different regions corresponding to different polarizations - linear (CPL) at the center and elliptical/circular (CPL) towards the edges.",
        "reference": "Figure 2. Illustration of the illumination scheme of double SIM using far-field optics. (a) Schematic of one possible approach to generate the illumination patterns with structured electric energy density and OC simultaneously. Structured OC patterns formed by the interference of (b) two L-CPL beams ( )C x+ and (c) two R-CPL beams ( )C x\u2212 . (d, e) Structured patterns of ( )eU x . The OC patterns in (b) and (c) possess the opposite handedness. The eU patterns in (d) and (e) are the same. Line-cut profile of the (f) OC and (g) eU along the dashed lines in (b-e). Scale bar in (b) is 02\u03c0/k and applicable for (d-e)."
    },
    "2202.03373v1-Figure5-1.png": {
        "prediction": "The diagram shows the architecture of CurveNLU, a neural network system that estimates curves from input data. The main components are:\n\n- Sigmoid curve estimation block which takes an input clip of data [0-1] and estimates curve parameters W.\n- A sequence of blocks A1 through An which each take the input clip and previous block's curve estimate C(X) to produce an updated curve estimate. \n- An input of a 2D grid with values 0 or 1.\n- The output is a 2D curve shown in the bottom right.\n\nThe [0-1] clipping operation and CurveNLU blocks are also detailed, showing they map a 2D binary input to a continuous output curve representation.",
        "reference": "Figure 5. An illustration of Curve Non-Linear Unit. This layer can be seen as a learnable non-linear activation function between 0 and 1. Based on Eq. 3, the learned function always follows concave down increasing curves to increase feature intensities."
    },
    "2207.07828v1-Figure2-1.png": {
        "prediction": "The image shows the architecture of a generative adversarial network (GAN) for image generation and manipulation. The key components are:\n\n- A Generator network that takes a noise vector z and a sentence embedding as input, and generates an image through a series of upsampling, convolution and normalization layers. Skip connections are used between mirrored layers.\n\n- A Structural Prior Estimator that takes the sentence embedding as input and predicts a segmentation map of the expected image layout. This acts as an additional conditioning input to the Generator at multiple scales.\n\n- Two Discriminator networks, Da and De, that try to distinguish between real and generated images. Da looks at the entire image while De looks at image patches. They are trained adversarially against the Generator.\n\nThe sentence embedding is replicated and concatenated with the image representation at multiple scales in the Generator and Discriminators. The overall architecture allows generating images that match a given text description, while the structural prior helps ensure the generated images have a plausible semantic layout matching the description.",
        "reference": "Fig. 2. Overview of the proposed Structural Prior guided Generative Adversarial Transformers (SPGAT). The SPGAT contains one generator, two discriminators, and one structural prior estimator. The generator is used to explore non-local information with the guidance of a structural prior estimator (SPE) for better clear image restoration. We build the skip connections between the generator and discriminators so that the discriminators can better discriminate between real and fake features in the generator for generating more natural images. Meanwhile, SPE is also utilized to guide the discriminators for better estimations. The generator takes a low-light image L as the input and outputs an enhanced image E\u0302. The SPE is input the corresponding structure S of L by gradient operation and outputs the estimated structural prior P\u0302. Both the two discriminators take the concatenation of the enhanced image E\u0302 and estimated structural P\u0302 as the input, which is regarded as the fake label. However, there is no real label feature. To that aim, we generate real label features by inputting the normal-light image to the generator and its corresponding structure to SPE. PW-STB illustrated in Fig. 3 is the basic block of the generator, discriminators, and SPE."
    },
    "2102.08827v1-Figure3-1.png": {
        "prediction": "The image shows the scene and skill ontology for an autonomous driving system. The scene ontology consists of 5 layers: L1 road-level, L2 traffic infrastructure, L3 temporary manipulation of L1 and L2, L4 objects, and L5 environment.\n\nThe scene elements in each layer determine the system, behavioral, and planning skills required. The skill ontology shows that the system skill determines the behavioral skill, which in turn determines the perception, action, data acquisition, and actuation skills. The connections between the skills are represented in a skill graph.",
        "reference": "Fig. 3. Class diagram of the connections between skills and scene elements."
    },
    "2204.08805v1-Figure5-1.png": {
        "prediction": "The image depicts a flowchart showing the components and flow of a system for analyzing and interacting with video data. The main components are:\n\nUser Interface - Allows the user to view and interact with the video, including viewpoint navigation and editing attributes.\n\nVideo Processing - Includes steps like 3D pose reconstruction, temporal segmentation, and pose normalization to analyze the video data.\n\nPose Analysis - Extracts information from the processed video like the pose sequence, aligns the sequences, retrieves relevant attributes, and makes comparisons.\n\nFeedback - Provides feedback to the video processing based on a timeline overview, body model augmentation, and suggestive viewpoints generated from the pose analysis.\n\nInteraction - Enables the user to navigate the viewpoint and edit attributes, with the changes feeding back into the video processing pipeline.\n\nThe arrows show how data and interactions flow between the user interface, video processing, pose analysis, and feedback components in a loop to analyze the video and allow the user to explore and modify the results.",
        "reference": "Fig. 5. The architecture of VCoach, which comprises five main modules."
    },
    "2207.13865v1-Figure3-1.png": {
        "prediction": "The image shows the workflow of an entity relation model (ERM). It starts with a ground set of domains, from which a subset of domains is randomly sampled. These sampled domains go through level-one-sampling to select a subset, which is then passed to the featurizer1 module. The output features from featurizer1 are used to initialize the initial model.\n\nIn parallel, batches are created from the sampled domains and go through level-two-sampling. The resulting batches are passed to the featurizer2 module. The output features from both featurizer1 (via the initial model) and featurizer2 are used to train the final ERM model using the batches inside the sampled domains.",
        "reference": "Figure 3: Illustration of the sampling framework of DOMI. The solid arrow indicates the actual sampling flow, while the dotted arrow only indicates the difference between randomly sampling and DOMI."
    },
    "1828713-Figure2-1.png": {
        "prediction": "The image depicts a neural network architecture with multiple layers:\n\nSoftmax layer at the top, which outputs gold ratings from 1-5 stars.\n\nPooling layer that combines uk, vd, and pj vectors.\n\nConvolutional layer that processes the outputs from the tanh layer.\n\nTanh layer with n hidden units h1 through hn, taking inputs from the linear layer.\n\nLinear layer that learns weight matrices W1 through Wn to transform the lookup layer outputs.\n\nLookup layer at the bottom, which maps the user (Uk), word (wi), and product (Pj) IDs to dense vector representations.\n\nThe lookup, linear, tanh and convolutional layers are repeated for each of the n words in the input sequence. The pooling combines the convolutional outputs to feed into the final softmax layer for predicting the star rating.",
        "reference": "Figure 2: An illustration of the neural network approach for sentiment classification. wi means the i-th word of a review text. uk and pj are continuous vector representations of user k and product j for capturing user-sentiment and product-sentiment consistencies. Uk and Pj are continuous matrix representations of user k and product j for capturing user-text and product-text consistencies."
    },
    "53776814-Figure4-1.png": {
        "prediction": "The image shows two computational stacks side by side, labeled Stack 1 and Stack 2. Each stack consists of:\n\nLevel 1: An STGCN (Spatio-Temporal Graph Convolutional Network) layer.\n\nLevel 2: A series of convolution and deconvolution layers. The convolution layers decrease the spatial dimensions while the deconvolution layers increase them back, forming an hourglass-like structure.\n\nThe key difference between the two stacks is that Stack 2 has one extra level of convolution and deconvolution compared to Stack 1. This allows Stack 2 to learn representations at a lower spatial resolution.\n\nThe parallel stacks enable the model to capture and combine features at different spatial scales. This multi-scale architecture is commonly used in deep learning models for tasks involving spatio-temporal data.",
        "reference": "Figure 4. Illustration of stacked hourglass STGCN with two levels."
    },
    "2107.01209v1-Figure5-1.png": {
        "prediction": "The image shows a diagram depicting the Earth-Sun geometry and the definition of the astronomical unit (au). The yellow circle represents the Sun and the blue circle represents the Earth. A line segment labeled \"D = 1 a.u.\" connects the centers of the Sun and Earth circles, representing the mean distance between the Earth and Sun which defines the astronomical unit. An angle labeled \u03b8 is shown between two line segments from the Sun to points on the Earth's circular orbit.",
        "reference": "Figure 5. Left: The DM particle enters the bulk of the sun with an angle \u03b8, collides with solar electrons and finally escapes from the Sun. For the purpose of illustration, the DM trajectory is projected onto a single plane. Right: The flux of reflected DM particles can be approximated as propagating along the radial direction from the center of the Sun. See text for details."
    },
    "2110.14160v2-Figure2-1.png": {
        "prediction": "The image shows a flowchart of the training pipeline for ResNet-50. The main steps are:\n\n1. Preprocessing the training data, which involves input resolution selection and enhancement method selection.\n\n2. Training strategies for the ResNet-50 model, including composition of data augmentation (DA) selection, batch sampling weight, and sampling strategy selection.\n\n3. The preprocessed training data is fed into the ResNet-50 model, which is trained using the selected training strategies.\n\n4. The trained model is used to make predictions on preprocessed testing data. This involves paired feature fusion to combine features and ensemble method selection to combine predictions from multiple models.\n\n5. Finally, the objective function used to train the model is selected from a few options, and different optimization configurations for the learning rate schedule are evaluated to tune the model.\n\nSo in summary, it depicts the end-to-end process of preprocessing data, training a ResNet-50 model with various strategies, and using the trained model to make predictions on test data.",
        "reference": "Figure 2: Components analyzed in our deep learning-based DR grading framework. The evaluation process of a framework can be divided into two parts: training (top) and testing (bottom). In the training phase, we first fix the architecture of the selected network (ResNet-50). Then we examine a collection of designs with respect to the training setting including preprocessing (image resizing and enhancement), training strategies (compositions of data augmentation (DA) and sampling strategies) and optimization configurations (objective functions and learning rate (LR) schedules). In the testing phase, we apply the same preprocessing as in the training phase and employ paired feature fusion to make use of the correlation between the two eyes (the training step of the fusion network is omitted in this figure). Then, we select the best ensemble method for the final prediction."
    },
    "2205.12022v1-Figure1-1.png": {
        "prediction": "The diagram illustrates the architecture of an image parsing generator that incorporates a residual Feature Pyramid Network (Res FFT-Conv) block.\n\nThe source image I_S is first encoded using an encoder F_s to extract feature maps F_k. These feature maps are passed through the Res FFT-Conv block, which performs a convolution operation in the frequency domain using the Fast Fourier Transform (FFT). The output features F_p from the Res FFT-Conv block are then decoded by a decoder to generate the parsing map P_t.\n\nIn parallel, an image generator pathway takes a latent code L_1 and progressively upsamples and normalizes it using per-region encoding, per-region normalization, and spatial-aware normalization blocks. The resulting normalized features are decoded to produce the generated image I_t. The L_CE, L_1, L_perc and L_style losses are used to train the generator to match the parsing map and appearance of the target image I_T.",
        "reference": "Fig. 1. Overview of our model. Our model consists of parsing generator and image generator, training parsing generator requires a pair of source-target images IS , IT , then obtaining human keypoints KS ,KT and human parsing map PS , PT respectively by using openpose and PGN framework. We concatenate KS , PS ,KT as the input of parsing generator, then the input is fed into an Unet-type network that generates a target parsing map with the same size of PT , which contains body shape information. To get the vivid image with detailed texture (e.g.style of clothing), we extract the Per-region feature information PSj from the source image via VGG type network, then we concatenate the parsing map to the above KS , PS ,KT and normalize it along with the region information PSj to combine the information of source texture and target parsing map. Finally, the target image can be generated by spatial normalization and decoder."
    },
    "2207.06080v2-Figure2-1.png": {
        "prediction": "The image shows a convolutional neural network architecture for classifying images of cats. The main components are:\n\n1. Feature Extraction: Takes a 32x32 RGB image as input and applies 3D convolutional layers with non-linear activations to extract a 64x8x8 feature volume from the input image.\n\n2. Non-linear mapping of Real Space to Feature Embeddings: The real space image is non-linearly mapped to a 64-dimensional feature embedding space. \n\n3. Classification: The 64-D feature embeddings go through a dense layer to produce 10 logits. These are passed through a softmax to get predicted class probabilities. The final output classifies the input image as \"Cat\" or not.\n\nThe left side is labeled \"Non-linear\" while the dense layer and softmax on the right constitute the \"Linear\" portion of the classification network.",
        "reference": "Fig. 2: Illustration of feature and classification embeddings, using the Resnet 32 architecture. Feature embeddigs (FE) are extracted by the CNN\u2019s non-linear layers and are combined with the final classification layer weights to form classification layer embeddings (CLE), before they are summed."
    },
    "2204.09225v1-Figure1-1.png": {
        "prediction": "The image shows an architecture for fMRI data analysis using twin transformers.\n\nTwo streams of fMRI data, labeled fMRI Data1 and fMRI Data2, go through separate but identical twin transformer modules. The twin transformers share weights and extract spatial and temporal features from the input data through matrix multiplication and recovery steps.\n\nThe spatial features from each stream go through a spatial similarity module to measure their similarity. Similarly, the temporal features are compared for temporal similarity.\n\nThe spatial and temporal similarities are then combined to recover the original fMRI data for each input stream.\n\nPart (b) shows more details of the twin transformer architecture. It contains a spatial transformer and temporal transformer. The spatial transformer has a linear projection followed by a spatial position projection. The temporal transformer consists of multiple linear layers. The transformers produce spatial and temporal feature representations of different dimensions.",
        "reference": "Fig. 1. Illustration of the proposed Twin-Transformers framework. (a) shows the overall picture of the proposed model, and (b) shows the details of the Twin-Transformers. The Twin-Transformers take the identical brain signal matrix as input. The spatial Transformer focuses on the space dimension and takes non-overlapping spatial patches as tokens to build attention in the spatial dimension and generate spatial features. Conversely, the temporal Transformer focuses on the temporal dimension and the non-overlapping temporal patches are used as tokens. Correspondingly, the temporal Transformer is designed to build attention in the time dimension and generate temporal features. The twin Transformers are designed for disentangling temporal and spatial features of the input signal matrix."
    },
    "2102.09837v1-Figure2-1.png": {
        "prediction": "The flowchart illustrates the calibration process for the e-calibrate technique. The main steps are:\n\n1. Start in the {Ready} state\n2. Initialize calibration by transitioning to the {Calibrating, Calibrating, s_calibrate} state with tp := 0\n3. Perform calibration, updating tp to 5, which transitions to the {Calibrating, Calibrated, e-calibrate} state\n4. Complete calibration by transitioning to the {Calibrated} end state\n\nThe key elements are the initialization of the calibration time tp to 0, updating it to 5 during calibration, and ending in the calibrated state once tp reaches 5 and the e-calibrate process finishes.",
        "reference": "Figure 2: The platform model of a robot arm."
    },
    "2202.01032v2-Figure5-1.png": {
        "prediction": "The image shows a Service Management and Orchestration Framework for a non-real-time RIC (RAN Intelligent Controller). It contains the following components:\n\n- O1 termination and A1 termination, which use an API/SDK to support multiple xApps (xApp 1 through xApp N)\n- The xApps communicate via an internal messaging infrastructure\n- Below the messaging infrastructure are components for conflict mitigation, subscription management, security, and SDL\n- At the bottom is the E2 termination which connects to the RAN (Radio Access Network)",
        "reference": "Fig. 5: Near-RT RIC architecture. The near-RT RIC connects to the RAN through the E2 interface, at the bottom of the figure (yellow), and to the non-RT RIC/SMO through the A1 and O1 interfaces, at the top of the figures (orange and green, respectively). The communication among the RIC components (in light blue) is mediated by an internal messaging infrastructure. The near-RT RIC can onboard custom logic as xApps (dark blue)."
    },
    "2202.01032v2-Figure6-1.png": {
        "prediction": "The image shows the Service Management and Orchestration (SMO) framework. It consists of several components:\n\n- Non-real-time RIC layer with rApp 1 through rApp N applications that interface with the R1 termination. This connects to the A1 termination.\n\n- SMO/Non-RT RIC framework functions including Data management and exposure, and AI/ML workflow.\n\n- Internal messaging infrastructure that sits between the SMO's functions (Policy, Inventory, Design, Configuration) and the external interfaces.\n\n- External interfaces including the terminations to/from external components like O-Cloud, O-CU/O-DU/O-RU, and Near-RT RIC.\n\nThe framework enables orchestration of rApps in the Non-RT RIC and provides interfaces to external components for service management functions.",
        "reference": "Fig. 6: Non-RT RIC and SMO architecture. The SMO functionalities (in green) enable connectivity to the O-Cloud (through the O2 interface) and the other RAN components (through O1) for management and orchestration. The non-RT RIC features custom logic (rApps, in red), and a termination of the A1 interface to the near-RT RIC (orange). Shared functionalities between the non-RT RIC and the SMO are in yellow."
    },
    "2202.02070v2-Figure2-1.png": {
        "prediction": "The image depicts the architecture of a system called KP-FCNN, which performs semantic segmentation on colored point clouds to generate segmented point clouds.\n\nThe input colored point cloud goes through a semantic encoder, which consists of a series of 1D convolution and KP Conv layers. The output of the semantic encoder then goes to a semantic decoder, which has a symmetric structure to the encoder with deconvolution and KP Conv layers.\n\nThe semantic decoder produces feature vectors (FC) that undergo feature concatenation and aggregation. The concatenated features are then fed into a NetVLAD layer consisting of a Conv layer, Softmax layer, VLAD Core, and Intra-Norm layer. The output of the NetVLAD layer goes through L2 normalization to generate a final descriptor vector representing the semantically segmented point cloud, with reduced dimensionality compared to the input.",
        "reference": "Fig. 2. The architecture of the proposed CGiS-Net."
    },
    "1319230-Figure1-1.png": {
        "prediction": "The flow diagram shows the following process for generating an answer given a query and document:\n\n1. The query (x) and document (d) are provided as inputs.\n\n2. Sentence Selection (Latent) is performed to select relevant sentences from the document. \n\n3. The selected sentences are summarized into a document summary (d\u0302).\n\n4. The document summary (d\u0302) is passed to an Answer Generation model, which is a Recurrent Neural Network (RNN).\n\n5. The RNN generates an answer (y) based on the document summary.\n\nSo in summary, relevant sentences are extracted from the input document, summarized, and then an RNN generates an answer from that document summary conditioned on the input query.",
        "reference": "Figure 1: Hierarchical question answering: the model first selects relevant sentences that produce a document summary (d\u0302) for the given query (x), and then generates an answer (y) based on the summary (d\u0302) and the query x."
    },
    "210713891-Figure1-1.png": {
        "prediction": "vggish",
        "reference": "Figure 1: An illustration of DSTC8 AVSD task. The goal of the task is generating accurate answer based on multiple modalities."
    },
    "2206.00726v1-Figure1-1.png": {
        "prediction": "The image shows a flowchart for generating multi-agent trajectories. It consists of the following steps:\n\n1. Random Sampling is performed multiple times to generate trajectories for each vehicle (Vehicle 1 to N).\n\n2. The sampled trajectories go through a Dynamics Classifier for each vehicle. \n\n3. The classified vehicle trajectories are passed to collision classifiers:\n   - Vehicle 1&2 Collision Classifier\n   - Vehicle 1&N Collision Classifier  \n   - Collision Avoidance Classifier\n\n4. The output of the collision classifiers is combined to generate the final Multi-agent Trajectories.\n\n5. An Acquisition Function is applied to the multi-agent trajectories.\n\n6. The trajectories are evaluated and used to update a Surrogate Model.\n\n7. The updated model is then used in the next iteration of the random sampling process to refine the generated trajectories.",
        "reference": "Fig. 1: Overview of our proposed algorithm. The BayesOpt model is composed of multiple Gaussian process models, which represent dynamic feasibility and collision avoidance constraints."
    },
    "2101.09353v2-Figure1-1.png": {
        "prediction": "The image illustrates a wireless sensor network setup. Radioprobes (1) are deployed over a monitored area and transmit signals via a radio link to two base stations (2). The base stations then send the collected data to a central processing machine (3) for analysis and storage.",
        "reference": "Figure 1. Working principle and radioprobe system architecture."
    },
    "2101.01418v1-Figure2-1.png": {
        "prediction": "The image shows a flowchart depicting various image segmentation techniques at the intermediate level of image processing, between low level pre-processing steps and high level recognition.\n\nThe process flow starts with the Problem Domain, then leads to Image Acquisition and Pre-processing at the low level. \n\nThe intermediate level focuses on Segmentation using techniques like Threshold Based, Region Based, Gradient Based, Classification Based, and Hybrid Based segmentation.\n\nThe Segmentation output feeds into Representation and Description, which then enables high level Recognition, Interpretation and generation of Results.\n\nThe chart also indicates feedback loops, with High Level Processing able to influence the Low Level Processing steps.",
        "reference": "Figure 2: Different levels in image processing process."
    },
    "2101.05151v2-Figure5-1.png": {
        "prediction": "The flowchart illustrates the steps involved in making predictions over time:\n\n1. The inference input H(t-t_k) is passed through a neural network layer to produce H(t-t_k-\u0394t).\n\n2. This is repeated for k steps of prediction. Each step advances the timeline by \u0394t.\n\n3. After the k prediction steps, a final neural network layer takes the output H(t) and generates the inference output H(t+\u0394T), representing the prediction \u0394T into the future.\n\nSo in summary, the model takes an input at time t-t_k, makes k iterative predictions each advancing by a small time step \u0394t to reach time t, then makes a final prediction to estimate the output at a future time t+\u0394T.",
        "reference": "Figure 5: Graphical illustration of long horizontal link forecasting. Given a sequence of graph snapshots G = {G(t\u2212tk), ...,G(t)}, whose length is k, test quadruples at t+ \u2206T are to be predicted."
    },
    "1482745-Figure1-1.png": {
        "prediction": "The image compares the architectures of a GAN Discriminator and PacGAN Discriminator.\n\nThe GAN Discriminator has an Input Layer consisting of several nodes feeding directly into the Discriminator network.\n\nIn contrast, the PacGAN Discriminator has an Input Layer where the nodes are first fed into two separate fully connected layers before entering the main Discriminator network. This allows the PacGAN architecture to operate on multiple samples simultaneously to determine if they are real or generated, enabling it to better detect mode collapse in the generator compared to a standard GAN discriminator looking at individual samples.",
        "reference": "Figure 1: PacGAN(m) augments the input layer by a factor of m. The number of edges between the first two layers are increased accordingly to preserve the connectivity of the mother architecture (typically fully-connected). Packed samples are fed to the input layer in a concatenated fashion; the grid-patterned nodes represent input nodes for the second input sample."
    },
    "2210.08812v1-Figure2-1.png": {
        "prediction": "The image compares two types of transformers for multi-modal content:\n\n(a) Aggregation based Explicit Transformer: This transformer takes in content vectors C and multiplies them element-wise. The result goes through a Softmax layer followed by an aggregation operation to produce the final content vector.\n\n(b) Modulation based Implicit Transformer: Here, the content vectors are multiplied element-wise and summed with coordinates vectors. A Sin function is applied to modulate the content before the final transform operation produces the output content vector.\n\nThe key difference is the aggregation step in (a) versus the modulation using Sin function in (b). The coordinates are an additional input only in the modulation based implicit transformer.",
        "reference": "Fig. 2. Illustration of aggregation based explicit transformer and modulation based implicit transformer."
    },
    "2101.07910v1-Figure2-1.png": {
        "prediction": "The flowchart shows the process for generating code snippets from an input population:\n\n1. Start with the Input Population\n2. Check if the iteration count is less than N\n   - If Yes, Calculate the Mutation Score for the current population\n   - Check if the sample is elite\n     - If Yes, add it to the Next Generation \n     - If No, Refactor the Input with a Mutation Rate\n   - Generate the Code Snippet\n   - Go back to check the iteration count again\n3. If the iteration count is not less than N, the process ends and outputs the Final Generated Code Snippets.",
        "reference": "Fig. 2. The guided mutation (GM) process for code adversarial generation."
    },
    "2012.06182v1-Figure1-1.png": {
        "prediction": "The image depicts various layers of a space network architecture, from the Terrestrial Layer at ground level up to the Deep Space Layer over 35,000 km above the Earth.\n\nThe layers shown, from bottom to top, are:\n- Terrestrial Layer (0.15 km)\n- Aeronautical Layer (17 km) \n- Stratospheric Layer (22 km)\n- LEO (Low Earth Orbit) Layer (200 km)\n- MEO (Medium Earth Orbit) Layer (2000 km)  \n- GEO (Geosynchronous Orbit) Layer (35,838 km)\n- Deep Space Layer (above 35,838 km)\n\nIntra-layer links are shown within the LEO layer. Inter-layer links connect the LEO, MEO and GEO layers.\n\nIcons represent various technologies operating at each layer, such as satellites, aircraft, airships/balloons, and terrestrial systems.",
        "reference": "Fig. 1: Illustration of a multilayered SN with satellites, HAPs, and UAVs."
    },
    "2012.06182v1-Figure8-1.png": {
        "prediction": "The image illustrates a high-level system architecture utilizing HAP (High Altitude Platform) technology for providing wireless network connectivity and IoT applications to a remote, populated area.\n\nA network of HAPs communicates wirelessly with a stand-alone HAP over the populated area. The stand-alone HAP provides wireless coverage, depicted by the orange cones, to the buildings and houses below.\n\nThe HAP wireless network backhauls traffic from IoT applications in the populated area. A swarm of UAVs (drones) is also shown operating in the remote area, likely collecting sensor data or providing additional coverage.\n\nThe diagram shows both RF (radio frequency) links and optical communication links between system components in the populated area and the remote HAP network.",
        "reference": "Fig. 8: An architecture of HAPs network with P2P HAP-to-HAP and backhauling links."
    },
    "2103.04910v1-Figure6-1.png": {
        "prediction": "The flowchart shows a system with input u(t) and output y(t). The system contains a regulator block h^(t), which takes the input u(t). The regulator output feeds into a control design block that determines the control input h(t) = k(\u03b8^(t)). This control input then goes into a recursive identifier block \u03b8^(t) that provides the parameter estimate back to the control design block, forming a closed loop.",
        "reference": "Figure 6: Model building approach"
    },
    "2202.06208v1-Figure1-1.png": {
        "prediction": "The image shows a Dynamic Hierarchical Tree representation for Optimal Transport for Feature Alignment and Metric Learning for Prediction Boundary.\n\nIn (a), the Optimal Transport moves the source domain features \u03a9s (blue dots) to align with the target domain features \u03a9t (gray triangles) by transporting the source features along the tree edges to match the target distribution.\n\nIn (b), Metric Learning adjusts the prediction boundary by pulling the positive class features (blue dots) closer together and pushing the negative class features (gray shapes) farther apart based on their labels. The legend indicates positive samples with dots and negative samples with triangles.",
        "reference": "Figure 1. The architecture of our proposed BROT. Sub-figure (a) depicts the OT process to align features from different domains. Sub-figure (b) shows that the metric learning aims to seek better decision boundaries with a dynamic hierarchical tree. The darkness of blue colors reflects the magnitude of molecular properties."
    },
    "2107.01104v1-Figure1-1.png": {
        "prediction": "The image depicts a parallel particle simulation across multiple processes (Process #0 through Process #P-1). Each process owns a vertical slice of the domain/grid, which contains Pp particles. Processes communicate by exchanging Sp particles with their left and right neighbors along the x3 dimension. This allows particles to move between processes as they cross the boundary slices during the simulation.",
        "reference": "Figure 1: Distribution of real-space data between MPI processes in TurTLE. Fields are split into slabs and distributed between P MPI processes along the x3 direction. The Np particles are also distributed, with each MPI process storing Pp particles on average. Within each MPI process the particle data is sorted according to its x3 location. This leads to a direct association between each of the Ps field slices to contiguous regions of the particle data arrays \u2014 in turn simplifying the interpolation procedure (see text for details). On average, S p particles are held within each such contiguous region."
    },
    "51865-Figure3-1.png": {
        "prediction": "The image shows a neural network architecture consisting of a CNN (Convolutional Neural Network) module followed by a CEC (Cross Entropy Coding) module.\n\nThe CNN module takes in the input data x and applies convolutional layers followed by pooling layers repeatedly. This extracts hierarchical features from the input data.\n\nThe extracted features from the CNN are then passed to the CEC module. The CEC contains a fully connected (FC) layer h1, another FC layer h2, and a softmax layer z. The CEC module is an autoencoder-like structure, with the FC layers acting as the encoder to map the CNN features to a latent representation z, and a decoder to reconstruct the original features from z.\n\nSo in summary, it is a CNN-based autoencoder architecture, where the CNN layers learn to extract useful features, and the CEC module learns a compact encoding of those features that allows reconstruction.",
        "reference": "Figure 3. Overview of DCEC. The network is built by cascading the CNN module and the CEC."
    },
    "2012.02113v1-Figure8.1-1.png": {
        "prediction": "The image shows a Venn diagram with two overlapping circles labeled H(X) and H(Y). The overlapping region in the center is labeled I(X;Y), representing the mutual information between X and Y. The remaining portions of the H(X) and H(Y) circles not overlapping are labeled H(X|Y) and H(Y|X) respectively, representing the conditional entropy of X given Y and vice versa.",
        "reference": "Figure 8.1 Venn diagram showing entropic quantities associated with a pair of random variables taking values in different sets: the Shannon entropies H(X) and H(Y), the joint entropy H(X,Y), the conditional entropies H(X | Y) and H(Y | X), and the mutual information I(X; Y)."
    },
    "2012.02113v1-Figure7.4-1.png": {
        "prediction": "The image shows a sequence of points p1, p2, p3, up to p', evenly spaced along a line segment. Below, this corresponds to a sequence D(p1), D(p2), D(p3), up to D(p'), where each D(pi) belongs to the set Q. Taking the limit as the number of points approaches infinity gives the definition of a line integral D(p) over the line segment from the initial point to p'.",
        "reference": "Figure 7.4 Schematic illustration of Lemma 7.4.6."
    },
    "2204.12696v1-Figure2-1.png": {
        "prediction": "The image illustrates how StyleGAN generates faces in a latent space. The micromotion space contains low-dimensional features that produce subtle variations in the generated faces, while moving in the latent space creates more significant changes in facial identity and appearance from one end of the space to the other.",
        "reference": "Figure 2: A tensor illustration of our low-rank micromotion subspace hypothesis. In the StyleGAN latent space, we hypothesize the same type of micromotion, at different quantitative levels but for the same identity, can be approximated by a low-rank subspace. We further hypothesize that subspaces for the same type of micromotion found at different identities are extremely similar to each other, and can hence be transferred across identities."
    },
    "206593096-Figure3-1.png": {
        "prediction": "The image compares two architectural designs for image segmentation, labeled as Design A and Design B.\n\nIn Design A, the input images are passed through convolutional layers (conv1 to conv5) to generate feature maps. These feature maps are then passed through two branches - one with an SPP (Spatial Pyramid Pooling) block and FC (fully connected) layers, and another with CFM (Coarse Feature Maps) and FC layers. The outputs from these two branches are concatenated and fed into a classifier to produce the final segmentation.\n\nIn Design B, the input images are first passed through a CNN backbone consisting of convolutional layers to extract feature maps at multiple scales (fc6 and fc7). These multi-scale feature maps are then fed into a CFM module followed by the classifier to generate the final segmentation output.\n\nThe key difference is that Design A uses parallel branches with SPP and CFM to process the CNN features before classification, while Design B directly feeds the multi-scale CNN features through CFM to the classifier. The image also shows intermediate segment proposal visualizations.",
        "reference": "Figure 3: Two network designs in this paper. The input image is processed as a whole at the convolutional layers from conv1 to conv5. Segments are exploited at a deeper hierarchy by: (Left) applying CFM on the feature map of conv5, where \u201c b\u201d means for \u201cbounding boxes\u201d and \u201c s\u201d means for segments; (Right) applying CFM on the finest feature map of the spatial pyramid pooling layer."
    },
    "2101.10868v1-Figure1-1.png": {
        "prediction": "The image depicts an information flow diagram showing the process of collecting and using personal data. The key steps are:\n\n1. The government orders and monitors data collection from users.\n2. User data flows to a central \"Data\" node. \n3. The collected data goes through processing.\n4. The processed data is then shared, including with social media platforms.\n5. Some of the data sharing flows to an \"Unknown\" destination, indicating a lack of transparency.\n\nThe bottom of the diagram lists some key concerns around this data collection and usage process, including restrictions and mass surveillance on users, changing users' behaviors through the collected data, and the aggregation of information from multiple sources.",
        "reference": "Figure 1: A Systematic Overview of Privacy Threats from Multiple Domains Related to the COVID-19 Pandemic"
    },
    "2102.04539v2-Figure10-1.png": {
        "prediction": "The diagram depicts how vertex corrections from single-particle Green's functions F, F', F'' are included in the vertex function v_i when calculating the electron-electron interaction vertex V_F. The dashed lines represent the bare Coulomb interaction. The vertex function v_i is obtained by summing over all possible pairings of the single-particle Green's functions F, F', F'' connected by the bare interaction lines, as shown by the equations below the diagram.",
        "reference": "Figure 10: Illustration for 2-Diam GBP. Here, V q denotes all sets V{q,\u00b7}."
    },
    "2102.04539v2-Figure6-1.png": {
        "prediction": "The image shows a neural network diagram with an input layer V_U, hidden layers V_1 through V_n, and an output layer V_F. The layers are connected by weight matrices v_F, v_F', and x. The diagram illustrates how information flows through the network, with the activations of each layer being a function of the previous layer's activations and the connecting weight matrices.",
        "reference": "Figure 6: Illustration for the construction in the proof of Proposition 8 for 2-Reach GBP with r = 1. In this example, U = {u1, . . . , un} and we have {u1, ui, uj , un} = F \u2208 F ."
    },
    "2102.04539v2-Figure4-1.png": {
        "prediction": "The image shows 5 diagrams (a) through (e) depicting an RNN network architecture with varying gating and connections:\n\n(a) Basic RNN architecture with input (x), hidden state (a), and output (y) at each timestep, connected sequentially.\n\n(b) Adds an x-to-y direct connection at each timestep.\n\n(c) Adds gating (circles) to the x-to-a, x-to-y, and a-to-a (recurrent) connections at each timestep. \n\n(d) Removes the x-to-y direct connections compared to (c).\n\n(e) Adds an additional x-to-a direct connection (skipping the gate) at each timestep compared to (d).\n\nThe diagrams show an increasing complexity of the RNN architecture by adding gating and additional weighted connections at each timestep of the recurrent network.",
        "reference": "Figure 4: Illustration to Constructions 3 & 4. Part (a) shows an exemplary directed graph which is a yes-instance for DHP. Applying Construction 3 on (a) yields (b). Applying Construction 4 on (b) yields the instance whose graph is depicted in (c) and two habitats of which are depicted in (d) and (e). Vertices marked yellow in (d) are contained in the habitat Xout. Vertices marked red in (e) are contained in the habitat Yout. The graph induced by Yout contains the red edges."
    },
    "2110.06487v1-Figure5-1.png": {
        "prediction": "The image shows a block diagram depicting the flow of information from Victim profiles on the public internet to Sources that cybercriminals use for their activities.\n\nThe diagram shows 5 victim profiles on the left. This information is fed into a \"Covid Unemployment Claim Site\" operated by the government for fund transfers. \n\nThe \"Scammer D/C\" (likely meaning Scammer Data Collection) obtains information from both the Covid Unemployment Claim Site as well as the victim profiles directly.\n\nThis collected scammer data is then used to fuel cybercriminal sources on the dark web and for online sales, likely of the stolen personal information and fraudulent unemployment claims.",
        "reference": "Fig. 5. Covid unemployment threat model"
    },
    "2103.00907v1-Figure4-1.png": {
        "prediction": "The image shows a neural network diagram with an input layer, multiple hidden layers, and an output layer. The input is represented by a vector p* and goes through a series of transformations in the hidden layers, with the output of each hidden layer feeding into the next. The final output is represented by the vector \u03c6*_uv. The connections between nodes in adjacent layers are depicted by criss-crossing lines, giving the network a densely interconnected structure.",
        "reference": "Figure 4. Diagram of the neural network(NN2) for the turbulent channel flow. The inputs are the pressure and velocity gradient and the output is the pressure strain term. The FCFF has 5 layers with 10 neurons in each layer."
    },
    "2206.00162v2-Figure2-1.png": {
        "prediction": "The image shows the architecture and process flow for an image super-resolution model using generative adversarial networks (GANs). Here is a summary of the key components and steps:\n\n1. A Gaussian Mixture Model (GMM) is used as the core generator to produce low-resolution images from a sample input. \n\n2. The low-resolution image is passed through an inverse SSL (Self-Supervised Learning) module to upsample it.\n\n3. A quality booster module enhances the resolution of the upsampled image in stages, from 8x8 to 16x16 to 32x32 resolution.\n\n4. The enhanced image is then passed to a Resolution Enhancer module that boosts the resolution further at each stage (8x8, 16x16, 32x32).\n\n5. Edge masks are computed from the upsampled images and DC components. The sampled AC components are masked and added back to generate the final enhanced high-resolution image.\n\n6. The enhanced image is cropped and a boosting residual is computed by subtracting the masked boosting residue. \n\n7. Locally linear embedding is applied to the clustered training data to produce the final boosted super-resolution output image.\n\nThe discriminator (DC) and generator networks are trained adversarially, with the process repeating to enhance each cropped portion of the input image.",
        "reference": "Figure 2: Overview of PAGER generation method."
    },
    "2011.03148v2-Figure4-1.png": {
        "prediction": "The image shows a diagram outlining the architecture of the Huber Regression Loss used to train an object detection model. The input image is passed through a Class Logits module to predict object classes and a BBox module to predict bounding box coordinates. These are fed into a Focal Similarity Loss and a Huber Regression Loss respectively. The Focal Similarity Loss handles class imbalance while the Huber Regression Loss is used for robust bounding box regression.",
        "reference": "Fig. 4. Diagram of perception consistency loss computation. An EfficientDet object detector predicts boxes and classes. Consistency of predictions between images is captured by losses similar to those in object detection training."
    },
    "2110.09749v2-Figure1-1.png": {
        "prediction": "The flowchart illustrates the process of Candidate Keyphrase Extraction using a RoBERTa model. The source document is fed into RoBERTa to generate word embeddings w1 to wM. Uni-grams h1 to hM are extracted from these embeddings.\n\nBi-grams are then generated from the uni-grams, with bi-gram h12 formed from h1 and h2, h23 from h2 and h3, and so on. This process continues up to N-grams.\n\nThe extracted keyphrases are evaluated along four dimensions in the Keyphrase Importance Estimation step: Syntactic Accuracy, Information Saliency, Concept Consistency, and Document Representation. The output of this estimation is used to make the final predictions.",
        "reference": "Figure 1: The KIEMP model architecture."
    },
    "2103.07592v2-Figure1-1.png": {
        "prediction": "The image depicts a schematic diagram showing the relationship between the dark matter (DM) field \u03c7 and the Standard Model (SM) fields \u03d5 and H. A cross symbol represents the interaction between the DM and SM fields, mediated by an intermediate field F\u03bc\u03bd\u03c1\u03c3.",
        "reference": "Figure 1: Schematic diagram for flux-mediated dark matter."
    },
    "2107.05680v1-Figure2-1.png": {
        "prediction": "The image depicts an architecture diagram showing how high resolution images X1 and X2 are downsampled through convolutional layers W1 and W2 to produce lower resolution representations Z. These are then combined via convolutional layers ZW*1 through ZW*4 to generate a final high resolution output image X5. The images D1, D2, and D5 represent training data at the input and output resolutions. Overall, it illustrates a deep learning architecture for super-resolution, taking low resolution input images X1 and X2 and producing a high resolution output X5.",
        "reference": "Figure 2: A modified architecture for progressive training of convex GANs (ProCoGAN). At each stage i, a linear generator Wi is used to model images at a given resolution Xi, attempting to fool quadratic-activation discriminator Di, for which the optimal solution can be found in closed-form via (15). Once stage i is trained, the input to stage i + 1 is given as the output of the previous stage with learned weights W\u2217i , which is then used to model higher-resolution images Xi+1. The procedure continues until high-resolution images can be generated from successive application of linear generators."
    },
    "2107.05214v2-Figure6-1.png": {
        "prediction": "The image shows a block diagram illustrating the process of binarizing the output of a multi-layer perceptron (MLP) neural network. The MLP takes a summation of l-1 input values m multiplied by weights as its input. The MLP output E is then passed through a binarization operation to generate the final binary output values m represented by a grid of black and white squares.",
        "reference": "Figure 6: The illustration of the attention mechanism. The prediction of current hidden state h\u0302t and the grid-level features E is used as query and key, respectively."
    },
    "2102.12058v2-Figure17-1.png": {
        "prediction": "The image shows a flowchart depicting the federated voting process in a system that uses quorum votes. The process starts with an \"uncommitted\" state. If a quorum votes for/accepts a value a, it moves to the \"voted a\" state. Once a node in each slice accepts a, it moves to the \"accepted a\" state. Finally, once a is confirmed, it reaches the \"confirmed a\" state, at which point a is considered valid.",
        "reference": "Fig. 17. Federated voting process [46] ."
    },
    "2103.05447v1-Figure2-1.png": {
        "prediction": "The image shows a schematic diagram of a hydraulic flume apparatus for conducting experiments. The main components are:\n\n- A 10 meter long flume tank\n- A flow straightener at the upstream end\n- A flow meter to measure the flow rate \n- A 3 meter long test section in the middle of the flume\n- 11 data collection locations marked along the test section at 60 cm intervals\n- A pump at the downstream end to recirculate the flow\n- A pool at the very downstream end\n\nThe side view shows the full length of the flume, while the plan view focuses on the test section area where measurements are taken. The diagram provides the key dimensions and an overview of how the experimental setup is configured.",
        "reference": "Figure 2. Experiment schematic."
    },
    "2010.00041v3-Figure2-1.png": {
        "prediction": "The flowchart depicts a genetic algorithm process, starting with the random creation of an initial population. The fitness of this population is evaluated, and if the stopping criteria are not satisfied, elites are passed to the next population while the rest of the new population is generated through roulette selection based on fitness, followed by cross-over and mutation operations. This iterative process continues, evaluating the fitness of each new population, until the stopping criteria are met and the algorithm returns the best individual found.",
        "reference": "Fig. 2. The optimization flowchart for the inverse material design."
    },
    "2202.00846v1-Figure2-1.png": {
        "prediction": "The flowchart shows the process for CVR (Customer Value Rating) Estimation. It starts with a Click/Purchase Log which feeds into CVR Estimation for each group, consisting of Group 1, Group 2, up to Group n. The results are then used to Compute Assignment Probability, which goes through a Stopping Rule before the final Declare Results are displayed to the user.",
        "reference": "Figure 2: Method Overview"
    },
    "2101.07424v2-Figure1-1.png": {
        "prediction": "The image shows a high-level overview of the process for representing a 3D scene. It starts with the coded scene, which is a low-resolution grid representation using different colors. This coded scene goes through an aperture, which focuses the view. The encoded scene on the other side of the aperture has higher spatial resolution. This then passes through a prism, which splits the scene into its spectral components. Finally, shifted encoded scene measurements are taken, with each measurement capturing a shifted spectral sampling of the high-resolution encoded scene.",
        "reference": "Fig. 1. Physical sensing phenomena in CASSI, which is the CSI prototype used to validate the proposed approach."
    },
    "2204.03873v1-Figure2-1.png": {
        "prediction": "The image shows the architecture of a Temporal Convolutional Network (TCN) for processing input sequences. The input sequence X goes through batch normalization, then is split into two paths - one path applies a spatial convolution (labeled \"ST\") while the other applies a standard 1D convolution (labeled \"TCN\"). The outputs of these two paths are concatenated and go through further processing steps including batch normalization, a mish activation function, a 1x1 convolution, dropout, and a linear transformation to produce the final output embedding f_out. The architecture also includes residual connections that add the input of each processing block to its output before passing it to the next block.",
        "reference": "Figure 2: Structure of gait-TR. TCN is the temporal convolutional network module, and ST is the spatial transformer module. FC denotes full connect layer. Batch-norm is BatchNorm2D for input Xtv \u2208 \u211d C\u00d7T\u00d7V , while Batch-norm* denotes BatchNorm1D for input Xtv \u2208 \u211d C\u2217V \u00d7T ."
    },
    "2204.10426v1-Figure1-1.png": {
        "prediction": "The image shows a state diagram with 3 states:\n\nHealthy (State 0)\nCognitive impairment (State 1)\nDeath (State 2)\n\nThere are transitions from Healthy to Cognitive impairment, and from both Healthy and Cognitive impairment to Death. The diagram illustrates possible progressions between these health states.",
        "reference": "Figure 1: Three-state illness-death model"
    }
}