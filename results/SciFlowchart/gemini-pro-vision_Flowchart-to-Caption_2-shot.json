{
    "2204.00424v1-Figure6-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. First, the direct call relations and indirect call patterns are extracted from the code. Then, the direct call patterns are used to generate the execution traces. Finally, the temporal relations between the execution traces are extracted.\n\nThe flowchart in Figure 7 illustrates the pre-training process of BERT. The input to the model is a sequence of tokens, which are embedded into vectors. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.\n\nThe flowchart in Figure 8 illustrates the image captioning process. The input to the model is an image, which is encoded into a vector. The vector is then passed through a series of transformer layers, which learn to attend to each other and to the input image. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.",
        "reference": "Figure 6. MSOPunet network. (S1t\u22121, S2t\u22121), (S1t, S2t) and (S1t+1, S2t+1) denotes the input pairs of optical and SAR images. S2t\u2032 denotes the reference image used to compute the loss (during training) and the metrics (at inference time from test data) from the reconstructed optical image S\u03022t."
    },
    "2202.03651v2-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are used to generate call graphs. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are represented by the edges in the direct call graph, while the indirect call relations are represented by the edges in the indirect call graph. The indirect call patterns are then extracted from the indirect call graph. Finally, the direct call patterns and the indirect call patterns are used to generate the call graph.",
        "reference": "Figure 1: Instead of retrospectively discovering individual failure cases for perception, we actively search for causal interventions (edits) to existing scenes that consistently result in perception failures. The middle shows an example of a single intervention causing perception failure, which we attribute to the intervention, as opposed to the left where a combinatorial set of factors could explain the error. Consistent failures through this type of intervention constitute a challenging group for the perception model as seen on the right."
    },
    "2011.15102v2-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are used to identify indirect call patterns. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to identify the direct callers and callees, while the indirect call relations are used to identify the indirect callers and callees. The indirect call patterns are then identified by analyzing the indirect call relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 illustrates the process of adversarial training. The learner is trying to find a solution that minimizes the loss function, while the tester is trying to find a test that maximizes the loss function. The learner and the tester iteratively improve their strategies until the learner is able to find a solution that is robust to the tester's attacks.",
        "reference": "Figure 1: Learning by passing tests. A tester model creates tests with increasing levels of difficulty from a test bank to evaluate a learner model. The learner continuously improves its learning ability to deliver better solutions for passing those difficult tests."
    },
    "14078997-Figure2-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of execution traces, we first extract direct call relations and indirect call relations from them. Then, we use the extracted relations to construct a call graph. Finally, we perform clustering on the call graph to identify the indirect call patterns.\n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of Transformer encoders, followed by an output layer. The input to BERT is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through the Transformer encoders, which learn to attend to each other and generate a contextualized representation of the input sequence. The output of the Transformer encoders is then passed through the output layer, which produces a probability distribution over the vocabulary for each token in the input sequence.\n\nFigure 8 shows the detailed architecture of a Transformer encoder. A Transformer encoder consists of a self-attention layer, followed by a feed-forward layer. The self-attention layer allows the Transformer encoder to attend to different parts of the input sequence and generate a weighted sum of the attended values. The feed-forward layer then applies a non-linear transformation to the output of the self-attention layer. The Transformer encoder is repeated multiple times in BERT, and the output of the final Transformer encoder is used to generate the probability distribution over the vocabulary for each token in the input sequence.",
        "reference": "Figure 2: Outline of the transformation-based model. The model is a CNN that takes as input a sequence of consecutive affine transforms between pairs of adjacent video frames. It predicts the affine transform between the last input frame and the next one in the sequence. We compute affine transforms (6 parameters per patch) for overlapping patches of size 8 \u00d7 8 in each video frame. Learning operates in the space of transformations as shown inside the dashed box. The front-end on the left is a module that estimates the affine transforms between pairs of consecutive input frames. The post-processor on the right reconstructs a frame from the predicted set of affine transforms and it is only used at test time."
    },
    "14078997-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. In the first step, the direct call relations are extracted from the code. In the second step, the indirect call relations are extracted. In the third step, the indirect call patterns are extracted. Finally, the direct call patterns are extracted.",
        "reference": "Figure 3: Outline of the system predicting 4 frames ahead in time. Only affine transforms A1, A2 and A3 are provided, and the model predicts A\u03034, A\u03035, A\u03036 and A\u03037, which are used to reconstruct the next 4 frames. Since affine parameters are continuous values and the whole chain of CNNs is differentiable, the whole unrolled system can be trained by back-propagation of the error. Note that CNNs all share the same parameters"
    },
    "2204.10509v1-Figure2-1.png": {
        "prediction": " The image shows the architecture of PEGE (Positive-Emotion Guided Empathetic) model. It consists of a Transformer-based encoder and a decoder. The encoder takes as input a sequence of context utterances and outputs a sequence of hidden states. The decoder takes as input the hidden states from the encoder and outputs a sequence of response tokens. The model is trained with three losses: NLL loss, emotional loss, and VAD loss. The NLL loss is the negative log-likelihood of the correct response tokens. The emotional loss is the sum of the cross-entropy losses of the positive and negative emotion terms. The VAD loss is the sum of the mean squared errors of the predicted valence, arousal, and dominance values and the ground truth values.",
        "reference": "Figure 2: Illustration of our PEGE Model"
    },
    "2202.05735v2-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given a set of traces, the direct call relations and indirect call relations are first extracted. Then, the indirect call patterns are generated based on the indirect call relations. Finally, the direct call patterns are generated based on the direct call relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 shows the architecture of a deep learning model for sleep stage classification. The model takes as input a time series of physiological signals, such as EEG, EOG, and EMG. The signals are first preprocessed to extract features, such as the average power in different frequency bands. The features are then passed through a series of convolutional layers, which learn to identify patterns in the data. The output of the convolutional layers is then passed through a fully connected layer to predict the sleep stage.",
        "reference": "Fig. 1: Three approaches to ML for sleep staging from PPG. Our new model, SleepPPG-Net takes as input the PPG waveform (WAV).The derived time series (DTS) and feature engineering (FE) approaches allow comparison with SOTA algorithms described in the literature."
    },
    "2101.02429v1-Figure6-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to extract temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the functions that are directly called by each function. The indirect call relations are the relations between the functions that are called by the functions that are called by each function. The indirect call patterns are the patterns of the indirect call relations. The figure shows an example of how the direct call relations, indirect call relations, and indirect call patterns can be extracted from two traces.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence. The model is trained to predict the next word in a sequence given the previous words in the sequence.\n\nFigure 8 shows the eigenvectors and neural decomposition of a 3D cone. The eigenvectors of the cone are the principal axes of the cone. The neural decomposition of the cone is the decomposition of the cone into a sum of three cylinders. The cylinders are aligned with the eigenvectors of the cone.",
        "reference": "Figure 6: Illustration of the neural decomposition procedure for d = 2 (i.e. Z\u2217 \u2208 R3\u00d73). The dashed red arrows correspond to the eigenvectors of Z\u2217 (q1, q2, q3) and the solid blue arrows show the decomposed vectors p1 and p2. In this example, the rank of Z \u2217 is 2 where q1 and q2 are its two principal eigenvectors. The eigenvalue corresponding to the eigenvector q1 is zero. The light blue colored surface shows the Lorentz cones z = \u221a x2 + y2 and z = \u2212 \u221a x2 + y2. We observe that the decomposed vectors p1 and p2 lie on the boundary of Lorentz cones."
    },
    "1246860-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a call graph, where the nodes represent functions and the edges represent the call relations. The indirect call relations are represented as a matrix, where the rows and columns represent functions and the values represent the probability of a function calling another function.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 shows the architecture of a sentiment analysis model. The model consists of two parts: a sentence encoder and a document encoder. The sentence encoder takes a sentence as input and outputs a vector representing the sentiment of the sentence. The document encoder takes the vectors from the sentence encoder and outputs a vector representing the sentiment of the document.",
        "reference": "Figure 1: Deep multi-instance transfer learning approach for review data."
    },
    "2102.10828v2-Figure7-1.png": {
        "prediction": " The image shows how traces are generated and used to learn temporal relations. Given two traces, trace 1 (C1) and trace 2 (C2), the direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph. The indirect call relations are used to construct the indirect call graph. Indirect call patterns are mined from the indirect call graph. Finally, the direct call patterns and indirect call patterns are used to learn temporal relations.",
        "reference": "Figure 7. Illustration of Equation (4.1)."
    },
    "1022460-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 (T1) and trace 2 (T2), the direct call relations and indirect call relations are extracted. The direct call relations are represented as a set of tuples, where each tuple consists of two elements, the caller and the callee. The indirect call relations are represented as a set of tuples, where each tuple consists of three elements, the caller, the indirect caller, and the callee. The use case is represented as a set of tuples, where each tuple consists of two elements, the caller and the callee. The flowchart in Figure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the encoders is a sequence of vectors, which are then used to predict the next token in the sequence. The flowchart in Figure 8 shows the curriculum learning approach for object detection. The model is first trained on a small set of easy images and then gradually trained on more difficult images. The difficulty of the images is determined by the objective function, which is a measure of the model's performance on the images.",
        "reference": "Figure 1. A feedback based learning model. The basic idea is to make predictions in an iterative manner based on a notion of the thus-far outcome. This provides several core advantages: I. enabling early predictions (given total inference time T , early predictions are made in fractions of T ); II. naturally conforming to a taxonomy in the output space; and III. better grounds for curriculum learning."
    },
    "2110.03031v3-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given a pair of traces (e.g., t1 and t2), the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the call relations. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values indicate the presence or absence of an indirect call relation.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to each other and to the input tokens. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.\n\nThe flowchart in Figure 8 shows the architecture of a neural network with skip connections. The network consists of a series of layers, each of which is followed by a skip connection. The skip connections allow the gradients to flow directly from the input to the output of the network, which can help to improve the training speed and accuracy.",
        "reference": "Figure 1. RieszNet architecture."
    },
    "2101.11189v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe image in Figure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the hidden state of the corresponding token. The model is trained to predict the masked tokens in the input sequence.\n\nThe image in Figure 8 shows the architecture of a YOLOv3 detector. The detector consists of a backbone network, a neck network, and a head network. The backbone network is responsible for extracting features from the input image. The neck network is responsible for combining the features from the backbone network and generating a feature pyramid. The head network is responsible for predicting the bounding boxes and class labels of the objects in the image.",
        "reference": "Fig. 2: The overall framework of our arbitrary-oriented ship detection method. Feature maps are first generated by using a fully convolutional network. Afterward, the peaks of the center feature map are selected as center points. Then, the center points offsets, object sizes and head regression locations are regressed on the corresponding feature maps on the position of each center point. The potential head points are collected by extracting peaks with confidence scores larger than 0.1 on the head feature map. The final head location is obtained by assigning each regressed location to its nearest potential head points."
    },
    "2204.01715v1-Figure7-1.png": {
        "prediction": " The image shows an end-to-end example of using Orca to train a PyTorch model on a K8s cluster. The data ingestion block shows how data can be loaded into an Orca DataFrame from various sources such as CSV, parquet, and images. The data can then be transformed using arbitrary Python libraries such as Pandas and NumPy. The transformed data is then used to train a PyTorch model. The model can be trained on a single node or distributed across a K8s cluster using Ray. The trained model can then be used for inference or deployed to a production environment.",
        "reference": "Figure 7. BigDL-Orca architecture"
    },
    "2202.07508v3-Figure3-1.png": {
        "prediction": " Figure 1 shows how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. Then, indirect call patterns are generated based on the indirect call relations. Finally, the direct call patterns are generated based on the direct call relations.",
        "reference": "Figure 3. Architecture of the dynamic deep linear kernel."
    },
    "2202.11824v1-Figure1-1.png": {
        "prediction": " An overview of the proposed approach. The attacker sends a crafted sample to the hospital for shotgun sequencing. The attacker can remotely control the sequencing machine through a biocybernetic hacking alert. The attacker then obtains the raw sequencing data and assembles it using a Trojanized DNA assembler. Finally, the attacker activates the Trojanized software to steal sensitive information.",
        "reference": "Fig. 1: Hybrid Trojan Bio-Cyber Hacking Attack. Steps 1 - 3 indicate a typical genetic sequencing operation for patients. Steps 4 - 6 indicate a situation where a hacker has embedded their IP address and Port number into a DNA that will trigger a remote connection from a Trojan-horse infected software tool leading to a connection to the attacker in Step 8. Our proposed approach utilizes Deep-Learning to detect Trojan payload in digital data using encoded into DNA strands that can prevent the attack."
    },
    "2011.04105v1-Figure3.5-1.png": {
        "prediction": " In Figure 1, the execution traces of two use cases are shown. Each use case consists of a sequence of API calls. The arrows between the API calls denote the direct call relations. For example, in the first use case, there is a direct call relation between API call C1 and API call C2, which means that C2 is called after C1. The dotted arrows between the API calls denote the indirect call relations. For example, in the first use case, there is an indirect call relation between API call C1 and API call C4, which means that C4 is called after C1, but there is no direct call relation between them. The indirect call relations are discovered by analyzing the call sequences in the execution traces.\n\nIn Figure 7, the overall architecture of BERT is shown. BERT consists of a stack of transformer encoders. Each transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, and the feed-forward layer allows the model to learn non-linear relationships between the input and output. The output of the final transformer encoder is a sequence of vectors, where each vector represents the learned representation of a token in the input sequence.\n\nIn Figure 8, the data plane, discovery plane, and decision plane of a software-defined network (SDN) are shown. The data plane is responsible for forwarding packets, the discovery plane is responsible for discovering the network topology, and the decision plane is responsible for making decisions about how to forward packets. The data plane and the discovery plane are directly controlled by the SDN controller, while the decision plane is indirectly controlled by the SDN controller.",
        "reference": "Figure 3.5: Network-level objectives."
    },
    "2202.01326v1-Figure5-1.png": {
        "prediction": " The flowchart describes the process to determine the role of triplets in enhancing the photovoltaic (PV) device. It starts with the measurement of the internal quantum efficiency (IQE). If the IQE is greater than 100%, then the triplets are enhancing the PV device. If the IQE is less than 100%, then the triplets are not enhancing the PV device.",
        "reference": "Figure 5: Flowchart describing the process for determining if SF is improving photocurrent in a device."
    },
    "2204.08696v2-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct execution traces and extract temporal relations. Given two traces Tr1 and Tr2 of the same program, we first identify the direct call relations and indirect call relations, denoted as C1 and C2, respectively. For C1, we directly connect the caller and callee functions in the calling context. For C2, we connect the caller function and the callee function through an intermediate caller function. In this example, C1={(C1,C2),(C1,C3),(C1,C5),(C2,C3),(C2,C4),(C3,C4),(C3,C5),(C3,C6),(C4,C6)}. C2={(C1,C4),(C1,C6)}. We then extract the indirect call patterns from C2, denoted as (C1,C2). In this example, (C1,C2)={(C1,C4),(C1,C6)}. Finally, we construct the direct call patterns from C1 and C2, denoted as (C1,C2). In this example, (C1,C2)={(C1,C2),(C1,C3),(C1,C5)}.\n\n\nFigure 7 shows the overall architecture of BERT. The input to BERT is a sequence of tokens, which are first embedded into vectors using a word embedding layer. The embedded tokens are then fed into a series of transformer encoder layers. Each transformer encoder layer consists of a self-attention mechanism and a feed-forward network. The self-attention mechanism allows the model to attend to different parts of the input sequence, while the feed-forward network allows the model to learn non-linear relationships between the input tokens. The output of the transformer encoder layers is then fed into a softmax layer to produce a probability distribution over the vocabulary. This probability distribution can be used to predict the next token in the sequence.\n\nThe left part of Figure 7 shows the inputs to the Transformer. The input consists of three parts: a segment embedding, a positional embedding, and a word embedding. The segment embedding is used to distinguish between the two segments of the input sequence. The positional embedding is used to provide information about the position of each token in the sequence. The word embedding is used to represent the meaning of each token.\n\nThe right part of Figure 7 shows the Transformer encoder. The Transformer encoder consists of six layers. Each layer consists of a self-attention mechanism and a feed-forward network. The self-attention mechanism allows the model to attend to different parts of the input sequence. The feed-forward network allows the model to learn non-linear relationships between the input tokens.\n\nThe Transformer decoder is used to generate the output sequence. The decoder consists of six layers. Each layer consists of a self-attention mechanism, a feed-forward network, and an attention mechanism. The self-attention mechanism allows the model to attend to different parts of the input sequence. The feed-forward network allows the model to learn non-linear relationships between the input tokens. The attention mechanism allows the model to attend to different parts of the output sequence.\n\nFigure 8 shows the architecture of the FSRCNN network. The network consists of three convolutional layers, each followed by a leaky ReLU activation function. The first convolutional layer has 96 filters of size 5x5, the second convolutional layer has 32 filters of size 3x3, and the third convolutional layer has 3 filters of size 3x3. The output of the third convolutional layer is then upsampled using a deconvolutional layer, followed by a leaky ReLU activation function. The final output of the network is a single-channel image.",
        "reference": "Fig. 1: The complete structure of the proposed CNN-Transformer Cooperation Network (CTCNet). CTCNet is a U-shaped symmetrical hierarchical network with three stages: encoding stag, bottleneck stage, and decoding stage. Among them, the encoding stage is designed to extract local and global features with different scales, and the decoding stage is designed for feature fusion and image reconstruction."
    },
    "2101.08366v2-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to learn temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph. The indirect call relations are used to construct the indirect call graph. The direct call graph and the indirect call graph are used to learn temporal relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors. The embeddings are then passed through the encoders, which learn to attend to each other and to the input sequence. The output of the encoders is a sequence of vectors, which are then used to predict the next token in the sequence.\n\nFigure (b) shows the architecture of a quadratic residual network (QRes) layer. The layer consists of two fully connected layers, with a residual connection between them. The first layer is a 2-layer feed-forward network with ReLU activation. The second layer is a linear layer. The residual connection is added to the output of the second layer, and the result is passed through a sigmoid function.",
        "reference": "Figure 2: Overview of our proposed Quadratic Residual Network (QRes) layer in comparison with plain DNN layer. Blue rectangular boxes represent trainable parameters and round boxes represent operations (purple \u201c\u00d7\u201d: multiplication, orange \u201c+\u201d: addition, green \u201d\u00b7\u201d: Hadamard product, and cyan \u201d\u03c3\u201d: activation operator)."
    },
    "2205.09510v2-Figure6.4-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the call relations. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values represent the probability of one function calling another.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through the encoders, which learn to extract contextual information from the sequence. The output of the encoders is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence.\n\nThe flowchart in Figure 8 shows the architecture of a quantum neural network. The network consists of a series of qubits, which are the basic units of quantum information. The qubits are connected to each other by gates, which are operations that can be performed on the qubits. The network is trained by applying a series of gates to the qubits and measuring the output. The goal of the training process is to find a set of gates that produces the desired output for a given input.",
        "reference": "Figure 6.4: An illustration of the \u201cQC\u201d setting of quantum machine learning, in which data are quantum and processing is classical."
    },
    "2205.09510v2-Figure6.11-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces t1 and t2 of the execution of a program, the direct call relations and indirect call relations can be extracted. The direct call relations are represented by the directed edges between the caller and callee functions in the call graph. The indirect call relations are represented by the dashed edges between the caller and callee functions in the call graph. The indirect call patterns are represented by the boxes with the labels C1, C2, ..., Cn. Each indirect call pattern represents a group of indirect call relations that have the same caller and callee functions.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence. The feed-forward layer allows the model to learn non-linear relationships between the input tokens. The model is trained to predict the masked tokens and the next sentence in a masked language modeling task.\n\nFigure 8 shows a quantum circuit for a variational quantum eigensolver (VQE). The circuit consists of a unitary operator U(\u03b8) that is applied to an initial state |0\u27e9. The unitary operator is parameterized by a set of parameters \u03b8. The circuit also includes a measurement operator F that is used to measure the energy of the system. The goal of the VQE is to find the parameters \u03b8 that minimize the expected energy of the system.",
        "reference": "Figure 6.11: Illustration of the operation of a VQE. The classical optimizer aims at minimizing the expected value \u3008F \u3009\u03c8(\u03b8)\u3009 = \u3008\u03c8(\u03b8)|F |\u03c8(\u03b8)\u3009 of the observable F ."
    },
    "2206.05240v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2, the direct call relations and indirect call relations can be extracted. Moreover, indirect call patterns can be further identified from the indirect call relations. The direct call patterns and indirect call patterns can be used to detect API misuse.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract meaningful features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nFigure 8 shows the overview of our approach. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract meaningful features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.",
        "reference": "Figure 2: Model Overview. We propose CBRL for RCB. Guided by a curriculum sequence, the agent learns to adaptively trade-off constraints and the objective in non-stationarymarkets. During deployment, the agent updates its belief over the market dynamics based on its past experience, and acts through posterior sampling."
    },
    "2210.02172v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to each other and generate a contextualized representation of the input sequence. The final output of the model is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence.\n\nThe flowchart in Figure 8 illustrates the DQN algorithm. The agent interacts with the environment by taking actions and receiving rewards. The agent learns to update its policy by minimizing the approximation error between the target Q-value and the current Q-value.",
        "reference": "Fig. 2: An illustration of deep Q-learning model for the ISRE-based 6G vehicular aided HetNet environment. The model uses a deep neural network to find the approximate Q-values. The model includes experience replay mechanism (to remove the correlation between different observations), a feature set (given as input to the deep neural network), a target Q-network for updating the primary Q-network and the simulation environment for extracting different parameters."
    },
    "14844989-Figure2-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct execution traces and extract temporal relations. Given two traces Tr1 and Tr2 of the same program, we can extract the direct call relations and indirect call relations. We use C1 and C2 to denote the caller and callee, respectively. For direct call relations, we have C1 \u2192 C2 if C1 directly calls C2. For indirect call relations, we have C1 \u2192 C2 if there exists a sequence of function calls C1 \u2192 C3 \u2192 \u00b7 \u00b7 \u00b7 \u2192 Cn \u2192 C2. We say that C1 and C2 have a temporal relation if there exists a direct or indirect call relation between them. \n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of Transformer encoders followed by an output layer. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer adds non-linearity to the model. The output layer is a fully connected layer that projects the output of the final Transformer encoder to the vocabulary size.\n\nThe input to BERT is a sequence of tokens, where each token is represented by a word embedding. The word embeddings are passed through the stack of Transformer encoders, which produce a sequence of hidden vectors. The hidden vectors are then projected to the vocabulary size by the output layer, which produces a probability distribution over the vocabulary for each token in the input sequence.\n\nBERT is trained using a masked language modeling objective. In masked language modeling, a certain percentage of the tokens in the input sequence are randomly masked, and the model is trained to predict the masked tokens based on the context. This objective helps the model to learn the relationships between different words in a sentence, and to capture the overall meaning of the sentence.\n\nFigure 8 shows the multi-task network for person re-identification. The architecture consists of three branches: a ranking branch, a classification branch, and a cross-domain branch. The ranking branch takes as input two images of the same person and outputs a ranking score indicating how similar the two images are. The classification branch takes as input a single image and outputs a probability distribution over the identities of the person in the image. The cross-domain branch takes as input two images of the same person from different domains (e.g., visible and infrared) and outputs a probability distribution over the identities of the person in the images.\n\nThe ranking branch and the classification branch share the same convolutional layers. The convolutional layers are followed by a fully connected layer for the ranking branch and a softmax layer for the classification branch. The cross-domain branch consists of a convolutional layer followed by a fully connected layer. The output of the fully connected layer is used to compute a triplet loss, which is used to train the network to learn a discriminative embedding space for person re-identification.",
        "reference": "Figure 2: The framework of the proposed multi-task deep network and the cross-domain architecture. The crossdomain architecture is only used when an auxiliary dataset is needed for training."
    },
    "2205.03661v1-Figure1-1.png": {
        "prediction": " In Figure 1, given two execution traces (denoted as Trace 1 and Trace 2) of a program, which contain the direct call relations (C1, C2, ..., C6), the indirect call relations (C1, C4), (C2, C4), (C2, C6), and (C1, C6), and the indirect call patterns ((C1, C2), (C1, C4)), the direct call patterns can be extracted as ((C1, C3), (C2, C3), (C2, C5), (C1, C5)).\n\nIn Figure 7, the input representations of a Transformer-based language model (e.g., BERT) are the sum of three vectors: (1) token embeddings, which are learned vectors for each token in the vocabulary; (2) segment embeddings, which are learned vectors for each segment in the input sequence; and (3) positional embeddings, which are learned vectors for each position in the input sequence. The token embeddings are first passed through a series of Transformer layers, which consist of self-attention layers and feed-forward layers. The self-attention layers allow the model to attend to different parts of the input sequence, while the feed-forward layers allow the model to learn non-linear relationships between the input tokens. The output of the Transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nIn Figure 3, the 1D-ECG signal is preprocessed before being fed into the deep learning model. The preprocessing step includes removing noise, normalizing the signal, and segmenting the signal into heartbeats. The preprocessed signal is then fed into the deep learning model, which consists of a series of convolutional layers and pooling layers. The convolutional layers learn to identify patterns in the signal, while the pooling layers reduce the dimensionality of the signal. The output of the deep learning model is a probability distribution over the different classes of heartbeats.",
        "reference": "Fig. 1. Overview of methods adopted"
    },
    "162169-Figure3-1.png": {
        "prediction": " The image shows a flowchart that describes the process of using a neural network to generate a response to a given input. The flowchart starts with the input being passed through an encoder, which converts the input into a sequence of vectors. These vectors are then passed through a decoder, which uses them to generate a sequence of output vectors. The output vectors are then converted back into a response, which is then passed to the user.",
        "reference": "Figure 3 Partial illustration of frames, roles, and lexical units related to the CAUSE TO MAKE NOISE frame, from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are unfilled bars. No particular significance is ascribed to the ordering of a frame\u2019s roles in its lexicon entry (the selection and ordering of roles above is for illustrative convenience). CAUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here."
    },
    "2202.03344v2-Figure2-1.png": {
        "prediction": " ## Figure 1: Illustration of execution traces and temporal relations.\n\nThe figure illustrates how execution traces and temporal relations are extracted from the program code. \nOn the left-hand side, there are two execution traces, denoted as Trace 1 and Trace 2. Each trace is a sequence of function calls, where each function call is represented by a node. The arrows between the nodes represent the order of the function calls. For example, in Trace 1, function C1 is called first, followed by function C2, and so on.\n\nOn the right-hand side, there are three different types of temporal relations that can be extracted from the execution traces. \n- The first type of relation is the direct call relation, which is represented by the solid arrows. A direct call relation exists between two functions if one function calls the other function directly. For example, in Trace 1, there is a direct call relation between function C1 and function C2.\n- The second type of relation is the indirect call relation, which is represented by the dashed arrows. An indirect call relation exists between two functions if one function calls another function, which in turn calls the second function. For example, in Trace 1, there is an indirect call relation between function C1 and function C4.\n- The third type of relation is the indirect call pattern, which is represented by the boxes. An indirect call pattern is a sequence of function calls that starts with one function and ends with another function. For example, in Trace 1, there is an indirect call pattern between function C1 and function C4, which consists of the function calls C1, C2, C3, and C4.\n\n\n## Figure 7: Pretraining BERT.\n\nThe figure shows the architecture of BERT in pretraining. \n- The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. \n- The token embeddings are then passed through a series of transformer layers, which attend to each other and to the positional embeddings. \n- The output of the transformer layers is then passed through a softmax layer to predict the next word in the sequence. \n- The model is trained to minimize the cross-entropy loss between the predicted next words and the actual next words in the training data.\n\n## Figure : Latent variable estimation flowchart.\n\nThe flowchart describes the estimation of latent variables \\(\\mathbf{z}\\) for a given data set \\(D\\) with \\(N\\) data points, a set of candidate distributions \\(\\mathcal{P}\\), and a set of candidate q-norms \\(\\mathcal{Q}\\). The estimation procedure starts with the initialization of the latent variable \\(z\\) and the iteration index \\(i_p\\) (line 1). Then, for each latent variable \\(z_i\\) (line 2), the algorithm estimates the coefficients \\(\\beta\\) for each distribution \\(p \\in \\mathcal{P}\\) using the current latent variable \\(z\\) (line 3). Next, for each q-norm \\(q \\in \\mathcal{Q}\\), the algorithm computes the cross-validation (CV) score (line 4) and updates \\(z\\) by setting it to \\(z_i^{\\ast}\\), the value that minimizes the CV score (line 5). If the current latent variable \\(z_i\\) does not improve the CV score compared to the previous latent variable \\(z_{i-1}\\) (line 6), the algorithm increases the degree of adaptivity \\(i_q\\) (line 7) and repeats the estimation of the coefficients \\(\\beta\\) and the computation of the CV score (lines 3-5). If the CV score does not improve for two consecutive increases in the degree of adaptivity (line 8), the algorithm reduces the degree of adaptivity \\(i_q\\) (line 9) and repeats the estimation of the coefficients \\(\\beta\\) and the computation of the CV score (lines 3-5). The algorithm stops when the CV score converges (line 10).",
        "reference": "Figure 2: Flow chart of the procedure to adaptively build a stochastic PCE"
    },
    "2207.12764v1-Figure1-1.png": {
        "prediction": " The figure shows the architecture of the proposed approach. It consists of three main steps: (1) **OCEL Extraction**: In this step, the raw event data from Celonis is extracted and transformed into a format that can be processed by the clustering algorithm. (2) **Enrich OCEL with Object and Graph Related Attributes**: In this step, the extracted OCEL is enriched with additional information about the objects and their relationships. This information is used to improve the accuracy of the clustering algorithm. (3) **Clustering**: In this step, the enriched OCEL is clustered using a hierarchical clustering algorithm. The resulting clusters are then used to identify the complex event patterns.",
        "reference": "Fig. 1: Overview of the proposed framework."
    },
    "2103.13446v3-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect call relations between the functions.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the encoders is a sequence of vectors, which are then used to predict the next token in the sequence and to predict whether the current token is the start of a new sentence.\n\nFigure 8 shows the architecture of GraphSAGE. The model consists of a message passing layer, which aggregates the features of a node's neighbors, and a readout layer, which produces the final representation of the node. The message passing layer is repeated multiple times, and the output of the final message passing layer is used to compute the node representations.",
        "reference": "Fig. 2: ModGNN\u2019s message aggregation module. In this diagram, the system is shown from a centralized point of view. First, the raw observation or output from the last layer is transformed by finput. Then, for each transmission up to K hops, the data from the neighboring agents is passed through fcom and then aggregated. The output is the set of the data from each k-hop neighborhood up to K."
    },
    "2107.06007v1-Figure14-1.png": {
        "prediction": " 1. Start with two traces.\n2. Identify direct call relations and indirect call patterns.\n3. For each indirect call pattern, identify the callers and callees.\n4. For each caller-callee pair, create a direct call relation.\n5. Remove all indirect call patterns.\n6. The resulting graph represents the direct call relations between the functions in the program.",
        "reference": "Figure 14. The EXO-200 slow control architecture consisted of three major parts- the MySQL backbone, the embedded system, and the distributed system."
    },
    "2107.12859v2-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe image in Figure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the input embedding layer. The embedded tokens are then passed through the encoders, which learn to extract contextual information from the sequence. The output of the encoders is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence.\n\nThe image in Figure 8 shows the architecture of a graph neural network (GNN) for point cloud part segmentation. The GNN consists of two parts: a forward sequential encoding module and a reverse sequential encoding module. The forward sequential encoding module takes as input a point cloud and a set of part labels. The module then iteratively updates the features of each point in the point cloud by aggregating information from its neighboring points. The reverse sequential encoding module takes as input the updated features of the points and a set of part labels. The module then iteratively updates the features of each point in the point cloud by aggregating information from its neighboring points. The output of the GNN is a set of part labels for each point in the point cloud.",
        "reference": "Figure 3: One iteration of our Recurrent Graph Learning framework. (a) We process part features and compute a graph message. (b) The message is encoded sequentially in our bidirectional GRU framework. (c) The features generated by the forward and reverse GRU are used to regress part-pose. We use three such iterations in our framework."
    },
    "2012.00248v1-Figure1-1.png": {
        "prediction": " The image shows an overview of the approach. Given a set of traces, we first extract the direct call relations and indirect call relations. Then, we use the direct call relations to construct the direct call graph, and use the indirect call relations to construct the indirect call graph. Finally, we use the direct call graph and the indirect call graph to generate the temporal relation graph.",
        "reference": "Fig. 1. Schematic diagram for synthesizing the digital Fourier hologram. The letter objects are placed at a distance d in front of the Fourier lens in the direction of propagation of light. The real-valued hologram are drawn in the logarithmic scale."
    },
    "2010.06310v1-Figure2-1.png": {
        "prediction": " The image shows an overview of the proposed neural cross-supervised learning framework for joint entity and trigger extraction. Given a sequence of words, the model first embeds each word into a vector. Then, a bi-LSTM is used to encode the sequence of vectors. The output of the bi-LSTM is then fed into a CRF layer to predict the sequence of entity and trigger tags. In addition to the sequence-to-sequence labeling loss, two cross-supervision losses are also computed: one between the predicted entity distribution and the predicted trigger distribution, and another between the predicted entity-trigger adjacency matrix and the ground-truth entity-trigger adjacency matrix. These two cross-supervision losses are used to regularize the model and improve its performance.",
        "reference": "Fig. 2: The framework of the joint-event-extraction model with our proposed cross-supervised mechanism."
    },
    "2202.05336v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces t1 and t2 for a use case u1, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 shows the steps involved in the image segmentation process. The input to the model is an image, which is first preprocessed to remove any noise or unwanted artifacts. The image is then segmented into a foreground and background using a graph-based segmentation algorithm. The foreground and background are then refined using a morphological operation. Finally, the segmented image is post-processed to remove any small holes or artifacts.",
        "reference": "Fig. 1. Block diagram of DBSGen"
    },
    "2107.03026v1-Figure7-1.png": {
        "prediction": " An ecological system and a social-economic system are depicted. The ecological system influences the social-economic system, and vice versa.",
        "reference": "Figure 7: Influence matrix schematic graph, based on [5, Figure 5]"
    },
    "1381187-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. First, the program code is parsed to identify the direct call relations between functions. Then, the indirect call relations are identified by analyzing the control flow graph of the program. Finally, the temporal relations between the functions are extracted by analyzing the execution traces of the program.\n\nThe flowchart in Figure 7 illustrates the architecture of BERT in pretraining. The model consists of a stack of transformer encoders, followed by a softmax layer. The input to the model is a sequence of tokens, and the output is a probability distribution over the vocabulary. The model is trained to predict the masked tokens in the input sequence.\n\nThe flowchart in Figure 8 illustrates the architecture of a convolutional neural network (CNN) for semantic segmentation. The model consists of an encoder and a decoder. The encoder is a stack of convolutional layers, and the decoder is a stack of transposed convolutional layers. The input to the model is an image, and the output is a segmentation mask. The model is trained to predict the class label for each pixel in the image.",
        "reference": "Fig. 1: Illustration of the SegNet architecture applied to EO data."
    },
    "13567899-Figure3-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to extract temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph. The indirect call relations are used to construct the indirect call graph. The direct call graph and the indirect call graph are then used to extract the temporal relations between the functions.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence. The feed-forward layer allows the model to learn non-linear relationships between the input tokens. The output of the final encoder is used to predict the masked tokens in the input sequence.\n\nFigure 8 shows the architecture of a Transformer encoder. The encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence. The feed-forward layer allows the model to learn non-linear relationships between the input tokens.",
        "reference": "Figure 3. V2V Architecture for Voxel Prediction. The lower part (below dashed line) consists of layers from C3D [28]. Connected to these layers we have three 3D convolution layers: Conv3c,Conv4c,Conv-pre use filters of size 3\u00d7 3\u00d7 3 with stride 1\u00d7 1\u00d7 1. Both Deconv5 and Deconv4 are deconvolutional layers employing kernels of size 4 \u00d7 4 \u00d7 4 with output stride of 2 \u00d7 2 \u00d7 2. Deconv3 has kernel size 8 \u00d7 4 \u00d7 4 and output stride of 4 \u00d7 2 \u00d7 2. The numbers inside the boxes represent the number of learning filters in that layer, while the numbers near the boxes (above or below) represent the size of output signals produced by that layer. The part inside the thick-dashed box is application-dependent."
    },
    "2204.10374v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from a given program. In the first step, the program is executed twice, with different inputs, to generate two execution traces. Each trace is represented as a sequence of method calls, where each method call is represented by a node in the graph. The direct call relations between methods are represented by edges in the graph. In the second step, the temporal relations between method calls are extracted. This is done by analyzing the order in which method calls occur in the traces. The temporal relations are represented by dashed edges in the graph. Finally, the direct call relations and temporal relations are used to identify indirect call patterns. An indirect call pattern is a sequence of method calls that occurs in both traces, but with different temporal relations. The indirect call patterns are represented by boxes in the graph.",
        "reference": "Figure 1 | Gesture Hierarchy. The architecture used for the Android applications is based on a 3-layer hierarchy: (1) The lowest level operates over GVFs corresponding to all supported gestures; (2) The middle layer selects a gesture GVF given the latest pixel image in AndroidEnv and its agent is trained to maximize the return associated with the task that the agent is trained on; and (3) The top layer selects a single gesture class for the task and the agent is trained to maximize the average per step reward. All levels are operated by distributed DQN agents."
    },
    "2010.00041v1-Figure3-1.png": {
        "prediction": " ## Figure 1: Illustration of execution traces and temporal relations.\n\nThe figure shows how to extract features that capture the temporal relations between API calls. Given two traces of API calls, denoted as Trace 1 and Trace 2, we can extract the direct call relations and indirect call relations. The direct call relations are the pairs of API calls that are directly called one after another, while the indirect call relations are the pairs of API calls that are not directly called one after another but have a temporal relation. For example, in Trace 1, the API calls C1 and C2 have a direct call relation, while the API calls C1 and C4 have an indirect call relation.\n\n## Figure 7: Pretraining BERT.\n\nThe figure shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the encoder layers is a sequence of vectors, each of which represents the hidden state of the model at a particular position in the input sequence. These hidden states can then be used to predict the next word in the sequence or to perform other tasks such as question answering or sentiment analysis.\n\n## Figure 2: The flowchart of the genetic algorithm.\n\nThe flowchart of the genetic algorithm is as follows:\n1. Start\n2. Random creation of the initial population\n3. Evaluation: calculating fitness of population\n4. Stopping criteria: if the stopping criteria are satisfied, then go to step 8.\n5. Passing Elites to next population\n6. Roulette Selection of operator based on fitness\n7. Cross-over\n8. Mutation\n9. New population\n10. End: return the best individual",
        "reference": "Fig. 3. The optimization flowchart for the inverse material design."
    },
    "2103.14005v2-Figure9-1.png": {
        "prediction": " The image shows how to use traces to extract direct and indirect relations between functions. Given two traces, the algorithm first extracts the direct call relations from each trace. Then, it computes the indirect call relations between the functions in the two traces. Finally, it uses the direct and indirect call relations to identify the common call patterns between the two traces.",
        "reference": "Figure 9. The Multi Input Fusion Classifier end task architecture. The orange box shows the frozen encoder."
    },
    "2110.14764v2-Figure1-1.png": {
        "prediction": " Figure 8. Multilingual document classification. The input documents in different languages are encoded by the language-specific TF-IDF encoders into vectors. These vectors are then fed into the first-tier classifiers, which are followed by the stacking layer and the meta-classifier.",
        "reference": "Fig. 1. The Fun architecture, exemplified with |L|=3 languages (Chinese, Italian, English). Note that the different term-document matrices in the 1st-tier may contain different numbers of documents and/or different numbers of terms. The three grey diamonds on the left represent calibrated classifiers that map the original vectors (e.g., TFIDF vectors) into |Y|-dimensional spaces. The resulting vectors are thus aligned and can all be used for training the meta-classifier, which is represented by the grey diamond on the right."
    },
    "1422748-Figure1-1.png": {
        "prediction": " The flowchart describes the process of extracting entities from text. The input to the process is a document, which is a collection of text. The first step is to detect the language of the document. This is done using a language detection algorithm. The language of the document is used to determine the appropriate text normalization and tokenization algorithms to be used. The next step is to normalize the text. This involves removing any punctuation or special characters from the text, and converting all characters to lowercase. The normalized text is then tokenized. This involves breaking the text into a sequence of tokens, where each token is a word or a punctuation mark. The final step is to extract the entities from the text. This is done using an entity extraction algorithm. The entity extraction algorithm identifies the entities in the text and their corresponding types. The output of the process is a list of entities and their types.",
        "reference": "Figure 1: Overview of the Lithium NLP pipeline"
    },
    "2101.07004v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the callers and callees in the traces. The indirect call relations are the relations between the callers and callees in the traces, but there is no direct call relation between them. The indirect call relations are transitive relations. For example, if A calls B and B calls C, then there is an indirect call relation between A and C.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence. The model is trained to predict the next word in a sequence, given the previous words in the sequence.\n\nFigure 8 shows the timing of CSI measurements and data transmission in a TDD system. The CSI measurements are performed in the uplink, and the data is transmitted in the downlink. The CSI measurements are used to estimate the channel state information, which is used to improve the quality of the data transmission.",
        "reference": "Figure 2. Block diagram of one transmission block."
    },
    "1061647-Figure5-1.png": {
        "prediction": " The image shows the architecture of a deep neural network for facial landmark detection. It consists of a series of convolutional layers, followed by a deconvolution layer and a final 1x1 convolutional layer. The input to the network is a 256x256 pixel grayscale image, and the output is a 60x60 pixel heatmap, where each pixel corresponds to the probability of a facial landmark being present at that location.",
        "reference": "Fig. 5. Architecture of the pose estimation network. First 5 layers copied from VGGM, followed by a deconvolution layer. A 1x1 convolution layer is then used to predict each output heat map."
    },
    "2103.06081v1-Figure2-1.png": {
        "prediction": " Figure 1: Schematic of the proposed method. (a) Illustration of the structured illumination. The sample is illuminated by two counter-propagating beams with wave vectors ks and \u2212ks, respectively. The polarization of each beam is controlled by a quarter-wave plate (QWP) and a linear polarizer (LP). The polarization states of the two beams are orthogonal to each other. The sample is placed at the front focal plane of the objective. The reflected light is collected by the objective and forms an interference pattern at the back focal plane. The interference pattern is captured by a camera. (b)\u2013(e) Interference patterns captured by the camera for different combinations of the polarization states of the two beams. (f) The cross-section intensity profile along the dashed yellow line in (b). (g) The corresponding visibility as a function of the spatial frequency. The red and blue curves are the theoretical fitting curves for TE- and TM-polarized light, respectively.",
        "reference": "Figure 2. Illustration of the illumination scheme of double SIM using far-field optics. (a) Schematic of one possible approach to generate the illumination patterns with structured electric energy density and OC simultaneously. Structured OC patterns formed by the interference of (b) two L-CPL beams ( )C x+ and (c) two R-CPL beams ( )C x\u2212 . (d, e) Structured patterns of ( )eU x . The OC patterns in (b) and (c) possess the opposite handedness. The eU patterns in (d) and (e) are the same. Line-cut profile of the (f) OC and (g) eU along the dashed lines in (b-e). Scale bar in (b) is 02\u03c0/k and applicable for (d-e)."
    },
    "2202.03373v1-Figure5-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the functions that are directly called by each other. The indirect call relations are the relations between the functions that are not directly called by each other, but are called by the same function. The indirect call patterns are then generated based on the indirect call relations. The indirect call patterns are the sequences of functions that are called by the same function.",
        "reference": "Figure 5. An illustration of Curve Non-Linear Unit. This layer can be seen as a learnable non-linear activation function between 0 and 1. Based on Eq. 3, the learned function always follows concave down increasing curves to increase feature intensities."
    },
    "2207.07828v1-Figure2-1.png": {
        "prediction": " The flowchart shows how to use the trace information to extract direct and indirect relations. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations can be extracted, respectively. Note that the indirect call relations are derived based on the assumption that if A calls B and B calls C, then A might call C. In addition, the direct call patterns and indirect call patterns can be further extracted from the direct call relations and indirect call relations, respectively.",
        "reference": "Fig. 2. Overview of the proposed Structural Prior guided Generative Adversarial Transformers (SPGAT). The SPGAT contains one generator, two discriminators, and one structural prior estimator. The generator is used to explore non-local information with the guidance of a structural prior estimator (SPE) for better clear image restoration. We build the skip connections between the generator and discriminators so that the discriminators can better discriminate between real and fake features in the generator for generating more natural images. Meanwhile, SPE is also utilized to guide the discriminators for better estimations. The generator takes a low-light image L as the input and outputs an enhanced image E\u0302. The SPE is input the corresponding structure S of L by gradient operation and outputs the estimated structural prior P\u0302. Both the two discriminators take the concatenation of the enhanced image E\u0302 and estimated structural P\u0302 as the input, which is regarded as the fake label. However, there is no real label feature. To that aim, we generate real label features by inputting the normal-light image to the generator and its corresponding structure to SPE. PW-STB illustrated in Fig. 3 is the basic block of the generator, discriminators, and SPE."
    },
    "2102.08827v1-Figure3-1.png": {
        "prediction": " An illustration of execution traces and temporal relations. Given two traces (left), the direct call relations (middle) and indirect call relations (right) can be extracted. The direct call relations are extracted by identifying the caller and callee functions in each trace. The indirect call relations are extracted by identifying the caller and callee functions in each trace, and then identifying the transitive call relations between them. For example, in the first trace, there is a direct call relation from function C1 to function C2, and an indirect call relation from function C1 to function C6.",
        "reference": "Fig. 3. Class diagram of the connections between skills and scene elements."
    },
    "2204.08805v1-Figure5-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 (C1) and trace 2 (C2), the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the functions that are called directly by each function, while the indirect call relations are the relations between the functions that are called indirectly by each function. The indirect call patterns are then generated based on the indirect call relations. Finally, the direct call patterns and indirect call patterns are used to generate the temporal relations between the functions.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through the encoders, which learn to extract contextual information from the sequence. The output of the encoders is a sequence of vectors, each of which represents the contextualized representation of the corresponding token.\n\nFigure 8 shows the components of a typical 3D pose estimation system. The system consists of a user interface, a video processing module, a pose analysis module, and a feedback module. The user interface allows the user to interact with the system and to provide input data, such as videos or images. The video processing module is responsible for extracting relevant information from the input data, such as object detection and tracking. The pose analysis module is responsible for estimating the 3D pose of the object of interest. The feedback module provides the user with feedback on the estimated pose, such as visualization of the pose or comparison with a reference pose.",
        "reference": "Fig. 5. The architecture of VCoach, which comprises five main modules."
    },
    "2207.13865v1-Figure3-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to extract temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph, and the indirect call relations are used to construct the indirect call graph. The direct call graph and the indirect call graph are then used to extract temporal relations between the functions.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the meaning of the corresponding token in the input sequence.\n\nFigure 8 illustrates the process of domain adaptation using the proposed InvDANN method. The method consists of two stages: level-one sampling and level-two sampling. In level-one sampling, a batch of source domains and a batch of target domains are randomly sampled from the ground set of domains. In level-two sampling, a batch of instances is randomly sampled from each of the selected source domains and target domains. The sampled instances are then used to train the InvDANN model.",
        "reference": "Figure 3: Illustration of the sampling framework of DOMI. The solid arrow indicates the actual sampling flow, while the dotted arrow only indicates the difference between randomly sampling and DOMI."
    },
    "1828713-Figure2-1.png": {
        "prediction": " The image shows an overview of training a neural network model. The model has a stack of Transformer encoders on the left, followed by a pooling layer, a softmax layer, and a linear layer. On the right, there is an input embedding layer, followed by a series of convolutional layers, a tanh layer, and a linear layer. The output of the linear layer is then fed into the softmax layer to produce a probability distribution over the output classes.",
        "reference": "Figure 2: An illustration of the neural network approach for sentiment classification. wi means the i-th word of a review text. uk and pj are continuous vector representations of user k and product j for capturing user-sentiment and product-sentiment consistencies. Uk and Pj are continuous matrix representations of user k and product j for capturing user-text and product-text consistencies."
    },
    "53776814-Figure4-1.png": {
        "prediction": " In Figure 1, we illustrate how execution traces and temporal relations are extracted. Given two traces, trace 1 and trace 2, we can extract direct call relations and indirect call relations. For example, in trace 1, there is a direct call relation from C1 to C2, and an indirect call relation from C1 to C6 via C2 and C5. We can also extract indirect call patterns, such as (C1, C2), (C2, C3), (C2, C5), (C2, C6), (C3, C4), (C4, C5), (C4, C6), (C5, C6). In Figure 7, we show the architecture of BERT in pretraining. The model consists of a stack of encoders, followed by a final input representation layer and an output layer. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The final input representation layer is a fully connected layer that projects the output of the last encoder to a fixed-size vector. The output layer is a softmax layer that predicts the probability of each token in the output vocabulary. In Figure 8, we show the architecture of a STGCN. The model consists of two stacks of STGCN layers, followed by a fully connected layer. Each STGCN layer consists of a spatial graph convolution layer and a temporal graph convolution layer. The spatial graph convolution layer learns the relationships between nodes in the spatial graph, while the temporal graph convolution layer learns the relationships between nodes in the temporal graph. The fully connected layer projects the output of the last STGCN layer to a fixed-size vector.",
        "reference": "Figure 4. Illustration of stacked hourglass STGCN with two levels."
    },
    "2107.01209v1-Figure5-1.png": {
        "prediction": " The image shows how rays of light from the Sun are bent by the gravity of the Earth. The Sun is a large, hot ball of gas that emits light in all directions. The Earth is a smaller, cooler planet that orbits the Sun. The Earth's gravity pulls on the light from the Sun, bending it towards the Earth. This bending of light is called refraction.\n\nThe image shows the Sun and the Earth, with the Earth's orbit around the Sun indicated by a dotted line. The Sun is on the left side of the image, and the Earth is on the right side of the image. The Earth is shown as a blue sphere, and the Sun is shown as a yellow sphere. The rays of light from the Sun are shown as lines, and the arrows indicate the direction of the light. The rays of light are bent towards the Earth by the Earth's gravity. This bending of light is called refraction.",
        "reference": "Figure 5. Left: The DM particle enters the bulk of the sun with an angle \u03b8, collides with solar electrons and finally escapes from the Sun. For the purpose of illustration, the DM trajectory is projected onto a single plane. Right: The flux of reflected DM particles can be approximated as propagating along the radial direction from the center of the Sun. See text for details."
    },
    "2110.14160v2-Figure2-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of traces, we first extract the direct call relations and indirect call relations from the traces. Then, we use the direct call relations to construct a direct call graph, and use the indirect call relations to construct an indirect call graph. Finally, we use the direct call graph and the indirect call graph to identify the indirect call patterns.\n\nFigure 7: Pretraining BERT. Architecture of BERT in pretraining. Assume that in the lowercased example sequence consisting of the segment pair \u2018he starts to speak. the nervous crowd is watch-ing him.\u2019 the tokens \u2018speak\u2019 and \u2018nervous\u2019 were sampled to be masked. \u2018speak\u2019 is replaced by the \u2018[MASK]\u2019 token and \u2018nervous\u2019 is replaced by the random token \u2018that\u2019. The model\u2019s task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.\n\nFigure 8: Fine-tuning BERT. Architecture of BERT in fine-tuning. Given a sequence of tokens, BERT first converts it into a sequence of vectors via token embeddings, positional embeddings, and segment embeddings. Then, it uses a stack of Transformer encoders to encode the sequence of vectors into a sequence of hidden vectors. Finally, it uses a feedforward neural network to predict the label of the sequence.",
        "reference": "Figure 2: Components analyzed in our deep learning-based DR grading framework. The evaluation process of a framework can be divided into two parts: training (top) and testing (bottom). In the training phase, we first fix the architecture of the selected network (ResNet-50). Then we examine a collection of designs with respect to the training setting including preprocessing (image resizing and enhancement), training strategies (compositions of data augmentation (DA) and sampling strategies) and optimization configurations (objective functions and learning rate (LR) schedules). In the testing phase, we apply the same preprocessing as in the training phase and employ paired feature fusion to make use of the correlation between the two eyes (the training step of the fusion network is omitted in this figure). Then, we select the best ensemble method for the final prediction."
    },
    "2205.12022v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 (C1) and trace 2 (C2), the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nThe flowchart in Figure 8 shows the parsing generator with Res FFT-Conv Block. The input to the model is an image, which is passed through an encoder to produce a feature map. The feature map is then passed through a series of Res FFT-Conv blocks, which learn to extract features from the image. The output of the Res FFT-Conv blocks is then passed through a decoder to produce a parse tree.",
        "reference": "Fig. 1. Overview of our model. Our model consists of parsing generator and image generator, training parsing generator requires a pair of source-target images IS , IT , then obtaining human keypoints KS ,KT and human parsing map PS , PT respectively by using openpose and PGN framework. We concatenate KS , PS ,KT as the input of parsing generator, then the input is fed into an Unet-type network that generates a target parsing map with the same size of PT , which contains body shape information. To get the vivid image with detailed texture (e.g.style of clothing), we extract the Per-region feature information PSj from the source image via VGG type network, then we concatenate the parsing map to the above KS , PS ,KT and normalize it along with the region information PSj to combine the information of source texture and target parsing map. Finally, the target image can be generated by spatial normalization and decoder."
    },
    "2207.06080v2-Figure2-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the functions that are directly called by each function. The indirect call relations are the relations between the functions that are called by the functions that are called by each function. The indirect call patterns are the patterns of the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by a final output layer. The model is trained to predict the masked tokens in a sequence of text.\n\nFigure 8 shows how to use embeddings to map images into a feature space. A stack of convolutional layers is used to extract features from the images. The features are then mapped into a feature space using a non-linear mapping. The feature embeddings are then used to classify the images.",
        "reference": "Fig. 2: Illustration of feature and classification embeddings, using the Resnet 32 architecture. Feature embeddigs (FE) are extracted by the CNN\u2019s non-linear layers and are combined with the final classification layer weights to form classification layer embeddings (CLE), before they are summed."
    },
    "2204.09225v1-Figure1-1.png": {
        "prediction": " In Figure 1, given two traces Trace1 and Trace2, the direct call relations and indirect call relations are extracted. The direct call relations are represented by the directed edges between the caller and callee functions. The indirect call relations are represented by the dashed edges between the caller and callee functions, which are mediated by other functions. The indirect call patterns are also extracted, which are represented by the boxes with the indirect call relations. For example, the indirect call pattern (C1, C2) represents the indirect call relation between C1 and C2, which is mediated by C3, C4, C5, and C6.\n\nIn Figure 7, the input sequence is first embedded into a sequence of vectors using word embeddings, segment embeddings, and positional embeddings. The word embeddings are learned using a masked language model. The segment embeddings are learned using a next sentence prediction task. The positional embeddings are learned using a sinusoidal function. The embedded sequence is then passed through a series of transformer layers. Each transformer layer consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the sequence. The feed-forward layer allows the model to learn non-linear relationships between the elements of the sequence. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nIn Figure 8, (a) shows the overall architecture of our proposed method, which consists of twin transformer networks and a recovery module. Each twin transformer network takes a single fMRI volume as input and outputs a latent representation. (b) illustrates the architecture of the twin transformer. It consists of two spatial transformer layers and two temporal transformer layers. The spatial transformer layer applies a linear projection to the input features to obtain the spatial attention weights. The temporal transformer layer applies a linear projection to the input features to obtain the temporal attention weights.",
        "reference": "Fig. 1. Illustration of the proposed Twin-Transformers framework. (a) shows the overall picture of the proposed model, and (b) shows the details of the Twin-Transformers. The Twin-Transformers take the identical brain signal matrix as input. The spatial Transformer focuses on the space dimension and takes non-overlapping spatial patches as tokens to build attention in the spatial dimension and generate spatial features. Conversely, the temporal Transformer focuses on the temporal dimension and the non-overlapping temporal patches are used as tokens. Correspondingly, the temporal Transformer is designed to build attention in the time dimension and generate temporal features. The twin Transformers are designed for disentangling temporal and spatial features of the input signal matrix."
    },
    "2102.09837v1-Figure2-1.png": {
        "prediction": " Figure 3: Calibration procedure for each ToF sensor. State Init indicates the initial state, where the ToF is in the idle mode. In the Calibrating state, the calibration procedure is performed. After calibration, the ToF sensor transits to the Calibrated state. The dashed arrow represents the error handling when the calibration fails.",
        "reference": "Figure 2: The platform model of a robot arm."
    },
    "2202.01032v2-Figure5-1.png": {
        "prediction": " ## Non-RT RIC Service Management and Orchestration Framework\n\nThe figure shows the architecture of the Non-RT RIC Service Management and Orchestration Framework. The framework consists of three layers:\n\n- The Service Management and Orchestration Framework layer is responsible for managing and orchestrating the xApps. It provides APIs and SDKs to support the xApps.\n- The Internal Messaging Infrastructure layer is responsible for providing communication between the xApps and the framework. It also provides conflict mitigation, subscription management, and security mechanisms.\n- The RAN layer is responsible for providing the underlying network connectivity to the xApps.\n\nThe framework is designed to be flexible and extensible, allowing new xApps to be added and deployed easily. It is also designed to be scalable, able to support a large number of xApps and devices.",
        "reference": "Fig. 5: Near-RT RIC architecture. The near-RT RIC connects to the RAN through the E2 interface, at the bottom of the figure (yellow), and to the non-RT RIC/SMO through the A1 and O1 interfaces, at the top of the figures (orange and green, respectively). The communication among the RIC components (in light blue) is mediated by an internal messaging infrastructure. The near-RT RIC can onboard custom logic as xApps (dark blue)."
    },
    "2202.01032v2-Figure6-1.png": {
        "prediction": " This image shows a service management and orchestration (SMO) framework for near real-time and non-real-time RIC. The SMO framework consists of three main functional blocks:\n\n1. Data management and exposure\n2. AI/ML workflow\n3. Internal messaging infrastructure\n\nThe SMO framework also has four external interfaces:\n\n1. O2 termination\n2. O1 termination\n3. A1 termination\n4. R1 termination\n\nThe SMO framework interacts with the following components:\n\n1. O-Cloud\n2. O-CU\n3. O-DU\n4. O-RU\n\nThe SMO framework is responsible for the following functions:\n\n1. Policy\n2. Inventory\n3. Design\n4. Configuration",
        "reference": "Fig. 6: Non-RT RIC and SMO architecture. The SMO functionalities (in green) enable connectivity to the O-Cloud (through the O2 interface) and the other RAN components (through O1) for management and orchestration. The non-RT RIC features custom logic (rApps, in red), and a termination of the A1 interface to the near-RT RIC (orange). Shared functionalities between the non-RT RIC and the SMO are in yellow."
    },
    "2202.02070v2-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a graph, where the nodes are the functions and the edges are the direct calls between the functions. The indirect call relations are represented as a matrix, where the rows and columns are the functions and the values are the number of indirect calls between the functions.\n\nThe next step is to identify the indirect call patterns. This is done by clustering the indirect call relations matrix. The clusters are then used to generate the indirect call patterns.\n\nFinally, the direct call relations and the indirect call patterns are used to generate the temporal relations. The temporal relations are represented as a graph, where the nodes are the functions and the edges are the temporal relations between the functions.",
        "reference": "Fig. 2. The architecture of the proposed CGiS-Net."
    },
    "1319230-Figure1-1.png": {
        "prediction": " The image shows the flowchart of a question answering system. The system takes a query x as input and outputs an answer y. The first step is to select a sentence from the document d that is relevant to the query. This is done by using a sentence selection algorithm. The next step is to generate a summary of the selected sentence. This is done by using a document summarization algorithm. The final step is to generate an answer to the question using the summary. This is done by using an answer generation algorithm.",
        "reference": "Figure 1: Hierarchical question answering: the model first selects relevant sentences that produce a document summary (d\u0302) for the given query (x), and then generates an answer (y) based on the summary (d\u0302) and the query x."
    },
    "210713891-Figure1-1.png": {
        "prediction": " The figure shows an end-to-end trainable neural dialog system. Given an image and a history of the dialog, the system predicts a natural language response. i3d-flow, i3d-rgb, and vggish are image encoders. We use two types of inputs for the dialog encoder: (1) word embeddings and (2) an embedding that encodes the dialog history. We use a stack of self-attention layers to encode the dialog. The output of the dialog encoder is used to predict the next word in the dialog.",
        "reference": "Figure 1: An illustration of DSTC8 AVSD task. The goal of the task is generating accurate answer based on multiple modalities."
    },
    "2206.00726v1-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of the proposed approach. Given a set of traces, we first extract the direct call relations and indirect call relations. Then, we use these relations to construct the direct call graph and indirect call graph. Finally, we use the direct call graph and indirect call graph to identify the indirect call patterns.\n\nFigure 7 shows the overall architecture of BERT. \n\nFigure 8 shows the Behavior Cloning (BC) training process. In BC, a dataset of expert demonstrations is used to train a policy network. The policy network is trained to predict the actions that the expert would have taken in each state. Once the policy network is trained, it can be used to control the robot.",
        "reference": "Fig. 1: Overview of our proposed algorithm. The BayesOpt model is composed of multiple Gaussian process models, which represent dynamic feasibility and collision avoidance constraints."
    },
    "2101.09353v2-Figure1-1.png": {
        "prediction": " The flowchart describes the process of using a neural network to predict the next word in a sequence. The input to the network is a sequence of words, and the output is a probability distribution over the next word in the sequence.\n\nThe first step is to tokenize the input sequence. This means breaking the sequence into a list of individual words. The next step is to embed the words in the sequence. This means converting each word into a vector of real numbers. The embedding vectors are then used to initialize the hidden state of the neural network.\n\nThe neural network then processes the sequence of embedded words one word at a time. At each time step, the network takes the current hidden state and the current input word embedding as input, and it outputs a new hidden state and a probability distribution over the next word in the sequence. The network then repeats this process until it has processed all of the words in the sequence.\n\nThe final output of the network is a probability distribution over the next word in the sequence. This distribution can be used to generate the next word in the sequence by sampling from the distribution.",
        "reference": "Figure 1. Working principle and radioprobe system architecture."
    },
    "2101.01418v1-Figure2-1.png": {
        "prediction": " An overview of the different stages of a typical image processing pipeline. The pipeline consists of four main stages: image acquisition, pre-processing, intermediate level processing and high level processing. The input to the pipeline is a raw image, and the output is a set of results that can be used to make a decision or take an action. The pre-processing stage enhances the image and removes noise. The intermediate level processing stage segments the image, extracts features, and classifies the objects in the image. The high level processing stage interprets the results of the intermediate level processing stage and makes a decision or takes an action.",
        "reference": "Figure 2: Different levels in image processing process."
    },
    "2101.05151v2-Figure5-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations can be inferred. For example, if C1 calls C2 in Tr1 and C2 calls C3 in Tr2, then an indirect call relation from C1 to C3 can be inferred. In addition, indirect call patterns can be mined from the indirect call relations. For example, if there are multiple indirect call relations from C1 to C2, C1 and C2 can be considered to have an indirect call pattern.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the encoders is a sequence of vectors, each of which represents the meaning of the corresponding token in the context of the entire sequence.\n\nFigure 8 shows the inference process of a Transformer model. The model takes a sequence of tokens as input and outputs a sequence of probabilities over the vocabulary. The model is composed of a stack of Transformer blocks, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn complex relationships between the tokens. The output of the Transformer model is a sequence of vectors, each of which represents the probability of the corresponding token in the output sequence.",
        "reference": "Figure 5: Graphical illustration of long horizontal link forecasting. Given a sequence of graph snapshots G = {G(t\u2212tk), ...,G(t)}, whose length is k, test quadruples at t+ \u2206T are to be predicted."
    },
    "1482745-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations can be inferred. For example, if C1 calls C2 in Tr1, and C2 calls C3 in Tr2, then there is an indirect call relation from C1 to C3. \n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The output of the final encoder is used to predict the masked tokens in the input sequence.\n\nFigure 8 shows the architectures of the GAN discriminator and the PacGAN discriminator. The GAN discriminator is a simple feed-forward neural network that takes as input a real image or a generated image and outputs a probability that the input image is real. The PacGAN discriminator is a more complex neural network that takes as input a real image, a generated image, and a label indicating whether the input image is real or generated. The PacGAN discriminator outputs a probability that the input image is real and a probability that the input image is generated.",
        "reference": "Figure 1: PacGAN(m) augments the input layer by a factor of m. The number of edges between the first two layers are increased accordingly to preserve the connectivity of the mother architecture (typically fully-connected). Packed samples are fed to the input layer in a concatenated fashion; the grid-patterned nodes represent input nodes for the second input sample."
    },
    "2210.08812v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values represent the probability of an indirect call relation between the two functions. The use case is represented as a set of indirect call patterns. Each indirect call pattern is represented as a sequence of functions, where the first function is the caller and the last function is the callee. The probability of each indirect call pattern is also represented.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to each other and to generate a contextualized representation of the input sequence. The final output of the model is a sequence of vectors, where each vector represents the contextualized representation of the corresponding token in the input sequence.\n\nFigure 8 shows the two types of transformer layers used in BERT. The first type is the self-attention layer, which allows the model to attend to different parts of the input sequence and to generate a contextualized representation of each token. The second type is the feed-forward layer, which applies a fully connected layer to the output of the self-attention layer. The two types of layers are stacked together to form a transformer block.\n\nFigure 9 shows how BERT can be used for masked language modeling. In masked language modeling, the model is given a sequence of tokens, where some of the tokens are masked. The model is then trained to predict the masked tokens based on the context of the unmasked tokens. This task helps the model to learn to understand the meaning of words and phrases in context.",
        "reference": "Fig. 2. Illustration of aggregation based explicit transformer and modulation based implicit transformer."
    },
    "2101.07910v1-Figure2-1.png": {
        "prediction": " 1. Start with an input population of code snippets.\n2. Calculate the mutation score for each code snippet.\n3. If the mutation score is less than N, go to step 4. Otherwise, go to step 6.\n4. Refactor the input code snippet with a mutation rate.\n5. Add the refactored code snippet to the next generation.\n6. Check if the sample is elite.\n7. If the sample is elite, add it to the next generation.\n8. If the number of iterations is less than N, go to step 2. Otherwise, go to step 9.\n9. Return the final generated code snippets.",
        "reference": "Fig. 2. The guided mutation (GM) process for code adversarial generation."
    },
    "2012.06182v1-Figure1-1.png": {
        "prediction": " The figure shows a layered architecture for satellite networks. The layers, from top to bottom, are:\n\n- Deep Space: >35,838 km\n- GEO: 35,786 km\n- MEO: 2,000\u201312,000 km\n- LEO: 200\u20132,000 km\n- Stratosphere: 17\u201322 km\n- Aeronautical: 0.15\u201317 km\n- Terrestrial: <0.15 km\n\nThe figure depicts links between the layers as follows:\n- Inter-Layer Links: Deep Space to LEO, GEO to LEO, MEO to LEO, LEO to Stratosphere, Stratosphere to Aeronautical, Aeronautical to Terrestrial\n- Intra-Layer Links: LEO to LEO, MEO to MEO, GEO to GEO",
        "reference": "Fig. 1: Illustration of a multilayered SN with satellites, HAPs, and UAVs."
    },
    "2012.06182v1-Figure8-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from a given program. The process starts with identifying direct call relations between functions in the program. Then, indirect call relations are identified by analyzing the control flow graph of the program. Finally, the temporal relations between the functions are extracted by analyzing the execution traces of the program.\n\nThe flowchart in Figure 7 illustrates the architecture of a BERT model in pretraining. The model consists of a stack of transformer encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by a word embedding layer. The output of the final transformer encoder is fed into a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 illustrates the architecture of a UAV-mounted MEC system. The system consists of a swarm of UAVs, each of which is equipped with a MEC server. The UAVs are connected to each other via a wireless network, and they can also connect to ground users via a backhaul link. The MEC servers can provide computing and storage resources to the ground users, and they can also help to improve the network performance by caching content and providing edge computing services.",
        "reference": "Fig. 8: An architecture of HAPs network with P2P HAP-to-HAP and backhauling links."
    },
    "2103.04910v1-Figure6-1.png": {
        "prediction": " The figure shows a block diagram of a feedback control system. The system is represented by the block labeled \"System\", and it takes an input \\(u(t)\\) and produces an output \\(y(t)\\). The controller is represented by the block labeled \"Regulator\", and it takes the output \\(y(t)\\) and the reference input \\(r(t)\\) as inputs, and produces a control signal \\(u(t)\\) as output. The reference input is the desired output of the system. The controller is designed using a control design block, which takes the current state of the system \\(x(t)\\) and the desired state \\(x_d(t)\\) as inputs, and produces the control signal \\(u(t)\\) as output. The state of the system is estimated by a recursive identifier, which takes the input \\(u(t)\\) and the output \\(y(t)\\) as inputs, and produces an estimate \\(\\hat{x}(t)\\) of the state \\(x(t)\\) as output.",
        "reference": "Figure 6: Model building approach"
    },
    "2202.06208v1-Figure1-1.png": {
        "prediction": " The flowchart shows the process of using a machine learning model to predict the next word in a sentence. The model is first trained on a dataset of sentences, and then it can be used to predict the next word in a new sentence. The flowchart shows the steps involved in this process.\n\nThe first step is to preprocess the data. This involves removing any punctuation or special characters from the data, and then tokenizing the data into words. The next step is to create a vocabulary of the words in the data. This vocabulary is used to represent the words as integers. The next step is to create a feature vector for each word in the data. This feature vector represents the word's frequency in the data, as well as its position in the sentence. The next step is to train the machine learning model. This involves using the feature vectors to train a model to predict the next word in a sentence. The final step is to use the trained model to predict the next word in a new sentence. This involves using the feature vector of the new sentence to predict the next word.",
        "reference": "Figure 1. The architecture of our proposed BROT. Sub-figure (a) depicts the OT process to align features from different domains. Sub-figure (b) shows that the metric learning aims to seek better decision boundaries with a dynamic hierarchical tree. The darkness of blue colors reflects the magnitude of molecular properties."
    },
    "2107.01104v1-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct execution traces and extract temporal relations. Given two traces of API calls Tr1 and Tr2, where each trace Ti =< C1, C2, . . . , Cn > is an ordered sequence of API calls, we build a bipartite graph G =< V1 \u222a V2, E >, where V1 and V2 are the sets of API calls in Tr1 and Tr2, respectively, and E is the set of edges between API calls in V1 and V2. We then extract two types of temporal relations: (1) direct call relations, which capture the order of API calls within each trace; and (2) indirect call relations, which capture the order of API calls between the two traces. \n\nThe direct call relations are represented by the edges in the bipartite graph G. For each API call Ci \u2208 V1, we create an edge from Ci to all API calls Cj \u2208 V2 that are reachable from Ci in Tr1. Similarly, for each API call Cj \u2208 V2, we create an edge from Cj to all API calls Ci \u2208 V1 that are reachable from Cj in Tr2. \n\nThe indirect call relations are represented by the edges in the complete bipartite graph K1,2 =< V1, V2, E\u2032 >, where E\u2032 is the set of all possible edges between V1 and V2. For each edge (Ci, Cj) \u2208 E\u2032, we assign a weight that represents the strength of the temporal relation between Ci and Cj. The weight is calculated as the number of times that Ci and Cj are executed in the same order in the two traces.",
        "reference": "Figure 1: Distribution of real-space data between MPI processes in TurTLE. Fields are split into slabs and distributed between P MPI processes along the x3 direction. The Np particles are also distributed, with each MPI process storing Pp particles on average. Within each MPI process the particle data is sorted according to its x3 location. This leads to a direct association between each of the Ps field slices to contiguous regions of the particle data arrays \u2014 in turn simplifying the interpolation procedure (see text for details). On average, S p particles are held within each such contiguous region."
    },
    "51865-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values represent the number of indirect calls between the functions. The use case is represented as a set of indirect call patterns. Each indirect call pattern is a sequence of functions that are called indirectly from each other. The figure shows an example of a use case with two indirect call patterns: C1, C2 and C1, C4.",
        "reference": "Figure 3. Overview of DCEC. The network is built by cascading the CNN module and the CEC."
    },
    "2012.02113v1-Figure8.1-1.png": {
        "prediction": " Mutual information Venn diagram. The diagram shows the relationship between the entropies \\(H(X)\\) and \\(H(Y)\\), the joint entropy \\(H(X,Y)\\), and the mutual information \\(I(X;Y)\\).",
        "reference": "Figure 8.1 Venn diagram showing entropic quantities associated with a pair of random variables taking values in different sets: the Shannon entropies H(X) and H(Y), the joint entropy H(X,Y), the conditional entropies H(X | Y) and H(Y | X), and the mutual information I(X; Y)."
    },
    "2012.02113v1-Figure7.4-1.png": {
        "prediction": " The image shows how to use traces to learn temporal relations between events. Given two traces, trace 1 and trace 2, we can extract the direct call relations and indirect call relations. The direct call relations are the relations between the events in the same trace. The indirect call relations are the relations between the events in different traces. We can use these relations to learn a model that can predict the temporal relations between events.\n\nThe image also shows how a neural network can be used to learn temporal relations between events. The neural network has two inputs, the direct call relations and the indirect call relations. The neural network outputs the temporal relations between the events. The neural network is trained using the traces. The neural network can be used to predict the temporal relations between events in new traces.",
        "reference": "Figure 7.4 Schematic illustration of Lemma 7.4.6."
    },
    "2204.12696v1-Figure2-1.png": {
        "prediction": " Figure 1 shows the call graph of a program with two functions `C1` and `C2`, and two traces of execution through the program. Each node in the call graph represents a function, and each edge represents a call from one function to another. The two traces are shown as sequences of function calls, with each call represented by an arrow.\n\nThe first trace, `Trace 1`, starts at function `C1`, which calls function `C2`, which in turn calls function `C3`. `C3` then calls back to `C2`, which returns to `C1`. The second trace, `Trace 2`, starts at function `C2`, which calls function `C4`, which calls function `C5`. `C5` then calls back to `C4`, which returns to `C2`. `C2` then calls function `C6`, which returns to `C2`. Finally, `C2` returns to the starting function `C1`.\n\nThe figure also shows the indirect call relations between functions. For example, there is an indirect call relation between `C1` and `C3`, because `C1` calls `C2`, which in turn calls `C3`. Similarly, there is an indirect call relation between `C2` and `C5`, because `C2` calls `C4`, which in turn calls `C5`.",
        "reference": "Figure 2: A tensor illustration of our low-rank micromotion subspace hypothesis. In the StyleGAN latent space, we hypothesize the same type of micromotion, at different quantitative levels but for the same identity, can be approximated by a low-rank subspace. We further hypothesize that subspaces for the same type of micromotion found at different identities are extremely similar to each other, and can hence be transferred across identities."
    },
    "206593096-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns. The direct call patterns and indirect call patterns are then used to generate the temporal relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the input embedding layer. The embedded tokens are then passed through the encoders, which learn to extract contextual information from the sequence. The output of the encoders is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence.\n\nFigure 8 shows two designs for object detection. Design A is a two-stage detector, which consists of a region proposal network (RPN) and a Fast R-CNN detector. The RPN takes an image as input and outputs a set of rectangular object proposals. The Fast R-CNN detector then takes the object proposals from the RPN and classifies each proposal into one of the foreground classes (e.g., person, car, bicycle) or the background class. Design B is a one-stage detector, which takes an image as input and directly outputs the bounding boxes and class labels of the objects in the image.",
        "reference": "Figure 3: Two network designs in this paper. The input image is processed as a whole at the convolutional layers from conv1 to conv5. Segments are exploited at a deeper hierarchy by: (Left) applying CFM on the feature map of conv5, where \u201c b\u201d means for \u201cbounding boxes\u201d and \u201c s\u201d means for segments; (Right) applying CFM on the finest feature map of the spatial pyramid pooling layer."
    },
    "2101.10868v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations can be inferred. For example, if C1 calls C2 in Tr1 and C2 calls C3 in Tr2, then there is an indirect call relation from C1 to C3. The direct and indirect call relations can be used to infer the indirect call patterns. For example, if there is an indirect call relation from C1 to C3 and an indirect call relation from C2 to C4, then there is an indirect call pattern (C1, C2).\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of transformer encoders, which are each composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the model is a sequence of vectors, which represent the hidden states of the model.\n\nFigure 8 illustrates the data flow in a typical social media platform. Users generate data, which is collected by the platform. The platform processes the data and shares it with third parties, such as advertisers. The government can also order the platform to share data with them. This data can be used for a variety of purposes, such as mass surveillance, changing users' behaviors, and multiple information aggregation.",
        "reference": "Figure 1: A Systematic Overview of Privacy Threats from Multiple Domains Related to the COVID-19 Pandemic"
    },
    "2102.04539v2-Figure10-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between API calls that are directly called, while the indirect call relations are the relations between API calls that are not directly called but have a transitive relation through other API calls. The indirect call patterns are the patterns of indirect call relations. For example, the indirect call pattern (C1, C2) indicates that there is an indirect call relation between API call C1 and API call C2. The direct call patterns are the patterns of direct call relations. For example, the direct call pattern (C1, C2) indicates that there is a direct call relation between API call C1 and API call C2.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between different parts of the sequence. The output of the encoder layers is a sequence of vectors, each of which represents a word in the input sequence. These vectors are then used to predict the next word in the sequence.\n\nFigure 8 shows an example of how BERT can be used to generate text. The input to the model is a sequence of tokens, which are the words in the sentence. The model then uses its self-attention layers to attend to different parts of the input sequence, and its feed-forward layers to learn relationships between different parts of the sequence. The output of the model is a sequence of vectors, each of which represents a word in the output sentence. These vectors are then used to generate the output sentence.",
        "reference": "Figure 10: Illustration for 2-Diam GBP. Here, V q denotes all sets V{q,\u00b7}."
    },
    "2102.04539v2-Figure6-1.png": {
        "prediction": " The image shows how traces are generated and used to learn temporal relations. Given a set of traces (left), we can extract direct call relations (C1) and indirect call relations (C2). We can then use these relations to learn a model of indirect call patterns (C1, C2). This model can then be used to predict the next call in a trace.",
        "reference": "Figure 6: Illustration for the construction in the proof of Proposition 8 for 2-Reach GBP with r = 1. In this example, U = {u1, . . . , un} and we have {u1, ui, uj , un} = F \u2208 F ."
    },
    "2102.04539v2-Figure4-1.png": {
        "prediction": " The image shows how to compute the probability of a word given its preceding words in a sentence. The model is a neural network that takes as input a sequence of word embeddings and outputs a probability distribution over the next word in the sequence. The network is trained on a large corpus of text data.\n\nThe first step is to embed the input words into vectors. This is done using a lookup table that maps each word to a vector of real numbers. The vectors are then passed through a series of layers of neural networks. Each layer consists of a linear transformation followed by a non-linear activation function. The output of the final layer is a probability distribution over the next word in the sequence.\n\nThe model is trained using a maximum likelihood objective function. This means that the model is trained to minimize the negative log-likelihood of the correct next word. The model is trained on a large corpus of text data.\n\nOnce the model is trained, it can be used to generate text. This is done by starting with a seed word and then using the model to predict the next word in the sequence. The process is repeated until a desired length of text has been generated.",
        "reference": "Figure 4: Illustration to Constructions 3 & 4. Part (a) shows an exemplary directed graph which is a yes-instance for DHP. Applying Construction 3 on (a) yields (b). Applying Construction 4 on (b) yields the instance whose graph is depicted in (c) and two habitats of which are depicted in (d) and (e). Vertices marked yellow in (d) are contained in the habitat Xout. Vertices marked red in (e) are contained in the habitat Yout. The graph induced by Yout contains the red edges."
    },
    "2110.06487v1-Figure5-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. The left part shows two execution traces, denoted as Trace 1 and Trace 2, respectively. Each trace consists of a sequence of API calls, where each API call is represented by a node. The right part shows the direct call relations and indirect call relations between the APIs. The direct call relations are represented by the edges between the nodes, while the indirect call relations are represented by the dashed edges.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors. The embedded vectors are then passed through the stack of encoders. The output of the final encoder is a sequence of vectors, which are then used to predict the next sentence and the masked tokens.\n\nFigure 8 shows the flowchart of the proposed approach. The approach consists of three main steps: data collection, feature extraction, and classification. In the data collection step, data is collected from various sources. In the feature extraction step, features are extracted from the collected data. In the classification step, the extracted features are used to classify the data into different categories.",
        "reference": "Fig. 5. Covid unemployment threat model"
    },
    "2103.00907v1-Figure4-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to extract temporal relations between API calls. Given two traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent API calls and the edges represent the direct call relations between them. The indirect call relations are represented as a bipartite graph, where one set of nodes represents API calls and the other set of nodes represents indirect call patterns. The edges between the two sets of nodes represent the indirect call relations between API calls and indirect call patterns. \n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using word embeddings. The embeddings are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract contextual information. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nFigure 8 shows the architecture of a neural network with one hidden layer. The network has an input layer, a hidden layer, and an output layer. The input layer consists of  p input neurons, the hidden layer consists of u hidden neurons, and the output layer consists of v output neurons. The network is trained to learn the mapping from the input to the output.",
        "reference": "Figure 4. Diagram of the neural network(NN2) for the turbulent channel flow. The inputs are the pressure and velocity gradient and the output is the pressure strain term. The FCFF has 5 layers with 10 neurons in each layer."
    },
    "2206.00162v2-Figure2-1.png": {
        "prediction": " Figure 1 shows the computation of temporal relations between events in traces. Given two traces Trace 1 and Trace 2, we first identify the direct call relations and indirect call relations in each trace. A direct call relation (C1, C2) indicates that component C1 directly calls component C2, while an indirect call relation (C1, C2) indicates that C1 calls C2 through a sequence of other components. We then use these relations to compute the temporal relations between events in the traces. For example, if there is a direct call relation (C1, C2) and an event e1 in C1 happens before an event e2 in C2, we can infer that e1 happens before e2.\n\n\nFigure 7 illustrates the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the final encoder layer is a sequence of vectors, each of which represents the meaning of a word in the input sequence. These vectors are then used to predict the next word in the sequence.\n\nFigure 8 shows the architecture of a GAN. A GAN consists of two neural networks: a generator and a discriminator. The generator takes a random noise vector as input and generates an image. The discriminator takes the generated image and a real image as input and outputs a probability that the input image is real. The generator and discriminator are trained together in an adversarial manner: the generator tries to generate images that the discriminator cannot distinguish from real images, while the discriminator tries to distinguish between real images and generated images.",
        "reference": "Figure 2: Overview of PAGER generation method."
    },
    "2011.03148v2-Figure4-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the call graph. Given a pair of traces (e.g., Trace 1 and Trace 2), direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph, and the indirect call relations are used to construct the indirect call graph. The direct call graph and the indirect call graph are then used to extract temporal relations between the functions.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the first encoder is a sequence of word embeddings, and the output of the last encoder is a sequence of hidden vectors. These hidden vectors are then used to predict the masked tokens in the input sequence.\n\nThe flowchart in Figure 8 illustrates the training procedure of Faster R-CNN. The Faster R-CNN consists of two main components: a region proposal network (RPN) and a Fast R-CNN detector. The RPN takes an image as input and outputs a set of rectangular object proposals. Each object proposal is a bounding box that is likely to contain an object. The Fast R-CNN detector then takes the object proposals from the RPN and classifies each proposal into one of the foreground classes (e.g., person, car, bicycle) or the background class.",
        "reference": "Fig. 4. Diagram of perception consistency loss computation. An EfficientDet object detector predicts boxes and classes. Consistency of predictions between images is captured by losses similar to those in object detection training."
    },
    "2110.09749v2-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.\n\nThe flowchart in Figure 8 shows the architecture of a neural network for keyphrase extraction. The model consists of two main components: a candidate keyphrase extractor and a keyphrase importance estimator. The candidate keyphrase extractor takes as input a document and outputs a list of candidate keyphrases. The keyphrase importance estimator then takes as input the candidate keyphrases and outputs a score for each keyphrase. The keyphrases with the highest scores are then selected as the final output.",
        "reference": "Figure 1: The KIEMP model architecture."
    },
    "2103.07592v2-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call patterns and indirect call patterns can be further extracted from the direct call relations and indirect call relations, respectively.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which attend to each other and to the input vectors. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nThe figure shows a diagram of a dark matter (DM) candidate interacting with the Standard Model (SM) through a new heavy mediator particle, denoted by \\(H\\). The interaction is mediated by a new force, which is described by the coupling \\(F_{\\mu\\nu\\rho\\sigma}\\). The dark matter particle is a fermion, denoted by \\(\\chi\\), which is a candidate for cold dark matter.",
        "reference": "Figure 1: Schematic diagram for flux-mediated dark matter."
    },
    "2107.05680v1-Figure2-1.png": {
        "prediction": " Figure 1 shows the call graph of a program with two functions `C1` and `C2`, and two traces `t1` and `t2`. Each node in the call graph represents a function, and each edge represents a call relation between two functions. A trace is a sequence of function calls, and each node in the trace graph represents a function call event. The arrows between trace nodes represent the temporal order of function calls. \n\nThe figure illustrates how the direct and indirect call relations between functions can be extracted from execution traces. The direct call relations are represented by the edges in the call graph, while the indirect call relations are represented by the paths in the trace graph. For example, there is a direct call relation between `C1` and `C2`, and there is also an indirect call relation between `C1` and `C2` via `C3`.",
        "reference": "Figure 2: A modified architecture for progressive training of convex GANs (ProCoGAN). At each stage i, a linear generator Wi is used to model images at a given resolution Xi, attempting to fool quadratic-activation discriminator Di, for which the optimal solution can be found in closed-form via (15). Once stage i is trained, the input to stage i + 1 is given as the output of the previous stage with learned weights W\u2217i , which is then used to model higher-resolution images Xi+1. The procedure continues until high-resolution images can be generated from successive application of linear generators."
    },
    "2107.05214v2-Figure6-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call patterns and indirect call patterns can then be obtained by abstracting the direct call relations and indirect call relations, respectively.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The output of the final encoder is used to predict the masked tokens in the input sequence.\n\nFigure 8 illustrates the process of binarizing a mask m. The mask m is first multiplied by a weight vector h, and then passed through a multi-layer perceptron (MLP). The output of the MLP is a binary vector, which is the binarized mask.",
        "reference": "Figure 6: The illustration of the attention mechanism. The prediction of current hidden state h\u0302t and the grid-level features E is used as query and key, respectively."
    },
    "2102.12058v2-Figure17-1.png": {
        "prediction": " In federated learning, each node has a local dataset and collaboratively trains a global model. The training process involves multiple rounds of communication between the nodes. In each round, the nodes first train the local model using their local dataset. Then, the nodes exchange their local model updates with each other. Finally, the nodes aggregate the local model updates to update the global model. The process continues until the global model converges.",
        "reference": "Fig. 17. Federated voting process [46] ."
    },
    "2103.05447v1-Figure2-1.png": {
        "prediction": " The image shows a flowchart that illustrates how to use the indirect call patterns to detect vulnerabilities in a program. The flowchart starts with the user providing two traces of the program's execution. These traces are then used to generate direct call relations and indirect call relations. The direct call relations are used to identify the functions that are called directly by other functions. The indirect call relations are used to identify the functions that are called indirectly by other functions. The next step is to identify the indirect call patterns. These patterns are used to identify the vulnerabilities in the program. Finally, the vulnerabilities are reported to the user.",
        "reference": "Figure 2. Experiment schematic."
    },
    "2010.00041v3-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given a set of traces, the algorithm first identifies all the direct call relations between the functions. Then, for each pair of functions that have a direct call relation, the algorithm checks if there is an indirect call relation between them. If there is an indirect call relation, the algorithm adds it to the set of indirect call relations. The algorithm then outputs the set of direct call relations and the set of indirect call relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the hidden state of the corresponding token. The model is trained to predict the masked tokens in the input sequence.\n\nThe flowchart in Figure 2 shows the training process of the genetic algorithm. The algorithm starts by creating a random population of individuals. Each individual represents a potential solution to the optimization problem. The population is then evaluated, and the individuals are ranked according to their fitness. The best individuals are then selected to be parents for the next generation. The parents are then used to create a new population of individuals through crossover and mutation. The process is repeated until the stopping criterion is met. The best individual in the final population is then returned as the solution to the optimization problem.",
        "reference": "Fig. 2. The optimization flowchart for the inverse material design."
    },
    "2202.00846v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces (Trace 1 and Trace 2) of direct call relations (left), indirect call relations (middle), and indirect call patterns (right) are extracted. In particular, C1 directly calls C2, and C1 indirectly calls C4 via C2. In addition, there are two indirect call patterns: C1 calls C2 followed by C4, and C1 calls C2 followed by C6.\n\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The token embeddings are then passed through a series of transformer layers, which learn to attend to other tokens in the sequence and extract contextual information. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary. The model is trained to predict the masked tokens in the input sequence.\n\nThe CVR estimation process is illustrated in the flowchart in Figure 8. The process starts with collecting click/purchase logs. The logs are then used to train a model to estimate the conversion rate for each group. The model is then used to compute the assignment probability for each group. The assignment probability is used to determine which group a user should be assigned to. The results are then displayed to the user.",
        "reference": "Figure 2: Method Overview"
    },
    "2101.07424v2-Figure1-1.png": {
        "prediction": " The flowchart shows how to generate an indirect call graph from traces. Given two traces, trace 1 and trace 2, the direct call relations and indirect call patterns are extracted. The direct call relations are used to generate the direct call graph. The indirect call patterns are used to generate the indirect call graph. The direct call graph and the indirect call graph are then combined to generate the indirect call graph.",
        "reference": "Fig. 1. Physical sensing phenomena in CASSI, which is the CSI prototype used to validate the proposed approach."
    },
    "2204.03873v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted. Given two traces, trace 1 and trace 2, which contain the execution order of a set of components C={c1,c2,...,c6}, we can extract the direct call relations and indirect call relations. The direct call relations are represented by the edges between the components in the traces. For example, in trace 1, there is a direct call relation from component c1 to component c2, denoted as c1\u2192c2. The indirect call relations are represented by the dashed edges between the components in the traces. For example, in trace 1, there is an indirect call relation from component c1 to component c4, denoted as c1\u2192c4. The indirect call patterns are extracted based on the indirect call relations. For example, in trace 1, there is an indirect call pattern c1\u2192c2\u2192c4.\n\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the model is a sequence of vectors, each of which represents the hidden state of the corresponding token in the input sequence.\n\nThe self-attention layer allows the model to attend to different parts of the input sequence when generating the output. This is done by calculating a weighted sum of the hidden states of the input tokens, where the weights are determined by a query vector. The feed-forward layer is a fully connected layer that is applied to the output of the self-attention layer. This layer allows the model to learn non-linear relationships between the input tokens.\n\nThe model is trained using a masked language modeling objective. This objective is to predict the masked tokens in a sequence of text, given the context of the surrounding tokens. The model is trained to minimize the cross-entropy loss between the predicted tokens and the actual tokens.",
        "reference": "Figure 2: Structure of gait-TR. TCN is the temporal convolutional network module, and ST is the spatial transformer module. FC denotes full connect layer. Batch-norm is BatchNorm2D for input Xtv \u2208 \u211d C\u00d7T\u00d7V , while Batch-norm* denotes BatchNorm1D for input Xtv \u2208 \u211d C\u2217V \u00d7T ."
    },
    "2204.10426v1-Figure1-1.png": {
        "prediction": " The flowchart shows the process of using a deep learning model to predict the next word in a sequence. The model is first trained on a large dataset of text, and then it can be used to predict the next word in a sequence of text. The flowchart is as follows:\n\n1. Start with a sequence of text.\n2. Use a deep learning model to predict the next word in the sequence.\n3. Repeat steps 2 and 3 until the end of the sequence is reached.\n\nThe flowchart can be used to generate text, translate languages, and answer questions.",
        "reference": "Figure 1: Three-state illness-death model"
    },
    "2204.09537v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations can be extracted. The direct call relations are the pairs of APIs that are directly called one after another in the traces. The indirect call relations are the pairs of APIs that are not directly called one after another in the traces, but there is a path between them in the call graph. The indirect call patterns are the patterns of indirect call relations. For example, the pattern C1->C2->C3 means that C1 calls C2 and C2 calls C3.\n\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between different parts of the sequence. The output of the encoder layers is a sequence of vectors, each of which represents a word in the input sequence. These vectors are then used to predict the next word in the sequence.\n\nFigure 2 shows the experimental setup. A mode-locked Ti:sapphire laser produces 75 fs pulses at a repetition rate of 80 MHz. The laser beam is split into two parts. One part is used to pump an optical parametric amplifier (OPA), which generates tunable visible light. The other part is used to generate extreme ultraviolet (XUV) light by high-harmonic generation (HHG). The XUV light is focused into a gas cell containing the sample. The sample is ionized by the XUV light, and the resulting ions are accelerated by an electric field. The ions are then detected by a time-of-flight mass spectrometer.",
        "reference": "Figure 1: Top: Schematic of the hydrogen beamline including the electric field ionizer (FI) and the microchannel plate (MCP) particle detectors. The electric and magnetic fields required for optical Rydberg excitation (cf. section 2.2.1) yet need to be experimentally implemented. Bottom: Schematic of the injection seeded Ti:Sa Rydberg excitation laser."
    },
    "2107.06268v1-Figure1-1.png": {
        "prediction": " The flowchart\u4e00\u958b\u59cb\u662f\u8cc7\u6599\u6e05\u7406\u548c\u9810\u8655\u7406\uff0c\u63a5\u8457\u662f\u5047\u65e5\u8abf\u6574\u7a0b\u5e8f\uff0c\u7136\u5f8c\u662f\u8a13\u7df4\u500b\u5225\u9810\u6e2c\u6a21\u578b\uff0c\u6700\u5f8c\u662f\u901a\u904e\u5e73\u6ed1\u4f2f\u6069\u65af\u5766\u5728\u7dda\u805a\u5408\u9810\u6e2c\u7d44\u5408\u3002",
        "reference": "Fig. 1: Structure of forecasting approach used for the forecasting competition."
    },
    "2011.14684v2-Figure4-1.png": {
        "prediction": " Figure 1 shows the call relations between functions. The left part shows the direct call relations and indirect call relations of the program. The right part shows the indirect call patterns between functions. Each node in the graph represents a function, and each edge represents a call relation between two functions. The direct call relations are represented by solid arrows, and the indirect call relations are represented by dashed arrows. The indirect call patterns are represented by boxes, and each box contains a set of indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, and the feed-forward layer allows the model to learn complex relationships between the input tokens. The output of the final encoder is a sequence of vectors, where each vector represents the learned representation of a token in the input sequence.\n\nFigure 4 shows the residual reduction module used in the Swin Transformer. The module consists of a stack of residual blocks, each of which consists of a self-attention layer, a normalization layer, and a residual connection. The self-attention layer allows the model to attend to different parts of the input sequence, and the normalization layer helps to stabilize the training process. The residual connection allows the model to learn long-term dependencies in the input sequence.",
        "reference": "Fig. 4: Overview of the REMnet architecture. The input of the model is the K\u00d71 tensor representing the CIR of the measurement. The dimensionality is reduced by N subsequent Residual Reduction Modules (RRM) with a feature attention mechanism. Finally, a fully connected layer composes the high-level extracted features and outputs the range error estimation."
    },
    "2206.08316v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. In the first step, the program code is parsed to identify the call relations between functions. In the second step, the identified call relations are categorized into direct call relations and indirect call relations. In the third step, the temporal relations between the functions are extracted based on the direct and indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the final encoder layer is a sequence of vectors, each of which represents a word in the input sequence. These vectors are then used to predict the next word in the sequence.\n\nThe flowchart in Figure 8 illustrates the training, generating, and attacking stages of an adversarial attack on a deep neural network (DNN) model. In the training stage, a normal surrogate model and a dark surrogate model are trained on the raw dataset. The normal surrogate model is trained to mimic the behavior of the victim model, while the dark surrogate model is trained to generate adversarial examples that can misclassify the victim model. In the generating stage, adversarial examples are generated using the dark surrogate model. In the attacking stage, the adversarial examples are used to attack the victim model.",
        "reference": "Fig. 1 An illustration of transfer-based adversarial attack and the proposed method. The two images in Raw Dataset are from ImageNet, labeled as \u201cpersian cat\u201d and \u201cpapillon\u201d respectively. Note that they also have features of other cats and dogs, as well as pillow and car. The normal surrogate model is trained by one-hot labels, and its adversarial transferability is relatively weak. In contrast, the dark surrogate model we proposed is trained with enhanced dark knowledge. Thus, it demonstrates stronger adversarial transferability."
    },
    "2010.03420v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given a set of traces, the algorithm first identifies direct call relations between functions. Then, for each pair of functions that have a direct call relation, the algorithm identifies indirect call patterns. Finally, the algorithm extracts temporal relations between functions based on the identified direct call relations and indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the hidden state of the corresponding token. The model is trained to predict the masked tokens in the input sequence.\n\nThe flowchart in the Figure depicts a Multi-Layer Perceptron (MLP) neural network. It consists of an input layer, an output layer, and multiple hidden layers. The input layer receives the input data, and the output layer produces the output. Each hidden layer consists of a number of neurons that are connected to the neurons in the previous and subsequent layers. The neurons in each layer are typically fully connected, meaning that each neuron in the layer is connected to every neuron in the previous and subsequent layers.",
        "reference": "Fig 2: Fully connected convolutional neural network with four hidden layers."
    },
    "2107.00465v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given two traces Tr1 and Tr2 of API call sequences, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent API calls and the edges represent the direct call relations between them. The indirect call relations are represented as a bipartite graph, where one set of nodes represents API calls and the other set of nodes represents indirect call patterns. The edges between the two sets of nodes represent the indirect call relations between API calls and indirect call patterns. \n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence. The model is trained to predict the next word in a sequence, given the previous words in the sequence. This task is known as masked language modeling.\n\nFigure 8 shows the architecture of a neural network with K hidden layers. The network has an input layer, K hidden layers, and an output layer. The input layer consists of N neurons, where N is the number of features in the input data. The hidden layers each consist of Z neurons, where Z is the number of hidden units in the network. The output layer consists of M neurons, where M is the number of classes in the output data. The network is trained to learn the relationship between the input data and the output data. This task is known as supervised learning.",
        "reference": "Fig. 1: Illustration of the neural network architecture to predict the optimal generation outputs P\u0302g using the active power demand Pd as input: There are K hidden layers in the neural network withNk neurons each. Where k = 1, ...,K."
    },
    "210701837-Figure2-1.png": {
        "prediction": " Figure 1 shows an overview of the proposed approach. Given a set of traces, we first extract the direct call relations and indirect call relations from the traces. Then, we use the direct call relations to construct a direct call graph, and use the indirect call relations to construct an indirect call graph. Finally, we use the direct call graph and the indirect call graph to identify the indirect call patterns.\n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of transformer encoders, with each encoder consisting of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the final encoder is a sequence of vectors, which are then used to predict the next word in the sequence.\n\nFigure 8 shows the training and prediction processes of the proposed method. In the training process, three modified U-Nets are trained on sagittal, coronal, and axial views, respectively. In the prediction process, the three modified U-Nets are used to segment the brain tumor from different views. The final segmentation result is obtained by fusing the outputs of the three U-Nets.",
        "reference": "Figure 2: The final segmentation volume is generated by taking into account activations from three FCNNs specialized on each 2D orientation. Neighboring slices are taken into account in a multi-channel approach. Full slices are used in prediction time, but training uses patches."
    },
    "2107.09388v2-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence. The feed-forward layer is a fully connected layer that is used to project the output of the self-attention layer to a higher dimension.\n\nThe input to the model is a sequence of tokens. Each token is represented by a vector of real numbers. The vectors are passed through the stack of transformer encoders. The output of the final encoder is a sequence of vectors. Each vector represents the probability distribution over the vocabulary for the corresponding token.\n\nThe model is trained to predict the masked tokens in the input sequence. The masked tokens are replaced with a special token, [MASK]. The model is trained to minimize the cross-entropy loss between the predicted probabilities and the true probabilities of the masked tokens.\n\nFigure 8 shows the architecture of a transformer encoder. The transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence. The feed-forward layer is a fully connected layer that is used to project the output of the self-attention layer to a higher dimension.\n\nThe input to the transformer encoder is a sequence of vectors. Each vector represents the embedding of a token in the input sequence. The vectors are passed through the self-attention layer. The output of the self-attention layer is a sequence of vectors. Each vector represents the weighted sum of the input vectors, where the weights are determined by the attention scores.\n\nThe output of the self-attention layer is passed through the feed-forward layer. The output of the feed-forward layer is a sequence of vectors. Each vector represents the output of the transformer encoder for the corresponding token in the input sequence.",
        "reference": "Figure 1: MHSA model configuration for SELD task."
    },
    "2103.10114v1-Figure10-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted from the dynamic call graph. Given two traces, trace t1 and trace t2, the direct call relations and indirect call patterns are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call patterns are represented as a set of triples, where each triple represents a pattern of indirect calls between three functions.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through the stack of transformer encoders, which learn to attend to each other and to the input tokens. The output of the final transformer encoder is a sequence of vectors, each of which represents the hidden state of the corresponding token. These hidden states are then used to predict the next token in the sequence, as well as to predict whether the current token is the start or end of a sentence.\n\nThe flowchart in Figure 7 works as follows: Given an input sequence of tokens, the model first converts the tokens into a sequence of vectors using a word embedding layer. The embedded tokens are then passed through a stack of transformer encoders, which learn to attend to each other and to the input tokens. The output of the final transformer encoder is a sequence of vectors, each of which represents the hidden state of the corresponding token. These hidden states are then used to predict the next token in the sequence, as well as to predict whether the current token is the start or end of a sentence.",
        "reference": "Fig. 10: Iteration procedure in dynamical core."
    },
    "2011.02216v2-Figure10-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values represent the probability of an indirect call from one function to another.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 shows the rounds of the entity linking process. In the first round, the model takes the query as input and generates a set of candidate entities. In the second round, the model takes the candidate entities and the context as input and generates a set of ranked entities. In the third round, the model takes the ranked entities and the context as input and generates a set of final entities.",
        "reference": "FIG. 10 Example of entanglement detection with bounded configuration space. In this illustration, every measurement in the first n \u2212 1 rounds is a 4-outcome POVM, where one outcome leads directly to guess SEP, and 3 outcomes continue with the measurement process."
    },
    "2102.12327v1-Figure4-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee in a direct call. The indirect call relations are the relations between the caller and the callee in an indirect call. The indirect call patterns are the patterns of the indirect call relations. The direct call patterns are the patterns of the direct call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, each of which is represented by a vector. The output of the model is a sequence of vectors, each of which represents the corresponding token in the input sequence.\n\nFigure 8 shows the test results of a test suite. The test suite consists of two test cases. The first test case failed. The second test case passed. The test suite diagnosis shows that the constraints usage = Scientific & cpu = CPUD, mb = MBSilver are inconsistent with the following test cases: usage = Scientific, cpu = CPUD, mb = MBSilver.",
        "reference": "Figure 4: PC recommender knowledge base: result of the diagnosis process presented in WEEVIS."
    },
    "2010.01919v2-Figure6-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph, and the indirect call relations are used to construct the indirect call graph. The direct call patterns and indirect call patterns are extracted from the direct call graph and indirect call graph, respectively.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 illustrates the training and evaluation process of BERT. The model is first pre-trained on a large corpus of text. During pre-training, the model is trained to predict the next token in a sequence of text, as well as to predict whether two sentences are semantically related. After pre-training, the model is fine-tuned on a smaller corpus of text that is specific to the task that the model will be used for. During fine-tuning, the model is trained to perform a specific task, such as answering questions or classifying text.",
        "reference": "Fig. 6: Training process. Data augmentation in pre-training. No data augmentation in fine-tuning."
    },
    "2107.12156v1-Figure2-1.png": {
        "prediction": " Figure 1: a Machine-learning-based methodology for exploring the growth parameter space of complex oxide thin films. a Bayesian optimization algorithm recommends the next thin film growth conditions based on the evaluation results of the previously grown thin films. The recommended growth conditions are then used to grow a new thin film sample, which is subsequently evaluated. This cycle is repeated until the desired thin film properties are achieved. b Schematic illustration of a typical thin film growth system equipped with combinatorial libraries for the deposition of thin film materials. c X-ray diffraction patterns of the as-grown thin films. The crystalline phase and crystallographic orientation of the thin films can be determined by analyzing the X-ray diffraction patterns.",
        "reference": "Figure 2. Schematic illustration of a machine-learning-integrated closed-loop process to optimize thin film fabrication parameters [13]"
    },
    "2107.00710v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to extract temporal relations. Given two traces, trace 1 and trace 2, which contain the execution order of components C1-C6, we can extract direct call relations and indirect call relations. For example, in trace 1, C1 calls C2, C3, and C4 directly, and C1 calls C5 and C6 indirectly through C4. We can also identify the indirect call patterns between C1 and C2, which are (C1, C4, C2) and (C1, C3, C2).\n\n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the first encoder is a sequence of tokens, each of which is represented by a word embedding. The output of the last encoder is a sequence of vectors, each of which represents the corresponding token in the input sequence.\n\n\nFigure 8 shows two variants of the EEG-based emotion recognition model. (A) The architecture of the short-term EEG-based emotion recognition model. (B) The architecture of the long-term EEG-based emotion recognition model.",
        "reference": "Figure 1. (A) The network\u2019s architecture employed for Mood-State Bipolar classification using short intervals containing 2830 learnable parameters. In this figure, Ri refers to the ith residual block (i \u2208 {1, 2, 3}), while Bj refers to the jth InceptionTime block (j \u2208 {1, 2, 3}). Conv refers to a convolutional layer and GAP refers to the Global Average Pooling operation. Finally, the plus signs refer to an element-wise summation. (B) Short-Long Network\u2019s architecture using 5689 parameters. The features from the Short Network corresponds to the output of the global average pooling operation in the Short Network."
    },
    "1369182-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call patterns and indirect call patterns are then generated based on the direct call relations and indirect call relations, respectively.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using word embeddings, segment embeddings, and positional embeddings. The embedded tokens are then passed through a series of Transformer layers, which learn to attend to each other and generate a contextualized representation of the input sequence. The final output of the model is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence.\n\nThe flowchart in Figure 8 illustrates the reinforcement learning process. The agent observes the environment and takes an action. The environment then provides the agent with a reward and the next observation. The agent uses the reward and the next observation to update its parameters. This process is repeated until the agent reaches its goal.",
        "reference": "Figure 1: Structure of a hierarchical sub-policy agent. \u03b8 represents the master policy, which selects a sub-policy to be active. In the diagram, \u03c63 is the active sub-policy, and actions are taken according to its output."
    },
    "2107.09510v1-Figure2-1.png": {
        "prediction": " Figure 1 shows how traces are generated and used to learn temporal relations. Given a set of traces, we first identify the direct call relations between API functions. For each pair of API functions \\(C_i\\) and \\(C_j\\) that have a direct call relation, we create a node in the direct call relation graph. We then identify the indirect call relations between API functions. For each pair of API functions \\(C_i\\) and \\(C_j\\) that have an indirect call relation, we create a node in the indirect call relation graph. We then use the direct and indirect call relation graphs to learn temporal relations between API functions.\n\nFigure 7 shows the architecture of BERT in pretraining. \n\nFigure 8 shows the architecture of the self-attention block used in the TransformerXL model.",
        "reference": "Fig. 2. The structure of the self-attention network (SAN) in this study. X is the input sequential physiological data. Four layers including one multi-head attention layer, one dense layer with two add & normalization layers form a self-attention block. After positional encoding, the model passes input data through 3 self-attention blocks and outputs the stress estimation result."
    },
    "2103.10312v2-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted from traces. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the functions that are directly called from one function to another. The indirect call relations are the relations between the functions that are called from one function to another through other functions. The indirect call patterns are the patterns of the indirect call relations. For example, the indirect call pattern (C1, C2) means that function C1 calls function C2 through other functions. The figure shows an example of how the direct call relations, indirect call relations, and indirect call patterns are extracted from two traces. In this example, there are two traces, trace 1 and trace 2. Trace 1 contains the following function calls: C1 calls C2, C2 calls C3, C3 calls C4, and C4 calls C5. Trace 2 contains the following function calls: C1 calls C2, C2 calls C3, C3 calls C6, and C6 calls C5. From these two traces, the direct call relations are extracted as follows: C1 calls C2, C2 calls C3, C3 calls C4, C4 calls C5, C1 calls C2, C2 calls C3, C3 calls C6, and C6 calls C5. The indirect call relations are extracted as follows: C1 calls C4, C1 calls C6, C2 calls C5, and C2 calls C6. The indirect call patterns are extracted as follows: (C1, C4), (C1, C6), (C2, C5), and (C2, C6).",
        "reference": "Figure 2. The Deep Autofocus architecture for a mini-batch size of one (recall our mini-batch size is thirty-two during training). The network inputs a single-look complex (SLC) image, ge, which is dynamic range compressed (DRC) and passed into a feature extraction network (blue) along with the SLC\u2019s phase. The features are then fed to a regression network (green) which outputs the phase corruption model parameters, which in this case are the coefficients of ten-degree polynomial modeling the phase error. The phase error is then applied in the k-space domain, computed by the fast Fourier transform (FFT), of the SLC and then inverse Fourier transformed back to the spatial domain. Finally, the relative improvement in sharpness between the input and output magnitude images is measured and weights are backpropagated to minimize this quantity (recall minimization of this term equates to maximisation of relative image sharpness, see Eq 8). Our formulation is end-to-end differentiable and trained on a graphics processing unit (GPU). During deployment, only a single forward pass is needed to compute g\u0302 which is represented by the red path."
    },
    "204955470-Figure1-1.png": {
        "prediction": " The flowchart describes the training procedure of a video topic classification model. Given a set of videos, the model first generates candidate topics for each video. Then, it ranks the candidate topics and selects the top 20 topics for each video. Finally, the model fine-tunes a video-level classifier and a segment-level classifier to predict the video topic.",
        "reference": "Figure 1. Solution overview."
    },
    "204955470-Figure6-1.png": {
        "prediction": " The image shows an overview of the proposed method. Given a video, we first extract frame-level features using a 3D CNN. Then, we use three NextVLAD models to encode the frame-level features into three different semantic spaces. Each NextVLAD model consists of a stack of self-attention layers and a global average pooling layer. The outputs of the three NextVLAD models are then averaged and passed through a gate to obtain the final prediction.",
        "reference": "Figure 6. Overview of a mixture of 3 NeXtVLAD models(MixNeXtVLAD) with online knowledge distillation. The orange arrows indicate the distillation of knowledge from the mixture prediction to the predictions of sub-models."
    },
    "12938495-Figure1-1.png": {
        "prediction": " The image shows an overview of the QA system. Given a question, the system first encodes the question and subgraph into binary vectors. Then, it uses an embedding matrix to embed the binary vectors into a continuous vector space. The continuous vectors are then used to compute a relevance score between the question and each candidate answer. The candidate answer with the highest relevance score is selected as the final answer.",
        "reference": "Fig. 1. Illustration of the subgraph embedding model scoring a candidate answer: (i) locate entity in the question; (ii) compute path from entity to answer; (iii) represent answer as path plus all connected entities to the answer (the subgraph); (iv) embed both the question and the answer subgraph separately using the learnt embedding vectors, and score the match via their dot product."
    },
    "2202.06988v2-Figure22-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces t1 and t2, the direct call relations C1 and C2 are extracted. Then, the indirect call relations between C1 and C2 are computed, resulting in the set of indirect call patterns. Finally, the direct call patterns between C1 and C2 are generated.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are first embedded into vectors. These vectors are then passed through a series of Transformer layers, which learn to attend to different parts of the sequence and extract meaningful features. The output of the Transformer layers is then passed through a softmax layer, which predicts the probability of each token in the sequence.\n\nThe flowchart in Figure 8 shows the two training stages of the proposed differentiable physics engine. In the first stage, the model is trained in an unsupervised manner, where the only objective is to minimize the loss function. In the second stage, the model is trained in a supervised manner, where the loss function is a combination of the unsupervised loss and a supervised loss.",
        "reference": "Figure 22: Visualisation of gradient back-propagation, comparing differentiable and supervised setups; displayed is a 3-step setup; the loss gradients from the last step are propagated through all previous steps and towards all previous network outputs; if the back-propagation is split into subranges, the gradients of the simulation state are set to zero, visualised by \u2019\\\u2019"
    },
    "2102.06744v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect calls. The edges between the two sets of nodes represent the indirect call relations.\n\nThe image in Figure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by a softmax layer that outputs a probability distribution over the vocabulary.\n\nThe input to the model is a sequence of tokens, which are represented as vectors. The vectors are passed through the embedding layer, which converts them into a dense representation. The embedded vectors are then passed through the encoders, which learn to attend to different parts of the sequence and to extract meaningful features from the text. The output of the encoders is a sequence of vectors, each of which represents the hidden state of the model at a particular position in the sequence. The hidden states are then passed through the softmax layer, which outputs a probability distribution over the vocabulary.\n\nThe model is trained to maximize the likelihood of the correct tokens in the input sequence. The loss function is calculated by comparing the model's output to the target tokens. The model is trained using a gradient-based optimization algorithm, such as Adam.\n\nThe image in Figure 8 shows the architecture of a neural network for speech recognition. The network consists of an embedding layer, an LSTM layer, a max pooling layer, and a dense layer. The embedding layer converts the input speech signal into a sequence of vectors. The LSTM layer learns to recognize patterns in the sequence of vectors. The max pooling layer reduces the dimensionality of the output of the LSTM layer. The dense layer outputs a probability distribution over the possible words in the speech signal.",
        "reference": "Fig. 1. Neural classifier model"
    },
    "2206.05406v1-Figure2-1.png": {
        "prediction": " The left part of the figure shows how to obtain traces and temporal relations from execution traces. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations can be obtained, respectively. In particular, for each function f in trace 1, if there exists a function g in trace 2 such that f directly calls g, then there is a direct call relation from f to g. For each pair of functions f in trace 1 and g in trace 2, if there exists a sequence of functions <f1, f2, ..., fn> such that f1 = f, fn = g, and for each i in [1, n-1], fi directly calls fi+1, then there is an indirect call relation from f to g. The right part of the figure shows how to obtain direct call patterns from direct call relations. Specifically, for each pair of functions f and g, if there is a direct call relation from f to g, then the direct call pattern (f, g) is obtained.",
        "reference": "Figure 2: Illustration of a free-rider attack. The free-rider does not perform normal training, but transmits fake model updates to the server by adding opportune stochastic perturbations \u03b5 based on Gaussian noise N(0, \u03c3). Finally, the global model issued by the server will be distributed to the free-rider."
    },
    "2101.11685v2-Figure3-4-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to extract temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the functions that are directly called by each function in the traces. The indirect call relations are the relations between the functions that are called by the functions that are called by each function in the traces. The extracted relations are then used to generate the indirect call patterns. The indirect call patterns are the patterns of the indirect call relations between the functions. For example, the indirect call pattern (C1, C2) indicates that function C1 calls function C2 indirectly.",
        "reference": "Figure 3-4: The overview of the modified squeeze-and-excitation block augmented with the memory layer. FGB is the function of global pooling which reduces the dimension of 3 dimensional feature map to the signle dimension, and Fadd is channel-wise addition of a resulting vector from memory to the original feature tensor."
    },
    "214727773-Figure1-1.png": {
        "prediction": " In Figure 1, we illustrate how execution traces and temporal relations are extracted from the program code. Given the execution traces, i.e., C1\u2192C2\u2192C3\u2192C4\u2192C5\u2192C6 and C1\u2192C2\u2192C5\u2192C6, the direct call relations can be obtained as C1\u2192C2, C2\u2192C3, C3\u2192C4, C4\u2192C5, C2\u2192C5, and C5\u2192C6. In addition, indirect call relations can be inferred from the direct call relations. For example, since there is a direct call relation from C1 to C2 and a direct call relation from C2 to C3, we can infer an indirect call relation from C1 to C3. Similarly, we can infer the indirect call relations from C1 to C4, C1 to C5, and C1 to C6. \n\nIn Figure 7, we show the Transformer-based language model BERT. The model consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The model is trained to predict the masked tokens in the input sequence.\n\nIn Figure 8, we illustrate the two types of knowledge distillation methods for training a student model with a teacher model. In the knowledge distillation for classification tasks (KD), the student model learns to mimic the soft targets of the teacher model. In the knowledge distillation for sequence generation tasks (TKD), the student model learns to mimic the logits of the teacher model at each time step.",
        "reference": "Figure 1. Illustration of LGKD, t(x,\u03b8) (left) and LTKD, t(x,\u03b8) (right)"
    },
    "2202.13121v1-Figure14-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect call patterns. The edges between the two sets of nodes represent the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the encoder layers is a sequence of vectors, each of which represents the hidden state of the model at a particular position in the input sequence. These vectors are then used to predict the next word in the sequence.\n\nThe figure shows how BERT can be used to generate text. The input to the model is a sequence of tokens, which are represented as vectors. The model then uses the self-attention layers to attend to different parts of the input sequence and the feed-forward layers to learn relationships between the different parts of the sequence. The output of the model is a sequence of vectors, each of which represents the probability of the next word in the sequence. These probabilities are then used to generate text.\n\nFigure 8 shows the architecture of a CNN-based re-identification model. The model consists of a series of convolutional layers, each of which is followed by a pooling layer. The convolutional layers learn to extract features from the input images, while the pooling layers reduce the dimensionality of the feature maps. The output of the convolutional layers is a feature vector, which is then used to classify the input image.",
        "reference": "Figure 14: An approach to achieve generalization."
    },
    "2011.07233v2-Figure3-1.png": {
        "prediction": " The flowchart shows the process of how a neural network model is trained. The model is first trained on a set of labeled data, which is then used to make predictions on a new set of unlabeled data. The predictions are then evaluated to determine the accuracy of the model. If the accuracy is not satisfactory, the model is retrained on a larger set of labeled data. This process is repeated until the accuracy of the model is satisfactory.\n\nThe flowchart also shows the different types of data that are used to train the model. The labeled data is used to train the model to learn the relationship between the input and output data. The unlabeled data is used to test the accuracy of the model. The evaluation data is used to determine the final accuracy of the model.\n\nThe flowchart also shows the different steps that are involved in training the model. The first step is to preprocess the data, which involves cleaning the data and removing any errors. The next step is to train the model, which involves feeding the data into the model and adjusting the weights of the model to minimize the error. The final step is to evaluate the model, which involves testing the accuracy of the model on a new set of data.",
        "reference": "Figure 3: On-surface aggregation. A 3D point x on the geometric scaffold \u0393 is seen in a set of source images. Each such image contributes a feature vector fk along a ray vk (green). On-surface aggregation uses a differentiable set network to process this data and produces a feature vector g for the target ray u (red)."
    },
    "2205.08180v1-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the functions that are directly called by each function. The indirect call relations are the relations between the functions that are called by the functions that are called by each function. The use case 1 represents the scenario where the functions C1 and C2 are called directly and indirectly. The indirect call patterns are the patterns of the indirect call relations. For example, the indirect call pattern (C1, C2) represents the pattern where C1 calls C2 indirectly. The direct call patterns are the patterns of the direct call relations. For example, the direct call pattern (C1, C2) represents the pattern where C1 calls C2 directly.",
        "reference": "Fig. 3: An illustration of the multimodal training framework"
    },
    "2010.03990v1-Figure5-1.png": {
        "prediction": " The flowchart\u4e00\u958b\u59cb\u662f\u5169\u500b\u865b\u7dda\u6846\uff0c\u5de6\u908a\u7684\u865b\u7dda\u6846\u5305\u542b\u4e86\u300cuse case u1\u300d\u548c\u300cTrace t1 Trace t2\u300d\uff0c\u53f3\u908a\u7684\u865b\u7dda\u6846\u5305\u542b\u4e86\u300cuse case u1\u300d\u548c\u300cindirect call patterns\u300d\u3002\n\n\u5de6\u908a\u7684\u865b\u7dda\u6846\u7684\u5de6\u908a\u662f\u300cC1\u300d\uff0c\u53f3\u908a\u662f\u300cC2\u300d\u3002C1\u548cC2\u4e4b\u9593\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC1\uff0c\u7d42\u9ede\u662fC2\u3002C1\u7684\u5de6\u908a\u9084\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC1\uff0c\u7d42\u9ede\u662fC1\u3002C2\u7684\u53f3\u908a\u9084\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC2\uff0c\u7d42\u9ede\u662fC2\u3002\n\n\u53f3\u908a\u7684\u865b\u7dda\u6846\u7684\u5de6\u908a\u662f\u300cC1\u300d\uff0c\u53f3\u908a\u662f\u300cC2\u300d\u3002C1\u548cC2\u4e4b\u9593\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC1\uff0c\u7d42\u9ede\u662fC2\u3002C1\u7684\u5de6\u908a\u9084\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC1\uff0c\u7d42\u9ede\u662fC1\u3002C2\u7684\u53f3\u908a\u9084\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC2\uff0c\u7d42\u9ede\u662fC2\u3002\n\n\u865b\u7dda\u6846\u7684\u4e0b\u9762\u662f\u5be6\u7dda\u6846\uff0c\u5be6\u7dda\u6846\u7684\u5de6\u908a\u662f\u300cdirect call relations\u300d\uff0c\u53f3\u908a\u662f\u300cindirect call patterns\u300d\u3002\n\n\u300cdirect call relations\u300d\u7684\u5de6\u908a\u662f\u300cC1\u300d\uff0c\u53f3\u908a\u662f\u300cC2\u300d\u3002C1\u548cC2\u4e4b\u9593\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC1\uff0c\u7d42\u9ede\u662fC2\u3002C1\u7684\u5de6\u908a\u9084\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC1\uff0c\u7d42\u9ede\u662fC1\u3002C2\u7684\u53f3\u908a\u9084\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC2\uff0c\u7d42\u9ede\u662fC2\u3002\n\n\u300cindirect call patterns\u300d\u7684\u5de6\u908a\u662f\u300cC1\u300d\uff0c\u53f3\u908a\u662f\u300cC2\u300d\u3002C1\u548cC2\u4e4b\u9593\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC1\uff0c\u7d42\u9ede\u662fC2\u3002C1\u7684\u5de6\u908a\u9084\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC1\uff0c\u7d42\u9ede\u662fC1\u3002C2\u7684\u53f3\u908a\u9084\u6709\u4e00\u689d\u7bad\u982d\uff0c\u7bad\u982d\u7684\u8d77\u9ede\u662fC2\uff0c\u7d42\u9ede\u662fC2\u3002",
        "reference": "Fig. 5: Architecture of FRCNN [33]"
    },
    "2010.03990v1-Figure6-1.png": {
        "prediction": " The flowchart\u4e00\u958b\u59cb\u662f\u5169\u500b\u7bc0\u9ede\uff0c\u5206\u5225\u662f\u300c\u4f7f\u7528\u6848\u4f8b1\u300d\u548c\u300c\u4f7f\u7528\u6848\u4f8b2\u300d\u3002\n\n\u300c\u4f7f\u7528\u6848\u4f8b1\u300d\u7684\u6d41\u7a0b\u5982\u4e0b\uff1a\n1. \u8f38\u5165\u300c\u8ddf\u8e641\u300d\u548c\u300c\u8ddf\u8e642\u300d\u3002\n2. \u8a08\u7b97\u300c\u76f4\u63a5\u547c\u53eb\u95dc\u806f\u300d\u3002\n3. \u8a08\u7b97\u300c\u9593\u63a5\u547c\u53eb\u95dc\u806f\u300d\u3002\n4. \u8a08\u7b97\u300c\u9593\u63a5\u547c\u53eb\u6a21\u5f0f\u300d\u3002\n5. \u8f38\u51fa\u300c\u76f4\u63a5\u547c\u53eb\u6a21\u5f0f\u300d\u3002\n\n\u300c\u4f7f\u7528\u6848\u4f8b2\u300d\u7684\u6d41\u7a0b\u5982\u4e0b\uff1a\n1. \u8f38\u5165\u300c\u8ddf\u8e641\u300d\u548c\u300c\u8ddf\u8e642\u300d\u3002\n2. \u8a08\u7b97\u300c\u76f4\u63a5\u547c\u53eb\u95dc\u806f\u300d\u3002\n3. \u8a08\u7b97\u300c\u9593\u63a5\u547c\u53eb\u95dc\u806f\u300d\u3002\n4. \u8a08\u7b97\u300c\u9593\u63a5\u547c\u53eb\u6a21\u5f0f\u300d\u3002\n5. \u8f38\u51fa\u300c\u9593\u63a5\u547c\u53eb\u6a21\u5f0f\u300d\u3002",
        "reference": "Fig. 6: Architecture of SSD [21]"
    },
    "2010.03990v1-Figure7-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect calls. The edges between the two sets of nodes represent the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through the encoders, which learn to extract contextual representations of the tokens. The output of the encoders is a sequence of vectors, each of which represents the contextual representation of a token in the input sequence.\n\nFigure 8 shows the architecture of a face detection model. The model consists of two main components: a feature extraction module and a detection module. The feature extraction module is composed of a convolutional neural network (CNN), which is used to extract features from the input image. The detection module is composed of a fully connected layer, which is used to classify the extracted features into different classes.",
        "reference": "Fig. 7: Architecture of UESegNet-1"
    },
    "2202.07376v1-Figure3-1.png": {
        "prediction": " In Figure 1, given two traces Tr1 and Tr2, the direct call relations and indirect call relations can be extracted. The direct call relations are C1->C2, C2->C3, C3->C4, C4->C5, and C5->C6. The indirect call relations are C1->C4, C1->C5, C1->C6, C2->C5, and C2->C6. The indirect call patterns are (C1, C4), (C1, C5), (C1, C6), (C2, C5), and (C2, C6). In Figure 7, the input sequence is converted into a sequence of word embeddings, which are then passed through a series of transformer layers. The output of the transformer layers is then used to predict the next word in the sequence. In Figure 8, the architecture of a 3D CNN is shown. The 3D CNN consists of a stack of 3D convolutional layers, each followed by a max pooling layer. The output of the 3D CNN is then flattened and passed through a fully connected layer to produce the final output.",
        "reference": "Figure 3: Our proposed end-to-end QPP model comprising a Siamese network of shared parameters of layered convolutional feature extraction, followed by either i) merge (concatenation) and a fully connected (FC) layer with a Sigmoid loss for pairwise testing (Equation 7) yielding a binary comparison indicator between a pair, or ii) a linear activation layer with pairwise hinge loss for pointwise testing yielding a score for a given query (Equation 8). Since the interaction for MDMQ and SDSQ are matrices with a single row only, the two layers of convolution filter sizes for these approaches are 1 \u00d7 5 and 1 \u00d7 3 (see Section 2.4)."
    },
    "2205.02090v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns. The direct call patterns and indirect call patterns are then used to generate the temporal relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 shows the overall architecture of the discourse dependency model. The model consists of three main components: a sentence splitter, an intra-model dependency parser, and an inter-model dependency parser. The sentence splitter takes a document as input and splits it into a sequence of sentences. The intra-model dependency parser then takes each sentence and parses it to identify the dependencies between the words in the sentence. The inter-model dependency parser then takes the output of the intra-model dependency parser and identifies the dependencies between the sentences in the document. The output of the inter-model dependency parser is a set of final relations between the sentences in the document.",
        "reference": "Figure 2: An overview of our model. Intra-sentential dependencies are discovered first and inter-sentential dependencies are constructed after that to form a complete dependency tree."
    },
    "2102.00713v1-Figure3-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a pair of consecutive frames \\(I_t\\) and \\(I_{t+1}\\), we first extract appearance features using a CNN encoder \\(S\\). We then compute the direct and indirect relations between the feature maps of \\(I_t\\) and \\(I_{t+1}\\). Specifically, the direct relation is computed as the element-wise product of the feature maps, and the indirect relation is computed as the element-wise product of the feature maps after applying a 1x1 convolution. We use three different indirect relation patterns, as shown in the figure. Finally, we concatenate the direct relation and the three indirect relations, and feed them into a fully-connected layer to predict the disparity map.\n\nFigure 7 illustrates the Transformer-based language model. The model consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input features. The model is trained to predict the masked tokens in the input sequence.",
        "reference": "Figure 3: The architecture details of the proposed multi-task CNN. Here n denotes the number of output feature maps."
    },
    "1505637-Figure4-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to learn temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. Then, the indirect call patterns are generated based on the indirect call relations. Finally, the direct call patterns and indirect call patterns are used to learn temporal relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to each other and generate a contextualized representation of the input sequence. The final output of the model is a sequence of vectors, where each vector represents the contextualized representation of the corresponding token in the input sequence.\n\nFigure 8 illustrates the training procedure of a Transformer model. The model is trained to predict the masked tokens in a sequence of text. The masked tokens are replaced with a special token, and the model is trained to predict the original tokens based on the context. The model is trained using a cross-entropy loss function, and the parameters of the model are updated using gradient descent.",
        "reference": "Figure 4: Method to select the kernel height kh. We project the receptive field of the target kernel to equirectangular projection Ie and increase kh until it is taller than the target kernel in Ie. The kernel width kw is determined using the same procedure after kh is set. We restrict the kernel size kw \u00d7 kh by an upper bound Uk."
    },
    "2011.03197v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 (C1) and trace 2 (C2), the direct call relations and indirect call relations are extracted. The direct call relations are represented as a matrix, where each row represents a caller and each column represents a callee. The indirect call relations are represented as a matrix, where each row represents a caller and each column represents a callee. The indirect call patterns are then generated by identifying the common callees between the direct call relations and the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the word embeddings layer. The embedded tokens are then passed through the encoders, which learn to attend to each other and to generate a representation of the sequence. The final output of the model is a vector, which is used to predict the next sentence in the sequence.\n\nFigure 8 shows the architecture of a transformer model. The model consists of a stack of encoder layers and a stack of decoder layers. The encoder layers are used to encode the input sequence into a vector, and the decoder layers are used to decode the vector into an output sequence. Each encoder layer consists of a self-attention layer and a feed-forward layer, and each decoder layer consists of a self-attention layer, an encoder-decoder attention layer, and a feed-forward layer.",
        "reference": "Figure 2: Reliability block diagram of series-parallel system."
    },
    "2011.03307v2-Figure11-1.png": {
        "prediction": " The image shows the geometry and the main physical processes in the lamppost model for AGN. The X-ray corona is located above the accretion disk and Compton upscatters the soft X-rays emitted from the disk. Part of the upscattered X-rays illuminate the inner wall of the molecular torus which partially covers the central region and re-emit the X-rays in all directions. The scattered X-rays are observed as the reflection component in the AGN X-ray spectra.",
        "reference": "Fig. 11. Illustration of the considered scenario. Above a spinning black hole, X-rays are emitted isotropically. Because of the compact corona very close to the black hole, the majority of the photons either hit the accretion disc or fall into the black hole. From the high- to the low-flux state, a partial coverer is obscuring and increasing part of the emitted X-ray radiation."
    },
    "2010.09125v2-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the call relations. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values represent the probability of a function calling another function. The use case u1 represents the scenario where the indirect call relations are used to predict the direct call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence. The model is trained using a masked language modeling objective, where a certain percentage of the tokens in the input sequence are masked and the model is trained to predict the masked tokens.\n\nFigure 8 shows the architecture of a GAN-based approach for disentangling 3D shape and appearance. The generator network takes a latent code as input and generates a 3D shape and an appearance code. The discriminator network takes the generated shape and appearance code as input and outputs a probability of the input being real or fake. The encoder network takes a real image as input and outputs a latent code. The decoder network takes the latent code as input and outputs a reconstructed image. The cycle consistency loss is used to ensure that the reconstructed image is similar to the input image. The perceptual loss is used to ensure that the generated image is visually similar to the real image. The KL divergence loss is used to ensure that the latent code is distributed according to a prior distribution.",
        "reference": "Figure 1: We employ two \u201crenderers\u201d: a GAN (StyleGAN in our work), and a differentiable graphics renderer (DIB-R in our work). We exploit StyleGAN as a synthetic data generator, and we label this data extremely efficiently. This \u201cdataset\u201d is used to train an inverse graphics network that predicts 3D properties from images. We use this network to disentangle StyleGAN\u2019s latent code through a carefully designed mapping network."
    },
    "2102.06271v1-Figure5-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct the execution traces and derive temporal relations from them. Given two traces of API calls Tr1 and Tr2, we first identify the direct call relations and indirect call relations, which are represented as two graphs G1 and G2, respectively. Each node in the graphs represents an API function, and a directed edge between two nodes indicates that the source node calls the target node. We then identify the indirect call patterns from G1 and G2, which are represented as a set of triples (C1, C2, P), where C1 and C2 are API functions and P is a path from C1 to C2 in G2. Finally, we derive the direct call patterns from G1, which are represented as a set of pairs (C1, C2), where C1 and C2 are API functions and there is a directed edge from C1 to C2 in G1.\n\nFigure 7 shows the overall architecture of BERT. The input to BERT is a sequence of tokens, which are embedded into vectors using word embeddings, positional embeddings, and segment embeddings. The embedded tokens are then fed into a series of transformer layers, which attend to each other and to the input embeddings. The output of the transformer layers is then fed into a softmax layer to produce a probability distribution over the vocabulary.\n\nThe NCI algorithm takes as input a graph G and a set of target nodes T. It intervenes on T by setting the values of the target nodes to zero. It then computes the new probability distribution over the output nodes Y, conditioned on the intervention. The difference between the original probability distribution and the new probability distribution is the NCI score.",
        "reference": "Figure 5. Schematic demonstrating calculation of NCI ."
    },
    "212029-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe image in Figure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The output of the encoders is a sequence of vectors, which are then used by the output layer to predict the next token in the sequence.\n\nThe image in Figure 8 shows the architecture of a convolutional neural network (CNN) for semantic segmentation. The CNN consists of a series of convolutional layers, each of which is followed by a pooling layer. The convolutional layers learn to extract features from the input image, while the pooling layers reduce the dimensionality of the feature maps. The output of the CNN is a set of feature maps, which are then used by the fully connected layer to predict the semantic segmentation of the input image.",
        "reference": "Figure 2. An overview of the proposed FastMask architecture."
    },
    "174869-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from traces. Given a set of traces, the algorithm first extracts direct call relations and indirect call relations. Then, it computes the indirect call patterns. Finally, it derives the direct call patterns.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The embeddings are then passed through the encoders, which learn to extract contextual information from the sequence. The output of the encoders is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence.\n\nThe figure shows the overall architecture of the pruning algorithm. It consists of two main components: (1) an importance evaluation module and (2) a pruning module. The importance evaluation module evaluates the importance of each weight in the network. The pruning module then prunes the network by removing the least important weights. The dashed lines in the figure indicate the connections that are pruned by the algorithm.",
        "reference": "Figure 2. Illustration of \u201cSparse Shrink\u201d algorithm. We evaluate the importance factor of each channel of feature maps f `, and prune the least important channels (dashed box). The pruning operation involves removing corresponding channels in W ` (dashed line), and modifying convolutional kernel W ` (blue line)."
    },
    "2103.05900v1-Figure5-1.png": {
        "prediction": " The image shows a flowchart that illustrates how to use the trace data to generate the topology of the underlying system. The flowchart takes as input two traces, trace1 and trace2, and outputs the topology of the system. The first step is to identify the direct call relations from the traces. This is done by looking for pairs of events in the traces that have a direct call relationship, i.e., one event calls the other event. The next step is to identify the indirect call relations from the traces. This is done by looking for pairs of events in the traces that have an indirect call relationship, i.e., one event calls a function that calls the other event. The final step is to use the direct and indirect call relations to generate the topology of the system. This is done by creating a graph where the nodes are the functions in the system and the edges are the call relations between the functions.",
        "reference": "Fig. 5: Overview of the DPN model for diagram classification."
    },
    "2107.03491v1-Figure5-1.png": {
        "prediction": " In Figure 1, given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the functions that are directly called by each function. The indirect call relations are the relations between the functions that are called by the functions that are called by each function. The indirect call patterns are the patterns of the indirect call relations. For example, the indirect call pattern (C1, C2) means that C1 calls C2 indirectly. In Figure 7, the input sequence is \"he starts to [MASK] the nervous crowd is watch-ing him.\". The model is trained to predict the masked token \"speak\" and the next sentence \"the nervous crowd is watching him.\". In Figure 8, coherent RM search is performed to find the optimal rotation matrices for a set of coherent poses. The RM-synthesis step is then performed to compute the optimal rotation matrix for the entire sequence.",
        "reference": "Figure 5. A schematic of the semi-coherent RM search routine that offsets the deleterious effects of intra-channel depolarization by coherently rotating to discrete trial RMs and applying RM-synthesis. The separation between trial values, \u03b4RM \u223c 700 rad m\u22122, ensures that no more than 10% depolarization can occur across the semi-coherent search range. This amounts to several thousand coherent de-rotation operations to search out to RM amplitudes as large as 106 radm\u22122."
    },
    "2011.09318v2-Figure1-1.png": {
        "prediction": " The figure shows a high-level view of the cryptocurrency transaction network analysis. The analysis can be categorized into three types based on the contents: network modeling, network profiling, and network-based detection. Besides, the analysis can be categorized into three types based on the methods: network property analysis, market effect analysis, and transaction pattern recognition.",
        "reference": "Fig. 1: The outline of cryptocurrency transaction network analysis methods."
    },
    "1294630-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call patterns are then generated based on the direct call relations. Similarly, the indirect call patterns are generated based on the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe model is trained using a masked language modeling objective. This means that a certain percentage of the tokens in the input sequence are masked, and the model is trained to predict the masked tokens based on the context. This objective helps the model to learn to understand the meaning of words and phrases in context.\n\nFigure 8 shows an example of how BERT can be used to generate text. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe model is trained using a masked language modeling objective. This means that a certain percentage of the tokens in the input sequence are masked, and the model is trained to predict the masked tokens based on the context. This objective helps the model to learn to understand the meaning of words and phrases in context.",
        "reference": "Figure 2: Architecture of our neural EL system. The input to the system are: a document D containing the query mention m and the corresponding Wikipedia candidate link li \u2208 L, where L is the set of all possible links extracted from the fast match step described in Section ."
    },
    "2107.03234v5-Figure5-1.png": {
        "prediction": " The image shows how traces are generated and how they are used to learn temporal relations. In particular, the image shows how traces are generated by following the execution of a program, and how these traces can be used to learn temporal relations between events in the program. The image also shows how these temporal relations can be used to predict the next event in a trace.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is trained on a large corpus of text. The model is composed of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence. The model is trained to predict the next word in a sequence given the previous words in the sequence. This training objective allows the model to learn the relationships between words and to generate coherent text.\n\nThe image shows a railway station with three platforms. There are two tracks, one between platforms 1 and 2, and another between platforms 2 and 3. There is a depot connected to platform 1 by a single track. There are two trains, one on track 1 and another on track 2. The train on track 1 is going to platform 1, and the train on track 2 is going to platform 2.",
        "reference": "Figure 5: The demonstrative model."
    },
    "2011.06150v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations can be extracted. In this example, C1 directly calls C2, C3, C4, and C5, and indirectly calls C6 through C4. The direct call patterns and indirect call patterns can then be used to detect vulnerabilities. For example, if C1 is a vulnerable function and C2 calls C1, then C2 may also be vulnerable.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence. The model is trained to predict the next word in a sequence given the previous words in the sequence. This training objective helps the model to learn the relationships between words and to generate coherent text.\n\nFigure 8 shows how the schedules S and S\u2032 are constructed. In schedule S, each job is assigned to a different machine in each time slot. In schedule S\u2032, job j1 is assigned to machine m1 in time slot 1, job j2 is assigned to machine m2 in time slot 1, and so on.",
        "reference": "Figure 1 An illustration of an application of Algorithm 1. Let the set of cliques be given by"
    },
    "2207.10397v1-Figure1-1.png": {
        "prediction": " The image shows the process of pretraining a language model. The model is first trained on a large corpus of text data. Then, it is fine-tuned on a smaller dataset of code. The fine-tuned model can then be used to generate code.\n\nThe flowchart is as follows:\n\n1. Start with a programming problem.\n2. Use a pre-trained language model to generate code solutions.\n3. Use a dual execution agreement to test the code solutions.\n4. Select the best code solution.",
        "reference": "Figure 1: The illustration of CODET. Both the code solutions and the test cases are generated by the pre-trained language model. The best code solution is then selected by a dual execution agreement."
    },
    "2110.08422v1-Figure1-1.png": {
        "prediction": " The figure illustrates the system model and attack vectors. The content consumer tries to access the content through the Internet. The censor can monitor and block the access to the content. The content publisher can publish the content on the uWeb and the Bitcoin blockchain. The integrity of the content can be ensured by the Satoshi blockchain.",
        "reference": "Fig. 1. System and adversary model. A client in the censored region cannot directly access sensitive services and news. Instead, content publishers embed content on commercial blockchains; clients access data with privacy and anonymity through standard Satoshi clients. The adversary can censor select client communications, monitor communications and perform integrity attacks."
    },
    "2011.01713v2-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 (T1) and trace 2 (T2), the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the functions that are directly called from one function to another. The indirect call relations are the relations between the functions that are called from one function to another through a chain of other functions. The indirect call patterns are then generated based on the indirect call relations. The indirect call patterns are the patterns of how the functions are called from one function to another through a chain of other functions. Finally, the direct call patterns are generated based on the direct call relations. The direct call patterns are the patterns of how the functions are called from one function to another.\n\nThe flowchart in Figure 7 illustrates the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are converted into embeddings by the input embedding layer. The embeddings are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.\n\nThe flowchart in Figure 8 illustrates the dataflow of a DPU accelerator. The accelerator consists of a number of processing elements (PEs), which are responsible for executing the DPU instructions. The PEs are organized into a grid, and each PE is connected to its nearest neighbors. The dataflow is controlled by a central controller, which is responsible for scheduling the instructions and allocating the data to the PEs. The PEs execute the instructions in parallel, and the results are then returned to the central controller.",
        "reference": "Fig. 2. Data-path schematic view of the accelerator core and its embedding into an SoC-level system. The diagram shows the unrolled compute architecture and encoding/decoding blocks, as well as the weight and feature map memories and tile buffer module. The dataflow of the accelerator is scheduled to first buffer full feature map windows in the tilebuffer and then compute the convolution result with pre-loaded weights in the compute units after which they are saved back to the feature map memory."
    },
    "2010.15032v3-Figure1-1.png": {
        "prediction": " Figure 1 shows the detailed architecture of our approach. It consists of three main components: the invocation sources, the invocation controller, and the scale controller. The invocation sources are responsible for receiving and distributing invocations. The invocation controller is responsible for creating and removing function instances, and for distributing invocations to them. The scale controller is responsible for monitoring the system and adjusting the number of function instances based on the load.",
        "reference": "Figure 1: Abstract FaaS architecture."
    },
    "2010.09258v1-Figure3-1.png": {
        "prediction": " The figure shows the architecture of a federated learning system. The system consists of a parameter server, a data manager, and multiple sub-models. The parameter server is responsible for storing the global model and updating it with the gradients received from the sub-models. The data manager is responsible for partitioning the data and allocating it to the sub-models. The sub-models are responsible for training on their respective data partitions and sending the gradients to the parameter server. The process of training a federated learning model involves the following steps:\n\n1. The data manager partitions the data and allocates it to the sub-models.\n2. The sub-models train on their respective data partitions and send the gradients to the parameter server.\n3. The parameter server updates the global model with the gradients received from the sub-models.\n4. The updated global model is sent back to the sub-models.\n5. The sub-models continue training on their respective data partitions using the updated global model.\n6. The process repeats until the model converges.",
        "reference": "FIGURE 3 The architecture and data flow of distributed machine learning"
    },
    "1090603-Figure3-1.png": {
        "prediction": " The flowchart describes the training of a neural network. The training process starts with the collection of a dataset. Then, the dataset is split into two subsets: a substitute dataset and a labeling dataset. The substitute dataset is used to train a substitute neural network, while the labeling dataset is used to label the data. The substitute neural network is then used to generate synthetic data, which is added to the substitute dataset. This process is repeated until the substitute dataset is large enough. Finally, the substitute neural network is trained on the substitute dataset.",
        "reference": "Figure 3: Training of the substitute DNN F : the attacker (1) collects an initial substitute training set S0 and (2) selects an architecture F . Using oracle O\u0303, the attacker (3) labels S0 and (4) trains substitute F . After (5) Jacobian-based dataset augmentation, steps (3) through (5) are repeated for several substitute epochs \u03c1."
    },
    "2210.08554v1-Figure8-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns, while the indirect call relations are used to generate the indirect call patterns. The direct call patterns and indirect call patterns are then used to generate the temporal relations.\n\nThe architecture of BERT in pretraining is shown in Figure 7. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the input embedding layer. The embedded tokens are then passed through the encoders, which learn to extract contextual information from the sequence. The output of the encoders is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence.\n\nFigure 8 illustrates the approach for computing the textual similarity between images. Given two images, image A and image B, the scene text in each image is extracted. Then, candidate entities are generated for each image. Next, the visual features of each image are extracted. Finally, the textual similarity between the two images is computed by comparing the scene text and visual features of the images.",
        "reference": "Figure 8: Overview of Image Wikification (visual entity linking) method in KRAMT. To recognize named visual entities in images, we use available methods such as CRAFT+CRNN, VGG-Face, and Landmark ArcFace for brands, celebrities, and landmarks respectively. Using these experts, we measure similarity against several thousands of reference entities to obtain a set of high ranking candidates. This open-set recognition approaches allow for addition or removal of any number of reference entities without a need to re-train."
    },
    "2107.06007v2-Figure13-1.png": {
        "prediction": " The image shows a process flow diagram of the helium recovery system. The process starts with the isolation valves being closed and the standpipe being open. The HFE storage is then pressurized to 15 psig and the HFE is allowed to flow into the ballast bottle. The isolation valves are then opened and the standpipe is closed. The HFE is then allowed to flow into the inner cryostat through the lead wall. The liquid helium is then evaporated and the gas is returned to the HFE storage through the gas line. The process is repeated until all of the liquid helium has been evaporated.",
        "reference": "Figure 13. A simplified schematic of the HFE system. The major system components are the storage dewar, ballast bottle, and inner cryostat vessel (IV)."
    },
    "2107.05307v2-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the functions that are directly called by each function, while the indirect call relations are the relations between the functions that are called by the functions that are called by each function. The indirect call patterns are the patterns of the indirect call relations. The figure shows an example of the direct call relations, indirect call relations, and indirect call patterns.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is trained to predict the masked words in a sentence, and to predict the next sentence in a sequence of sentences. The figure shows the different layers of the BERT model, including the input embedding layer, the transformer encoder layers, and the output layer.\n\nFigure 8 shows the architecture of a super-resolution network (SRNet). SRNet is a deep learning model that is used to upscale low-resolution images to high-resolution images. The model is composed of a feature extraction network (FNet) and a super-resolution network (SRNet). The FNet is used to extract features from the low-resolution image, and the SRNet is used to upscale the features to a high-resolution image.",
        "reference": "Fig. 1. Overview of EGVSR network."
    },
    "2103.07052v1-Figure4-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces (trace1 and trace2) with their respective direct call relations (C1 and C2), the indirect call relations between C1 and C2 are generated. The direct call patterns (C1, C2) and indirect call patterns (C1, C2) are then used to generate the temporal relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the input embedding layer. The embedded tokens are then passed through the encoders, which learn to attend to each other and to generate a representation of the input sequence. The final output of the model is a sequence of vectors, each of which represents a token in the input sequence.\n\nFigure 8 illustrates the two-stage training procedure of DocBERT. In the first stage, the model is trained on a known document to learn the parameters of the document encoder and the fusion layer. In the second stage, the model is trained on an unknown document to learn the parameters of the unknown document encoder. The two stages are iterated until the model converges.",
        "reference": "Figure 4: Network architecture of the DV-Projection method. Vectors \ud835\udc38\ud835\udc40\ud835\udc35, \ud835\udc3f\ud835\udc40 and \ud835\udc37\ud835\udc49 are represented using rounded rectangle shape. Fully connected layers are represented using trapezoid shape. Element-wise math operations are represented using circles."
    },
    "2107.03564v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. \n\nThe direct call relations and indirect call relations are extracted from the program code. The direct call relations are represented by the directed edges between the caller and callee functions, while the indirect call relations are represented by the dashed edges between the caller and callee functions. \n\nThe execution traces are extracted from the program code by traversing the call graph. Each execution trace is a sequence of function calls, representing a possible execution path of the program. \n\nThe temporal relations between the function calls are extracted from the execution traces. The temporal relations are represented by the arrows between the function calls, where the arrow indicates the order of execution.",
        "reference": "Figure 2: The overall architecture of ProxySR."
    },
    "2110.03090v1-Figure3-1.png": {
        "prediction": " The image shows the architecture of a Transformer model. The model consists of a stack of encoder and decoder layers. The encoder layers are responsible for converting the input sequence into a sequence of hidden states. The decoder layers are responsible for generating the output sequence from the hidden states.\n\nThe input to the model is a sequence of tokens. The tokens are first embedded into vectors by the embedding layer. The embedded tokens are then passed through the encoder layers. The encoder layers consist of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence. The feed-forward layer is a fully connected layer that is used to project the hidden states to a different dimension.\n\nThe output of the encoder layers is a sequence of hidden states. The hidden states are then passed through the decoder layers. The decoder layers consist of a self-attention layer, a feed-forward layer, and an output layer. The self-attention layer allows the model to attend to different parts of the hidden states. The feed-forward layer is a fully connected layer that is used to project the hidden states to a different dimension. The output layer is a fully connected layer that is used to generate the output sequence.",
        "reference": "Fig. 3: Network architecture for the player identification model. The networks accepts a player tracklet as input. Each tracklet image is passed through a ResNet18 to obtain time ordered features F . The features F are input into three 1D convolutional blocks, each consisting of a 1D convolutional layer, batch normalization, and ReLU activation. In this figure, k and s are the kernel size and stride of convolution operation. The activations obtained from the convolutions blocks are mean-pooled and passed through a fully connected layer and a softmax layer to output the probability distribution of jersey number pjn."
    },
    "2204.04370v1-Figure1-1.png": {
        "prediction": " The image shows a flowchart for a quantum music generation system. The system is composed of three main stages: preprocessing, quantum circuit, and beat construction. In the preprocessing stage, the input audio is filtered and features are extracted. In the quantum circuit stage, an encoding matrix is used to encode the features into a quantum state. The quantum circuit is then executed to generate a result. In the beat construction stage, the result is decoded and used to construct a beat. The beat is then stored in a database.",
        "reference": "Figure 1: QuiKo Architecture"
    },
    "2103.01209v3-Figure3-1.png": {
        "prediction": " Latent GAN: The generator takes a latent vector and produces an image. The discriminator takes an image and produces a probability that the image is real.\n\nStyleGAN: The generator takes a latent vector and produces an image. The discriminator takes an image and produces a probability that the image is real. The latent vector is used to control the style of the image.",
        "reference": "Figure 3. Model Overview. Left: The GANformer layer is composed of a bipartite attention operation to propagate information from the latents to the image grid, followed by convolution and upsampling. These are stacked multiple times starting from a 4\u00d74 grid and up to producing a final high-resolution image. Right: The latents and image features attend to each other to capture the scene structure. The GANformer\u2019s compositional latent space contrasts with the StyleGAN monolithic one (where a single latent modulates the whole scene uniformly)."
    },
    "2102.05956v1-Figure3-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations between the functions can be inferred. For example, if C1 calls C2 in Tr1, and C2 calls C3 in Tr2, then we can infer that C1 indirectly calls C3. The indirect call patterns can be further extracted from the indirect call relations. For example, if C1 indirectly calls C2 and C3, and C2 indirectly calls C4, then we can infer that there is an indirect call pattern (C1, C2, C4).\n\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn complex relationships between the input tokens. The output of the final encoder layer is a sequence of vectors, each of which represents a token in the input sequence. These vectors are then used to predict the next token in the sequence, as well as to predict whether the current token is the start or end of a sentence.\n\n\nFigure 8 illustrates how dropout works. Dropout is a technique that is used to prevent neural networks from overfitting. During training, dropout randomly drops out some of the neurons in the network, which forces the network to learn to rely on multiple features in the data rather than relying too heavily on a few features. This helps to prevent the network from becoming too specialized to the training data, and it also helps to improve the generalization performance of the network.",
        "reference": "Fig. 3: Monte Carlo dropout performed in fully-connected and convolutional neural networks. Keeping dropout during inference creates an implicit ensemble of models. As shown it requires running the same network with different dropout masks in order to provide uncertainty estimations."
    },
    "2205.10889v1-Figure1-1.png": {
        "prediction": " The flowchart shows how to use the trace data to generate the indirect call patterns. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations can be extracted. Then, for each indirect call relation, the indirect call patterns can be generated. Finally, the direct call patterns can be generated.",
        "reference": "Fig. 1: Overview of the proposed many-core wireless-enabled IMC platform. Orange encoders map to our wireless TX, while green IMCs map to our wireless-augmented IMCs. Bit-wise majority operation maps to the wireless OTA computation."
    },
    "2202.10590v1-Figure1-1.png": {
        "prediction": " The image shows an overview of the proposed caching and routing scheme. The system consists of a single root server, multiple edge caches, and a set of users. Each edge cache is connected to the root server through a dedicated link and to the users through wireless links. The root server stores a complete copy of the content library, while each edge cache stores a subset of the content library. When a user requests a content item, the request is first routed to the edge cache that is closest to the user. If the content item is available in the cache, it is served directly to the user. Otherwise, the request is forwarded to the root server, which then serves the content item to the user.",
        "reference": "Fig. 1. System Model. A network of J caches serves file requests from a set I of user locations. Unserved requests are routed to the Root Server. Caching decisions are aided via the recommendations provided by the rec-sys."
    },
    "17589422-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee in the calling context. The indirect call relations are the relations between the caller and the callee in the callee context. The indirect call patterns are the patterns of the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence. The model is trained to predict the next word in a sequence given the previous words in the sequence.\n\nFigure 8 shows the hierarchical structure of the CVMs. Each CVM is responsible for modeling the correlations between a pair of views. The green nodes represent the view-specific CVMs, which model the correlations between the views within each document. The red nodes represent the document-level CVMs, which model the correlations between the views across different documents. The gray nodes represent the corpus-level CVM, which models the correlations between the views across the entire corpus.",
        "reference": "Figure 2: Description of a parallel document-level compositional vector model (DOC). The model recursively computes semantic representations for each sentence of a document and then for the document itself, treating the sentence vectors as inputs for a second CVM."
    },
    "2103.03875v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to learn temporal relations between API calls. Given two traces (denoted as Trace 1 and Trace 2), each containing a sequence of API calls (denoted as C1, C2, C3, C4, C5, C6), we can identify direct call relations (e.g., C1 calls C2) and indirect call relations (e.g., C1 calls C4, C4 calls C6). \n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the first encoder is a sequence of word embeddings, which are learned by a word embedding layer. The output of the last encoder is a sequence of hidden vectors, which are used to predict the masked tokens in the input sequence.\n\nFigure 8 shows how to use a frozen BERT model to perform QA. Given a question and a context, we first use the tokenizer to convert them into a sequence of tokens. We then use the BERT model to encode the sequence of tokens into a sequence of hidden vectors. Finally, we use a feed-forward layer to predict the answer to the question.",
        "reference": "Figure 1. Transfer CNN model in sandwich-shape encoding, Ls and Le are tunable parameters to determine the boundary of trainable layers"
    },
    "2012.14111v1-Figure6-1.png": {
        "prediction": " The image shows how a proxy server can be used to intercept HTTPS traffic. The proxy server is configured to listen on port 80 and 443, which are the standard ports for HTTP and HTTPS traffic. When a client sends a request to a website, the proxy server intercepts the request and sends it to the C-icap server. The C-icap server then sends the request to the web server. The web server sends the response back to the C-icap server, which then sends the response back to the client. Because the proxy server is intercepting all of the traffic, it can be used to monitor and filter the traffic.",
        "reference": "Figure 6: Logical Diagram showing how DLP solution is deployed within the organization"
    },
    "2204.05751v2-Figure2-1.png": {
        "prediction": " The flowchart shows how to generate execution traces and temporal relations from a given program. The input to the flowchart is a program, and the output is a set of execution traces and temporal relations. The flowchart first uses the direct call relations between functions to identify the potential execution traces. Then, it uses the indirect call patterns to further refine the execution traces. Finally, it generates the temporal relations between the functions in the execution traces.",
        "reference": "Figure 2: Case study of span detection. Sup-Span: train a span detector in the fully supervised manner on available data from all training episodes, and then directly use it for span detection. Sup-Span-f.t.: further fine-tune the model learned by Sup-Span as in the proposed approach."
    },
    "2107.04367v2-Figure4-1.png": {
        "prediction": " The image shows an overview of the proposed method. Given a layout, the corresponding spectral domain data is obtained via the discrete cosine transform (DCT). Then, a group of 2D convolutional layers is applied to extract discriminative features. The extracted features are fed into a group lasso layer for penalizing the similarity among different feature channels. Finally, the critical feature channels are selected for further processing.",
        "reference": "Fig. 4: Procedure of the proposed feature selection."
    },
    "2107.04367v2-Figure3-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call patterns and indirect call patterns can then be derived.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input features. The model is trained to predict the masked tokens in the input sequence.\n\nFigure 8 shows the architecture of Federated Averaging. The model is trained on a number of clients. Each client has its own local data and a local model. The local models are trained on the local data. The global model is updated by aggregating the local models. The local models are updated by downloading the global model.",
        "reference": "Fig. 3: Overview of the proposed LHD framework using heterogeneous federated learning with local adaptation."
    },
    "2107.04367v2-Figure5-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to learn temporal relations between API calls. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph where each node represents an API call and each edge represents a direct call from one API call to another. The indirect call relations are represented as a bipartite graph where one set of nodes represents API calls and the other set of nodes represents indirect relations between API calls. The indirect call patterns are represented as a set of triplets (C1, C2, P), where C1 and C2 are API calls and P is a path between C1 and C2 in the indirect call graph. Finally, the direct call patterns are represented as a set of pairs (C1, C2), where C1 and C2 are API calls and there is a direct call from C1 to C2.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the final Transformer encoder is a sequence of vectors, each of which represents the hidden state of the corresponding token in the input sequence. These hidden states are then used to predict the masked tokens in the input sequence.\n\nFigure 8 shows the architecture of a deep neural network for hotspot detection. The model consists of a convolutional layer, a max-pooling layer, and a fully connected layer. The input to the model is a 2D array of values, which represents the features of the hotspots. The output of the model is a binary value, which indicates whether or not the input array is a hotspot.",
        "reference": "Fig. 5: Neural network architecture example at the client."
    },
    "2010.06536v1-Figure5-1.png": {
        "prediction": " The flowchart describes the process of 3D building reconstruction from 2D images. The input is a 2D image of a building. The first step is to use an annotation tool to label the facade of the building. The second step is to use a facade parsing and rectification algorithm to extract the facade of the building from the image. The third step is to use an inverse procedural modeling algorithm to generate a 3D model of the building. The fourth step is to use a footprint extrusion algorithm to generate a 3D footprint of the building. The fifth step is to use a 3D mesh generation algorithm to generate a 3D mesh of the building. The sixth step is to use a 3D rendering algorithm to generate a 3D rendering of the building. The final step is to store the 3D model in a 3D repository.",
        "reference": "Figure 5. System diagram for the 3D models module."
    },
    "2107.02220v2-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the call relations. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values represent the probability of a function calling another function.\n\nThe direct call relations and indirect call relations are used to generate the temporal relations. The temporal relations are represented as a matrix, where the rows and columns represent the functions and the values represent the probability of a function being called before another function.",
        "reference": "Fig. 2: The pipeline of the proposed graph convolution based re-ranking (GCR) method."
    },
    "2103.10630v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations can be extracted. The direct call patterns and indirect call patterns can be further derived.\n\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by a word embedding layer. The output of the encoders is a sequence of vectors, which are then used to predict the next sentence and the masked tokens.\n\n\nFigure 3 shows the cryo-EM single-particle reconstruction procedure. A volume with an unknown orientation and offset is randomly projected to generate a CTF image. The CTF image is then propagated to the Fourier space to obtain the power spectrum. The final projection is obtained by applying the inverse Fourier transform to the power spectrum.",
        "reference": "Fig. 2. Illustration of the forward model used for the cryo-EM MBIR method. It involves a 3D projection at a fixed orientation (with appropriate offset for the center of rotation) followed by a propagation operator that depends on the contrast transfer function (CTF) of the system. The figure shows an example of the magnitude of the Fourier transform of a typical CTF, illustrating that the CTF typically zeros out several frequency components of the projection data. While this can pose challenges for typical pre-process and reconstruct approaches, we use this model in order to perform the reconstruction."
    },
    "2101.03561v3-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where each node represents a function and each edge represents a direct call relation between two functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the callers and the other set of nodes represents the callees. An edge between two nodes indicates an indirect call relation between the caller and the callee. The extracted direct call relations and indirect call relations are used to identify indirect call patterns. \n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The model is trained to predict masked tokens in the input sequence. The masked tokens are replaced with a special [MASK] token. The model learns to predict the masked tokens by attending to the other tokens in the sequence.\n\nFigure 8 shows the dependency graph of a mathematical proof. The graph has 12 nodes, each of which represents a proposition or a lemma. The nodes are connected by directed edges, which indicate the logical relationships between the propositions and lemmas. The root node of the graph is Proposition 12, which is the main proposition that is being proved. The other nodes are lemmas and propositions that are used to prove Proposition 12. The edges in the graph indicate the logical relationships between the propositions and lemmas. For example, the edge from Proposition 6 to Lemma 19 indicates that Lemma 19 is used to prove Proposition 6.",
        "reference": "Figure 1. Main Lemmas and Theorems diagram."
    },
    "2210.01191v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates the process of extracting temporal relations between events from execution traces. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the events that are directly called by each other. The indirect call relations are the relations between the events that are called by the same event. The indirect call patterns are then extracted from the indirect call relations. The indirect call patterns are the patterns of the indirect call relations. Finally, the direct call patterns are extracted from the direct call relations. The direct call patterns are the patterns of the direct call relations.\n\nThe image in Figure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is trained to predict the masked words in a sentence. The input to the model is a sequence of tokens, and the output is a sequence of probabilities over the vocabulary. The model is trained to maximize the likelihood of the correct words being predicted.\n\nThe image in Figure 8 shows the architecture of a MAC-X model. MAC-X is a neural network model that is used for video question answering. The model is composed of two parts: a video encoder and a question encoder. The video encoder encodes the video into a sequence of vectors. The question encoder encodes the question into a vector. The two vectors are then concatenated and passed through a multilayer perceptron to produce the answer.",
        "reference": "Figure 2: Overview of the proposed end-to-end architecture, centered around the MAC-X Network: On the left, the question (Q), visual frames (V ), dialogue transcript (T ), acoustic input (Ac) as well as correct (A1) and incorrect (A2) answers are shown for the binary task. Their features are encoded with LSTMs, before use in MAC-X or in final classification along with last memory mp. Two identical classifiers make the predictions y1, y2 which are then used to calculate the loss in equation (7)."
    },
    "2101.08621v1-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from a given program. In the first step, the program is executed twice to generate two execution traces, denoted as Trace 1 and Trace 2. Each trace is a sequence of function calls, where each function call is represented by a node in the graph. The nodes in the graph are labeled with the names of the functions, and the edges in the graph represent the order in which the functions are called.\n\nIn the second step, the direct call relations between the functions are extracted from the execution traces. A direct call relation exists between two functions if one function calls the other function directly. The direct call relations are represented by the solid edges in the graph.\n\nIn the third step, the indirect call relations between the functions are extracted from the execution traces. An indirect call relation exists between two functions if one function calls another function, which in turn calls the second function. The indirect call relations are represented by the dashed edges in the graph.\n\nIn the fourth step, the indirect call patterns are extracted from the indirect call relations. An indirect call pattern is a sequence of function calls that starts with one function and ends with another function. The indirect call patterns are represented by the boxes in the graph.\n\nIn the fifth step, the direct call patterns are extracted from the direct call relations. A direct call pattern is a sequence of function calls that starts with one function and ends with another function. The direct call patterns are represented by the boxes in the graph.",
        "reference": "Figure 3: Architecture of the entire system we implemented for the second experiment."
    },
    "2205.10635v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe architecture of BERT in pretraining is shown in Figure 7. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The embedded tokens are then passed through the encoders, which learn to attend to each other and to generate a representation of the input sequence. The final output of the model is a sequence of vectors, each of which represents a token in the input sequence.\n\nFigure 8 shows how layer-wise splitting and semantic splitting can be used to improve the response time and accuracy of a neural network. Layer-wise splitting involves splitting the network into multiple layers, each of which is responsible for a different part of the computation. This can improve the response time of the network by reducing the amount of computation that needs to be performed in each layer. Semantic splitting involves splitting the network into multiple parts, each of which is responsible for a different part of the input data. This can improve the accuracy of the network by allowing each part of the network to focus on a smaller part of the input data.",
        "reference": "Figure 1: Overview of layer and semantic splitting strategies"
    },
    "2103.03189v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee functions in the same trace. The indirect call relations are the relations between the caller and the callee functions in different traces. The indirect call patterns are the patterns of the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is trained to predict the masked words in a sentence. The input to the model is a sequence of tokens, and the output is a sequence of probabilities over the vocabulary. The model is trained to maximize the likelihood of the correct words being predicted.\n\nFigure 8 shows the experimental setup for measuring the transient OCT signal. The system consists of a slit lamp, a contact lens with a piezo transducer, a laser, a photodiode, and a computer. The laser is used to generate a pulsed beam of light that is directed to the eye through the contact lens. The piezo transducer is used to vibrate the contact lens, which causes the OCT signal to be modulated. The photodiode is used to detect the OCT signal, which is then digitized by the computer.",
        "reference": "Fig. 1. Schematic sketch of the experimental setup."
    },
    "2101.07327v1-Figure5-1.png": {
        "prediction": " An illustration of execution traces and temporal relations. Given two traces Trace 1 and Trace 2, the direct call relations and indirect call relations are extracted. The indirect call patterns are mined based on the indirect call relations. Finally, the direct call patterns are extracted based on the direct call relations.",
        "reference": "Fig. 5. The baseline architecture in a modern UVR system"
    },
    "2101.07327v1-Figure7-1.png": {
        "prediction": " Figure 1 shows the system architecture of OpenUVR. It consists of two major components: the host PC and the mobile user device. On the host PC, the VR application is responsible for rendering the VR scene and capturing user interactions. The OpenUVR Host API provides a communication interface between the VR application and the OpenUVR Content Capturing Module. The OpenUVR Content Capturing Module is responsible for capturing the VR content, which includes the scene geometry, textures, and lighting information. The captured content is then transmitted to the mobile user device over a wireless network. On the mobile user device, the OpenUVR MUD API provides a communication interface between the OpenUVR MUD Program and the OpenUVR Content Presentation/User Input Module. The OpenUVR MUD Program is responsible for managing the user\u2019s interaction with the VR content. The OpenUVR Content Presentation/User Input Module is responsible for presenting the VR content to the user and capturing user input. Both the host PC and the mobile user device have an OpenUVR Network Module, which is responsible for handling the network communication between the two devices.",
        "reference": "Fig. 7. The OpenUVR system architecture"
    },
    "2110.08043v1-Figure14-1.png": {
        "prediction": " The figure shows how the execution traces are generated. Given two traces, trace 1 and trace 2, which consist of direct call relations, indirect call relations, and indirect call patterns, the direct call relations and indirect call relations are extracted. Then, the direct call patterns are generated based on the direct call relations. Finally, the indirect call patterns are generated based on the indirect call relations.",
        "reference": "Figure 14: Mode I (left) and Mode I+II (right) for the study of the crack path under thermal expansion and the loading process. Here, the initial damage z\u2217(x) is illustrated by the red initial crack in the figures."
    },
    "1125974-Figure2-1.png": {
        "prediction": " The flowchart shows how to use the trace information to extract direct and indirect relations between API calls. Given two traces, the direct call relations and indirect call relations can be extracted. For indirect call relations, the indirect call patterns can be further extracted. In addition, the direct call patterns between the two traces can also be extracted.",
        "reference": "Figure 2: Model architecture of RNN."
    },
    "812092-Figure7-1.png": {
        "prediction": " The figure shows an overview of the Transformer model. \n\nThe Transformer model consists of an encoder and a decoder. \n\nThe encoder takes a sequence of input tokens and converts them into a sequence of hidden vectors. \n\nThe decoder takes the sequence of hidden vectors and converts them back into a sequence of output tokens. \n\nEach layer in the Transformer consists of a self-attention mechanism and a feed-forward network. \n\nThe self-attention mechanism allows each token in the sequence to attend to other tokens in the sequence. \n\nThe feed-forward network is a fully connected neural network that is applied to each token in the sequence. \n\nThe Transformer model is trained using a masked language modeling objective. \n\nDuring training, some of the tokens in the input sequence are masked, and the model is trained to predict the masked tokens. \n\nThis objective encourages the model to learn to attend to all of the tokens in the sequence, and not just the tokens that are immediately adjacent to each other.",
        "reference": "Fig. 7. Illustration of the decoder model. \u201c<bos>\u201d is a tag denoting the beginning of a sentence, and \u201c<eos>\u201d is the end of a sentence."
    },
    "2101.11032v5-Figure1-1.png": {
        "prediction": " The image shows a flowchart that illustrates how to use a machine learning model to predict the next word in a sentence. The flowchart has three main steps:\n\n1. **Load data:** The first step is to load the data that will be used to train the model. This data can be in the form of a text file, a CSV file, or a database.\n2. **Train model:** The next step is to train the model using the data that was loaded in the previous step. This involves using a machine learning algorithm to learn the patterns in the data and how to use those patterns to predict the next word in a sentence.\n3. **Use model:** The final step is to use the trained model to predict the next word in a sentence. This can be done by providing the model with a sentence and then using the model to predict the next word in the sentence.\n\nThe flowchart also shows how the model can be used to generate new text. This can be done by providing the model with a seed sentence and then using the model to generate new words that follow the seed sentence.",
        "reference": "FIG. 1. Top: A physicist with limited control of a gas in a box predicts and observes an irreversible (to her) increase of entropy in a spontaneous equilibration process. A cheeky demon with full knowledge of positions and momenta of all particles can reverse the process bringing the gas (and in principle her too) back to the original state. Bottom: Wigner\u2019s friend predicts and observes an irreversible (to her) collapse of the quantum state during a measurement process. A cheeky demon named Wigner with full knowledge of the global quantum state can reverse the process bringing system, measuring device and her back to the original state. In this note we highlight and discuss the analogies between these two setups and their significance for the debate surrounding the original Wigner\u2019s friend gedankenexperiment."
    },
    "2110.13367v1-Figure4-1.png": {
        "prediction": " The image shows the flowchart of the proposed method. The first step is to load the data, which consists of traces and their corresponding labels. The data is then split into training and testing sets. The next step is to train the model. The model is trained using a deep learning algorithm, such as a convolutional neural network (CNN). The model is trained to identify the patterns in the traces that correspond to the different labels. Once the model is trained, it can be used to classify new traces. The final step is to evaluate the model. The model is evaluated using a variety of metrics, such as accuracy, precision, and recall.",
        "reference": "Figure 4: Workflow of the detection (in the third step, red is the output of model and yellow cube is the final output of the method)"
    },
    "2011.06192v3-Figure5-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, where Tr1 =<C1, C2, C3, C4, C5, C6> and Tr2 =<C1, C2, C5>, we can derive indirect relations between functions as follows: C4->C3, C6->C5. In addition, we can also derive indirect call patterns between functions, such as (C1, C2), (C1, C4), (C2, C3), (C2, C5), (C2, C6). \n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input features. The output of the final Transformer encoder is a sequence of vectors, where each vector represents the learned representation of the corresponding input token.\n\nFigure 8 shows the reinforcement learning approach for sample-efficient imitation learning with a single master and multiple slaves. (a) In general imitation learning, the master directly controls the slave to perform a task. The delays from the master's command to the slave's response are estimated and used to train the AI controller. (b) In bilateral control-based imitation learning, the master first demonstrates a task, and then the slave autonomously follows the master's demonstration. The delays from the master's demonstration to the slave's response are estimated and used to train the AI controller.",
        "reference": "Fig. 5. Overview of general IL and our bilateral control-based IL. In general, the delays caused during the demonstration and autonomous operation are different. Therefore, a general IL can realize only slow motion, which can ignore delays. In the bilateral control-based IL, the delays caused during the demonstration and autonomous operation are the same. Thus, in our bilateral control-based IL, fast motion with delays can be achieved."
    },
    "2107.07277v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the call graph. Given two traces Tr1 and Tr2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a set of tuples (C1, C2), where C1 and C2 are the caller and callee functions, respectively. The indirect call relations are represented as a set of tuples (C1, C2), where C1 and C2 are the caller and callee functions, respectively, and there is a path from C1 to C2 in the call graph. The indirect call patterns are then extracted from the indirect call relations. An indirect call pattern is a sequence of function calls, where each function call is represented by a tuple (C1, C2).",
        "reference": "Figure 1: Electric circuit representing the averaged model of a DC/DC buck converter connected to the microgrid."
    },
    "2011.08946v1-Figure8-1.png": {
        "prediction": " The flowchart\u4e00\u958b\u59cb\u662f\u5b9a\u7fa9\u4e86\u8f38\u5165\u53c3\u6578\uff0c\u5305\u62ec\u4e86\u76ee\u6a19\u50b3\u64ad\u7387\u3001\u76ee\u6a19\u7528\u6236\u898f\u6a21\u3001\u4ee5\u53ca\u96a8\u6a5f\u9078\u64c7\u7684\u521d\u59cb\u7bc0\u9ede\u6578\u91cf\u3002\u96a8\u5f8c\uff0c\u6839\u64da\u8f38\u5165\u53c3\u6578\uff0c\u5229\u7528\u64f4\u6563\u6a21\u578b\u9032\u884c\u64f4\u6563\uff0c\u5f97\u5230\u64f4\u6563\u5f8c\u7684\u7bc0\u9ede\u72c0\u614b\u3002\u7136\u5f8c\u6839\u64da\u7bc0\u9ede\u7684\u72c0\u614b\uff0c\u5229\u7528PageRank\u6216HITS\u7b49\u6f14\u7b97\u6cd5\u5c0d\u7bc0\u9ede\u9032\u884c\u6392\u5e8f\uff0c\u5f97\u5230\u6392\u5e8f\u5f8c\u7684\u7bc0\u9ede\u5217\u8868\u3002\u6700\u5f8c\uff0c\u5f9e\u6392\u5e8f\u5f8c\u7684\u7bc0\u9ede\u5217\u8868\u4e2d\uff0c\u9078\u64c7\u524dK\u500b\u7bc0\u9ede\u4f5c\u70ba\u7a2e\u5b50\u7bc0\u9ede\u3002",
        "reference": "Figure 8: The framework illustration of Disparity Seeding."
    },
    "999120-Figure2-1.png": {
        "prediction": " The image shows an overview of the bottom-up part-based object detection framework. Given an input image, the baseline network is applied to extract the global feature map. Then, the part proposals are generated by applying the ROI pooling layer to the global feature map. For each part proposal, we crop the corresponding part image and feed it into the part network to extract the part feature. After that, the part loss is computed by comparing the part feature with the ground-truth part label. Finally, the global loss and the part loss are combined to obtain the total loss.",
        "reference": "Figure 2. Overview of part loss networks (PL-Net), which is composed of a baseline network and a part loss computation extension. \u201cGAP\u201d denotes the Global Average Pooling. Given an input image, we firstly extract its feature maps X , then compute the global loss and person part loss based on X . The person part loss is computed on K parts generated with an unsupervised method."
    },
    "2012.05825v3-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of traces, we first extract direct call relations and indirect call relations between functions. Then, we use the extracted relations to construct a call graph. Finally, we perform anomaly detection on the constructed call graph.\n\nFigure 7 illustrates the overall architecture of BERT. The input to BERT is a sequence of tokens, which are converted into vectors by the input embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input vectors. The output of the transformer layers is then passed through a final softmax layer, which produces a probability distribution over the vocabulary.\n\nFigure 8 shows how BERT can be used for text classification. The input to the classifier is a sequence of tokens, which are converted into vectors by the input embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input vectors. The output of the transformer layers is then passed through a final softmax layer, which produces a probability distribution over the classes.",
        "reference": "Figure 1. Novelty detection is challenging since X-rays of novel diseases are remarkably similar to known conditions. The unlabeled batch of inference-time data can be used to adapt a semi-supervised novelty detection approach to emerging novel diseases."
    },
    "200059-Figure4-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph where each node represents an API call and each edge represents a direct call from one API call to another. The indirect call relations are represented as a bipartite graph where one set of nodes represents API calls and the other set of nodes represents indirect call patterns. Each edge in the bipartite graph represents an indirect call relation between an API call and an indirect call pattern. \n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The output of the final encoder is a sequence of vectors, where each vector represents the learned representation of a token in the input sequence.\n\nFigure 8 shows an example of how BERT can be used for text classification. The input to the model is a sequence of tokens, which are converted into vectors using word embeddings. The vectors are then passed through the BERT encoders, which learn a representation of the text. The output of the final encoder is a sequence of vectors, where each vector represents the learned representation of a token in the input sequence. The vectors are then passed through a feed-forward layer to produce a classification score for each class.",
        "reference": "Figure 4. A genuine pair {x1,x2} that share the same identity but different pose is fed into the recognition network \u03b8r to obtain the rich embedding er1 and er2. By regularizing the self and cross reconstruction, er11 and er21, the identity and non-identity features are eventually disentangled to make the non-frontal peer ei2 to be similar to its near-frontal reference ei1."
    },
    "2207.10758v1-Figure7-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of traces, we first extract direct call relations and indirect call relations between functions. Then, we use these relations to construct a call graph. Finally, we use the call graph to identify indirect call patterns.\n\nFigure 7 shows the overall architecture of BERT. \n\nFigure 8 shows an illustration of our approach.",
        "reference": "Fig. 7: Overview of Theorem 1 (Example 13.2 from [30]), which links the planarity and projective transformations in the continuous world."
    },
    "2202.04333v1-Figure7-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations can be extracted from a program. The process starts with two traces, trace 1 and trace 2, which are then used to identify direct call relations and indirect call relations. The direct call relations are then used to identify indirect call patterns. Finally, the direct call patterns and the indirect call patterns are used to generate the temporal relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the input embedding layer. The embedded tokens are then passed through the stack of encoders, which learns to extract contextual representations of the tokens. The final output of the model is a sequence of vectors, each of which represents the contextual representation of a token in the input sequence.\n\nThe flowchart in Figure 8 shows the architecture of the real-time prediction server. The server receives a traffic request from a user, which includes the user's ID and the anchor ID. The server then uses the user ID to retrieve the user's browsing history and user demographics from the user database. The server also uses the anchor ID to retrieve the anchor's information from the anchor database. The user's browsing history, user demographics, and anchor's information are then used to generate a real-time prediction of the user's next action. The real-time prediction is then sent back to the user.",
        "reference": "Figure 7: Online live broadcast recommender system with proposed TWINS model that partly shares similar idea with [20]. The new system lightens the TWINSmodel, and builds tree structures for user browsing histories and anchor histories in a offline fashion to save computation and latency costs for online serving."
    },
    "1358694-Figure1-1.png": {
        "prediction": " The image shows an overview of the proposed approach. Given an input image pair \\(I_{in}\\), the geometric predictor first estimates the homography \\(H\\) between the two images. Then, the estimated homography is used to warp the input image \\(I_{in}\\) to the reference image \\(I_{out}\\).",
        "reference": "Figure 1: Network module of Spatial Transformers [7]. The blue arrows indicate information passing of appearance, and the purple one indicate that of geometry. The yellow 3D trapezoid denotes the geometric predictor, which contains the learnable parameters."
    },
    "2202.06670v2-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of traces, we first extract the direct call relations and indirect call relations from each trace. Then, we use the direct call relations to construct a direct call graph, and use the indirect call relations to construct an indirect call graph. Finally, we use the direct call graph and the indirect call graph to identify the indirect call patterns.\n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of Transformer encoders, with each encoder consisting of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the final encoder is a sequence of vectors, which are then used to predict the next word in the sequence.\n\nFigure 8 shows the three different methods for self-supervised learning, auxiliary information, and our method. Self-supervision is a method that uses unlabeled data to learn a model. In this case, we use image augmentation to generate different views of the same image, and then use these views to train the model. Auxiliary information is a method that uses additional information to help the model learn. In this case, we use the attributes of the bird to help the model learn. Our method is a contrastive learning method that uses two views of the same image to learn a model. In this case, we use the contrastive loss to learn a model that can distinguish between the two views.",
        "reference": "Figure 1: Left: Self-supervision. Self-supervised learning (SSL) uses self-supervision (the supervision from the data itself) for learning representations. An example of self-supervision is the augmented variant of the original data. Middle: Auxiliary Information. This paper aims to leverage auxiliary information of data for weakly-supervised representation learning. We consider data attributes (e.g., binary indicators of attributes) as auxiliary information. Right: Our Weakly-supervised Contrastive Learning Method. We first construct data clusters according to auxiliary information. We argue the formed clusters can provide valuable structural information of data for learning better representations. Second, we present a contrastive learning approach - the clustering InfoNCE (Cl-InfoNCE) objective to leverage the constructed clusters."
    },
    "2012.14294v1-Figure1-1.png": {
        "prediction": " In Figure 1, we show how execution traces are generated and how temporal relations are extracted. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee methods, while the indirect call relations are the relations between the methods that are called by the same caller method. The indirect call patterns are the patterns of indirect call relations. For example, in Figure 1, there are two indirect call patterns: C1->C4->C6 and C2->C4->C6.",
        "reference": "Fig. 1. The proposed I-Health system architecture."
    },
    "2110.06875v2-Figure3-1.png": {
        "prediction": " The image shows how the TransformerXL model uses relative positional embeddings to model long-range dependencies. On the left, the TransformerXL model is applied to a sequence of length 6, and on the right, the self-attention mechanism is applied to the same sequence with relative positional embeddings. The green line indicates the attention between tokens, and the diamond indicates the relative positional embedding.",
        "reference": "Fig. 3. Illustration of the possible steps performed during the iteration by HM-Improve. The edges of the current sub-allocation Y are depicted using bold, green lines, while edges of the envy graph are shown by dashed, red lines. Source and sink vertices of Y are depicted with a white black diamond, respectively. Vertices of R as well as all edges incident to them are shown in grey."
    },
    "210911552-Figure3-1.png": {
        "prediction": " The image shows an encoder-decoder model for image captioning. The encoder consists of a convolutional neural network (CNN) that encodes the visual content of the image, and a long short-term memory (LSTM) network that encodes the textual content of the caption. The decoder is an LSTM network that generates the output caption.",
        "reference": "Figure 3. Multi-Cue Bayesian Moderator Network. We first use a Bayesian CNN/LSTM to obtain the embeddings gi, gp, gc, gt and then fuse those using the Fusion Module to get \u00b5p, \u00b5c, \u00b5t. These embeddings are then passed to the Moderator network. These are then fed to the decoder to get the questions for each image."
    },
    "2205.03371v1-Figure5-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct execution traces and derive temporal relations from them. Given two traces Tr1 and Tr2 of API calls, we can extract the direct call relations and indirect call relations, denoted as C1 and C2, respectively. C1 is a set of pairs (c1, c2), where c1 and c2 are API calls and c1 directly calls c2 in at least one execution trace. C2 is a set of pairs (c1, c2), where c1 and c2 are API calls and there exists a sequence of API calls c1, c3, . . . , cn, c2 such that c1 calls c3, c3 calls c4, . . . , cn\u22121 calls cn, and cn calls c2 in at least one execution trace. We can also derive indirect call patterns from C2. An indirect call pattern is a set of API calls that are frequently called together. For example, in Figure 1, we have two indirect call patterns {c1, c4} and {c2, c6}. The former means that c1 and c4 are frequently called together, and the latter means that c2 and c6 are frequently called together.\n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The output of the final Transformer encoder is fed to a softmax layer to predict the next token in the sequence.\n\nFigure 8 shows how BERT can be used for text classification. The input to BERT is a sequence of tokens, and the output is a vector of probabilities over the different classes. The vector of probabilities is then used to predict the class of the input sequence.",
        "reference": "Fig. 5: Illustration on the instance representation and the generation of bag probability distribution."
    },
    "1189091-Figure3-1.png": {
        "prediction": " The image shows an overview of the Faster R-CNN object detection framework. The framework consists of two main components: a region proposal network (RPN) and a Fast R-CNN detector.\n\nThe RPN takes an image as input and outputs a set of rectangular object proposals. Each proposal is a bounding box that is likely to contain an object. The RPN is a small convolutional neural network that is trained to predict the objectness score and the bounding box coordinates for each location in the image.\n\nThe Fast R-CNN detector takes the object proposals from the RPN and classifies each proposal into one of the foreground classes (e.g., person, car, bicycle) or the background class. The Fast R-CNN detector is a convolutional neural network that is trained to extract features from the image and to classify each proposal.",
        "reference": "Figure 3: Our framework consists of two stages: a region detection network and a localization and captioning network."
    },
    "2101.04989v1-Figure2-1.png": {
        "prediction": " Figure 2: (A) H&E image of a breast cancer sample with regions 1, 2, and 3 marked. (B) Corresponding regions in the segmented image.",
        "reference": "Fig. 2. Steps in processing esophageal biopsy images to produce patches. (A) A typical image of a hemotoxylin and eosin (H&E)-stained esophageal biopsy section obtained from an individual with active EoE. The image was taken at 80X magnification. (B) The same image after background removal with an illustration of tissue coverage criteria per patch size to meet the threshold for inclusion in training or validation sets. Box 1 (red): patch of 224X224 pixels with less than 10% tissue coverage. Box 2 (yellow): patch of 224X224 pixels with greater than 10% tissue coverage. Box 3 (red): patch of 448X448 pixels with less than 10% tissue coverage. Box 4 (yellow): patch of 448X448 pixels with greater than 10% tissue coverage."
    },
    "2011.05643v1-Figure2-1.png": {
        "prediction": " In Figure 1, we illustrate the call relations and execution traces. Given two traces Tr1 and Tr2 of API calls, we aim to learn the relations between the APIs. For each API call, we extract thecallee and caller functions, and construct a node for each unique API. We connect the caller and callee functions with a directed edge, and label the edge with the type of the relation (direct or indirect). For example, in Figure 1, there is a direct call relation from C1 to C2, and an indirect call relation from C1 to C4. We denote the set of direct call relations as CD, and the set of indirect call relations as CI. \n\nIn Figure 7, we show the overall architecture of BERT. The input to BERT is a sequence of tokens, which are embedded into vectors using word embeddings, positional embeddings, and segment embeddings. The embedded tokens are then fed into a stack of transformer encoders. Each transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The output of the transformer encoders is a sequence of vectors, which are then fed into a softmax layer to produce a probability distribution over the vocabulary.\n\nIn Figure 8, we show the detailed architecture of a transformer encoder. The transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer is composed of three multi-head attention layers. Each multi-head attention layer consists of a number of scaled dot-product attention heads. The scaled dot-product attention head computes a weighted sum of the values in the query vector, where the weights are determined by the similarity between the query vector and the key vectors. The output of the multi-head attention layer is a vector that contains information from all of the input vectors. The feed-forward layer is a two-layer fully connected neural network. The output of the feed-forward layer is a vector that contains the final representation of the input vectors.",
        "reference": "Figure 2: Block level schematic diagram of an ASIC channel [8]."
    },
    "2012.06186v1-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the functions that are directly called from one function to another. The indirect call relations are the relations between the functions that are called through a chain of other functions. The indirect call patterns are then generated based on the indirect call relations. The indirect call patterns are the sequences of functions that are called from one function to another. The direct call patterns are the sequences of functions that are directly called from one function to another.",
        "reference": "Fig. 3: CNN architecture composed of the ResNet-20 followed by the NetVLAD layer. Numbers in each rectangle denote kernel size, number of output filters, padding, and size of stride, respectively. The N -way fully connected is dropped and instead the 1\u00d7 1\u00d7 64 output vector is passed to the NetVLAD layer."
    },
    "113704-Figure2-1.png": {
        "prediction": " The image shows an overview of the training procedure of the proposed approach. Given a sentence s, the predictor P first retrieves the most relevant WEs from the knowledge base and projects them into the same space as the sentence embedding. Then, the concatenated sentence and WEs are fed into a multi-layer perceptron (MLP) to predict a relevance score rs. Meanwhile, the judge J takes the sentence and the predicted rs as input and outputs a reward rpos if the prediction is correct, or rneg otherwise. The reward is then used to update the parameters of P and J.",
        "reference": "Figure 2: DAN Architecture for Text Classification"
    },
    "2102.07774v2-Figure4-1.png": {
        "prediction": " The image shows how to use traces to learn temporal relations between events. Given two traces, trace 1 and trace 2, we can extract the direct call relations and indirect call relations. The direct call relations are the relations between events that are directly connected in the traces. The indirect call relations are the relations between events that are not directly connected in the traces, but are connected through other events. We can then use these relations to learn a model that can predict the temporal relations between events.",
        "reference": "Figure 4. An illustration of the approach to the limit point Clim, where CA and CB are infinitesimally close to the limiting point."
    },
    "2206.01335v2-Figure1-1.png": {
        "prediction": " The image shows an overview of the training procedure of a large language model (LLM). The training data consists of a huge code corpus. First, instances are extracted from the source code and documentation. Second, prompts are generated for the extracted instances. Third, the prompts are used to fine-tune a pre-trained language model. Finally, the model is post-processed to generate code.",
        "reference": "Figure 1: Overview of a general framework for generating code analysis tools using few-shot, pre-trained language models."
    },
    "2210.01384v1-Figure3-1.png": {
        "prediction": " The figure shows the overall architecture of EDNAS, which consists of three major components: 1) a search space of edge-friendly neural architectures; 2) a hardware-aware multi-task training framework; and 3) a joint absolute-relative depth loss function. Given the search space, the search algorithm samples and evaluates different edge-efficient candidate architectures. The hardware-aware multi-task training framework then trains the sampled architectures on multiple tasks and obtains the corresponding multi-task accuracy and latency. The joint absolute-relative depth loss function is used to guide the training process to find the optimal edge architecture that achieves high accuracy under latency constraints.",
        "reference": "Figure 3: A system-level overview of our proposed methods. We leverage multi-objective, hardware-aware neural architecture search to discover optimal neural components suitable for multi-task dense predictions, while simultaneously ensuring efficient edge inference."
    },
    "2010.06791v3-Figure4-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. The left part shows two execution traces. Each trace consists of a sequence of API calls, where each API call is represented by a node. The right part shows the direct and indirect relations between the API calls. The direct relations are represented by solid arrows, and the indirect relations are represented by dashed arrows.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence.\n\nFigure 8 shows the architecture of a neural network-based GN decoding (NND) receiver. The receiver consists of two main components: a CSI estimator and an optimal GN decoder. The CSI estimator uses the received signal to estimate the channel state information (CSI). The optimal GN decoder uses the estimated CSI to decode the received message.",
        "reference": "Fig. 4. Comparison of the conventional decoder and the optimal GNNDR, for channels with imperfect receiver CSI. The left block diagram is part of the channel model in Figure 1. The upper right block diagram illustrates the conventional approach for realizing the decoder, where the imperfect receiver CSI is utilized to estimate the true channel state, and the estimated channel state is fed into the NNDR for decoding; the lower right block diagram illustrates the optimal GNNDR, which directly obtains the output processing function and the codeword scaling function, without estimating the channel state. We remark that channel estimation is still a critical module in practical wireless communication systems, and that the optimal GNNDR structure bypassing the channel estimation step is only valid for the specific information transmission system model in this paper."
    },
    "2101.03244v5-Figure2-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of traces, we first extract the direct call relations and indirect call relations from them. Then, we use the direct call relations to construct a direct call graph, and use the indirect call relations to construct an indirect call graph. Finally, we use the direct call graph and the indirect call graph to identify the indirect call patterns.\n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of transformer encoders, with each encoder consisting of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the final encoder is a sequence of vectors, which are then used to predict the next token in the sequence.\n\nFigure 8 shows the detailed architecture of the transformer encoder. The self-attention layer consists of a number of attention heads, each of which attends to a different part of the input sequence. The feed-forward layer consists of two linear layers with a ReLU activation function.",
        "reference": "Fig. 2. Proposed end-to-end framework for computing voxel-level detections of csPCa in validation/test samples of prostate bpMRI. The model center-crops two ROIs from the multi-channel concatenation of the patient\u2019s T2W, DWI and ADC scans for the input of its detection and classification 3D CNN sub-models (M1, M2). M1 leverages an anatomical prior P in its input x1 to synthesize spatial priori and generate a preliminary detection y1. M2 infers on a set of overlapping patches x2 and maps them to a set of probabilistic malignancy scores y2. Decision fusion node NDF aggregates y1, y2 to produce the model output yDF in the form of a post-processed csPCa detection map with high sensitivity and reduced false positives."
    },
    "2012.06168v4-Figure4-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a graph where each node represents an API call and an edge between two nodes indicates that one API call directly calls the other. The indirect call relations are represented as a matrix, where each entry indicates the probability that one API call indirectly calls the other. In addition, indirect call patterns are extracted, which capture the patterns of indirect call relations between API calls. \n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that predicts the next word in the sequence. The model is trained to predict the next word in a masked sequence, where some of the words in the sequence are replaced with a special [MASK] token.\n\nFigure 8 shows the architecture of a reinforcement learning agent that is used to play the game of Go. The agent consists of two convolutional neural networks (CNNs) that are used to extract features from the game board, and a fully connected network (FCN) that is used to predict the next move. The agent is trained using a policy gradient algorithm, which is a type of reinforcement learning algorithm that maximizes the expected reward of the agent.",
        "reference": "Fig. 4. End-to-end learning architecture of our deep RL based AI ARL."
    },
    "2204.10485v1-Figure2-1.png": {
        "prediction": " In Figure 1, we illustrate how execution traces are generated and how temporal relations are extracted. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations can be extracted. The direct call relations are the pairs of APIs that are directly called one after another in the traces. The indirect call relations are the pairs of APIs that are not directly called one after another in the traces, but there is a path between them in the call graph. \n\nIn Figure 7, we show the overall architecture of BERT. BERT consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The output of the final Transformer encoder is a sequence of vectors, where each vector represents the hidden state of the corresponding input token. These hidden states can then be used to perform various tasks, such as text classification, question answering, and machine translation.\n\nIn Figure 8, we show the detailed architecture of the Transformer encoder. The Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output.",
        "reference": "Figure 2. Overview of AHIQ. The proposed model takes a pair of the reference image and distortion image as input and then obtains feature maps through ViT [11] and CNN, respectively. The feature maps of reference image from ViT are used as global information to obtain the offset map of the deformable convolution [8]. After the feature fusion module which fuses the feature maps, we use a patch-wise prediction module to predict a score for each image patch. The final output is the weighted sum of the scores."
    },
    "2102.08921v2-Figure2-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct the execution traces and derive temporal relations from them. Given two traces Tr1 and Tr2 of API calls, we first identify the direct caller-callee relations between API calls. For example, in Tr1, C1 directly calls C2, and C2 directly calls C3. We then identify the indirect caller-callee relations between API calls. For example, in Tr1, although C1 does not directly call C4, C4 is indirectly called by C1 through C2. We identify such indirect relations by constructing the call graph based on the direct caller-callee relations. For each pair of API calls (C1, C2), we construct three sets: C1_Callees, C2_Callers, and C1_Callees \u2229 C2_Callers. If C1_Callees \u2229 C2_Callers is not empty, then there is an indirect relation from C1 to C2. For example, in Figure 1, C1_Callees = {C2, C3, C4, C5, C6}, C2_Callers = {C1, C3}, and C1_Callees \u2229 C2_Callers = {C3}, so there is an indirect relation from C1 to C2. We repeat this process for all pairs of API calls to identify all the indirect caller-callee relations.\n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence and capture long-range dependencies. The feed-forward layer adds non-linearity to the model. The output of the final Transformer encoder is fed to a softmax layer to predict the next token in the sequence.\n\nFigure 8 shows the evaluation pipeline for the generated samples. (a) Given a synthetic dataset Dsynth and a real dataset Dreal, we first use the evaluation embedding \u03a6 to embed the samples from both datasets into a common embedding space. Then, we compute the \u03b1-precision, authenticity, and \u03b2-recall between the embedded synthetic and real samples. The \u03b1-precision measures the fraction of synthetic samples that are classified as real. The authenticity measures the fraction of real samples that are classified as real. The \u03b2-recall measures the fraction of real samples that are retrieved by the synthetic samples. We use these metrics to assess the quality of the generated samples. (b) We use a generative model Pg to generate synthetic samples. Then, we use the evaluation pipeline to evaluate the generated samples. If the generated samples are of high quality, we accept them. Otherwise, we reject them.",
        "reference": "Figure 2. Illustration for the evaluation and auditing pipelines."
    },
    "2010.11991v2-Figure8-1.png": {
        "prediction": " ## Figure 1: Illustration of execution traces and temporal relations.\n\nThe figure illustrates how execution traces and temporal relations are generated. \nOn the left-hand side, the execution traces are shown. Each trace consists of a sequence of calls to the functions `C1`, `C2`, `C3`, `C4`, `C5`, and `C6`. The arrows between the calls indicate the order in which the functions are called. For example, the arrow from `C1` to `C2` indicates that `C2` is called after `C1`.\n\nOn the right-hand side, the temporal relations between the functions are shown. The relations are represented by a matrix, where each row corresponds to a function and each column corresponds to another function. The entry at the intersection of row `i` and column `j` indicates the temporal relation between functions `i` and `j`. For example, the entry at the intersection of row `C1` and column `C2` indicates the temporal relation between functions `C1` and `C2`. The temporal relations are determined by the arrows in the execution traces. For example, the arrow from `C1` to `C2` in the execution traces indicates that `C2` is called after `C1`, which is represented by the entry `C2 -> C1` in the temporal relations matrix.\n\n\n## Figure 7: Pretraining BERT.\n\nThe figure shows the architecture of BERT in pretraining. \nThe model consists of a stack of Transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The output of the final encoder is a sequence of vectors, where each vector represents the learned representation of a token in the input sequence.\n\n## Figure 7: Point Cloud Processing Pipeline.\n\nThe figure shows a point cloud processing pipeline. \nThe pipeline consists of three main stages: point cloud preprocessing, point cloud feature extraction, and point cloud segmentation. In the preprocessing stage, the point cloud is filtered to remove noise and outliers. In the feature extraction stage, features are extracted from the point cloud, such as the point coordinates, normals, and colors. In the segmentation stage, the point cloud is segmented into different objects.",
        "reference": "Fig. 8. Schematic of the point cloud aggregation pipeline. The input data are filtered from redundant points, downsampled to reduce the computational complexity. At the same time, the current agent\u2019s position, and the position for the previous scan is taken from history. The PointCloudExtrapolator splits the entire scan into the smaller batches, and for every batch is calculates linearly interpolated transformation that is proportional to the when the point has been scanned. Finally, all the bathes are aggregated in the PointCloudAggregator"
    },
    "2204.05928v2-Figure3-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct execution traces and extract temporal relations. Given two traces, trace t1 and trace t2, we first identify the direct call relations between the functions in each trace. A direct call relation (C1, C2) indicates that function C1 directly calls function C2. We then identify the indirect call relations between the functions in the two traces. An indirect call relation (C1, C4) indicates that function C1 calls function C2, and function C2 calls function C4. Finally, we use the direct and indirect call relations to construct the temporal relations between the functions in the two traces. A temporal relation (C1, C2) indicates that function C1 happens before function C2.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of transformer encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the meaning of the corresponding token in the input sequence.\n\nFigure 8 shows the architecture of a neural network model for natural language understanding (NLU). The model consists of three components: a domain classifier, an intent classifier, and a slot tagger. The domain classifier predicts the domain of the input sentence, the intent classifier predicts the intent of the input sentence, and the slot tagger predicts the values of the slots in the input sentence. The model is trained on a dataset of annotated sentences, and it can be used to classify new sentences into domains, intents, and slots.",
        "reference": "Figure 3: Proposed action prediction in DDPT using a transformer decoder. In every decoding step, a token embedding for domain, intent or slot informs the model what needs to be predicted and the previous output is fed into the decoder. In case of domain prediction, we propose a domain gate that decides whether to choose a domain that the user currently talks about."
    },
    "2110.02775v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the call graph. Given two traces Tr1 and Tr2, the direct call relations and indirect call relations are extracted. The direct call relations are represented by the directed edges between the caller and callee functions in the call graph. The indirect call relations are represented by the dashed edges between the caller and callee functions in the call graph. The indirect call patterns are represented by the boxes in the rightmost part of the figure. Each box represents a unique indirect call pattern. The numbers inside the boxes represent the number of occurrences of the corresponding indirect call pattern in the traces.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence. The feed-forward layer is a fully connected layer that is used to transform the output of the self-attention layer. The model is trained to predict the masked tokens in the input sequence. The output of the model is a vector of probabilities over the vocabulary.\n\nThe flowchart in Figure 8 illustrates the training procedure of BERT. The model is trained on a large corpus of text. The training data is divided into batches. Each batch consists of a number of text sequences. The model is trained to predict the masked tokens in each sequence in the batch. The loss function is calculated as the cross-entropy loss between the predicted probabilities and the true labels. The model is updated using the Adam optimizer.",
        "reference": "Figure 1: An example of a network for the MONK-2 dataset. xi are the inputs, y is the output. The red and blue rectangles represent the plot of functions, with input range on the x-axis and output on the y-axis. The green rectangles contain the aggregation function. The numbers in bold represent the thresholds for the step functions."
    },
    "2010.15764v1-Figure4-1.png": {
        "prediction": " The image shows a flowchart that illustrates how to use the trace information to extract direct and indirect relations between API calls. Given two traces, the first step is to identify the common API calls between the two traces. This is done by comparing the API calls in the two traces and finding the ones that are present in both traces. The next step is to identify the direct relations between the common API calls. This is done by finding the API calls in the first trace that are followed by an API call in the second trace. The third step is to identify the indirect relations between the common API calls. This is done by finding the API calls in the first trace that are followed by an API call in the second trace, and then finding the API calls in the second trace that are followed by an API call in the first trace. The fourth step is to group the direct and indirect relations into indirect call patterns. This is done by grouping the relations based on the API calls that are involved in the relations. The fifth step is to use the indirect call patterns to identify the API calls that are related to each other. This is done by finding the API calls that are involved in the same indirect call pattern.",
        "reference": "Figure 4: The causal diagram for mixed-causal-anticausal domain adaptation."
    },
    "2202.09450v1-Figure38-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of traces, we first extract the direct call relations and indirect call relations between functions. Then, we use these relations to construct a call graph. Finally, we use the call graph to identify the indirect call patterns.\n\nFigure 7 shows the overall architecture of BERT. \n\nFigure 8 shows the information bottleneck framework for learning disentangled representations.",
        "reference": "Fig. 38. Illustration of AttGAN extension for attribute style manipulation. Courtesy of [52]"
    },
    "2202.09450v1-Figure59-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given a program with multiple execution traces, the direct call relations and indirect call relations are extracted. Then, the indirect call patterns are extracted from the indirect call relations. Finally, the direct call patterns are extracted from the direct call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of Transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the Transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 shows the steps involved in training a neural network. The first step is to load the data, which is typically stored in a file. The data is then preprocessed, which may involve cleaning the data, removing outliers, and normalizing the data. The preprocessed data is then split into training and testing sets. The training set is used to train the neural network, and the testing set is used to evaluate the performance of the neural network. The neural network is trained by iteratively updating its weights until it learns to make accurate predictions on the training set. Once the neural network is trained, it can be used to make predictions on new data.",
        "reference": "Fig. 59. The overview of GeoNet Framework. It consists of rigid structure reconstructor for estimating static scene geometry and non-rigid motion localizer for capturing dynamic objects. Courtesy of [89]"
    },
    "2202.09450v1-Figure31-1.png": {
        "prediction": " In Figure 1, given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee functions that are directly called. The indirect call relations are the relations between the caller and the callee functions that are not directly called, but through other functions. The indirect call patterns are the patterns of the indirect call relations. For example, in the figure, there are two indirect call patterns: C1->C4->C6 and C2->C5->C6.",
        "reference": "Fig. 31. The high-level architecture of . Courtesy of [41]"
    },
    "2202.09450v1-Figure32-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations can be extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions in Trace 1, the other set of nodes represents the functions in Trace 2, and the edges represent the indirect call relations between the functions in Trace 1 and Trace 2. In addition, indirect call patterns can be extracted from the indirect call relations. An indirect call pattern is a sequence of functions that are called indirectly from one function to another. For example, in Figure 1, there are two indirect call patterns: C1->C4->C6 and C2->C5->C6.\n\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the encoder layers is then fed to a softmax layer to produce a probability distribution over the vocabulary.\n\nThe input to the model is a sequence of tokens, which are represented as vectors. The vectors are then passed through the encoder layers, which produce a sequence of hidden vectors. The hidden vectors are then passed through the softmax layer to produce a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct probability distribution.",
        "reference": "Fig. 32. CycleGAN model contains two mapping functions G : X \u2192 Y and F : Y \u2192 X , and associated adversarial discriminators DY and DX . DY encourages G to translate X into outputs indistinguishable from domain Y , and vice versa for DX , F, and X. To further regularize the mappings, they introduced two \u201ccycle consistency losses\u201d that capture the intuition that if they translate from one domain to the other and back again we should arrive where they started. Courtesy of [47]"
    },
    "2202.09450v1-Figure21-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a pair of images, we first use a pre-trained deep neural network (DNN) model to extract a set of keypoints from each image. We then use these keypoints to compute a dense correspondence map between the two images. Finally, we use this correspondence map to transfer the texture from the source image to the target image.\n\nThe DNN model that we use to extract keypoints is a variant of the Hourglass network. This network is specifically designed for human pose estimation, and it has been shown to achieve state-of-the-art results on this task. The network takes as input an image and outputs a set of keypoints, each of which corresponds to a specific location on the human body.\n\nOnce we have extracted the keypoints from each image, we use them to compute a dense correspondence map between the two images. This correspondence map specifies, for each pixel in the source image, the corresponding pixel in the target image. We compute the correspondence map using a technique called thin plate splines (TPS). TPS is a non-linear transformation that can be used to warp one image to another.\n\nOnce we have computed the correspondence map, we use it to transfer the texture from the source image to the target image. We do this by simply copying the pixel values from the source image to the target image, according to the correspondence map.\n\nThe result of our approach is a new image that has the texture of the source image and the shape of the target image. This image can be used for a variety of purposes, such as creating photorealistic composites or changing the appearance of a person in a video.",
        "reference": "Fig. 21. Overview of M3D-VTON architecture. Courtesy of [20]"
    },
    "2202.09450v1-Figure47-1.png": {
        "prediction": " The image shows an overview of the proposed gaze estimation approach. The image is processed by a backbone network to extract discriminative features. A depth estimation network estimates the depth of each pixel in the image. The estimated depth map is used to rebase the image features into a 3D space. A dual attention module is employed to capture the spatial and channel-wise attention. The output of the dual attention module is fed into a head pose estimation network to estimate the 3D gaze direction.",
        "reference": "Fig. 47. The Architecture of Dual Attention Guided Gaze Target Detection. Courtesy of [62]"
    },
    "2102.10313v1-Figure1-1.png": {
        "prediction": " The image shows a quadrotor in flight in an indoor environment. The quadrotor is equipped with a spray-painting system. The quadrotor is shown spraying a painting on a wall.",
        "reference": "Fig. 1. Left: Illustration of a typical planning run with all planners on the hilo scenario. Red is the proposed, green the DGEO, and brown the RRT*Con planner. Blue shades correspond to the RRT*-Sam, yellow shades to the RRT*-Pro, and pink to CHOMP. Right: Example of an OMAV inspecting a surface using the proposed planner."
    },
    "2207.10392v2-Figure3-1.png": {
        "prediction": " Figure 1 shows an overview of the proposed approach. Given a trace, we first extract the direct call relations and indirect call relations from the trace. Then, we construct the indirect call patterns from the indirect call relations. Finally, we use the direct call relations, indirect call relations, and indirect call patterns to predict the next system call.\n\nFigure 7 shows the overall architecture of BERT. The input to BERT is a sequence of tokens, which are converted into vectors by the input embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the next token in the sequence.\n\nFigure 7 shows the architecture of the FADE model. The model consists of an encoder, a decoder, and a gated feature refinement module. The encoder takes as input a sequence of 2D feature maps and produces a sequence of high-level feature maps. The decoder takes as input the high-level feature maps and produces a sequence of low-level feature maps. The gated feature refinement module takes as input the low-level feature maps and produces a refined sequence of low-level feature maps. The refined feature maps are then used to predict the next frame in the video.",
        "reference": "Figure 3: Technical pipeline of FADE. From (a) the overview of FADE, feature upsampling is executed by jointly exploiting the encoder and decoder feature with two key modules. In (b) dynamic feature pre-upsampling, they are used to generate upsampling kernels using a semi-shift convolutional operator (Fig. 5). The kernels are then used to reassemble the decoder feature into pre-upsampled feature. In (c) gated feature refinement, the encoder and pre-upsampled features are modulated by a decoder-dependent gating mechanism to enhance detail delineation before generating the final upsampled feature."
    },
    "2011.07831v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee, while the indirect call relations are the relations between the caller of the caller and the callee. The indirect call patterns are the patterns of the indirect call relations. For example, the indirect call pattern (C1, C2) means that C1 calls C2 indirectly.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence. The model is trained to predict the next word in a sequence given the previous words in the sequence.\n\nFigure 8 shows the architecture of a neural machine translation model. The model consists of an encoder and a decoder. The encoder is a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are used to encode the source sentence into a sequence of vectors. The decoder is a stack of decoders, each of which consists of a self-attention layer, an attention layer, and a feed-forward layer. The decoders are used to decode the sequence of vectors into the target sentence.",
        "reference": "Figure 1: A simplified illustration of our proposed method where \u039b refers to the write mechanism described in section 3.1.1. Ft are the recurrent weights of the FWM which have been generated by the LSTM. The LSTM is a regular slow RNN. The residual connection between the FWM and the LSTM is not depicted."
    },
    "1262057-Figure5-1.png": {
        "prediction": " The flowchart\u4e00\u958b\u59cb\u662f\u5169\u500b\u4e26\u3093\u3067\u7684\u300cuse case u1\u300d\u548c\u300cuse case u2\u300d\uff0c\u7bad\u982d\u6307\u5411\u300ctrace t1\u300d\u548c\u300ctrace t2\u300d\u3002\n\n\u300ctrace t1\u300d\u548c\u300ctrace t2\u300d\u5404\u81ea\u9023\u5230\u4e00\u500b\u65b9\u6846\uff0c\u5167\u542b\u6587\u5b57\u300cC1\u300d\uff0c\u518d\u7531\u65b9\u6846\u300cC1\u300d\u5404\u81ea\u9023\u5230\u4e0b\u4e00\u500b\u65b9\u6846\u300cC2\u300d\uff0c\u300cC2\u300d\u518d\u9023\u5230\u300cC3\u300d\uff0c\u300cC3\u300d\u9023\u5230\u300cC4\u300d\uff0c\u300cC4\u300d\u9023\u5230\u300cC5\u300d\uff0c\u300cC5\u300d\u9023\u5230\u300cC6\u300d\u3002\n\n\u300ctrace t1\u300d\u548c\u300ctrace t2\u300d\u4e5f\u5404\u81ea\u9023\u5230\u4e00\u500b\u65b9\u6846\uff0c\u5167\u542b\u6587\u5b57\u300cC1\u300d\uff0c\u518d\u7531\u65b9\u6846\u300cC1\u300d\u5404\u81ea\u9023\u5230\u4e0b\u4e00\u500b\u65b9\u6846\u300cC2\u300d\uff0c\u300cC2\u300d\u518d\u9023\u5230\u300cC3\u300d\uff0c\u300cC3\u300d\u9023\u5230\u300cC6\u300d\u3002\n\n\u65b9\u6846\u300cC1\u300d\u548c\u300cC2\u300d\u4e4b\u9593\u4e5f\u6709\u7bad\u982d\u4e92\u76f8\u9023\u63a5\u3002\n\n\u5728\u300cuse case u1\u300d\u7684\u53f3\u65b9\uff0c\u6709\u4e00\u500b\u5927\u62ec\u5f27\uff0c\u5167\u542b\u6587\u5b57\u300cdirect call relations\u300d\u3002\u5927\u62ec\u5f27\u7684\u5de6\u65b9\u4e5f\u6709\u7bad\u982d\u9023\u5230\u300cC1\u300d\u548c\u300cC2\u300d\u3002\n\n\u5728\u5927\u62ec\u5f27\u7684\u53f3\u65b9\uff0c\u6709\u4e09\u500b\u5c0f\u65b9\u6846\uff0c\u5206\u5225\u662f\u300cC1\u300d\uff0c\u300cC4\u300d\u548c\u300cC2\u300d\uff0c\u300cC4\u300d\u3002\n\n\u5728\u300cuse case u2\u300d\u7684\u53f3\u65b9\uff0c\u4e5f\u6709\u500b\u5927\u62ec\u5f27\uff0c\u5167\u542b\u6587\u5b57\u300cindirect call patterns\u300d\u3002\u5927\u62ec\u5f27\u7684\u5de6\u65b9\u4e5f\u6709\u7bad\u982d\u9023\u5230\u300cC1\u300d\u548c\u300cC2\u300d\u3002\n\n\u5728\u5927\u62ec\u5f27\u7684\u53f3\u65b9\uff0c\u6709\u4e5d\u500b\u5c0f\u65b9\u6846\uff0c\u5206\u5225\u662f\u300cC1\u300d\uff0c\u300cC4\u300d\uff0c\u300cC2\u300d\uff0c\u300cC4\u300d\uff0c\u300cC1\u300d\uff0c\u300cC2\u300d\uff0c\u300cC6\u300d\uff0c\u300cC1\u300d\uff0c\u300cC2\u300d\uff0c\u300cC6\u300d\u3002\n\n\u5728\u300cindirect call patterns\u300d\u7684\u4e0b\u65b9\uff0c\u9084\u6709\u4e00\u500b\u5927\u62ec\u5f27\uff0c\u5167\u542b\u6587\u5b57\u300cdirect call patterns\u300d\u3002\n\n\u5728\u5927\u62ec\u5f27\u7684\u53f3\u65b9\uff0c\u6709\u4e94\u500b\u5c0f\u65b9\u6846\uff0c\u5206\u5225\u662f\u300cC1\u300d\uff0c\u300cC3\u300d\uff0c\u300cC2\u300d\uff0c\u300cC3\u300d\uff0c\u300cC5\u300d\u3002",
        "reference": "Figure 5. Overview of our approach for unbiased human action recognition on a sample of the Guiabot robot dataset."
    },
    "2107.00638v1-Figure5-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations can be extracted. The direct call relations are represented by the directed edges between the caller and the callee, while the indirect call relations are represented by the dashed edges between the caller and the callee. The indirect call patterns are extracted based on the indirect call relations. For example, the indirect call pattern (C1, C2) indicates that C1 calls C2 indirectly. The direct call patterns are extracted based on the direct call relations. For example, the direct call pattern (C1, C2) indicates that C1 calls C2 directly.",
        "reference": "Figure 5: Charge transfer (CT) due to relative orbital positions in metal-organic interfaces. Panel a) shows the situation before contact. The Fermi-level of the metal lies above the LUMO of the adsorbate. Panel b) intermediate stage (purely for illustration, this does not exist in reality). Adsorbate and substrate are in contact, adsorbate orbitals broaden IP and EA are changed due to band gap renormalization, charge is transferred to enforce a common Fermi level. Panel c) Final situation. Charge transfer is finished, result is a common Fermi level and a dipole that shifts the adsorbate levels by \ud835\udee5\ud835\udef7. Reproduced with permission from [62]."
    },
    "2205.15979v1-Figure7-1.png": {
        "prediction": " In Figure 1, given two traces Tr1 and Tr2, the direct call relations and indirect call patterns are extracted. The direct call relations are extracted by identifying the caller and callee functions in each trace. The indirect call patterns are extracted by identifying the caller and callee functions in each trace, and then identifying the indirect call patterns between the caller and callee functions.",
        "reference": "Figure 7: Overview of the short-term and long-term planning steps for the local trajectory planning."
    },
    "2206.00991v1-Figure3-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of traces, we first extract the direct call relations and indirect call relations from each trace. Then, we construct the indirect call patterns by merging the indirect call relations from all traces. Finally, we use the direct call relations and indirect call patterns to predict the direct call relations in new traces.\n\nFigure 7 shows the overall architecture of BERT. The model consists of a stack of Transformer encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the final encoder is fed into a softmax layer to predict the probability distribution over the vocabulary for each token.\n\nFigure 8 shows the architecture of Occupancy Grid Decoder. It consists of two convolutional neural networks (CNNs), one for predicting the occupancy grid of vehicles and the other for predicting the occupancy grid of pedestrians. The input to the CNNs is a feature map that is computed by concatenating the sparse representation of the current agent's occupancy and the whole-scene features. The output of the CNNs is a two-dimensional occupancy grid that represents the predicted occupancy of vehicles and pedestrians in the scene.",
        "reference": "Fig. 3. An overview of the StopNet architecture. The encoder processes the input point set P and produces a feature map, which is used to predict both per-agent trajectories and whole-scene occupancy grids for each agent type. Input agent boxes at t = 0 are also rendered in BEV as binary features and fed to the trajectory and occupancy grid decoders."
    },
    "1356654-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect call relations between the functions. The indirect call patterns are then extracted from the indirect call relations. The indirect call patterns are represented as a set of triples, where each triple represents an indirect call pattern between three functions.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence. The model is trained to predict the next word in a sequence, given the previous words in the sequence.\n\nFigure 8 shows the architecture of a fully convolutional network (FCN) for semantic segmentation. The FCN takes an image as input and produces a segmentation mask as output. The segmentation mask is a binary image that indicates the class of each pixel in the input image. The FCN consists of a stack of convolutional layers, each of which is followed by a pooling layer. The convolutional layers are used to extract features from the input image, and the pooling layers are used to reduce the dimensionality of the feature maps. The final layer of the FCN is a fully connected layer that is used to predict the class of each pixel in the input image.",
        "reference": "Fig. 2. (Top): Overview of our semantic segmentation training framework. (Bottom): Different levels of training supervision. For full supervision, the class of every pixel is provided. For image-level supervision, the class labels are known but their locations are not. We introduce point-level supervision, where each class is only associated with one or a few pixels, corresponding to humans pointing to objects of that class. We include an objectness prior in our training loss function to accurately infer the object extent."
    },
    "980236-Figure2-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee functions in the calling stack. The indirect call relations are the relations between the caller and the callee functions that are not in the calling stack. For example, in trace t1, the direct call relations are C1->C2, C2->C3, C3->C4, and C4->C5. The indirect call relations are C1->C4 and C2->C5.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the final encoder is a sequence of vectors, where each vector represents the hidden state of the corresponding token in the input sequence. These vectors are then used to predict the masked tokens in the input sequence.\n\nFigure 8 shows the architecture of a style transfer network. The network consists of a content encoder, a style encoder, and a decoder. The content encoder encodes the content image into a content vector. The style encoder encodes the style image into a style vector. The decoder uses the content vector and the style vector to generate the stylized image.",
        "reference": "Fig. 2. System overview. We train an image transformation network to transform input images into output images. We use a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process."
    },
    "204823930-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given a pair of traces (e.g., t1 and t2), the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the call relations. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values represent the probability of a function calling another function.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which attend to each other and learn to extract features from the sequence. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 shows the architecture of a neural network for evidence selection and joint evidence reasoning. The model takes as input a set of evidence nodes and a claim, and outputs a probability distribution over the possible labels for the claim. The evidence nodes are first embedded into vectors using a node embedding layer. The embedded evidence nodes are then passed through a series of graph convolutional layers, which learn to extract features from the graph of evidence nodes. The output of the graph convolutional layers is then passed through a multilayer perceptron (MLP) to predict the probability distribution over the possible labels for the claim.",
        "reference": "Figure 2: KGAT Architecture."
    },
    "336802-Figure1-1.png": {
        "prediction": " The figure shows an overview of the PHOCNet architecture. Given an input image, a deep CNN is used to extract visual features. These features are then fed into the PHOCNet, which consists of a stack of convolutional layers. The output of the PHOCNet is a binary vector, which represents the estimated PHOC.",
        "reference": "Fig. 1. Overview over the proposed PHOCNet approach for Query-byExample (QbE) and Query-by-String (QbS) word spotting."
    },
    "2206.10801v3-Figure1-1.png": {
        "prediction": " Figure 1 illustrates the traces and temporal relations. Given two traces Tr1 and Tr2, the direct call relations and indirect call relations can be extracted. The direct call patterns C1 and C2 are extracted from Tr1 and Tr2, respectively. The indirect call patterns between C1 and C2 are also extracted. \n\nFigure 7 shows the architecture of BERT in pretraining. The input consists of a masked sequence of tokens, and the model is tasked with predicting the masked tokens and the next sentence. The model consists of a stack of Transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The output of the final Transformer encoder is fed into a softmax layer to predict the masked tokens and the next sentence.\n\nFigure 8 shows the architecture of a VAE. The encoder network takes as input a data sample x and outputs a latent representation z. The decoder network takes as input the latent representation z and outputs a reconstruction of the input data sample x. The loss function of the VAE is the sum of the reconstruction loss and the KL divergence between the prior distribution and the posterior distribution.",
        "reference": "Fig. 1: Overview of the proposed system. D denotes the discriminator, G denotes the generator."
    },
    "2101.04223v2-Figure2-1.png": {
        "prediction": " In Figure 1, we illustrate how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee in the same trace. The indirect call relations are the relations between the callers and callees in different traces. The indirect call patterns are the patterns of the indirect call relations. For example, the indirect call pattern (C1, C2) means that C1 calls C2 in different traces.\n\nIn Figure 7, we show the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the meaning of the corresponding token in the input sequence.\n\nIn Figure 3, we show three different types of ESNs: (A) the standard ESN, (B) the hierarchical ESN, and (C) the parallel ESN. The standard ESN has a single reservoir, while the hierarchical ESN has a hierarchy of reservoirs. The parallel ESN has multiple reservoirs that are connected in parallel.",
        "reference": "Figure 2: Single and hierarchical echo-state network (ESN) architectures. A: A single ESN with internally connected nodes with a single set of hyper-parameters \u03b1 and \u03c1. B: A hierarchical ESN composed of 2 connected reservoirs where the input is fed into reservoir 1 only and the connection is unidirectional from R1 to R2, which is identical to the 2 layer DeepESN of [27]. C: A parallel (or unconnected hierarchical) ESN where the network is partitioned into 2 reservoirs, R1 and R2, which each receive the input and provide output but have distinct hyper-parameters."
    },
    "2101.11508v8-Figure2-1.png": {
        "prediction": " Figure 1 shows how execution traces are generated and used to extract temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph, and the indirect call relations are used to construct the indirect call graph. The direct call graph and the indirect call graph are then used to extract the temporal relations between the functions.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoder layers and a stack of decoder layers. The encoder layers are used to encode the input text into a sequence of vectors. The decoder layers are used to decode the sequence of vectors into a sequence of output tokens. The model is trained to predict the next token in a sequence of text, given the previous tokens in the sequence.\n\nFigure 8 shows the architecture of a U-Net model. U-Net is a convolutional neural network that is used for image segmentation. The model is composed of a series of encoder layers and a series of decoder layers. The encoder layers are used to encode the input image into a sequence of feature maps. The decoder layers are used to decode the sequence of feature maps into a segmentation mask. The model is trained to predict the segmentation mask for an input image.",
        "reference": "Figure 2. U-net architecture. Conv means convolution. ReLU is rectified linear unit. DepthConv is depth concatenation. UpConv means up-convolution or transposed convolution. MaxPool is Max Pooling."
    },
    "2210.09757v1-Figure4-1.png": {
        "prediction": " In Figure 1, we illustrate how traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, we can extract direct call relations and indirect call relations. We use C1 and C2 to denote the caller and callee, respectively. For example, in trace t1, we have the direct call relations C1->C2, C1->C3, C1->C4, C2->C5, and C3->C6. We also have the indirect call relations C1->C5 and C1->C6. The indirect call relations are obtained by transitive closure of the direct call relations. For example, C1->C5 is an indirect call relation because there is a path C1->C2->C5 in the direct call graph.\n\nIn Figure 7, we show the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between different parts of the sequence. The output of the encoder layers is a sequence of vectors, each of which represents a word in the input sequence. These vectors are then used to predict the next word in the sequence.\n\nIn Figure 8, we show how VLS and VIO observations are generated. VLS observations are generated by a visual language model, which is a neural network that is trained to predict the next word in a sentence given an image. VIO observations are generated by a visual inertial odometry system, which is a sensor that is used to estimate the position and orientation of a moving object. The VLS and VIO observations are then used to train a neural network to predict the future trajectory of the object.",
        "reference": "Fig. 4. Illustration of drift RANSAC. Drift RANSAC randomly samples local key frames and calculate errors of each set. Then we estimates T lo \u2032 between VIO and VLS coordinates of least error among T lo,k . Finally we calculate error and weight of each VLS edge."
    },
    "2102.03117v2-Figure2-1.png": {
        "prediction": " The image shows the flowchart of the algorithm. It consists of four steps:\n\n1. Start with traces.\n2. Extract direct call relations.\n3. Extract indirect call relations.\n4. Generate indirect call patterns.",
        "reference": "Figure 2 A more detailed proof diagram."
    },
    "2010.11078v2-Figure2-1.png": {
        "prediction": " The flowchart illustrates the process of extracting indirect call relations from traces. Given two traces, trace 1 and trace 2, the direct call relations are first extracted. Then, the indirect call relations are extracted based on the direct call relations. Finally, the indirect call patterns are generated based on the indirect call relations.",
        "reference": "Fig. 2. A conceptual illustration of the natural match between the discrete task planner and the low-level distributed trajectory optimization. In the move action without an object in hand, velocity and dynamics constraints are enforced. When the gripper is in the push action, the optimization naturally adds a contact constraint. Note that qgoal,move = qinit,push."
    },
    "2204.06433v1-Figure3-1.png": {
        "prediction": " Figure 1 shows the components used in our approach. We use MoveIt and RViz to visualize the robot\u2019s motion plan and the environment. The path planning module Pathfinder takes the target object\u2019s position as input and outputs a collision-free path. The robot controller receives the planned path and controls the robot\u2019s arm to move along the path. The object handler receives the target object\u2019s position and updates the object\u2019s position in the simulation environment. The blue arrows indicate the data flow between components, and the orange arrows indicate the control flow.",
        "reference": "Fig. 3. Implementation Diagram"
    },
    "2204.12346v1-Figure2-1.png": {
        "prediction": " Figure 1 shows how execution traces are generated and how temporal relations are extracted. Given two traces, t1 and t2, the direct call relations and indirect call relations can be extracted. Based on these relations, indirect call patterns can be identified.\n\n\nFigure 7 illustrates the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer, which predicts the probability of each token in the sequence.\n\nFigure 2(a) shows the SIR model. The SIR model is a compartmental model, which means that it divides the population into three compartments: susceptible, infected, and recovered. The susceptible compartment contains the individuals who are susceptible to the disease. The infected compartment contains the individuals who are infected with the disease. The recovered compartment contains the individuals who have recovered from the disease.\n\nFigure 2(b) shows the SIRS model. The SIRS model is a compartmental model that is an extension of the SIR model. The SIRS model includes an additional compartment, the removed compartment. The removed compartment contains the individuals who have been removed from the population, either through death or recovery.",
        "reference": "Figure 2. A schematic representation of the SIR and the SIRD model."
    },
    "2011.12745v4-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. Then, the indirect call patterns are generated based on the indirect call relations. Finally, the direct call patterns are generated based on the direct call relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input is a masked sequence of tokens. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The output of the final encoder is fed to a softmax layer to predict the masked tokens.\n\nThe flowchart in Figure 8 shows the architecture of a point cloud upsampling network. The input is a sparse point cloud. The network consists of a feature extractor, a distance encoder, an interpolation learning module, an interpolation matrix, and a coordinate refinement module. The output of the network is a dense point cloud.",
        "reference": "Fig. 3. The flowchart of the proposed method. Given a sparse point cloud with N points, it first learns a U -dimensional feature for each point (i.e., the feature learning module) and also embeds the relative position information of K nearest neighbouring (KNN) points into the U -dimensional features (i.e., the distance encoder module). Then the two types of high-dimensional features are concatenated to regress unified and sorted interpolation weights (i.e., the weight learning module), which coarsely interpolate the input sparse point cloud into a dense one. The coarse point cloud is finally refined via a self-attention-based refinement module, which regresses an offset for each point from the corresponding feature (see Fig. 4 for the detailed network architecture). After one-time end-to-end training, the proposed method is capable of handling flexible upsampling factors not greater than the maximum factor Rmax."
    },
    "2210.06361v3-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values represent the indirect call relations between the functions.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe image shows a high-level overview of the MViT architecture. Given multi-view images \\(I^d, I^v, I^c\\), the feature pyramid encoder \\(f_\\theta\\) extracts hierarchical features. Specifically, it consists of five convolutional blocks, each followed by a max-pooling layer. The output of the feature pyramid encoder is a set of feature maps of different scales. These feature maps are then fed into the view combining layer, which consists of five co-attention modules (CAMs). Each CAM takes two feature maps from different views as input and outputs a refined feature map. The output of the view combining layer is a single feature map, which is then fed into the hierarchical channel fusion decoder. The decoder consists of five convolutional blocks, each followed by a transposed convolution layer. The output of the decoder is a high-resolution feature map, which is then passed through a final convolutional layer to produce the segmentation result.",
        "reference": "Figure 2: Overview of our model structure. We generate multiple views (Diagonally and Vertically flipped views, Close looking views) of the data by different transformation methods. The shared pyramid feature encoder is applied to extract hierarchical features of different scales corresponding to different view choices. The view combining layer concatenates features of same level from different views (fDi , f V i , f O i , f C1 i , f C2 i ) channel-wisely and output multi-view feature tensors (mvtensors). The model feeds mv-tensors into CAMVs and obtain multi-view enhanced feature tensor (en-tensor) fi. CAMV is adopted to fuse features and aggregate vital clues between different views by a two-stage co-attention mechanism. The entensors are further decoded and the contextual correlation are exploited by hierarchical channel fusion unit simultaneously. In the end, a probability map of camouflaged object in the input image is computed by several convolutional blocks."
    },
    "2011.09340v3-Figure9-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations can be inferred. For example, if A calls B in Tr1, and B calls C in Tr2, then an indirect call relation from A to C can be inferred. In this example, the inferred indirect call relations are (C1, C4), (C1, C5), (C1, C6), (C2, C4), (C2, C5), and (C2, C6).\n\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence.\n\n\nFigure 8 shows the circuit for the Grover search algorithm. The Grover search algorithm is a quantum algorithm that can be used to search for an item in a database. The algorithm works by iteratively applying a Grover operator to the database. The Grover operator is a unitary operator that amplifies the probability of finding the item that is being searched for.",
        "reference": "Figure 9: Process with an entanglement breaking map on at least one of its spaces. If the circuit of a process can be represented with an entanglement breaking (EB) channel on one of its wires, then the resulting comb \u03a5ABC is separable in the corresponding cut. For example, an entanglement breaking channel on the environment R implies that \u03a5ABC is separable in the splitting A : BC. If there are two entanglement breaking channels (independent of what two wires they act on), then the resulting comb is fully separable. For better tracking of the involved spaces, the input and output spaces of the EB channels are labelled differently."
    },
    "2011.03178v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. Then, the indirect call patterns are generated based on the indirect call relations. Finally, the direct call patterns are generated based on the direct call relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nThe flowchart in Figure 8 shows the training process of a neural network. The model is first trained on a training dataset, which is used to optimize the model's parameters. The model is then evaluated on a validation dataset, which is used to assess the model's performance. The model is then deployed to a production environment, where it is used to make predictions on new data.",
        "reference": "Figure 1: A diagram for the pipeline of (transductive) active learning."
    },
    "2101.07079v1-Figure5-1.png": {
        "prediction": " The flowchart shows how to compute the indirect call relations between functions. \n\nGiven two traces `Tr1` and `Tr2`, the direct call relations and indirect call patterns are extracted. The direct call relations are used to construct the call graph. The indirect call patterns are used to infer the indirect call relations. The inferred indirect call relations are used to update the call graph. The process is repeated until the call graph converges.",
        "reference": "Figure 5: Illustration for the notations in the beginning of Section 5.1."
    },
    "1580300-Figure2.2-1.png": {
        "prediction": " The image shows an end-to-end trainable neural network architecture for VQA. The architecture consists of two major components: a parameter prediction network and a classification network. The parameter prediction network takes as input the question and outputs a set of parameters that control the dynamic parameters of the classification network. The classification network takes as input the image and the output of the parameter prediction network, and outputs a classification score for each answer candidate.",
        "reference": "Figure 2.2: DPPnet, the question is used to learn how to predict parameters for a dynamic parameter layer in the classification network."
    },
    "2207.14299v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the source code. Given a pair of traces (e.g., trace t1 and trace t2), the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent functions and the edges represent the call relations between functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents functions and the other set of nodes represents call sites. The edges between the two sets of nodes represent the indirect call relations between functions and call sites. Finally, the direct call patterns and indirect call patterns are extracted from the direct call relations and indirect call relations, respectively.",
        "reference": "Figure 2: Overview. We extract object bounding boxes from video sequences using an off-the-shelf detector, and construct a graph abstraction of the scene. We model graph-abstracted object interactions using Interaction Networks [5], and learn a reward function by aligning video embeddings temporally. We then train image-based RL policies using our learned reward function, and deploy on a real robot."
    },
    "2107.04225v2-Figure1-1.png": {
        "prediction": " Figure 8. Multi-task learning with unlabeled data. The student network learns from both labeled and unlabeled data with multi-task supervision. The teacher network is used to generate pseudo labels for unlabeled data.",
        "reference": "Fig. 1. Framework for multi-task affective behavior analysis model"
    },
    "2012.04925v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. First, the direct call relations are extracted from the code. Then, the indirect call relations are extracted by analyzing the control flow of the program. Finally, the temporal relations between the statements are extracted by combining the direct and indirect call relations.\n\nThe flowchart in Figure 7 illustrates the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, each of which represents the meaning of the corresponding token in the input sequence.\n\nThe flowchart in Figure 8 illustrates the process of machine translation using a neural network. The model consists of an encoder and a decoder. The encoder converts the input sentence into a vector, and the decoder converts the vector into an output sentence in the target language.",
        "reference": "Figure 2. Conceptual illustration of the three proposed metrics. Given a caption \ud835\udc66\ud835\udc61 generated by a crosslingual image captioning model, we propose WMDRel and CLinRel to measure the semantic relevance between \ud835\udc66\ud835\udc61 and \ud835\udc66\ud835\udc60 , the reference in a source language (English here), and CMedRel to measure the semantic relevance between \ud835\udc66\ud835\udc61 and the visual content. Different from previous works, no reference caption in the target language (Chinese here) is needed."
    },
    "1592947-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns. The direct call patterns and indirect call patterns are then used to generate the temporal relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.\n\nThe flowchart in Figure 8 shows the architecture of a BiLSTM-CRF model for named entity recognition. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The vectors are then passed through a series of BiLSTM layers, which learn to extract features from the sequence. The output of the BiLSTM layers is then passed through a CRF layer, which produces a sequence of labels, where each label corresponds to a named entity. The model is trained to minimize the negative log-likelihood of the correct label sequence.",
        "reference": "Figure 2. A BI-LSTM-CRF model for identifying FS."
    },
    "2011.14903v1-Figure3-1.png": {
        "prediction": " The flowchart\u4e00\u958b\u59cb\u662f\u5b9a\u7fa9\u4e86\u56db\u500b\u8b8a\u6578\uff0c\u5206\u5225\u662f\uff1a`\u8d44\u4ea7\u7ba1\u7406`\u3001`\u7535\u6c60\u8001\u5316`\u3001`\u5e76\u884c\u8d44\u4ea7\u7ba1\u7406\u6a21\u578b`\u548c`\u975e\u7ebf\u6027\u5bb9\u91cf\u8870\u51cf\u6a21\u578b`\u3002\n\n\u7136\u5f8c\uff0c\u6d41\u7a0b\u5716\u5b9a\u7fa9\u4e86\u5169\u500b\u51fd\u6578\uff0c\u5206\u5225\u662f\uff1a`\u6700\u5c0f\u5316\u66f4\u6362\u6210\u672c\u7684\u6a21\u578b`\u548c`\u7535\u6c60\u5bff\u547d\u9884\u6d4b`\u3002\n\n\u6700\u5f8c\uff0c\u6d41\u7a0b\u5716\u7d66\u51fa\u4e86\u9019\u5169\u500b\u51fd\u6578\u7684\u8f38\u51fa\uff0c\u5206\u5225\u662f\uff1a`\u8d44\u4ea7\u7ba1\u7406\u6a21\u578b`\u548c`\u7535\u6c60\u8001\u5316`\u3002",
        "reference": "Fig. 3. Battery Degradation Model and Asset Management Model Framework"
    },
    "2110.15553v2-Figure4-1.png": {
        "prediction": " Figure 1 shows an overview of the approach. Given two traces Tr1 and Tr2, we first extract the direct call relations and indirect call relations from each trace. Then, we use the direct call relations to construct the direct call graph G1 and use the indirect call relations to construct the indirect call graph G2. Finally, we use the graph edit distance to measure the similarity between G1 and G2.\n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the final transformer encoder is a sequence of vectors, each of which represents the meaning of the corresponding token in the input sequence.\n\nFigure 8 shows the pre-training procedure of BERT. The model is trained on a large corpus of text data, and the training objective is to predict the masked tokens in the input sequence. The model is trained using a masked language modeling objective, which means that it is trained to predict the words that have been masked out in the input sequence. This objective helps the model to learn the relationships between words in a sentence, and it also helps the model to learn the meaning of words in context.",
        "reference": "Figure 4. A schematic illustration of the proposed data-driven surrogate modeling framework for UQ of computational head models: in the first stage (see Section 3.2), the available material properties of each of the four substructures, \ud835\udcb3M \ud835\udc56 \u2208 \u211d4\u00d7\ud835\udc41\ud835\udc56 (\ud835\udc41\ud835\udc56 denotes number of voxels for substructure \ud835\udc56) are used to generate 300 realizations of the input random vector of material properties for the 2D head model (i.e., \ud835\udcb3M). Simulations of these realizations yields input-output (\ud835\udcb3M \u2212 \ud835\udcb4M MAS) data sets for training the surrogate model in the second stage. The surrogate model is developed in three steps (see Section 3.3): 1. perform nonlinear dimensionality reduction on the output via"
    },
    "214611580-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the call graph. Given a pair of traces (Trace 1 and Trace 2), direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph, and the indirect call relations are used to construct the indirect call graph. The direct call patterns and indirect call patterns are then extracted from the direct call graph and indirect call graph, respectively.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 illustrates the contrastive learning framework for representation learning. The framework consists of three main components: a representation learning network, an n-tuplet sampling network, and a robust anchor generation network. The representation learning network learns to embed data points into a latent space, where similar data points are close to each other and dissimilar data points are far apart. The n-tuplet sampling network samples triplets of data points that are used to train the representation learning network. The robust anchor generation network generates anchors that are used to ensure that the representation learning network learns to embed data points in a way that is robust to noise and outliers.",
        "reference": "Figure 1. The overview of the NeuCrowd framework. The dash line indicates that the SRL network passes its training loss to the n-tuplet sampling network. The blue dots represent original crowdsourced examples and red dots represent the robust anchors."
    },
    "1047598-Figure4-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates the process of extracting temporal relations between events from execution traces. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the events that are directly called by each other, while the indirect call relations are the relations between the events that are called by the same event. The indirect call patterns are then extracted from the indirect call relations. The indirect call patterns are the patterns of the indirect call relations that occur frequently. Finally, the direct call patterns are extracted from the direct call relations. The direct call patterns are the patterns of the direct call relations that occur frequently.",
        "reference": "Figure 4. Schematic of recognition in proposed model."
    },
    "2107.07001v1-Figure13-1.png": {
        "prediction": " The flowchart shows the process of how a neural network model is trained. The model is first trained on a dataset of labeled data. The model is then evaluated on a test set of labeled data. If the model performs well on the test set, it is then deployed for use.\n\nThe flowchart also shows the different types of data that are used to train the model. The model is trained on a dataset of labeled data, which is data that has been annotated with the correct answers. The model is also trained on a dataset of unlabeled data, which is data that has not been annotated with the correct answers. The unlabeled data is used to help the model learn the structure of the data.\n\nThe flowchart also shows the different types of models that can be trained. The model can be a supervised learning model, which is a model that is trained on a dataset of labeled data. The model can also be an unsupervised learning model, which is a model that is trained on a dataset of unlabeled data.\n\nThe flowchart also shows the different types of tasks that the model can be trained to perform. The model can be trained to perform a classification task, which is a task where the model is given a set of inputs and the model must predict the correct output. The model can also be trained to perform a regression task, which is a task where the model is given a set of inputs and the model must predict a continuous value.",
        "reference": "Fig. 13 Illustration of the Apollo CSM Transposition and Docking maneuver with the LM housed inside the Saturn S-IVB third stage [35, Figure 2-11]."
    },
    "2204.03065v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the API calls and the edges represent the direct call relations between them. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the API calls and the other set of nodes represents the indirect call patterns. The edges between the two sets of nodes represent the indirect call relations between the API calls and the indirect call patterns.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input features. The output of the final encoder is used to predict the next sentence and the masked words in the input sequence.\n\nFigure 8 shows the general architecture of a neural network-based recommender system. The recommender system consists of three main components: an extractor, a transformer, and a task processing component. The extractor component extracts features from the input items. The transformer component then transforms the extracted features into a new representation. Finally, the task processing component uses the transformed features to predict the output.",
        "reference": "Fig. 2: Generic designs of networks that act on sets of inputs. These cover relevant architectures, e.g. for few-shot-classification and clustering. Left: A generic network for processing a set of input items typically follows the depicted structure: (i) Each item separately goes through a common feature extractor F . (ii) The set of extracted features is the input to a downstream task processing module G. ; Right: A more general structure in which the extracted features undergo a joint processing by a transform T . Our SOT transform (as well as other attention mechanisms) is of this type and its high-level design (within the \u2018green\u2019 module) is detailed in Fig. 1."
    },
    "137199-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given a set of traces, the first step is to identify the direct call relations between the functions. This is done by parsing the traces and extracting the function calls. The direct call relations are then used to construct a call graph. In the second step, the indirect call relations between the functions are extracted. This is done by analyzing the control flow of the program and identifying the cases where a function is called indirectly through another function. The indirect call relations are then used to construct an indirect call graph. Finally, the direct and indirect call relations are combined to generate the temporal relations between the functions.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The model consists of a stack of transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The output of the final encoder is used to predict the masked tokens in the input sequence.\n\nThe flowchart in Figure 8 shows the different components and training procedure of a typical machine learning pipeline. The pipeline starts with data preprocessing, where the data is cleaned and transformed into a format that is suitable for modeling. The next step is feature engineering, where the data is transformed into a set of features that are relevant to the modeling task. The features are then used to train a machine learning model. The model is then evaluated on a test set to assess its performance. If the performance is satisfactory, the model can be deployed for use.",
        "reference": "Figure 3. Illustration for a exemplary subspace decomposition {\u039b(m)}3m=1."
    },
    "2012.01644v2-Figure5-1.png": {
        "prediction": " The image shows the architecture of a 3D convolutional neural network (CNN) for point cloud classification. The input to the network is a point cloud, which is a collection of 3D points. The network first samples a number of anchor points from the point cloud. For each anchor point, the network then extracts a local region around the point and applies a 3D CNN to the region. The output of the CNN is a feature vector for the anchor point. The feature vectors for all of the anchor points are then concatenated and fed into a fully connected layer to produce the final classification.",
        "reference": "Figure 5: Example of multi-patch sampling procedure with sampled anchor patch, positive child, and negative child."
    },
    "2012.01249v2-Figure16-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input is a sequence of tokens, which are embedded into vectors by the word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary. The model is trained to maximize the likelihood of the correct tokens in the input sequence.\n\nThe flowchart in Figure 8 shows the architecture of a graph neural network (GNN) for node classification. The GNN consists of a series of layers, each of which takes as input the node features and the edge features, and produces as output a new set of node features. The final layer of the GNN produces a probability distribution over the class labels for each node. The model is trained to minimize the cross-entropy loss between the predicted class labels and the true class labels.",
        "reference": "Fig. 16. ABCNet architecture used for pileup identification in Ref. 16. FC layer and encoding node sizes are denoted inside curly brackets. For each graph attention pooling layer (GAPLayer), the number of k-nearest neighbors and attention heads (h) are given."
    },
    "2012.01249v2-Figure12-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given a pair of traces (e.g., Trace 1 and Trace 2 in the figure), the direct call relations and indirect call relations are identified. The direct call relations are represented by the edges in the leftmost graph, where each node represents a function. The indirect call relations are represented by the dashed edges in the middle graph, where each node represents a function pair (caller, callee). The indirect call patterns are extracted from the indirect call relations and represented by the hyperedges in the rightmost graph. Each hyperedge represents a set of function pairs that share the same caller and callee.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using word embeddings, positional embeddings, and segment embeddings. The embedded tokens are then passed through a series of Transformer layers, which learn to attend to each other and generate a contextualized representation of the input sequence. The final output of the model is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence.\n\nFigure 8 shows the overall architecture of Graphormer. It consists of an encoder module, several graph modules, and an output module. The encoder module takes as input a graph and generates a node embedding for each node in the graph. Each graph module then takes as input the node embeddings from the previous graph module and updates them by attending to the neighboring nodes in the graph. The output module then takes as input the final node embeddings from the last graph module and generates an output for each node in the graph.",
        "reference": "Fig. 12. Graph neural network architecture for particle tracking [21]. The initial latent features of the nodes and edges after the encoder network are named H0. The graph module is applied repeatedly to the latent features. For the ith iteration, the initial features H0 are concatenated with the current features Hi\u22121. After 8 iterations, the output network takes the last latent features H8 to produce classification scores for every edge."
    },
    "2012.01249v2-Figure14-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. \n\nThe direct call relations are extracted by identifying the caller and callee functions in the code. The indirect call relations are extracted by identifying the functions that are called through pointers or function pointers. The temporal relations between the function calls are extracted by analyzing the order in which the functions are called.\n\nFigure 7 shows the architecture of BERT in pretraining. \n\nThe input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which attend to each other and to the input tokens. The output of the transformer layers is a sequence of hidden vectors, which are then passed through a final softmax layer to produce a probability distribution over the vocabulary.\n\nThe model is trained to predict the masked tokens in the input sequence. The masked tokens are randomly selected, and the model is trained to predict the correct tokens based on the context. The model is also trained to predict the next sentence in the input sequence. The next sentence is randomly selected, and the model is trained to predict whether the next sentence is the correct next sentence.",
        "reference": "Fig. 14. The Set2Graph [36, 37] model architecture (top) consisting of a set-to-set component \u03c6, which creates a hidden representation of each track, a broadcasting layer \u03b2, which creates a representation for each directed edge (ordered pair of tracks in the jet), and an edge classifier \u03c8. Edges whose symmetrized edge score is over a certain threshold are connected, resulting in the set partition."
    },
    "140529-Figure1-1.png": {
        "prediction": " The image shows an overview of the image captioning model. The model takes an image as input and generates a caption for the image. The model consists of two main components: an image encoder and a caption decoder. The image encoder extracts features from the image, and the caption decoder uses the features to generate a caption for the image. To train the model, we minimize the cross-entropy loss between the generated caption and the ground-truth caption.",
        "reference": "Figure 1: (Top) Model architecture: the network is split into two branches after the shared feature extraction layers. The top branch predicts a segmentation mask for the the object located at the center while the bottom branch predicts an object score for the input patch. (Bottom) Examples of training triplets: input patch x, mask m and label y. Green patches contain objects that satisfy the specified constraints and therefore are assigned the label y = 1. Note that masks for negative examples (shown in red) are not used and are shown for illustrative purposes only."
    },
    "2204.08626v1-Figure1-1.png": {
        "prediction": " In Figure 1, the flowchart illustrates how traces are generated and used to learn temporal relations. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to learn the direct call patterns, while the indirect call relations are used to learn the indirect call patterns. In this example, C1 and C2 have both direct and indirect relations, while C3 only has a direct relation with C2. C4, C5, and C6 only have indirect relations with C1 and C2.",
        "reference": "Fig. 1: The procedure of extracting features. The EEG signals are bandpass filtered using K different frequency ranges represented in F . The CSP algorithm is applied to the filtered signals to generate a feature vector. These vectors are then fused to feed a supervised autoencoder."
    },
    "2101.08520v3-Figure1-1.png": {
        "prediction": " The image shows how the trace of execution of a program can be represented as a graph. The nodes of the graph represent the statements in the program, and the edges represent the flow of control between the statements. The graph can be used to analyze the program's behavior and identify potential problems.\n\nThe first step is to identify the direct call relations between the functions. This can be done by looking at the call graph of the program. The call graph shows the caller and callee functions for each function in the program.\n\nOnce the direct call relations have been identified, the indirect call relations can be identified. This can be done by looking at the transitive closure of the direct call graph. The transitive closure of a graph is a graph that includes all of the paths between any two nodes in the original graph.\n\nThe direct and indirect call relations can be used to identify the call patterns in the program. A call pattern is a sequence of function calls that is repeated multiple times in the program. Call patterns can be used to identify common execution paths in the program and to optimize the program's performance.",
        "reference": "Figure 1. Pictorial description of the overall architecture."
    },
    "2012.14758v1-Figure1-1.png": {
        "prediction": " In the image, there are two use cases: *use case 1* and *use case 2*. \n\n*Use case 1* has two traces, *trace 1* and *trace 2*. Both traces contain 6 operations, labeled C1 to C6. The operations in *trace 1* are ordered as follows: C1, C2, C3, C4, C5, C6. The operations in *trace 2* are ordered as follows: C1, C2, C5, C3, C4, C6.\n\n*Use case 2* has three direct call relations and six indirect call patterns. The direct call relations are: C1 calls C2, C2 calls C3, and C3 calls C4. The indirect call patterns are: C1 calls C4, C2 calls C5, C2 calls C6, C3 calls C5, C3 calls C6, and C4 calls C6.",
        "reference": "Fig. 1: Block diagram of the proposed system."
    },
    "2012.14758v1-Figure2-1.png": {
        "prediction": " Figure 1 shows how execution traces are generated and used to learn temporal relations between API calls. Given two traces Tr1 and Tr2, the direct call relations and indirect call patterns are extracted. The direct call relations are used to construct the direct call graph, and the indirect call patterns are used to construct the indirect call graph. The direct call graph and the indirect call graph are then used to learn temporal relations between API calls.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, and the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The output of the final encoder is a sequence of vectors, where each vector represents the learned representation of a token in the input sequence.\n\nFigure 8 shows the architecture of a face recognition system. The system consists of two convolutional neural networks (CNNs), one for the face and one for the iris. The face CNN extracts features from the face image, and the iris CNN extracts features from the iris image. The features extracted by the two CNNs are then concatenated and fed into a fully connected layer to predict the identity of the person.",
        "reference": "Fig. 2: Proposed deep feature extraction and binarization (DFB) model for the fully concatenated architecture (FCA)."
    },
    "2012.05647v1-Figure2-1.png": {
        "prediction": " In Figure 1, the flowchart illustrates how execution traces are generated and used to learn temporal relations between API calls. Given a set of API calls, the first step is to identify the direct call relations between them. This is done by parsing the source code and identifying the API calls that are directly invoked by each other. The direct call relations are represented as a graph, where the nodes represent the API calls and the edges represent the direct call relations between them.\n\nIn the second step, the indirect call relations between the API calls are identified. This is done by analyzing the execution traces of the program. An execution trace is a sequence of API calls that are executed by the program. By analyzing the execution traces, it is possible to identify the API calls that are indirectly related to each other, even if they are not directly called by each other. The indirect call relations are also represented as a graph, where the nodes represent the API calls and the edges represent the indirect call relations between them.\n\nIn the third step, the direct and indirect call relations are combined to create a unified graph of API call relations. This graph represents the temporal relations between the API calls, and it can be used to identify the API calls that are most likely to be executed together.\n\nThe flowchart in Figure 7 shows the architecture of BERT, a pre-training model for natural language processing. The model consists of a stack of transformer encoders, which are used to learn the relationships between words in a sentence. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the meaning of a token in the sentence. The model is trained to predict the masked tokens in a sentence, and it can be used to perform a variety of natural language processing tasks, such as text classification and question answering.\n\nThe flowchart in Figure 8 shows the quasi-reinforcement learning loop used to train a neural network to control a physical system. The loop begins with the neural network receiving an input, which is a representation of the current state of the system. The neural network then uses this input to generate a control signal, which is sent to the physical system. The physical system then executes the control signal, and the resulting state of the system is observed. This observed state is then used to calculate a reward, which is a measure of how well the neural network is performing. The reward is then used to update the neural network's parameters, so that it can improve its performance in the future.",
        "reference": "Figure 2: Schematic of the specific reinforcement learning loop used for each phase pattern of the training data set, with a particular example of desired phase pattern."
    },
    "2101.00341v1-Figure1.1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations between the involved components can be inferred. For example, if C1 calls C2 in Tr1 and C2 calls C3 in Tr2, then there is an indirect relation from C1 to C3. In addition, indirect call patterns can be identified. For example, if C1 calls C2 and C2 calls C3 in both Tr1 and Tr2, then there is an indirect call pattern (C1, C2, C3). \n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The Transformer encoders are stacked together, and the output of the final Transformer encoder is used to predict the masked tokens in the input sequence.\n\nFigure 8 shows the popularity dynamics of the content in the network. Panel (a) shows the spatial popularity dynamics, i.e., the number of requests for each content at each SBS. Panel (b) shows the temporal popularity dynamics, i.e., the number of requests for each content at each SBS over time.",
        "reference": "Figure 1.1: An illustration of a UDCN and its intrinsic spatio-temporal popularity dynamics. (a) Spatially dynamics of popularity (b) Temporal dynamics where the content popularity changes for long-term and short-term duration. The long-term dynamics are captured by the Chinese restaurant process, which determines the mean popularity for a certain time period of T . During this period, the instantaneous popularity is captured by the mean reversion model following the OU process [43]."
    },
    "2205.13948v1-Figure4-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the caller and the callee in a direct call. The indirect call relations are the relations between the caller and the callee in an indirect call. The indirect call patterns are the patterns of the indirect call relations. The direct call patterns are the patterns of the direct call relations.",
        "reference": "Fig. 4. Overview of PEGA."
    },
    "2210.01528v1-Figure3-1.png": {
        "prediction": " The flowchart\u4e00\u958b\u59cb\u8b80\u53d6\u8f38\u5165\uff0c\u7136\u5f8c\u9032\u884c\u71b1\u566a\u8072\u53bb\u9664\u3001\u6591\u9ede\u904e\u6ffe\u3001\u5730\u5f62\u6821\u6b63\u548c\u7dda\u6027\u8f49\u63db\u3002\u63a5\u4e0b\u4f86\uff0c\u8a72\u7b97\u6cd5\u5c07\u61c9\u7528\u65bc\u5b50\u96c6\u3002\u6700\u5f8c\uff0c\u8f38\u51fa\u7d50\u679c\u3002",
        "reference": "Fig. 3: Illustration of one of the back-scattering SNAP pipeline we have integrated in the system."
    },
    "913274-Figure1-1.png": {
        "prediction": " The image shows an overview of training and evaluating a neural machine translation model. \n\nIn the training phase, the NMT model is trained to minimize the loss function over a dataset of parallel sentences in two languages. \n\nIn the evaluation phase, the trained NMT model is used to translate new sentences from the source language to the target language. The quality of the translation is evaluated using BLEU score.",
        "reference": "Figure 1. An illustration of the Bi-LSTM-RNN model. The example is \u201cHe had headachese1 from molde2 in the bedrooms.\u201d. e1 and e2 denote two target entities."
    },
    "914562-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given a set of traces, the algorithm first extracts direct call relations and indirect call relations. Then, it computes the indirect call patterns. Finally, it generates the direct call patterns.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, each of which is represented by a word embedding. The output of the model is a sequence of hidden states, each of which represents the meaning of the corresponding token in the context of the entire sequence.\n\nFigure 8 shows the architecture of a metric learning network. The network consists of two parts: a feature network and a metric network. The feature network extracts features from the input images. The metric network computes the similarity between the features of the two images.",
        "reference": "Figure 2: Illustration of our learning formulation. It tends to maximize the distance disparity between the matched pair and mismatched pair for each triplet, in terms of optimization."
    },
    "2101.02047v2-Figure2-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Trace1 and Trace2, the direct call relations and indirect call relations can be extracted. The direct call patterns and indirect call patterns can be further extracted based on the direct call relations and indirect call relations, respectively.\n\n\nFigure 7 shows the architecture of BERT in pretraining. The input is a sequence of tokens, which are embedded into vectors by the word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nFigure 8 shows how to use a stack of LSTMs to implement an encoder-decoder model for machine translation. The encoder reads the input sequence and produces a context vector. The decoder then uses the context vector to generate the output sequence.",
        "reference": "Figure 2: A block diagram of the unified gesture recognition and fingertip detection algorithm depicting the CNN architecture with input and output."
    },
    "2202.06065v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to learn temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. Then, the indirect call patterns are generated based on the indirect call relations. Finally, the direct call patterns and indirect call patterns are used to learn temporal relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.\n\nFigure 8 shows the overall architecture of the system. The system consists of two main components: a sentence encoder and a relation predictor. The sentence encoder encodes the input sentences into vectors, and the relation predictor uses these vectors to predict the relation between the sentences. The sentence encoder is a convolutional neural network, and the relation predictor is a multi-layer perceptron.",
        "reference": "Fig. 2. Illustration of the construction of (B , B ) . The straight edges are the five possible types for edges of % . The curvy edge corresponds to an edge of Alice, and the do ed edge to an edge of Bob."
    },
    "2012.15175v3-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors. The vectors are then passed through a series of Transformer layers, which learn to attend to each other and to the input sequence. The output of the Transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary.\n\nThe flowchart in Figure 8 shows the training and testing process of the proposed HPE method. In the training stage, the original ground truth (GT) is scaled to different scales to generate scale maps. Then, the heatmaps are generated by the HPE model. After that, the weight-adaptive loss is calculated between the heatmaps and the re-scaled GT. Besides, the scale-adaptive GT is generated from the original GT. Finally, the pushing and pulling loss is calculated between the scale-adaptive GT and the grouping result. In the testing stage, only the forward pass is performed to obtain the final result.",
        "reference": "Figure 2. During training, the ground-truth heatmaps are firstly scaled according to predicted scale maps and then are used to supervise the whole model via weight-adaptive loss. During testing, the predicted heatmaps and associative embeddings are used for grouping of individual persons."
    },
    "2103.06446v1-Figure1-1.png": {
        "prediction": " The flowchart describes the process of using a transformer model to predict the next word in a sequence. The input to the model is a sequence of tokens, and the output is a probability distribution over the next token in the sequence. The model is trained on a large dataset of text, and it learns to predict the next word in a sequence by looking at the words that came before it.\n\nThe flowchart first shows how the input sequence is converted into a sequence of embeddings. Each embedding is a vector of numbers that represents the meaning of the corresponding word. The embeddings are then passed through a series of transformer layers. Each transformer layer consists of a self-attention mechanism and a feed-forward network. The self-attention mechanism allows the model to attend to different parts of the input sequence, and the feed-forward network allows the model to learn relationships between the different parts of the sequence.\n\nThe output of the transformer layers is a sequence of vectors. Each vector represents the probability distribution over the next token in the sequence. The final step of the process is to convert the sequence of vectors into a sequence of words. This is done by taking the argmax of each vector. The argmax of a vector is the index of the element with the highest value. The sequence of words that corresponds to the argmaxes of the vectors is the predicted next word in the sequence.",
        "reference": "Figure 1: The procedures of our approach"
    },
    "18233504-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct execution traces and extract temporal relations. Given two traces Tr1 and Tr2 of API calls, we first identify the direct call relations and indirect call relations in each trace. A direct call relation (C1, C2) indicates that C2 is directly called by C1, while an indirect call relation (C1, C2) indicates that C2 is called by C1 through a sequence of other API calls. We then use these relations to construct a call graph for each trace. Finally, we extract the temporal relations between API calls by analyzing the call graphs.\n\nFigure 7 shows the overall architecture of BERT. \n\nFigure 8 shows the detailed architecture of the Transformer blocks used in BERT. \n\nFigure 9 shows the architecture of the DQN model.",
        "reference": "Figure 1: Iteratively decoding with DQN and LSTM; the encoder- coder LSTM network is depicted as gray-filled rectangles on the bottom; the top-leftis the graphical illustration of the DQN with bidirectional LSTMs; the dash arrow line on the right indicates the iteration loop."
    },
    "2010.13114v1-Figure2-1.png": {
        "prediction": " ## Figure 1: Illustration of execution traces and temporal relations.\n\nThe figure shows how to extract direct and indirect relations from execution traces. Given two traces `trace1` and `trace2`, where each trace is a sequence of function calls, the direct call relations and indirect call relations can be extracted. The direct call relations are represented by the edges in the leftmost graph, where each node represents a function. The indirect call relations are represented by the edges in the middle graph, where each node represents a pair of functions. The rightmost graph shows the indirect call patterns, where each node represents a pattern of indirect call relations.\n\n## Figure 7: Pretraining BERT.\n\nThe figure shows the architecture of BERT in pretraining. The model consists of a stack of transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors. The embeddings are then passed through the encoders, which learn to attend to each other and to the input tokens. The output of the encoders is a sequence of vectors, which are then used to predict the next token in the sequence.\n\n## Figure 8: Fine-tuning BERT for sequence classification.\n\nThe figure shows how to fine-tune BERT for sequence classification. The model consists of a stack of transformer encoders, followed by a classification layer. The input to the model is a sequence of tokens, which are embedded into vectors. The embeddings are then passed through the encoders, which learn to attend to each other and to the input tokens. The output of the encoders is a sequence of vectors, which are then used to predict the label of the sequence.",
        "reference": "Figure 2: A schematic of the proposed methodology for joint knowledge distillation and open set recognition. The outputs from teacher and student models are their respective representations, T and S, which are used for calculating CRD loss term. Next, we zS and zT represent student and teacher logits, which are used for calculating the KD loss term. Finally, we calculate class probabilities \u03c3S(zS), which are used for computing of cross-entropy loss term."
    },
    "988721-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns. The direct call patterns and indirect call patterns are then used to generate the temporal relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract meaningful features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 shows the architecture of a convolutional neural network (CNN) for semantic segmentation. The input to the network is an image, which is passed through a series of convolutional layers. The convolutional layers learn to extract features from the image, such as edges, corners, and objects. The output of the convolutional layers is then passed through a fully connected layer to predict the semantic segmentation of the image.",
        "reference": "Figure 1. Overview of the proposed method. Multi-scale fully convolutional network is used for image representation. The network consists of 5 different paths and each path extracts feature at a difference scale. At the end of each path, two convolutional layers extract feature for semantic segmentation and depth estimation. These feature maps are scaled and aggregated to form a comprehensive feature map for semantic segmentation and depth estimation. Depth values are estimated using Eq. 3. Estimated depth values along with semantic unaries and the image are passed through the fully connected CRF to get the final probabilities of semantic labels."
    },
    "2202.07568v4-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates the process of extracting temporal relations between events from execution traces. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the caller and the callee functions, while the indirect call relations are the relations between the caller of a function and the functions called by the callee function. The direct call patterns and indirect call patterns are then generated based on the direct call relations and indirect call relations, respectively.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The token embeddings are then passed through a series of transformer layers, which learn to attend to each other and generate a contextualized representation of the input sequence. The final output of the model is a sequence of vectors, where each vector represents the contextualized representation of the corresponding token in the input sequence.\n\nThe flowchart in Figure 8 illustrates the offline and online phases of the proposed approach. In the offline phase, a set of models is generated and evaluated against different attack intensities. The models that achieve the best performance are selected for deployment. In the online phase, when a user makes a prediction, a strategy vector is generated based on the current attack intensity and the defender's strategy. The strategy vector is used to select a model from the set of deployed models. The selected model is then used to make a prediction.",
        "reference": "Fig. 2: Overview of StratDef."
    },
    "201666925-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The indirect call patterns can be further categorized into C1 and C2.\n\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nThe model is trained to predict the masked tokens in the input sequence. This task is designed to help the model learn to understand the meaning of words in context. The model is also trained to predict the next sentence in a sequence. This task is designed to help the model learn to understand the relationships between sentences.\n\n\nFigure 8 shows the training procedure of GAN and our proposed Recon-GLGAN. The input of both models is a zero-filling (ZF) image. The GAN model directly takes the ZF image as input and tries to reconstruct the original FS image. On the other hand, our proposed Recon-GLGAN takes the input ZF image and first generates a coarse segmentation, which is used to guide the reconstruction of the FS image. The output of both models is a reconstructed FS image. The difference between the reconstructed FS image and the original FS image is calculated as the reconstruction error. The GAN model is trained to minimize the reconstruction error, while our proposed Recon-GLGAN is trained to minimize both the reconstruction error and a perceptual loss.",
        "reference": "Fig. 1. Illustration depicting the comparison between the baseline GAN model and our Recon-GLGAN model. In the training phase, the ZF image and the ROI are fed in as inputs to the Recon-GLGAN model, while the baseline GAN only takes the ZF image as input. In the testing stage, the ZF image is fed as input to either model to produce the reconstruction (Note: ROI is not used during testing stage). The reconstruction error of the Recon-GLGAN model is lesser than the baseline GAN model in the ROI"
    },
    "201666925-Figure2-1.png": {
        "prediction": " Figure 1 illustrates the execution traces and temporal relations. Given two traces, trace 1 (T1) and trace 2 (T2), the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the functions that are directly called by each function, while the indirect call relations are the relations between the functions that are called by the functions that are called by each function. The indirect call patterns are the patterns of the indirect call relations. For example, the indirect call pattern (C1, C2) means that function C1 calls function C2 indirectly. The direct call patterns are the patterns of the direct call relations. For example, the direct call pattern (C1, C2) means that function C1 calls function C2 directly.\n\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by a final output layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the meaning of the corresponding token in the input sequence.\n\nFigure 8 shows the architecture of the generator and the context discriminator in the CycleGAN model. The generator is a neural network that translates an input image from one domain to another. The context discriminator is a neural network that distinguishes between real images and fake images generated by the generator. The generator and the context discriminator are trained together in an adversarial manner.",
        "reference": "Fig. 2. Recon-GLGAN architecture"
    },
    "2107.05180v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given a pair of traces Tr1 and Tr2, the direct call relations and indirect call relations are extracted. Then, the indirect call patterns between functions are constructed based on the extracted indirect call relations. Finally, the direct call patterns between functions are constructed based on the extracted direct call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nThe flowchart in Figure 8 illustrates the multi-task learning framework for real estate appraisal. The framework consists of three main components: data warehouse, feature construction, and representation learning. The data warehouse stores data from multiple sources, such as transaction data, community data, and user profile data. The feature construction component extracts features from the data warehouse and constructs a feature matrix for each task. The representation learning component learns a representation for each task using a multi-task learning algorithm. The learned representations are then used to predict the target variable for each task.",
        "reference": "Figure 2: The framework overview of MugRep."
    },
    "2202.10337v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace 1 (C1 \u2192 C2 \u2192 C3 \u2192 C4 \u2192 C5 \u2192 C6) and trace 2 (C1 \u2192 C2 \u2192 C5), the direct call relations can be extracted as C1 \u2192 C2, C2 \u2192 C3, C3 \u2192 C4, C4 \u2192 C5, C2 \u2192 C5. In addition, the indirect call relations can be identified as C1 \u2192 C4, C1 \u2192 C5, C1 \u2192 C6. Note that C2 \u2192 C4 and C3 \u2192 C5 are not indirect call relations, because there exist direct call relations between them. The indirect call patterns can be further extracted from the indirect call relations. For example, the indirect call pattern (C1, C2) indicates that C1 and C2 are indirectly related through C2 \u2192 C3 \u2192 C4. \n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn complex relationships between the input tokens. The output of the final encoder is a sequence of vectors, where each vector represents the learned representation of the corresponding input token.\n\nThe figure shows the knowledge discovery process. The process starts with domain experts' knowledge, which is often qualitative and unstructured. This knowledge is then transformed into a quantitative and structured representation, which can be processed by machines. The machine learning-based knowledge discovery methods are then applied to the structured knowledge to extract useful patterns and relationships. Finally, the discovered knowledge is applied to real-world applications, such as simulation, inverse modeling, discovery of physical laws, and interpretation.",
        "reference": "Figure 1: Schematic diagram of the relationship between knowledge embedding and knowledge discovery"
    },
    "30595348-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to learn temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph, and the indirect call relations are used to construct the indirect call graph. The direct call graph and the indirect call graph are then used to learn temporal relations between the functions.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the final encoder layer is a sequence of vectors, each of which represents the meaning of a word in the input sequence. These vectors are then used to predict the next word in the sequence.\n\nFigure 8 shows how to use contrastive learning to train a Siamese network for face verification. The Siamese network consists of two identical subnetworks, each of which is a convolutional neural network (CNN). The two subnetworks share the same weights. The input to the Siamese network is a pair of images, one of which is a photo of a person and the other is a photo of a different person. The output of the Siamese network is a vector that represents the similarity between the two images. The Siamese network is trained to minimize the distance between the vectors that represent similar images and to maximize the distance between the vectors that represent dissimilar images.",
        "reference": "Fig. 1. An illustration of the proposed reversed attention network (RAN), where the lower and upper branches learn features and predictions that are and are not associated with a target class, respectively. The mid-branch focuses on local regions with complicated spatial patterns whose object responses are weaker and provide a mechanism to amplify the response. The predictions of all three branches are fused to yield the final prediction for the segmentation task."
    },
    "1233699-Figure4-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented by a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented by a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect calls. The edges between the two sets of nodes represent the indirect call relations. The use cases are represented by ovals. The arrows between the use cases and the graphs represent the relationships between the use cases and the graphs.",
        "reference": "Fig. 4. The proposed network architecture for skeleton extraction, which is converted from VGG 16-layer net [36]. (a) Multi-task Scale-associated side outputs (SSOs) learning. Our network has 4 stages with SSO layers connected to the convolutional layers. Each stage branches into two sibling SSO layers, one for skeleton localization and the other for scale prediction, denoted by Loc-SSO (the left multi-color blocks) and ScalePred-SSO (the right blue block), respectively. The SSOs in each stage are guided by a scale-associated groundtruth skeleton map (The skeleton pixels with different quantized scales are in different colors. Each block in a Loc-SSO is the activation map for one quantized scale, marked by the corresponding color). (b) Scale-specific fusion. Each Loc-SSO provides a certain number of scale-specific skeleton score maps (identified by stage number-quantized scale value pairs). The score maps of the same scales from different stages will be sliced and concatenated. Five scale-specific weighted-fusion layers are added to automatically fuse outputs from multiple stages."
    },
    "2012.09688v3-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given a set of traces, the first step is to identify the direct call relations between functions. This is done by parsing the traces and extracting the caller and callee information from each trace. The direct call relations are then used to construct a call graph, which represents the call hierarchy of the program.\n\nOnce the call graph has been constructed, the next step is to identify the indirect call relations between functions. This is done by traversing the call graph and identifying the cases where a function calls another function that is not directly called by the caller of the first function. The indirect call relations are then used to construct an indirect call graph, which represents the transitive call relationships between functions.\n\nFinally, the direct and indirect call relations are used to extract the temporal relations between functions. This is done by identifying the cases where a function calls another function that is executed after the caller of the first function. The temporal relations are then used to construct a temporal relation graph, which represents the temporal ordering of functions in the program.",
        "reference": "Figure 3. Architecture of Offset-Attention. Numbers above tensors are numbers of dimensions N and feature channels D/Da, with switches showing alternatives of Self-Attention or Offset-Attention: dotted lines indicate Self-Attention branches."
    },
    "2110.10072v1-Figure1-1.png": {
        "prediction": " The image shows a schematic of the experimental setup used to measure the dynamic response of the material. A laser pulse is used to generate a stress wave in the material, which is then detected by a strain gauge. The strain gauge is connected to an oscilloscope, which is used to record the strain wave. The experiment is repeated for different values of the angle of incidence of the laser pulse.",
        "reference": "Fig. 1 Schematic diagram of a Split Hopkinson Pressure bar and top view of the flattened Brazilian disc sample before mounting"
    },
    "688013-Figure7-1.png": {
        "prediction": " The image shows a flowchart that illustrates how to use indirect call relations to improve the performance of a deep learning model. The flowchart takes as input a set of traces and a set of indirect call relations. The traces are then used to generate a set of direct call relations. The direct call relations are then used to generate a set of indirect call patterns. The indirect call patterns are then used to improve the performance of a deep learning model.",
        "reference": "Figure 7. Illustration of our approach for spatio-temporal action localization. Region proposals are detected and regressed with Faster-RCNN on RGB keyframes. Spatio-temporal tubes are classified with two-stream I3D convolutions."
    },
    "1379674-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to each other and generate a contextualized representation of the input sequence. The final output of the model is a vector, which can be used to predict the next token in the sequence or to classify the sequence as a whole.\n\nThe flowchart in Figure 8 shows the architecture of a Transformer model. The Transformer model is a type of neural network that is used for natural language processing tasks. The model consists of a stack of encoder layers and a stack of decoder layers. The encoder layers encode the input sequence into a fixed-length vector. The decoder layers then use the encoder's output to generate the output sequence.",
        "reference": "Figure 1. Inference in a 3-Level Pyramid Network [15]: The network G0 computes the residual flow v0 at the highest level of the pyramid (smallest image) using the low resolution images {I10 , I20}. At each pyramid level, the network Gk computes a residual flow vk which propagates to each of the next lower levels of the pyramid in turn, to finally obtain the flow V2 at the highest resolution."
    },
    "2011.05655v1-Figure1-1.png": {
        "prediction": " The image shows the process flow diagram of a typical nuclear fuel reprocessing plant. The process starts with the dissolution of the irradiated fuel in nitric acid. The dissolved fuel is then purified by solvent extraction using tributyl phosphate (TBP) in an organic solvent. The purified uranium and plutonium are then separated by ion exchange chromatography. The uranium is then further purified by solvent extraction using di-2-ethylhexyl phosphoric acid (DEHPA) in an organic solvent. The plutonium is then purified by anion exchange chromatography. The purified uranium and plutonium are then converted to their respective oxides and stored. The process also includes the recovery of the nitric acid and the treatment of the waste generated during the process.",
        "reference": "Figure 1: Schematic of the pilot plant experiment"
    },
    "2101.08003v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces (denoted as t1 and t2) of direct function calls between components C1, C2, C3, C4, C5, and C6, we can derive indirect relations between the components. For example, if t1 shows a direct relation from C1 to C2 and t2 shows a direct relation from C2 to C3, we can infer an indirect relation from C1 to C3. The indirect relations can be further categorized into indirect call patterns, where the intermediate components (e.g., C2 in the aforementioned example) are the same across traces.\n\n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The Transformer encoders are stacked together, and the output of the final encoder is used to predict the masked tokens in the input sequence.\n\n\nFigure 2(a) shows the schematic of the sensing principle of the SEF sensor based on the Kretschmann configuration. A linearly polarized light is incident from the bottom of the BK7 glass substrate. The evanescent field generated at the interface between the BK7 glass and the gold film penetrates the thin birefringent film and is reflected by the gold film. The reflected light is then collected and analyzed. The sensing region is defined by the area where the evanescent field intensity is above a certain threshold. The thickness of the birefringent thin film is optimized to maximize the sensitivity of the sensor. Figure 2(b) shows the simulated distribution of the electric field intensity |E|z in the sensing region for the optimized thickness of the birefringent thin film. The electric field intensity is strongest at the interface between the BK7 glass and the gold film, and it decays exponentially with the distance from the interface. The sensing depth, defined as the distance from the interface where the electric field intensity drops to 1/e of its maximum value, is about 100 nm.",
        "reference": "FIG. 1. A schematic of (a) simulation setup showing the major components as depicted. The plane of polarization is XY. The angle of incidence of the monochromatic light (L) is \u03b8i, thickness of each material and detector (D) are shown and (b) a single layer of shape anisotropic molecules (rod shaped) tilted with respect to X-axis along Y-axis on the YZ plane. The projection of the molecules is shown in black. Such projection resembles nematic ordering on 2D plane with optic axis along Y-axis."
    },
    "2202.03587v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values represent the number of indirect calls between the functions. The use case u1 represents the scenario where the indirect call patterns are used to identify the potential vulnerabilities. The use case u2 represents the scenario where the direct call relations are used to identify the potential vulnerabilities.",
        "reference": "Figure 2: The acoustic tokens, as output by the SpecTran network pretrained via CALP, are employed together with (text) words in a multimodal transformer. The multimodal transformer encodes the set of tokens with modality and positional information."
    },
    "2010.13130v1-Figure1-1.png": {
        "prediction": " The flowchart describes the training procedure of a machine learning model. It starts with the ingestion of data, which is then used to train the model. The model then makes predictions on new data, which are then scored. The flowchart then checks if the score is greater than a certain threshold. If it is, the model is retrained. If it is not, the model is deployed.",
        "reference": "Figure 1: AutoSpeech Challenge\u2019s evaluation process for one task defined by the 5-tuple: Dtr, D \u2205 te, L,BT , BS . Participants need to submit a strategy implemented by Python scripts which must contain a file named \u201dmodel.py\u201d. forgiving In this file, the two functions named train and test implement the logic of AutoSpeech algorithm. These two functions are called by the ingestion program (defined in ingestion.py) orderly, to train on Dtr and produce a prediction Y t pred on D \u2205 te at the timestamp t respectively. The prediction Y tpred is then compared to true labels Yte in scoring program (defined by score.py and produces a score st. This ingestion/scoring loop is controlled by time budget BT and a flag in model.py, to determine whether the whole program ends. At any time,the score sequence st0, st1, ... is visualized as a learning curve and the area under learning curve is used as the evaluation for this task. Note that only the run time of ingestion program is accumulated, but the time of scoring is not counted for running in parallel."
    },
    "2103.14969v2-Figure7-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the functions that are directly called by a function. The indirect call relations are the relations between the functions that are called by the functions that are directly called by a function. The indirect call patterns are the patterns of the indirect call relations. The direct call patterns are the patterns of the direct call relations.",
        "reference": "Fig. 7. V-Net with autoencoder regularization: \u201cSchematic visualization of the network architecture. Input is a four channel 3D MRI crop, followed by initial 3x3x3 3D convolution with 32 filters. Each green block is a ResNet-like block with the GroupNorm normalization. The output of the segmentation decoder has three channels (with the same spatial size as the input) followed by a sigmoid for segmentation maps of the three tumor subregions (WT, TC, ET). The VAE branch reconstructs the input image into itself, and is used only during training to regularize the shared encoder.\u201d [33]"
    },
    "2010.00702v1-Figure4-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a pair of images \\(I_1\\) and \\(I_2\\), we first extract appearance features using a pre-trained CNN, denoted as \\(F_1\\) and \\(F_2\\). We then compute the optical flow between the two images, denoted as \\(\\Phi_{flow}\\). Next, we warp the appearance features of the second image \\(F_2\\) using the optical flow to obtain the warped features \\(F_{1\\rightarrow2}\\). We then compute the photometric consistency loss between the warped features \\(F_{1\\rightarrow2}\\) and the appearance features of the first image \\(F_1\\). We also compute the smoothness loss on the warped features \\(F_{1\\rightarrow2}\\) to encourage smoothness in the warped image. Finally, we use the warped features \\(F_{1\\rightarrow2}\\) to synthesize the novel view \\(T_1\\).",
        "reference": "Figure 4: Our dual-view reflection removal. Given images I1 and I2, our reflection-invariant optical flow network \u03c6flow estimates the motion F1 2 between the unknown transmissive layers of the inputs, which is then used to warp I2 towards I1 to \u201cundo\u201d that motion. Our synthesis network \u03c6syn can then use these aligned images to leverage the parallax between the reflective layers (and the lack thereof between the transmissive layers) to synthesize I1\u2019s transmissive layer T1."
    },
    "2205.04319v1-Figure1-1.png": {
        "prediction": " An illustration of a decentralized ride-sharing platform. A customer sends a request to a broker, which collects requests from multiple customers and sends them to operators. Operators match the requests with available vehicles and send offers to the broker, which forwards them to the customers. The legend on the right explains the symbols used in the figure.",
        "reference": "Figure 1: Illustration of an AMOD Broker System"
    },
    "2205.11782v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nThe flowchart in Figure 8 illustrates the training of a neural network. The input to the network is a set of training data, which consists of pairs of input and output values. The network is trained by iteratively adjusting its weights so as to minimize the error between the predicted output values and the true output values.",
        "reference": "Figure 1: Illustration of our fine-grained data poisoning attacks on LDP-based mean/variance estimation."
    },
    "2107.09786v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect call relations between the functions. Finally, the direct call patterns and indirect call patterns are extracted from the direct call relations and indirect call relations, respectively.\n\nThe architecture of BERT in pretraining is shown in Figure 7. The model consists of a stack of transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The model is trained to predict the masked tokens in the input sequence.\n\nIn federated learning, the clients collaboratively train a global model without sharing their data. As shown in Figure 7(a), each client has a local dataset and a local model. The local models are trained on the local datasets, and the gradients are sent to the server. The server aggregates the gradients and updates the global model. The updated global model is then sent back to the clients, and the process is repeated. In Figure 7(b), we show how the cut layer is used to reduce the communication cost. The cut layer is placed after the first few layers of the neural network. The activations of the cut layer are sent to the server, and the gradients are computed on the server. The gradients are then sent back to the clients, and the local models are updated.",
        "reference": "Fig. 1. Overview of split learning scheme. (a) Split learning with K clients. (b) Communication and computation at the client level."
    },
    "2210.09964v2-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph, and the indirect call relations are used to construct the indirect call graph. The direct call graph and the indirect call graph are then used to extract the temporal relations between the API calls.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The token embeddings are then passed through a series of transformer layers, which learn to attend to each other and to the positional embeddings. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nFigure 4 shows the steps involved in the approach. First, the requirements are analyzed to identify the security-relevant entities and actions. Second, the system model is built based on the identified security-relevant entities and actions. Third, the attack scenarios are generated based on the system model. Fourth, the attack scenarios are analyzed to identify the potential risks. Finally, the identified risks are evaluated and prioritized.",
        "reference": "Figure 1: Overview of our translation."
    },
    "2010.06417v2-Figure2.12-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to learn temporal relations between API calls. Given two traces (denoted as Trace 1 and Trace 2), which consist of API calls C1, C2, C3, C4, C5, and C6, the direct call relations and indirect call relations can be extracted. The direct call relations are represented by the edges between the API calls in the traces. For example, there is a direct call relation from C1 to C2 in both traces. The indirect call relations are represented by the dashed edges between the API calls in the traces. For example, there is an indirect call relation from C1 to C4 in Trace 1. The indirect call patterns are represented by the boxes on the right side of the figure. Each box represents a unique indirect call pattern. For example, the first box represents the indirect call pattern C1->C2->C4.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The model is trained to predict the masked tokens in the input sequence.\n\nFigure 8 shows an unfolded view of a recurrent neural network (RNN). The RNN is composed of a series of interconnected nodes, where each node represents a hidden state \\(h_t\\) at time step \\(t\\). The hidden state is updated at each time step based on the previous hidden state \\(h_{t-1}\\) and the input \\(x_t\\) at time step \\(t\\). The output of the RNN is a sequence of hidden states \\(h_1, h_2, ..., h_n\\).",
        "reference": "Figure 2.12: A recurrent network with no outputs. This recurrent network just processes information from the input x by incorporating it into the state h that is passed forward through time. (Left) Circuit diagram. The black square indicates a delay of a single time-step. Right The same network seen as an unfolded computational graph, where each node is now associated with one particular time instance."
    },
    "2110.02729v1-Figure3-1.png": {
        "prediction": " An illustration of execution traces and temporal relations. Given two traces t1 and t2 of the execution of a program, we can derive direct call relations (C1, C2) and indirect call relations (C1, C2) between functions. We can also derive indirect call patterns between functions, such as (C1, C2), (C1, C6), and (C2, C6).",
        "reference": "Figure 3. The block diagram of the proposed time domain offset cancelation technique."
    },
    "2103.04234v2-Figure6-1.png": {
        "prediction": " ## Figure 1: Illustration of execution traces and temporal relations.\n\nThe figure shows how to extract direct and indirect relations from execution traces. Given two traces `Tr1` and `Tr2`, where `Tr1` = `<C1, C2, C3, C4, C5, C6>` and `Tr2` = `<C1, C2, C5>`, we can identify direct call relations between methods, as shown in the leftmost box. For example, there is a direct call relation from `C1` to `C2` because `C2` is called after `C1` in both traces. We can also identify indirect call relations between methods, as shown in the middle box. For example, there is an indirect call relation from `C1` to `C4` because `C4` is called after `C2`, which is called after `C1` in `Tr1`. Finally, we can identify indirect call patterns between methods, as shown in the rightmost box. For example, there is an indirect call pattern `<C1, C2, C4>` because this pattern appears in both traces.\n\n## Figure 7: Pretraining BERT.\n\nThe figure shows the architecture of BERT in pretraining. The model consists of a stack of transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the model is a sequence of vectors, each of which represents the hidden state of the corresponding token in the input sequence.\n\n## Figure 8: Overview of the Raft consensus protocol.\n\nThe figure shows an overview of the Raft consensus protocol. The protocol consists of a set of servers, each of which can be in one of three states: leader, follower, or candidate. The leader is responsible for coordinating the replication of data between the servers. The followers replicate the data from the leader. The candidates compete to become the leader if the current leader fails. The figure shows the steps involved in the Raft consensus protocol. In the first step, the servers elect a leader. In the second step, the leader sends heartbeats to the followers. In the third step, the followers replicate the data from the leader. In the fourth step, the leader commits the data to the database.",
        "reference": "Fig. 6: The Paxi architecture"
    },
    "2206.10375v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to learn temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph, and the indirect call relations are used to construct the indirect call graph. The direct call graph and the indirect call graph are then used to learn temporal relations between the functions.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is trained to predict the masked words in a sentence, and to predict the next sentence in a sequence of sentences. The model is composed of a stack of transformer encoders, which are each composed of a self-attention layer and a feed-forward layer. The model also has a positional encoding layer, which is used to encode the position of each word in a sentence.\n\nFigure 8 shows the stereo matching pipeline of our approach. Given a pair of stereo images, we first convert the images to grayscale and apply a Gaussian filter to reduce noise. Then, we compute the disparity estimates for each pixel in the left and right images using a dual-channel CNN. The disparity estimates are then refined using a Laplacian filter and a contrast map. Finally, the refined disparity maps are combined to generate the final disparity map.",
        "reference": "Fig. 1. Overview of proposed ME2Stereo-Du2CNN architecture: The input consists of three stereo pair of the same scene captured at different camera exposure level and the output is a refined disparity map (R\u0302). The input is passed through novel Dual CNN architecture to get the disparity estimates at three exposure levels. The estimates are fused using two quality measures: well-exposedness and contrast, which in-turn are obtained respectively from the input left-view image and the disparity estimate of the corresponding stereo pair. The predicted refined disparity map (R\u0302) is a better estimate compared to three intermediate disparity estimates."
    },
    "2101.08386v4-Figure5-1.png": {
        "prediction": " The image shows how to use traces to learn temporal relations. Given two traces `Trace 1` and `Trace 2`, which consist of the direct call relations `C1`, `C2`, `C3`, `C4`, `C5`, and `C6`, we can learn both direct and indirect call patterns. That is, given a call `C1`, we can predict the next call `C2` and also predict indirect calls `C4` and `C6`.",
        "reference": "Figure 5: (Handwritten digits) Diagram of the learning model used in the handwritten digits experiment. The model is obtained by concatenating a computer vision (CV) model and an identity effect (IE) model. From right to left: The model takes a pair of images as input (right); then, the CV model classifies them or, equivalently, encodes them as two 10-dimensional probability vectors (center); finally, the IE model assigns a rating from 0 to 1 to the pair of probability (or encoding) vectors to identify whether the images represent identical digits or not (left)."
    },
    "2204.07837v2-Figure3-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given two traces Tr1 and Tr2 of a program execution, each containing a sequence of function calls, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where each node represents a function and each edge represents a direct call from one function to another. The indirect call relations are represented as a set of pairs of functions, where each pair represents two functions that may call each other indirectly. The figure also shows how the indirect call patterns are extracted from the indirect call relations. Each indirect call pattern is a sequence of functions, where each function in the sequence may call the next function in the sequence indirectly.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, each of which is represented by a word embedding. The output of the model is a sequence of hidden vectors, each of which represents the meaning of the corresponding token in the input sequence.\n\nFigure 8 shows the training procedure of BERT. The model is trained on a large corpus of text. The training objective is to maximize the probability of the correct next word in a sequence, given the previous words in the sequence. The model is trained using a masked language modeling task, in which a certain percentage of the words in the input sequence are masked and the model is trained to predict the masked words.",
        "reference": "Fig. 3. The illustration of our proposed self-supervised input representation (Section III-C) in sequence-to-sequence learning framework. We add two classifier to predict the token and position of perturbed tokens synthesized by the smooth augmented data generator in Section III-B. The meaning of blue rectangle and green rectangle is the same as in Figure 2. The red rectangles represent disturbed tokens\u2019 intermediate representation produced by the top layer of encoder."
    },
    "11277821-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call patterns and indirect call patterns are then extracted from the direct call relations and indirect call relations, respectively.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nFigure 8 shows how a variational autoencoder (VAE) can be used to learn a disentangled representation of a dataset. The VAE consists of an encoder and a decoder. The encoder takes as input a data point and produces a latent representation. The decoder takes as input the latent representation and produces a reconstruction of the data point. The VAE is trained to minimize the reconstruction error and the Kullback-Leibler (KL) divergence between the prior distribution and the posterior distribution.",
        "reference": "Figure 1: Architecture of the Wasserstein Deep Learning: two samples are drawn from the data distribution and set as input of the same network (\u03c6) that computes the embedding. The embedding is learnt such that the squared Euclidean distance in the embedding mimics the Wasserstein distance. The embedded representation of the data is then decoded with a different network (\u03c8), trained with a Kullback-Leibler divergence loss."
    },
    "2010.10246v2-Figure1-1.png": {
        "prediction": " Component diagram of the machine learning pipeline. The dataset repository contains the raw data. The library repository contains the code for data cleansing, feature extraction, and model training. The pipeline repository contains the code for the machine learning pipeline. The ML Platform is the platform on which the machine learning pipeline is deployed.",
        "reference": "Fig. 1. The architecture of MLCask for supporting collaborative pipeline development with version control semantics."
    },
    "2202.07638v2-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces (Trace 1 and Trace 2) with their corresponding direct call relations, the indirect call relations between the functions are inferred. The direct call relations and indirect call relations are then used to generate indirect call patterns.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe figure shows a\u5b95 three-layer neural network with  \\(l_1\\), \\(l_2\\), and \\(l_3\\) layers. Each layer consists of a number of neurons (depicted as circles), which are connected to the neurons in the previous and next layers (depicted as lines). The neurons in the input layer are connected to the external environment, and the neurons in the output layer are connected to the external environment. The disturbance \\(d\\) affects the neurons in the first hidden layer.",
        "reference": "Fig. 1. Schematic representation of the multiplex network considered in this paper. Black nodes denote the agents while blue nodes represent the (possible) leaders."
    },
    "2101.05329v1-Figure1-1.png": {
        "prediction": " The figure shows the architecture of the image compression algorithm. The algorithm takes an uncompressed byte array as input and produces a compressed byte array as output. The compression algorithm consists of the following steps:\n1. Burrows-Wheeler Transform\n2. Run-length encoding\n3. Huffman Encoding\n\nThe decompression algorithm consists of the following steps:\n1. Inverse Huffman decoding\n2. Inverse run-length decoding\n3. Inverse Burrows-Wheeler Transform",
        "reference": "Figure 1: Schematic illustration of the proposed algorithm."
    },
    "2110.06972v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given a pair of execution traces (e.g., trace t1 and trace t2), the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a set of triples, where each triple represents an indirect call relation between two functions through a third function. The extracted direct call relations and indirect call relations are used to generate the indirect call patterns. Each indirect call pattern represents a set of indirect call relations that share the same calling context. The direct call patterns are represented as a set of pairs, where each pair represents a direct call relation between two functions.",
        "reference": "Figure 2: Proposed ZeUS algorithm. The components shown in green (i.e. observation encoder, context encoder, dynamics model and reward model) are shared across tasks. Components/representations in red or blue belong to separate tasks."
    },
    "2110.14705v2-Figure10-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the callers and callees in the traces. The indirect call relations are the relations between the callers and callees that are not directly called. The indirect call patterns are the patterns of the indirect call relations. The figure shows an example of the direct call relations, indirect call relations, and indirect call patterns.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence. The model is trained to predict the next word in a sequence, given the previous words in the sequence. The figure shows the input to the model, which is a sequence of tokens. The tokens are embedded into vectors, and the vectors are then passed through the encoders. The output of the encoders is a sequence of vectors, which are then passed through the output layer to predict the next word in the sequence.\n\nFigure 8 shows the energy landscape of a spin glass. The energy landscape is a plot of the energy of the spin glass as a function of the configuration of the spins. The spins can be in either an up or down state, and the energy of the spin glass is determined by the interactions between the spins. The figure shows a spin glass with a double-well energy landscape. This means that there are two energy minima, and the spin glass can be in either of these minima. The figure also shows a bubble of size R, which is a region of the energy landscape where the energy is higher than the energy of the minima. The bubble of size R is a metastable state, and the spin glass can be trapped in this state for a long time.",
        "reference": "FIG. 10. A candidate low-order process leading to the meltdown of the order parameter. In the main text, we prove that energy resonance between the two configurations is not possible under a generic assumption of strong incommensurability of the couplings."
    },
    "2012.06275v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations can be inferred. For example, if C1 calls C2 in Tr1, and C2 calls C3 in Tr2, then an indirect call relation from C1 to C3 can be inferred. In this example, the indirect call patterns (C1, C2) and (C2, C3) can be extracted from traces Tr1 and Tr2, respectively. In addition, direct call patterns (C1, C2) and (C2, C3) can also be extracted.",
        "reference": "Fig. 1. The convolutional deep autoencoder (DAE(C)) architecture."
    },
    "2205.13113v1-Figure3-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct execution traces and extract temporal relations. Given two traces Tr1 and Tr2 of API calls, we first identify the direct call relations and indirect call relations, respectively. A direct call relation (C1, C2) indicates that C2 is called by C1. An indirect call relation (C1, C2) indicates that C2 is called by a function that is called by C1. We then extract the indirect call patterns from the indirect call relations. An indirect call pattern is a sequence of indirect call relations. For example, (C1, C2), (C2, C3), (C3, C4) is an indirect call pattern. Finally, we use the direct call relations and indirect call patterns to construct the execution traces. An execution trace is a sequence of API calls that captures the dynamic behavior of a program. For example, Tr1 =<C1, C2, C3, C5, C6> and Tr2 =<C1, C4, C6> are two execution traces.",
        "reference": "Fig. 3. Illustration of the training process."
    },
    "2206.10128v2-Figure1-1.png": {
        "prediction": " The flowchart describes the process of using a pre-trained language model to generate a response to a question. The question is first passed through a query generation model, which generates a query that is then used to retrieve relevant documents from a document store. The retrieved documents are then passed through a cross-encoder model, which generates a relevance score for each document. The documents are then ranked according to their relevance scores, and the top-ranked documents are passed to a decoder, which generates a response to the question.",
        "reference": "Figure 1: Overview of the proposed DSI-QG indexing framework, where a query generator (yellow box) is used to generate potential queries, which in turn are used to represent documents for indexing. The architecture of the method includes a cross-encoder ranker (gray box) that is used to select only promising queries to be sent for indexing."
    },
    "2202.13078v2-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call patterns and indirect call patterns are then generated.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe model is trained using a masked language modeling objective, which means that it is trained to predict the masked tokens in a sequence. This objective encourages the model to learn to attend to the context of a word in order to predict its meaning. The model is also trained using a next sentence prediction objective, which means that it is trained to predict whether two sentences are consecutive in a document. This objective encourages the model to learn to understand the relationships between sentences.",
        "reference": "Fig. 1: Model architecture used in the pretraining phase of the proposed method."
    },
    "2102.00719v1-Figure1-1.png": {
        "prediction": " The flowchart shows the process of how a neural network model is trained to perform video classification. The model is first trained on a dataset of videos that have been labeled with their corresponding classes. The model learns to identify the features in the videos that are most relevant to the classification task. Once the model has been trained, it can be used to classify new videos. The model takes a new video as input and produces a probability distribution over the possible classes. The class with the highest probability is the predicted class for the video.\n\nThe flowchart also shows how the model can be used to generate new videos. The model is first trained on a dataset of videos that have been labeled with their corresponding classes. The model learns to identify the features in the videos that are most relevant to the classification task. Once the model has been trained, it can be used to generate new videos by sampling from the probability distribution over the possible classes. The generated videos will have the same distribution of classes as the training data.",
        "reference": "Figure 1: Video Transformer Network architecture. Connecting three modules: A 2D spatial backbone (f(x)), used for feature extraction. Followed by a temporal attentionbased encoder (Longformer in this work), that uses the feature vectors (\u03c6i) combined with a position encoding. The [CLS] token is processed by a classification MLP head to get the final class prediction."
    },
    "2012.02670v2-Figure8-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2, the direct call relations and indirect call relations can be extracted. Moreover, indirect call patterns can be mined from the indirect call relations. Finally, direct call patterns can be extracted from the direct call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nFigure 8 shows the federated averaging algorithm. The algorithm begins by initializing a global model on the server. Each client then downloads the global model and uses it to train a local model on its own data. The local models are then uploaded back to the server, where they are averaged to produce a new global model. This process is repeated until the global model converges.",
        "reference": "Figure 8: Schematic representation of the training process of the server\u2019s networks for the attribute inference attack. In the figure, the network \ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61 substitutes \ud835\udc53 \u22121 and \ud835\udc52\ud835\udc5b refers to a suitable entropy measure for the classification task."
    },
    "2205.10688v1-Figure2-1.png": {
        "prediction": " The image shows an overview of the training pipeline of our approach. Given a set of agents and constraints, a single-agent policy is trained. Based on this policy, a group of agents with different morphologies is created. After evaluating the agents in the environment, the best agents are selected and used to create a new generation of agents through crossover and mutation. This process is repeated until the termination criterion is met. The arrows between the blocks indicate the data flow and the direction of the optimization process.",
        "reference": "Fig. 2. Overview: The agent is either generated randomly or with user support. The user also defined constraints (yellow arrows) (a). The initial Proximal Policy Optimization (PPO) trains the input agent to provide baseline agent policy (b). The system then creates variants of the initial model (c) and trains them all together with universal PPO (d). Selection (e), crossover, and mutation (f) create a new generation that is trained again. The system outputs the best(s) co-designed agents and their policies (g)."
    },
    "1241810-Figure5-1.png": {
        "prediction": " The flowchart\u4e00\u958b\u59cb\u662f\u6b0a\u91cd\u548c\u504f\u5dee\u8a13\u7df4\uff0c\u63a5\u8457\u662f\u6b0a\u91cd\u91cf\u5316\uff0c\u7136\u5f8c\u662f\u6b0a\u91cd\u51cd\u7d50\u548c\u504f\u5dee\u5b78\u7fd2\uff0c\u6700\u5f8c\u662f\u90e8\u7f72\u5230\u795e\u7d93\u5f62\u614b\u8a08\u7b97\u7cfb\u7d71\u548c\u5fc6\u963b\u5668\u7de8\u7a0b\u3001\u504f\u5dee\u7de8\u7a0b\u3002",
        "reference": "Fig. 5. The framework of proposed bias tuning method."
    },
    "2011.12453v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, we aim to learn indirect call patterns. We first construct the direct call graphs for Tr1 and Tr2, where each node represents a function and an edge indicates a direct call relation between two functions. Then, we identify the common subgraphs between the two graphs, which correspond to the indirect call patterns. \n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The output of the final Transformer encoder is fed to a softmax layer to predict the next token in the sequence.\n\nFigure 8 illustrates the architecture of the GNN employed in our method. The GNN consists of multiple layers. Each layer consists of a graph convolution network (GCN) layer and a readout layer. The GCN layer aggregates information from the neighboring nodes in the graph, while the readout layer produces a node embedding for each node in the graph. The output of the final GCN layer is fed to a softmax layer to predict the labels of the nodes in the graph.",
        "reference": "Figure 1: (a) Illustration of graph neural ansatz. (b) Distributed workflow of VMC using ISGO."
    },
    "2202.04161v1-Figure3-1.png": {
        "prediction": " In this paper, we propose a neural model for ad-hoc SQL query generation from context. Given a set of structured data and a user query in natural language, the goal is to generate an SQL query that retrieves the most relevant data items from the database. Our model consists of an encoder that encodes the structured data and the user query into fixed-length vectors, and a decoder that decodes the vectors into an SQL query. We train the model end-to-end using a reinforcement learning algorithm. Experiments on two real-world datasets show that our model outperforms strong baselines.",
        "reference": "Figure 3: Task structure for the generative model."
    },
    "2107.00466v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations can be inferred as C1 \u2192 C4, C2 \u2192 C4, C2 \u2192 C6, and C3 \u2192 C5. The inferred indirect call relations, together with the direct call relations, form the indirect call patterns {C1, C4}, {C2, C4}, {C2, C6}, and {C3, C5}.\n\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The token embeddings are then passed through a series of transformer layers, which attend to each other and to the positional embeddings. The output of the transformer layers is then fed into a softmax layer, which predicts the probability of each token in the vocabulary.\n\nThe next sentence prediction task is a binary classification task that predicts whether two sentences are consecutive in a document. The input to the model is a pair of sentences, which are embedded into vectors using a sentence embedding layer. The sentence embeddings are then passed through a series of transformer layers, which attend to each other and to the positional embeddings. The output of the transformer layers is then fed into a softmax layer, which predicts the probability that the two sentences are consecutive.",
        "reference": "FIG. 1. Illustration of the inverse renormalization group. Inverse transformations are applied to iteratively increase the size L in each dimension by a factor of b, therefore evading the critical slowing down effect."
    },
    "2204.00034v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces (Trace 1 and Trace 2) of direct call relations (left), the direct call patterns (C1, C2) between the callers (C1) and callees (C2) are extracted. Similarly, the indirect call relations and indirect call patterns are also extracted.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The token embeddings are then passed through a series of Transformer encoder layers, which learn to attend to each other and to the positional embeddings of the tokens. The output of the Transformer encoder layers is then passed through a feedforward network and a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 illustrates how masked language modeling is used to pretrain BERT. In masked language modeling, a certain percentage of the tokens in the input sequence are randomly masked, and the model is then trained to predict the masked tokens. This task helps the model to learn the relationships between different words in the sequence, and to make predictions based on context.",
        "reference": "Fig. 1. Schematic comparison of sequential proof-of-work (Bitcoin, left) and parallel proof-of-work (proposed, right). Boxes represent blocks, checkmarks represent proof-of-work solutions, and arrows represent hash-references."
    },
    "1508199-Figure2-1.png": {
        "prediction": " In Figure 1, given two traces Tr1 and Tr2 of direct call relations, the indirect call relations can be inferred. For example, since C1 calls C2 in Tr1, and C2 calls C3 in Tr2, we can infer that C1 indirectly calls C3. All the indirect call relations can be computed in this way. In addition, the direct call patterns can also be extracted from the traces. For example, from Tr1 and Tr2, we can extract the direct call patterns <C1, C2>, <C2, C3>, <C3, C4>, <C4, C5>, and <C5, C6>. \n\nIn Figure 7, the input to the Transformer-based language model is a sequence of tokens, where each token is represented by a word embedding. The word embeddings are then passed through a series of Transformer layers, which attend to each other and to the positional embeddings. The output of the Transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nIn Figure 8, the affinity matrix A is formed by computing the pairwise distances between all data points. Spectral clustering is then used to cluster the data points into k clusters. The test point x_t is then assigned to the cluster with the smallest mean distance to x_t. If the cluster contains a certain number of labeled data points, then x_t is assigned the label of the majority of the labeled data points in the cluster. Otherwise, x_t is actively queried for its label.",
        "reference": "Figure 2. Diagram of SUPERPAC algorithm for pairwise constrained clustering."
    },
    "2012.12683v1-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of the approach. Given two traces Tr1 and Tr2 of the same program execution, we aim to learn the direct and indirect relations between the components. We first construct the component-level call graphs for each trace, where nodes represent components and edges represent direct calls between components. We then identify indirect relations between components by analyzing the execution order of components in the traces. Specifically, we say that component A has an indirect relation with component B if there exists a sequence of components C1, C2, ..., Cn such that A directly calls C1, C1 directly calls C2, ..., Cn-1 directly calls Cn, and Cn directly calls B. We also consider indirect relations between components that are mediated by other components. For example, if component A calls component B, and component B calls component C, then we say that A has an indirect relation with C. We represent the direct and indirect relations between components using a bipartite graph, where one set of nodes represents components and the other set of nodes represents relations. The edges between the two sets of nodes represent the direct and indirect relations between components. We use this bipartite graph to learn a model that can predict the direct and indirect relations between components given a pair of traces.\n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the first transformer encoder is a sequence of token embeddings, which are learned by a word embedding layer and a positional embedding layer. The output of the last transformer encoder is a sequence of hidden state vectors, which are then used by a final output layer to predict the masked tokens.\n\nThe left-hand side of Figure 2 shows the deployment network of the CSP subsystem. The deployment network defines the physical connectivity between the components of the CSP subsystem. The right-hand side of Figure 2 shows the components of the CSP subsystem. Each component is represented by a box, and the boxes are connected by lines that represent the communication links between the components. The CSP subsystem consists of the following components:\n\n- The NCS component is responsible for controlling the operation of the CSP subsystem.\n- The ICS component is responsible for interfacing with the other subsystems of the spacecraft.\n- The ACS component is responsible for controlling the attitude of the spacecraft.\n- The EPS component is responsible for providing power to the spacecraft.\n- The Propulsion component is responsible for controlling the movement of the spacecraft.\n- The COM component is responsible for communicating with the ground station.",
        "reference": "Figure 1. Left: Network control architecture of SOXS. Right: Components of the SOXS software; red boxes represent software requiring custom configuration or development, green boxes represent VLTSW components that will be used without modifications."
    },
    "2103.07018v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are represented by the edges between the caller and callee functions in the direct call graph. The indirect call relations are represented by the edges between the caller and callee functions in the indirect call graph. The indirect call patterns are represented by the subgraphs of the indirect call graph.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nFigure 8 shows the interleaving learning and block learning strategies. In interleaving learning, the model is trained on one task at a time, but the tasks are interleaved in a random order. In block learning, the model is trained on all tasks at once, but the tasks are presented in blocks.",
        "reference": "Figure 1: Comparison between interleaving learning and block learning. In interleaving learning, we perform task 1 for a short while, then move to task 2, then task 3. Afterwards, we move from task 3 back to task 1. This process iterates where each task is performed for a short time period before switching to another task. In contrast, in block learning, we perform task 1 to the very end, then move to task 2, and later task 3. Each task is performed for a long time period until it is completed. Once a task is finished, it will not be performed again."
    },
    "2107.02704v1-Figure3-1.png": {
        "prediction": " Figure 1 shows an overview of how execution traces are generated and how temporal relations are extracted. Given two traces Tr1 and Tr2 of API calls, each node in the graph represents an API call, and a directed edge between two nodes indicates the calling relation between the corresponding API calls. We use C1 and C2 to denote the caller and callee, respectively. For example, in Tr1, C1 calls C2, C3, C4, and C5 in sequence, and C2 calls C6. Note that the direct call relations are extracted based on the traces. Besides, we can also derive indirect call relations. For example, since C1 calls C2 in Tr1, and C2 calls C6 in Tr1, we can infer that C1 indirectly calls C6. The indirect call relations are transitive. \n\nFigure 7 shows the overall architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The model is trained on a large corpus of text data, and the pre-trained model can be fine-tuned for various NLP tasks.\n\nFigure 2 shows the FLASH MRI acquisition and FLASH MRI forward model. The FLASH MRI acquisition is performed on an ex vivo human brain sample. The FLASH MRI forward model takes the tissue parameters as input and generates the simulated FLASH MRI signal as output. The tissue parameters are estimated using a two-step process. First, the proton density (PD) map is estimated using a conventional MRI sequence. Second, the T1 and T2* maps are estimated using the FLASH MRI forward model and a nonlinear least-squares optimization algorithm.",
        "reference": "Figure 3: Training Data simulation steps. We estimated tissue parameters p from a large 12-contrast FLASH MRI acquisition containing multiple flip angles (FA) and echo time (TE) scanner parameters for 22 different ex vivo human brain specimens using a dictionary-based method [20]. Estimated tissue parameters were used as ground truth to simulate training data."
    },
    "2202.06076v1-Figure2-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct execution traces and derive temporal relations from them. Given two traces Tr1 and Tr2 of API calls, we identify the direct call relations and indirect call relations between the APIs. We use these relations to construct the API dependency graph. \n\nFigure 7 shows the overall architecture of BERT. BERT consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input features. The output of the final Transformer encoder is a sequence of vectors, where each vector represents the learned representation of the corresponding input token.\n\nFigure 8 shows the Transformer encoder layer. The Transformer encoder layer consists of a self-attention sub-layer, a feed-forward sub-layer, and a residual connection between the input and output. The self-attention sub-layer allows the model to attend to different parts of the input sequence, while the feed-forward sub-layer allows the model to learn non-linear relationships between the input features. The residual connection allows the model to learn long-term dependencies in the input sequence.",
        "reference": "Fig. 2: The overview of method. We extend a multi-layer transformer pre-trained on textual data with imaging input. The images are provided as features extracted from a ResNet50 network. The features are reshaped to 49 vectors of 2048 dimensions each and combined with two embeddings describing segment (image or text) and position of the token."
    },
    "2010.00150v1-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates the process of extracting temporal relations between events from execution traces. Given two traces, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between them. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values represent the number of indirect calls between them.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nThe flowchart in Figure 8 shows the architecture of a neural machine translation model. The model consists of an encoder and a decoder. The encoder reads the input sentence and produces a vector representation of its meaning. The decoder then uses this vector to generate the output sentence. The attention layer allows the decoder to attend to different parts of the input sentence when generating each output token.",
        "reference": "Figure 3: Attentional Encoder-Decoder architecture with each supervision method shown."
    },
    "2205.00186v2-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. In the first step, the direct call relations are extracted from the code. In the second step, the indirect call relations are extracted. In the third step, the indirect call patterns are extracted. Finally, the direct call patterns are extracted. The extracted information is used to generate the system dependence graph (SDG).\nFigure 7 shows the architecture of BERT in pretraining. \nThe image in Figure 2 shows how ReCo works. In the warm-up stage, a CNN is trained to distinguish between clean and noisy images. In the ReCo stage, the noisy images are fed into the CNN to obtain their logits. The logits are used to calculate the similarity between the noisy images and the clean images. The noisy images are then assigned with pseudo labels according to their similarity to the clean images. The pseudo labels are used to train another CNN for image classification.",
        "reference": "Figure 2: An overview of the proposed LC-Booster framework. We first warm up the model for a few iterations, and then fit a GMM to the loss distribution to separate clean or noisy sets. We then adopt H-Aug, which enforces consistency between weak-weak and weak-strong views. At the middle of training, we perform ReCo to revise the labels for all samples. The revised labels are used for GMM and CE Loss in the rest of training."
    },
    "211532678-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nThe flowchart in Figure 8 shows the architecture of a neural network with an adversarial loss. The input to the network is a data point x, which is passed through a function f to produce an output y. The output y is then passed through a function G to produce an adversarial example x_adv. The adversarial example x_adv is then passed through the function f again to produce an output y_adv. The difference between y and y_adv is then used to compute an adversarial loss, which is used to train the network.",
        "reference": "Figure 1. Adversarial training framework of image reconstruction network f , jointly with another network G, generating the additive perturbations"
    },
    "14194315-Figure2-1.png": {
        "prediction": " The flowchart shows the steps to generate a video prediction model. \n\nThe first step is to take in a video as input. The video is then passed through a series of convolutional layers. The convolutional layers help to extract features from the video. The features are then passed through a fully connected layer. The fully connected layer helps to classify the video into different categories. The final step is to output the predicted labels.\n\nThere are three different types of video prediction models shown in the image. The first model is a simple convolutional neural network (CNN). The second model is a recurrent neural network (RNN). The third model is a convolutional LSTM (ConvLSTM). The ConvLSTM model is a combination of a CNN and an LSTM. The CNN helps to extract features from the video, and the LSTM helps to learn the temporal relationships between the features.",
        "reference": "Figure 2: Schematic representation of deep neural networks for supervised and unsupervised learning. We use pink boxes to denote convolutional layers, yellow boxes denote rectified linear unit layers and green boxes indicate normalization layers. Our technique combines unsupervised learning approaches (a) with labeled prediction (b) to predict gestures using massive amounts of unlabeled data and few labeled samples. Autoencoder (a) is used to initialize weights and then predictor (b) is fine tuned to predict labels."
    },
    "2207.06543v1-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given a pair of traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect calls. The edges between the two sets of nodes represent the indirect call relations. Finally, the direct call patterns and indirect call patterns are extracted from the direct call relations and indirect call relations, respectively. The direct call patterns are represented as a set of tuples, where each tuple represents a sequence of direct calls. The indirect call patterns are represented as a set of tuples, where each tuple represents a sequence of indirect calls.\n\nThe flowchart in Figure 7 illustrates the pre-training of the BERT model. The input to the model is a sequence of tokens, which are represented as word embeddings. The word embeddings are then passed through a series of Transformer layers. Each Transformer layer consists of a self-attention mechanism and a feed-forward network. The self-attention mechanism allows the model to attend to different parts of the input sequence, while the feed-forward network allows the model to learn non-linear relationships between the input tokens. The output of the Transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the true distribution of the next token in the sequence.\n\nThe flowchart in Figure 8 illustrates the process of continual learning in the multi-task setting. The model is first trained on a sequence of tasks, denoted as Task 1, ..., Task T. For each task, the model is trained to minimize the empirical loss on the task data. After the model has been trained on all the tasks, it is evaluated on a new task, denoted as Task T+1. The model is then retrained on the new task, and the process is repeated for the next task.",
        "reference": "Fig. 3. Illustration of simultaneously promoting learning plasticity and memory stability in continual learning, where arrows represent the tightening process."
    },
    "2012.10294v4-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of how traces are generated and used to learn temporal relations. Given a set of traces, we first identify the direct call relations between functions. We then use these direct call relations to infer indirect call relations. Finally, we use the indirect call relations to learn temporal relations between functions.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of transformer encoders, which are used to learn the relationships between words in a sentence. The output of the transformer encoders is then used to predict the next word in the sentence.\n\nFigure 8 shows the 20-fold cross-validation results of the proposed method on the ADNI dataset. The proposed method achieves an average accuracy of 90.5% and an average AUC of 95.2%. These results demonstrate that the proposed method is able to effectively classify AD and MCI patients from healthy controls.",
        "reference": "Figure 1 Data flow chart and convolutional neural network structure."
    },
    "2103.12279v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces (trace 1 and trace 2) of direct call relations (C1 and C2), the indirect call relations between C1 and C2 are generated. The direct call patterns between C1 and C2 are also generated.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between different parts of the sequence. The output of the final encoder is a sequence of vectors, each of which represents the meaning of a word in the input sequence. These vectors are then used to predict the next word in the sequence.\n\nFigure 8 shows the architecture of the local interpretable layer (LIL) and the global interpretable layer (GIL). The LIL is a linear layer that is applied to the output of the transformer encoder. The GIL is a weighted sum of the outputs of the LIL. The weights are determined by the attention mechanism. The output of the GIL is a vector that represents the global meaning of the input sequence. This vector is then used to predict the label of the input sequence.",
        "reference": "Figure 2: Model Architecture: Our architecture comprises a base encoder that encodes the input and its relative non-terminals. GIL then uses MIPS to retrieve the most influential concepts that globally explain the sample, while LIL computes a relevance score for each ntj that quantifies its relevance to predict the label. The model interpretability is enforced through regularization (example parse tree inspired from Zanzotto et al. (2020))."
    },
    "1189033-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given an input program, the direct call relations and indirect call patterns are extracted. The direct call relations are represented as a graph, where the nodes represent functions and the edges represent the call relations between them. The indirect call patterns are represented as a set of triples, where each triple represents a call pattern between three functions. The traces are extracted by traversing the direct call graph and recording the sequence of functions that are called. The temporal relations between the traces are extracted by analyzing the indirect call patterns.",
        "reference": "Figure 2. At test time, the input to the framework is a sequence of RGB video frames (a). A real-time optical flow (OF) algorithm (b) [16] takes the consecutive RGB frames as input to produce flow images (d). As an option, (c) a more accurate optical flow algorithm [1] can be used (although not in real time). (e) RGB and OF images are fed to two separate SSD detection [22] networks (\u00a7 3.2). (f) Each network outputs a set of detection boxes along with their class-specific confidence scores (\u00a7 3.2). (g) Appearance and flow detections are fused (\u00a7 3.3). Finally (h), multiple action tubes are built up in an online fashion by associating current detections with partial tubes (\u00a7 3.4)."
    },
    "2207.05544v1-Figure5-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations can be used to identify patterns in software behavior. The process starts with two traces, which are sequences of events that have been recorded during the execution of a program. These traces are then used to identify direct call relations, which are pairs of functions that call each other directly. Next, the direct call relations are used to identify indirect call relations, which are pairs of functions that call each other indirectly through a chain of other functions. Finally, the indirect call relations are used to identify indirect call patterns, which are groups of functions that are frequently called together.\n\nThe flowchart in Figure 7 shows the architecture of a BERT model, which is a type of neural network that is used for natural language processing. The model consists of a stack of encoders, which are used to process the input text and extract features from it. The encoders are followed by a final input representation, which is used to represent the input text as a single vector. The final input representation is then used by an output layer to predict the output of the model.\n\nThe flowchart in Figure 8 shows the architecture of a self-driving car. The car is controlled by a user, who interacts with the car through a user interface. The user interface is connected to an ackermann drive, which is responsible for controlling the car's steering and acceleration. The ackermann drive is connected to a navigation system, which is responsible for planning the car's route. The navigation system is connected to a mapping system, which is responsible for creating and maintaining a map of the car's surroundings. The mapping system is connected to a localization system, which is responsible for determining the car's current location. The localization system is connected to a wheel sensor, which is responsible for measuring the speed of the car's wheels. The localization system is also connected to a laser sensor, which is responsible for measuring the distance between the car and its surroundings.",
        "reference": "Fig. 5: Robot navigation system overview"
    },
    "2204.12527v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call patterns and indirect call patterns can be further extracted from the direct call relations and indirect call relations, respectively.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nFigure 8 shows the architecture of a GAN. The GAN consists of two neural networks: a generator and a discriminator. The generator takes a random noise vector as input and produces a fake data sample. The discriminator takes a data sample as input and outputs a probability that the sample is real. The generator and discriminator are trained together in an adversarial fashion: the generator tries to fool the discriminator by producing fake samples that look like real data, while the discriminator tries to correctly classify real and fake samples.",
        "reference": "Fig. 1. Overview of our recommender system (CFWGAN-GP)"
    },
    "2204.00352v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee in a direct call. The indirect call relations are the relations between the caller and the callee in an indirect call. The indirect call patterns are the patterns of the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors by the input embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nFigure 8 shows the meta-learning framework for few-shot learning. The framework consists of two main components: a self-supervised learning component and a meta-learning component. The self-supervised learning component learns a feature extractor from a large dataset of unlabeled data. The meta-learning component then learns to fine-tune the feature extractor on a small dataset of labeled data.",
        "reference": "Figure 1: The pipeline of our methods. First, we will decide whether to initial the encoder with the pre-trained self-supervised learning model and fix the parameters or not. Then, we will train our model using meta-learning algorithms. The white module blocks represent the randomly initialized parameters and the blue encoder block refer to the model pre-trained from SSL."
    },
    "2110.08607v1-Figure4-1.png": {
        "prediction": " Figure 1 illustrates how traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee in a direct call. The indirect call relations are the relations between the caller and the callee in an indirect call. The indirect call patterns are the patterns of the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the meaning of the corresponding token in the input sequence.\n\nFigure 8 shows the architecture of a bi-directional recurrent neural network (BRNN). A BRNN is a type of recurrent neural network that has two hidden layers, one for the forward pass and one for the backward pass. The forward hidden layer takes the input sequence as input and produces a sequence of hidden states. The backward hidden layer takes the reversed input sequence as input and produces a sequence of hidden states. The output of the BRNN is the concatenation of the forward and backward hidden states.",
        "reference": "Figure 4: Inference neural network for PgDMM: The approximate inference model qi\u03c6(z i 1:T |x1:T ) for zi1:T , where i stands for either phy or NN, since the structure of both inference models is the same, is derived using bidirectional recurrent neural networks (BRNNs). The BRNN admits x1:T as inputs and - through a series of hidden layers of the neural network, denoted by blue arrows - forms two sequences of deterministic hidden states hft and h b t . These express the flow of information in the forward (from left to right) and backward (from right to left) sense, respectively. Then hft and h b t , together with the previous latent state z i t\u22121, are fed into a further neural network NN i\u03c6, denoted by the red arrows. Finally, the inference network outputs two vectors \u00b5i\u03c6(z i t\u22121,x) and \u03a3 i \u03c6(z i t\u22121,x), which parameterize the mean and diagonal covariance of qi\u03c6(z i t|zit\u22121,x), as shown in Eq.(14a)."
    },
    "2011.14771v1-Figure6-1.png": {
        "prediction": " The image shows how direct and indirect relations are extracted from traces. Given two traces (left), the direct call relations (C1, C2) and indirect call patterns (C1, C2) are extracted (right). The direct call relations are extracted by identifying the caller and callee functions in each trace. The indirect call patterns are extracted by identifying the caller and callee functions in each trace, and then identifying the common callees between the two traces.",
        "reference": "Fig. 6. Schematic illustration of the silicon master cross-section with \ud835\udeff = 29.5\u25e6 as the blaze angle and \ud835\udf03 \u2248 70.5\u25e6 defined by the crystal structure of silicon. At a groove spacing of \ud835\udc51 / 160 nm, the flat-top regions have widths \ud835\udc64 ' 30 nm as a result of the etch undercut while the groove depth is \u210e \u2248 65 \u2212 70 nm by Eq. (3). Indicated by the circle, the indented portion of the etched topography cannot be described with a functional form for diffraction-efficiency analysis."
    },
    "1023605-Figure10-1.png": {
        "prediction": " The flowchart shows how traces are generated and used to learn temporal relations between events. \n\nThe process starts with two traces, trace 1 and trace 2. Each trace is a sequence of events, where each event is represented by a node in the graph. The nodes in trace 1 are C1, C2, C3, C4, C5, and C6. The nodes in trace 2 are C1, C2, C3, C5.\n\nThe next step is to find the direct call relations between the events in each trace. A direct call relation exists between two events if one event directly calls the other event. For example, in trace 1, there is a direct call relation between C1 and C2 because C1 calls C2.\n\nThe next step is to find the indirect call relations between the events in each trace. An indirect call relation exists between two events if there is a path between the two events in the graph. For example, in trace 1, there is an indirect call relation between C1 and C3 because there is a path C1 -> C2 -> C3.\n\nThe final step is to use the direct and indirect call relations to learn temporal relations between the events. The temporal relations are represented by a matrix, where each entry in the matrix represents the probability that one event will occur after another event.\n\nThe learned temporal relations can be used to predict the order of events in a new trace. For example, if we have a new trace that contains the events C1, C2, and C3, we can use the learned temporal relations to predict that C1 will occur before C2, and C2 will occur before C3.",
        "reference": "Figure 10. The schema for 35 \u00d7 35 grid (Inception-ResNet-A) module of Inception-ResNet-v1 network."
    },
    "1011245-Figure6-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to learn temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are the pairs of methods that are called directly from one method to another. The indirect call relations are the pairs of methods that are called indirectly through a chain of method calls. The direct call patterns and indirect call patterns are then used to learn temporal relations between methods.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the final encoder layer is a sequence of vectors, each of which represents the learned representation of a token in the input sequence.\n\nFigure 8 shows an example of how BERT can be used for text classification. The input to the model is a sequence of tokens, which are converted into vectors by the word embedding layer. The vectors are then passed through the stack of encoder layers, which learn to extract features from the sequence. The output of the final encoder layer is a sequence of vectors, each of which represents the learned representation of a token in the input sequence. The vectors are then passed through a fully connected layer, which outputs a probability distribution over the different classes.",
        "reference": "Figure 6: 2D object shape classification network architecture."
    },
    "2102.06872v1-Figure1-1.png": {
        "prediction": " The flowchart describes the process of collecting coverage information for a program. The process starts with running the program with a set of test inputs. The coverage information is then collected and stored in a data structure called the coverage array. The coverage array is then used to generate new test inputs that cover the parts of the program that were not covered by the original test inputs. This process is repeated until all parts of the program are covered. The flowchart for collecting coverage information is as follows:\n\n1. Run the program with a set of test inputs.\n2. Collect the coverage information and store it in a coverage array.\n3. Generate new test inputs that cover the parts of the program that were not covered by the original test inputs.\n4. Repeat steps 1-3 until all parts of the program are covered.",
        "reference": "Fig. 1: GenTree overview"
    },
    "2107.01496v2-Figure2-1.png": {
        "prediction": " The flowchart describes the training of a negotiation strategy recognition model. \n\nThe process starts with repeated negotiation, where a detector and an agent interact with each other. The detector detects the strategy of the agent, and the agent updates its strategy based on the feedback from the detector. \n\nThe negotiation traces are then fed into a feature engineering module, which extracts features from the traces. These features are then used to train a recognition model, which is used to recognize the strategy of an agent in a negotiation. \n\nThe flowchart also shows that the trained model can be used to generate synthetic negotiation traces, which can be used to train other models or to evaluate the performance of the trained model.",
        "reference": "Fig. 2. The overall approach of strategy recognition."
    },
    "2110.14532v2-Figure2-1.png": {
        "prediction": " The image shows an overview of the proposed approach. Given a tweet or text, we first use a semantic similarity model to find the top n related hoaxes. Then, we use a natural language inference model to determine whether each related hoax entails the input text. Finally, we rank the hoaxes by their entailment scores and present them to the user.",
        "reference": "Figure 2: Architecture for the evaluation of information pieces against hoaxes already identified by fact checkers. A first step allows to retrieve hoaxes that are semantically similar to the input text. In the second step, a Natural Language Inference model measures the degree of entailment against each hoax retrieved in step 1."
    },
    "2107.13826v1-Figure6-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to learn temporal relations. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations can be extracted. Based on these relations, three types of indirect call patterns can be identified.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors. These vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary.\n\nFigure 2 shows the process flowsheet of the chlor-alkali electrolysis process. The main components of the process are the electrolyzer, the brine tank, the chlorine tank, and the hydrogen tank. The electrolyzer is a device that uses electricity to split water into hydrogen and oxygen. The hydrogen is then used to produce hydrochloric acid, and the oxygen is released as a byproduct. The brine tank is a storage tank for the sodium chloride solution that is used in the electrolyzer. The chlorine tank is a storage tank for the chlorine gas that is produced in the electrolyzer. The hydrogen tank is a storage tank for the hydrogen gas that is produced in the electrolyzer.",
        "reference": "Figure 6. Model overview: a) Flowchart of chlor-alkali process model, dashed controllers are removed from model and associated manipulated variables are used as input variables in sampling algorithm. b) Structure of used recurrent neural networks. Each output is modeled separately. Parameters N and O are determined in hyperparameter tuning."
    },
    "2110.11334v1-Figure3-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations can be inferred. For example, if C1 calls C2 in Tr1, and C2 calls C3 in Tr2, then we can infer that C1 indirectly calls C3. The inferred indirect call relations can be further used to identify indirect call patterns. For example, if C1 calls C2 and C3, and C2 calls C4, then we can identify an indirect call pattern C1-C2-C4. \n\nFigure 7 shows the architecture of BERT in pretraining. \n\nFigure 8 summarizes the anomaly detection and novelty detection methods. In (a), we present a comprehensive overview of the existing anomaly detection and one-class novelty detection methods. In (b), we summarize the existing multi-class novelty detection and open set recognition methods. In (c), we present the existing out-of-distribution detection methods.",
        "reference": "Fig. 3: Timeline for representative methodologies of (a) anomaly detection & one-class novelty detection, details in Section 3, (b) multi-class novelty detection & open set recognition, details in Section 4, and (c) OOD detection, details in Section 5, under generalized OOD detection framework. Different colors indicate different categories of methodologies. Each method has its corresponding reference (inconspicuous white) in the lower right corner. We do not list outlier detection methods in this figure due to their limited number of works on computer vision in deep learning era."
    },
    "822804-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. \n\nThe direct call relations and indirect call relations are extracted from the program code. The direct call relations are represented as a graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the elements of the matrix represent the indirect call relations between the functions.\n\nThe next step is to identify the indirect call patterns. For each pair of functions that have an indirect call relation, the set of all paths between the two functions in the direct call graph is identified. Each path represents a potential indirect call pattern.\n\nThe indirect call patterns are then used to identify the potential vulnerabilities in the program code. For each indirect call pattern, the set of all functions that can be called through the pattern is identified. These functions are then analyzed to identify any potential vulnerabilities.",
        "reference": "Figure 1: The left hand side is an overview of the model. The right hand side shows the details about the different comparison functions. The rectangles in dark represent parameters to be learned. \u00d7 represents matrix multiplication."
    },
    "1576593-Figure4-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations can be inferred. For example, if there is a direct call relation from C1 to C2 in Tr1, and a direct call relation from C2 to C3 in Tr2, then there is an indirect call relation from C1 to C3. \n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoder layers, each of which consists of a self-attention mechanism and a feed-forward network. The self-attention mechanism allows the model to attend to different parts of the input sequence, while the feed-forward network allows the model to learn complex relationships between the input tokens. The output of the encoder layers is a sequence of vectors, each of which represents the meaning of a token in the input sequence.\n\nFigure 8 shows how to use a pretrained BERT model for text classification. The first step is to tokenize the input text and convert it into a sequence of integers. The next step is to pass the sequence of integers through the BERT model. The output of the BERT model is a sequence of vectors, each of which represents the meaning of a token in the input text. The final step is to use a classifier to predict the label of the input text based on the sequence of vectors.",
        "reference": "Figure 4: An overview of our approach for utilizing abstract examples for data augmentation and model training."
    },
    "2011.02573v1-Figure8-1.png": {
        "prediction": " Figure 3: An illustration of computing multi-head attention. Each attention head computes an attention distribution over the values \\(v_i\\) based on the query \\(q\\) and key \\(k\\) vectors. The \\(W_i^q\\), \\(W_i^k\\), and \\(W_i^v\\) are parameter matrices.",
        "reference": "Fig. 8 A general appraisal-emotion network with k appraisal variables and l emotion types."
    },
    "2011.02573v1-Figure7-1.png": {
        "prediction": " The image shows a flowchart that captures relations between concepts. It illustrates how direct call relations (left) and indirect call relations (right) can be extracted from traces. The extracted relations are then used to generate indirect call patterns. Additionally, direct call patterns can be generated from direct call relations. Both direct and indirect call patterns can be used for protocol reverse engineering.",
        "reference": "Fig. 7 An weighted appraisal-emotion network showing many-to-many relationship between appraisal variables and emotions"
    },
    "213980-Figure3-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to extract temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect call relations between the functions. The extracted temporal relations are then used to generate the indirect call patterns. Each indirect call pattern represents a set of indirect call relations that can occur between two functions. The indirect call patterns are used to identify the potential vulnerabilities in the software system.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by a softmax layer that outputs a probability distribution over the vocabulary. The model is trained to predict the masked tokens in a sequence of text.\n\nFigure 8 shows the architecture of a convolutional neural network (CNN) for image classification. The CNN consists of a stack of convolutional layers, each of which is followed by a pooling layer. The convolutional layers learn to extract features from the input image, and the pooling layers reduce the dimensionality of the feature maps. The final layer of the CNN is a fully connected layer that outputs a probability distribution over the classes of images.",
        "reference": "Figure 3. Ladder network architectures Rasmus et al. (2015). :"
    },
    "2102.05963v1-Figure4-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations can be extracted. The direct call patterns and indirect call patterns can then be obtained.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nFigure 8 shows the architecture of the BRDF model. The input to the model is a 32x1 vector of the normal vector of the surface. The model consists of two layers: a fully connected layer and a BRDF model layer. The fully connected layer takes the normal vector as input and produces a 2x1 vector. The BRDF model layer takes the 2x1 vector as input and produces a 3x1 vector of the BRDF parameters.",
        "reference": "Figure 4: Scheme for quick computation of inverse CDF from an NBRDF: we train a network to map from latent NBRDF embeddings to importance sampling parameters of a chosen analytic BRDF model."
    },
    "2011.05452v1-Figure2-1.png": {
        "prediction": " Figure 1 shows how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the functions that are directly called by each function. The indirect call relations are the relations between the functions that are called by the functions that are called by each function. The indirect call patterns are the patterns of the indirect call relations. For example, the indirect call pattern (C1, C2) means that C1 calls C2 indirectly. The direct call patterns are the patterns of the direct call relations. For example, the direct call pattern (C1, C2) means that C1 calls C2 directly.\n\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence. The model is trained to predict the next word in a sequence given the previous words in the sequence.\n\nFigure 8 shows two cases of the interaction between two agents, A and B. In the first case, which we call the *contiguous case*, agents A and B interact with each other directly. In the second case, which we call the *non-contiguous case*, agents A and B interact with each other indirectly through another agent, C. In the first case, the interaction between A and B is mediated by the communication channel between them. In the second case, the interaction between A and B is mediated by the communication channel between A and C, and the communication channel between C and B.",
        "reference": "Figure 2. Schematic of relation between parts of the AKLT state and the corresponding entanglement Hamiltonian. The entanglement Hamiltonian for a part of the state is similar to the Heisenberg Hamiltonian between the effective spin-1/2 particles at the boundaries of the blocks. The green line in Hint represents couplings between interblock effective spin-1/2 particles (J(2)E ), while the red lines represent the coupling between intra-block spins (J(1)E )."
    },
    "909872-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. Then, the indirect call patterns are generated based on the indirect call relations. Finally, the direct call patterns are generated based on the direct call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through a series of Transformer layers, which encode the relationships between the tokens in the sequence. The output of the Transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.\n\nThe flowchart in Figure 8 illustrates the training procedure of a neural network. The first step is to initialize the weights of the network. Then, the network is trained on a set of training data. The training data is divided into batches, and the network is trained on each batch. The weights of the network are updated after each batch. The training process is repeated until the network has converged.",
        "reference": "Fig. 2. (a) The flowchart of the sub-band reconstruction for image super-resolution. (b) A relaxed version of (a). Gi is set as the element-wise summation function. In this framework, only the MSE loss is used to constrain the recovery. (c) The deep network designed with the intuition of (b). Gi is the element-wise summation function and Fi is modeled by two layer convolutions."
    },
    "126819-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe architecture of BERT in pretraining is shown in Figure 7. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the word embeddings layer. The output of the encoders is a sequence of vectors, each of which represents the meaning of the corresponding token in the input sequence.\n\nThe DeepSurvey architecture is shown in Figure 8. The architecture consists of two main components: a crawler and a deep learning model. The crawler is responsible for collecting data from the web. The deep learning model is responsible for analyzing the data and identifying potential survey papers.",
        "reference": "Fig. 2 DeepSurvey architecture: ( ) shows the actual number of papers and the number of ideas and implementations."
    },
    "211678216-Figure8-1.png": {
        "prediction": " ## Pneumonia detection using Deep Learning\nThe image shows a flowchart of a deep learning model for pneumonia detection. The model takes a chest X-ray image as input and outputs a binary classification result: pneumonia or not.\n\nThe model consists of a convolutional neural network (CNN) with three convolutional layers, each followed by a max pooling layer. The convolutional layers use a 3x3 kernel size and a stride of 1. The max pooling layers use a 2x2 kernel size and a stride of 2. The output of the CNN is then flattened and passed to a fully connected layer with 128 neurons. The fully connected layer is followed by a ReLU activation function and a dropout layer with a rate of 0.5. The output of the dropout layer is then passed to a softmax layer with 2 neurons. The softmax layer is followed by an argmax layer, which outputs the final classification result.",
        "reference": "Fig. 8. Structural design for the model of vanilla CNN"
    },
    "2012.00020v3-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations can be extracted. Moreover, indirect call patterns between API sequences can be further identified. For example, the indirect call pattern (C1, C2) indicates that whenever C1 and C2 are called in this order in any trace, there must exist another call to C4. The direct call relations and indirect call patterns can be further utilized to detect API misuse.\n\nFigure 7 shows the architecture of BERT in pretraining. Masked language modeling (MLM) randomly masks some percentage of the tokens in the input sequence and the model is trained to predict the masked tokens. Next sentence prediction (NSP) is a binary classification task where the model predicts whether the second sentence is the next sentence in the original document.\n\nFigure 8 shows the detailed architecture of the Transformer encoder layer. Each Transformer encoder layer consists of a self-attention mechanism and a feed-forward network. The self-attention mechanism allows the model to attend to different parts of the input sequence and capture long-range dependencies. The feed-forward network is used to add non-linearity to the model.\n\nFigure 9 shows the detailed architecture of the Transformer decoder layer. Each Transformer decoder layer consists of a self-attention mechanism, an encoder-decoder attention mechanism, and a feed-forward network. The self-attention mechanism allows the model to attend to different parts of the output sequence and capture long-range dependencies. The encoder-decoder attention mechanism allows the model to attend to the input sequence and incorporate information from the encoder into the decoder. The feed-forward network is used to add non-linearity to the model.",
        "reference": "FIG. 1. Overview of the general algorithm to quantum compute high energy scattering cross-sections, including the values of the bare couplings \u03bb and m for simulation time t. Initial state preparation is discussed in Section IV A, time evolution in Section IV B, and measurement of particle cross-sections in Section IV C. The choice of (renormalized) couplings \u03bb(t), m(t) is discussed in Section IV D."
    },
    "2012.00020v3-Figure3-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes are the API calls and the edges are the call relations. The indirect call relations are represented as a bipartite graph, where one set of nodes are the API calls and the other set of nodes are the indirect call patterns. The edges between the two sets of nodes represent the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The model is trained to predict the masked tokens and the next sentence in a masked language modeling task.\n\nFigure 8 shows the circuit for the QAOA algorithm. The circuit consists of a series of Hadamard gates, CNOT gates, and controlled-Z gates. The Hadamard gates are used to put the qubits into a superposition state. The CNOT gates are used to entangle the qubits. The controlled-Z gates are used to apply a phase shift to the qubits depending on the state of the control qubit. The circuit is repeated for a number of iterations, and the final state of the qubits is measured to obtain the solution to the optimization problem.",
        "reference": "FIG. 3. Overview of the time evolution scheme for one Trotter-Suzuki step \u03b4. Here S (discussed in Section IV B 2) and qFT (discussed in Section IV B 3) denote the squeezing operation and the quantum Fourier transformation, respectively."
    },
    "2011.06125v3-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the functions that are directly called from one function to another. The indirect call relations are the relations between the functions that are called from one function to another through other functions. The indirect call patterns are the patterns of the indirect call relations. The direct call patterns are the patterns of the direct call relations.",
        "reference": "Fig. 2. Representation of our multimodal machine learning framework using the two data sources: statistical and reanalysis maps. During Step 1, we extract embeddings from the reanalysis maps. In particular, we use encoder-decoder architectures or tensor decomposition to obtain a one-dimensional representation. During Step 2, we concatenate the statistical data with the features extracted from the reanalysis maps. During Step 3, we train one XGBoost model for each of the prediction tasks: intensity in 24 h, latitude displacement in 24 h, and longitude displacement in 24 h."
    },
    "2202.10332v1-Figure2-1.png": {
        "prediction": " The flowchart describes the process of deploying a machine learning model to Azure Kubernetes Service (AKS).\n\nThe process starts with the data source, which can be a SQL database or Azure Blob Storage. The data is then loaded into Azure Blob Storage or Azure ML Storage Account.\n\nThe next step is to create a machine learning model. This can be done using the Azure Machine Learning service. The model is then trained on the data that was loaded into Azure Blob Storage or Azure ML Storage Account.\n\nOnce the model is trained, it is deployed to Azure Kubernetes Service. This is done using the Azure Kubernetes Service API. The model is then exposed as a REST API using the Apigee API Gateway.\n\nFinally, the model can be consumed by client applications using the REST API.",
        "reference": "Figure 2: Azure ML Deployment Solution Architecture"
    },
    "2202.09110v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.\n\nThe flowchart in Figure 8 shows the iterative process of model training and evaluation. The model is first trained on a bootstrapping set. Then, the model is evaluated on a test set. The evaluation results are used to update the model. This process is repeated until the model reaches a satisfactory level of performance.",
        "reference": "Fig. 1. Flowchart outlining the developed framework."
    },
    "2101.08122v1-Figure1-1.png": {
        "prediction": " Figure 1 shows how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. Then, indirect call patterns are generated based on the indirect call relations. Finally, the direct call patterns are generated based on the direct call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nFigure 8 shows the self-supervised pretraining and change detection frameworks. In self-supervised pretraining, a CNN is used to extract features from a set of patches from an image. A pretext task is used to learn the weights of the CNN. In change detection, the features extracted from the image patches are used to train a classifier to detect changes in the image.",
        "reference": "Fig. 1: Overview of the methodology."
    },
    "2011.06236v3-Figure4-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted from traces. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect call patterns. The edges between the two sets of nodes represent the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the encoders is a sequence of vectors, which are then used to predict the next sentence and the masked tokens.\n\nFigure 8 shows a block diagram of the proposed control architecture. The closed-loop comprises the robot, the QP force controller, and the adaptation law. The reference model is used to generate the desired position and velocity of the robot. The control input function computes the control input to the robot based on the desired position and velocity, the current position and velocity of the robot, and the estimated parameters. The QP force controller computes the desired force to be applied to the robot based on the control input. The adaptation law estimates the parameters of the robot based on the error between the desired position and velocity and the current position and velocity of the robot.",
        "reference": "Fig. 4: Block diagram of the proposed adaptive force-based controller."
    },
    "2110.07525v2-Figure1-1.png": {
        "prediction": " ## Indirect Call Patterns\nFigure 1 illustrates how execution traces and temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee in a single trace. The indirect call relations are the relations between the callers and callees across different traces. For example, in trace t1, the direct call relations are C1->C2, C2->C3, C3->C4, C4->C5, and C5->C6. The indirect call relations are C1->C4, C1->C5, and C1->C6.",
        "reference": "Fig. 1. ORAN architecture with distributed controllers located at CU and DU/RU, and intelligence controller RIC"
    },
    "2102.03357v1-Figure16-1.png": {
        "prediction": " Figure 1: Overview of the proposed approach. Given a set of traces, we first extract direct and indirect call relations. Then, we identify indirect call patterns. Finally, we classify the call relations based on their uniqueness and testability.\n\nFigure 7: Transformer-based language model.\n\nFigure 1: The overview of the proposed approach.",
        "reference": "Fig. 16. The flow of proposed method in [97]."
    },
    "2204.07810v1-Figure6-1.png": {
        "prediction": " Figure 1 shows an overview of the approach. Given a set of traces, we first extract the direct call relations and indirect call relations. Then, we use the direct call relations to construct the direct call graph, and use the indirect call relations to construct the indirect call graph. Finally, we use the direct call graph and the indirect call graph to identify the indirect call patterns.\n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by a word embedding layer. The output of the final transformer encoder is fed to a softmax layer to predict the probability distribution over the vocabulary for each token.\n\nFigure 8 shows the training and prediction processes of the machine learning model for the surrogate model. In the training process, the model is trained to minimize the discrepancy between the predictions of the surrogate model and the results of the turbulence equation. In the prediction process, the trained model is used to predict the turbulence quantities of interest.",
        "reference": "Fig. 6 Model training and prediction flow chart"
    },
    "2204.07810v1-Figure3-1.png": {
        "prediction": " In Figure 1, the execution traces of two use cases are shown. Each node represents a function call, and the arrows between the nodes represent the call relations. The labels of the nodes indicate the caller and callee functions. For example, the node labeled \u201cC1\u201d represents the function \u201cC1\u201d, and the arrow from \u201cC1\u201d to \u201cC2\u201d indicates that \u201cC1\u201d calls \u201cC2\u201d. The dashed lines in the figure group the functions into two clusters, representing two different use cases. The use case on the left has a single trace, while the use case on the right has two traces.\n\nThe figure shows how the traces of the two use cases are related to each other. The arrows between the traces indicate the indirect call relations between the functions in the two use cases. For example, the arrow from the trace on the left to the trace on the right indicates that there is an indirect call relation between the functions \u201cC1\u201d and \u201cC4\u201d. This means that \u201cC1\u201d in the first use case calls a function that calls \u201cC4\u201d in the second use case.\n\nThe figure also shows how the functions in the two use cases are related to each other. The boxes in the figure represent the functions, and the arrows between the boxes represent the direct call relations between the functions. For example, the arrow from \u201cC1\u201d to \u201cC2\u201d in the box on the left indicates that \u201cC1\u201d directly calls \u201cC2\u201d.",
        "reference": "Fig. 3 CFD solution process of the iterative framework"
    },
    "2110.03183v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given a trace, denoted as T, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect calls. The edges between the two sets of nodes represent the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the encoder layers is a sequence of vectors, each of which represents the hidden state of the model at a particular position in the input sequence. These hidden states are then used to predict the next token in the sequence.\n\nFigure 8 illustrates the architecture of a neural network for music generation. The network consists of an encoder, a decoder, and a loss function. The encoder converts the input audio signal into a sequence of vectors, each of which represents the frequency spectrum of the audio signal at a particular time instant. The decoder converts the sequence of vectors into an output audio signal. The loss function is used to measure the difference between the output audio signal and the input audio signal. The network is trained by minimizing the loss function.",
        "reference": "Fig. 1. Figure capturing our proposed method and learning different codes for different representations of mel-spectogram"
    },
    "2202.12108v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces (Trace 1 and Trace 2) as input, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between them. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values indicate the presence or absence of an indirect call relation between them. The use case is represented as a set of indirect call patterns. Each pattern is a sequence of function calls that can be used to achieve a specific task. The algorithm then generates the temporal relations between the functions in the use case by analyzing the direct and indirect call relations. The temporal relations are represented as a set of constraints that specify the order in which the functions in the use case must be executed.",
        "reference": "Fig. 2: Overview of the proposed fusion method."
    },
    "2101.04861v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces t1 and t2 of direct call relations, indirect call relations, and indirect call patterns are constructed. On the left, the direct call relations are shown as a graph, where nodes represent functions and edges represent direct calls. On the right, the indirect call relations are shown as a bipartite graph, where nodes on the left represent functions in trace t1 and nodes on the right represent functions in trace t2. Edges between the two sets of nodes represent indirect calls. The indirect call patterns are shown as a table, where each row represents a pattern of indirect calls and each column represents a function in trace t2. A checkmark in the table indicates that the corresponding function in trace t2 is called indirectly by the corresponding pattern of functions in trace t1. Finally, the direct call patterns are shown as a table, where each row represents a pattern of direct calls and each column represents a function in trace t2. A checkmark in the table indicates that the corresponding function in trace t2 is called directly by the corresponding pattern of functions in trace t1.",
        "reference": "FIG. 1. (a) Schematic illustration (top view) of the TE module made of a film of magnetic TI. A periodic array of small square hollows filled with two different dielectrics is introduced into the magnetic TI. Each hollow harbors gap-opened metallic surface states (yellow lines) and is connected in series by leads (black solid and dashed lines). Refrigeration mode is shown here. (b) Schematic geometry of the fundamental element (topological thermocouple) consisting of two connected hollows with different dielectrics (1 and 2), possessing the p- and n-types metallic surface states. d is the distance between the two adjoined surfaces. (c) Corresponding k-dependent surface band dispersions around the \u0393 point are depicted by blue lines in which \u00b5 denotes the chemical potential at equilibrium and USIA describes the structure inversion asymmetry (SIA) between the adjoined two surfaces due to band bending induced by the dielectrics."
    },
    "2107.12100v2-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given two traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect calls. The edges between the two sets of nodes represent the indirect call relations. Finally, the direct call patterns and indirect call patterns are extracted from the direct call relations and indirect call relations, respectively. The direct call patterns are represented as a set of sequences of function calls, where each sequence represents a possible path of execution through the program. The indirect call patterns are represented as a set of pairs of functions, where each pair represents a possible indirect call relation between two functions.",
        "reference": "Figure 3: Overview of our approach to predict in uential nodes and node sequences based on path data. We start from path data which we split into training and test sets. We learn three di erent models bases on the training data: (i) a network model containing all transitions from the training data, (ii) a multi-order generative model containing observed higher-order transitions up to a maximum order of K , which is determined by model selection, and (iii) a path model containing the full paths in the training set. Based on these models, we predict the in uence of node or node sequences according to a broad range of centrality measures. We compare the ranking of node sequences to the ground truth rankings obtained from the test paths using AUC-based evaluation."
    },
    "209217-Figure2-1.png": {
        "prediction": " The image shows an overview of the Transformer model. The model consists of a stack of Transformer blocks, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the Transformer model is a sequence of vectors, each of which represents the probability of a word in the output sequence.",
        "reference": "Figure 2: Illustration of the mQA model architecture. We input an image and a question about the image (i.e. \u201cWhat is the cat doing?\u201d) to the model. The model is trained to generate the answer to the question (i.e. \u201cSitting on the umbrella\u201d). The weight matrix in the word embedding layers of the two LSTMs (one for the question and one for the answer) are shared. In addition, as in [25], this weight matrix is also shared, in a transposed manner, with the weight matrix in the Softmax layer. Different colors in the figure represent different components of the model. (Best viewed in color.)"
    },
    "1504097-Figure2-1.png": {
        "prediction": " The image shows the architecture of an argumentative summarization system. The system consists of three main components: a pre-processor, a constraint generator, and a summary generator. The pre-processor takes the input text document and pre-processes it by removing stop words and stemming the words. The constraint generator then takes the pre-processed text and generates a set of constraints that will be used by the summary generator to generate the summary. The summary generator takes the pre-processed text and the constraints and generates a summary of the text.",
        "reference": "Fig. 2. ASDS Architecture"
    },
    "2010.15560v2-Figure9-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct execution traces and extract temporal relations. Given two traces Tr1 and Tr2 of the same program, we first identify the direct call relations and indirect call relations between the functions in the traces. A direct call relation (C1, C2) indicates that function C1 directly calls function C2, and an indirect call relation (C1, C2) indicates that function C1 indirectly calls function C2 (i.e., there is a call chain from C1 to C2). We then use these relations to construct the direct call graph and indirect call graph. Finally, we extract the temporal relations between the functions in the traces by analyzing the call relations.\n\nFigure 7 shows the overall architecture of BERT. BERT consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, and the feed-forward layer allows the model to learn non-linear relationships between the input and output. The Transformer encoders are stacked together, and the output of the final Transformer encoder is used to predict the next sentence.\n\nFigure 8 shows an example of how BERT can be used to answer a question about a text sequence. The input to BERT is a sequence of tokens, and the output is a sequence of vectors. Each vector represents the hidden state of the corresponding token in the input sequence. The hidden states can be used to answer questions about the text sequence. For example, the question \"What is the name of the person who is speaking?\" can be answered by looking at the hidden state of the token \"he\" in the input sequence.",
        "reference": "Fig. 9. The first best architecture."
    },
    "2202.01897v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the callers and callees in the traces. The indirect call relations are the relations between the callers and callees in the traces that are not direct. For example, in trace 1, there is a direct call relation between C1 and C2, and there is an indirect call relation between C1 and C4.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence.\n\nFigure 8 shows the architecture of a Swin Transformer. The Swin Transformer is a transformer-based image recognition model that is based on the Swin Transformer architecture. The model is composed of a stack of Swin Transformer blocks, each of which consists of a self-attention layer and a feed-forward layer. The Swin Transformer blocks are followed by a global average pooling layer and a fully connected layer that is used to classify the image.",
        "reference": "Figure 2: The AtmoDist network used for learning the pretext task. Numbers after layer names indicate the number of filters / feature maps of an operation. The comparison network is only required during training and can be discarded afterwards."
    },
    "1045792-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which attend to each other and to the input tokens. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nThe flowchart in Figure 8 shows the architecture of a gated recurrent unit (GRU) cell. The GRU cell is a type of recurrent neural network (RNN) that is used in a variety of natural language processing tasks. The GRU cell takes as input a vector of values, and it outputs a vector of values. The GRU cell also has a hidden state, which is a vector of values that is used to store information from previous time steps.",
        "reference": "Figure 3: Block diagram of the recurrent module of an LSTM network."
    },
    "2202.05262v2-Figure5-1.png": {
        "prediction": " In Figure 1, given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the functions that are directly called, while the indirect call relations are the relations between the functions that are called indirectly through other functions. The indirect call patterns are the patterns of the indirect call relations. In this example, there are two indirect call patterns, (C1, C4) and (C1, C6). The direct call patterns are the patterns of the direct call relations. In this example, there are three direct call patterns, (C1, C2), (C2, C3), and (C2, C5).",
        "reference": "Figure 5. The ROME method. To insert a fact (s, r, o\u2217) relating a subject s to an object o\u2217, a rank-one update \u039b(C\u22121k\u2217)T is applied to a single midlayer MLP projection, where (a) k\u2217 is chosen to select the last token of the subject name s, and (b) \u039b \u2208 RH is computed from v\u2217 that causes the prediction o\u2217 after text for (s, r)."
    },
    "2205.13038v2-Figure1-1.png": {
        "prediction": " The image shows an overview of the proposed graph pooling method. Given an input graph \\(G=(X, A)\\), where \\(X\\) is the set of nodes and \\(A\\) is the adjacency matrix, the method first generates multiple subgraphs by subgraph perturbation. Then, for each subgraph \\(G_i\\), the node embeddings are extracted and then pooled to obtain the subgraph embedding \\(e_{G_i}\\). Finally, the subgraph embeddings are used to update the node embeddings.",
        "reference": "Figure 1. Overview of our proposed subgraph augmentation approach. The two subgraphs in the original graph are colored in gree and orange. We first generate multi-subgraph views via stochastic augmentation. Following that we connect the augmented subgraph to the remaining part of the original graph, by adding edges that link the augmented subgraph and the whole graph. After feeding forward the whole graph into subgraph-specific GNNs, we extract the subgraph embeddings of different views, respectively (triangles and squares). Ultimately, we fuse the embeddings of different views by a pooling function and obtain the augmented subgraph embeddings (diamonds)."
    },
    "1872130-Figure1-1.png": {
        "prediction": " The image shows how to generate saliency maps using three different CNNs. The architecture of each CNN is the same, but the weights are different. The input to the CNNs is an image, and the output is a saliency map. The saliency map shows the importance of each pixel in the image for the classification task.\n\nThe first step is to forward propagate the image through the CNNs. This produces three saliency maps, one for each CNN. The second step is to revise the images using the gradients. The gradients are calculated by backpropagating the error from the classification task through the CNNs. The third step is to forward propagate the revised images through the CNNs again. This produces three new saliency maps, which are more accurate than the first three saliency maps.",
        "reference": "Figure 1. The proposed method to generate the object-specific saliency maps directly from DCNNs."
    },
    "2204.06981v1-Figure8-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call relations are represented by the directed edges between the caller and the callee, and the indirect call relations are represented by the dashed edges between the caller and the callee. The indirect call patterns are extracted based on the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The token embeddings are then passed through a series of transformer layers, which attend to each other and to the positional embeddings. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nFigure 8 shows the\u304c\u751f\u8d77\u3057\u307e\u3057\u305f\u3002Figure 8 shows the",
        "reference": "Figure 8: A 2D diagram of the integration region. The compact binary is the wiggly curve between (0, \u03b70) and (0, \u03b7coal), the merger takes place at (0, \u03b7coal), the merger pulse arrives at the detector at (r, \u03b7f ), GW emitted by the binary moves along the light cone and emits at (r \u2032, \u03b7\u2032) a subluminal tail pulse which arrives at the detector at r at the time \u03b7 > \u03b7f after the merger pulse. GW in the two shaded regions can also source a tail pulse to (r, \u03b7). Similar post-merger first order tail radiation (emission of dashed line from the wiggly binary curve) would also be possible but is canceled by the dynamics of the process (subsection 3.5)."
    },
    "2012.03418v1-Figure1-1.png": {
        "prediction": " The figure shows how execution traces are generated. Given two traces `t1` and `t2` with direct call relations and indirect call relations, the direct call patterns `(C1, C2)`, indirect call patterns `(C1, C2)`, and temporal relations can be extracted.",
        "reference": "Fig. 1. An example of a hypernym-hyponym pair in a definition: \u201csql is a language for querying databases\u201d. The definition is split into units (words and the corresponding PoS) for analysis. The word \u201clanguage\u201d and \u201cdatabases\u201d are two hypernym candidates. The PoS elements surround \u201clanguage\u201d and \u201cdatabases\u201d are different. Our model learns such features and identifies \u201clanguage\u201d as the hypernym of \u201csql\u201d."
    },
    "2107.10658v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations can be extracted. The direct call patterns and indirect call patterns can be further derived.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary.\n\nFigure 8 shows the architecture of a typical TTS system. The system consists of three main components: a text encoder, a prosody predictor, and a speech synthesizer. The text encoder converts the input text into a sequence of vectors, which are then used by the prosody predictor to predict the pitch and duration of each phoneme in the text. The speech synthesizer then uses the predicted pitch and duration to generate the final speech waveform.",
        "reference": "Figure 2: Cloud architecture for Synchronous TTS."
    },
    "2102.11262v5-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the functions that are directly called from one function to another. The indirect call relations are the relations between the functions that are called from one function to another through a chain of other functions. The indirect call patterns are then generated based on the indirect call relations. The indirect call patterns are the patterns of how the functions are called from one function to another through a chain of other functions. Finally, the direct call patterns are generated based on the direct call relations. The direct call patterns are the patterns of how the functions are called from one function to another.",
        "reference": "Fig. 2: Architecture of the proposed Adversarial Shape Learning Network (ASLNet) for building extraction. We designed an explicit shape regularizer to model the shape features, and a shape discriminator to guide the segmentation network."
    },
    "2107.12979v3-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how traces are generated and used to learn temporal relations. Given two traces, trace 1 (T1) and trace 2 (T2), which consist of the execution of API calls C1, C2, C3, C4, C5, and C6, we can derive direct call relations and indirect call relations. A direct call relation exists between two APIs if one API directly calls the other. For example, in T1, C2 directly calls C3, so there is a direct call relation between C2 and C3. An indirect call relation exists between two APIs if there is a sequence of API calls from one API to the other. For example, in T1, there is an indirect call relation between C1 and C3 because there is a sequence of API calls C1 \u2192 C2 \u2192 C3. We can use these relations to learn temporal relations between APIs. For example, we can learn that C1 always precedes C3 because there is an indirect call relation between C1 and C3.\n\nFigure 7 shows the architecture of BERT, a pre-training model for natural language processing. BERT is a transformer-based model that consists of a stack of encoder layers. Each encoder layer consists of a self-attention mechanism and a feed-forward network. The self-attention mechanism allows the model to attend to different parts of the input sequence, while the feed-forward network allows the model to learn relationships between the different parts of the sequence. The output of the final encoder layer is a sequence of vectors, where each vector represents the learned representation of a token in the input sequence. These vectors can be used for various natural language processing tasks, such as text classification and question answering.\n\nFigure 8 illustrates the computation of attention weights in the self-attention mechanism. The self-attention mechanism allows a model to attend to different parts of the input sequence. In this example, the model is attending to the word \"the\" in the input sequence. The attention weights are computed by a function of the query vector (Q), the key vector (K), and the value vector (V). The query vector is a representation of the word that the model is currently attending to. The key vector is a representation of the words in the input sequence. The value vector is a representation of the words in the input sequence. The attention weights are then used to compute a weighted sum of the value vectors, which is the output of the self-attention mechanism.",
        "reference": "Figure 2: The canonical microcircuit proposed by Bastos et al mapped onto the laminar connectivity of a cortical region (which comprises 6 layers). Here, for simplicity, we group layers L2 and L3 together into a broad \u2018superficial\u2019 layer and L5 and L6 together into a \u2018deep\u2019 layer. We ignore L1 entirely since there are few neurons there and they are not involved in the Bastos microcircuit. Bold lines are included in the canonoical microcircuit of Bastos et al. Dashed lines are connections which are known to exist in the cortex which are not explained by the model. Red text denotes the values which are computed in each part of the canonical microcircuit"
    },
    "2102.08893v1-Figure4-1.png": {
        "prediction": " The flowchart describes the steps to generate an image using a neural network. \n\nThe first step is to encode the input image using an encoder convolutional neural network (CNN). The output of the encoder CNN is a feature map that is then used to generate a Gaussian distribution. \n\nThe next step is to decode the Gaussian distribution using a decoder CNN. The output of the decoder CNN is an image that is then compared to the original input image using a mean squared error (MSE) loss function. \n\nThe MSE loss is then used to update the weights of the encoder and decoder CNNs using a gradient descent algorithm. This process is repeated until the MSE loss is minimized and the generated image is visually similar to the original input image.",
        "reference": "Fig. 4. Architecture of the training phase [2]"
    },
    "2202.13313v3-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 (T1) and trace 2 (T2), the direct call relations and indirect call relations are extracted. The direct call relations are represented as a matrix, where each row and column corresponds to a function, and a non-zero entry at position (i, j) indicates that function i directly calls function j. The indirect call relations are represented as a matrix, where each row and column corresponds to a function, and a non-zero entry at position (i, j) indicates that function i indirectly calls function j. The indirect call patterns are then generated by identifying the common indirect call relations between T1 and T2. Finally, the direct call patterns are generated by identifying the common direct call relations between T1 and T2.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The token embeddings are then passed through a series of transformer layers, which learn to attend to each other and generate a contextualized representation of the input sequence. The final output of the transformer layers is passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 illustrates the training process of a neural network using backpropagation. The network is first initialized with random weights. Then, a forward pass is performed, which computes the output of the network for a given input. The output is then compared to the desired output, and the error is calculated. The error is then backpropagated through the network, which updates the weights of the network. This process is repeated until the error is minimized.",
        "reference": "Fig. 2. Searching process of neural architecture."
    },
    "207468-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces (Trace 1 and Trace 2) of direct call relations, the direct call patterns (C1, C2) are extracted. Then, the indirect call relations are generated based on the direct call patterns. Finally, the indirect call patterns (C1, C2) are extracted.",
        "reference": "Figure 3: Hierarchical Sequence to Sequence Model with Attention."
    },
    "2102.12816v3-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of the execution of a program, the direct call relations and indirect call relations can be extracted. The direct call relations are represented by the directed edges between the caller and callee methods, and the indirect call relations are represented by the dashed edges between the caller and callee methods. The indirect call patterns are extracted based on the indirect call relations. For example, the indirect call pattern (C1, C2) indicates that C1 calls C2 indirectly. The direct call patterns are extracted based on the direct call relations. For example, the direct call pattern (C1, C2) indicates that C1 calls C2 directly.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through a series of transformer layers, which attend to each other and to the input embeddings. The output of the transformer layers is then passed through a final softmax layer to produce a probability distribution over the vocabulary.\n\nThe model is trained to predict the masked tokens in the input sequence. The masked tokens are replaced with a special [MASK] token, and the model is trained to predict the original token from the context. The model is also trained to predict the next sentence in the input sequence. The next sentence is separated from the input sequence by a special [SEP] token, and the model is trained to predict whether the next sentence is the correct next sentence.",
        "reference": "Fig. 1. An illustration of the relationship among T , Ti and delay in slot m. In this example, Pi,m \u2212 ni,m = 2 and tm+1 \u2212 \u03c4(tm+1) = 4, which satisfies (12)."
    },
    "90264-Figure5-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are used to identify API call patterns. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to construct the direct call graph, and the indirect call relations are used to construct the indirect call graph. The API call patterns are then identified by analyzing the direct call graph and the indirect call graph.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract meaningful features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 shows the architecture of a video action recognition model. The model consists of two streams: an appearance stream and a motion stream. The appearance stream processes the appearance information in the video, while the motion stream processes the motion information in the video. The output of the two streams is then fused to produce the final prediction.",
        "reference": "Fig. 5: Illustration of late-fusion. We encode, temporally pool the appearance and motion representation separately and fuse the two probability distributions."
    },
    "2102.10461v1-Figure4-1.png": {
        "prediction": " Figure 1 shows how execution traces are generated and how temporal relations are extracted. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations can be extracted. The direct call relations are the pairs of APIs that are directly called one after another in the traces. The indirect call relations are the pairs of APIs that are not directly called one after another in the traces, but there is a path between them in the call graph. The indirect call patterns are the patterns of indirect call relations. For example, the pattern C1->C2->C3 means that C1 calls C2 and C2 calls C3.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of transformer encoders, which are used to learn the relationships between the words in a sentence. The input to the model is a sequence of tokens, which are converted into vectors by the word embeddings layer. The output of the model is a sequence of vectors, which represent the learned representations of the tokens.\n\nFigure 8 shows the architecture of a U-Net. U-Net is a convolutional neural network that is used for image segmentation. The model consists of a series of convolutional layers, which are used to extract features from the image. The model also has a series of upsampling layers, which are used to reconstruct the image from the features. The input to the model is an image, which is converted into a feature map by the convolutional layers. The output of the model is a segmented image, which is produced by the upsampling layers.",
        "reference": "Figure 4: CelebA architecture for the injective portion g of TRUMPET. The input size to each layer is written below it."
    },
    "2110.06804v3-Figure4-1.png": {
        "prediction": " The flowchart shows how to use the Binarized Neural Network (BNN) accelerator. It takes a model as input and then quantizes it to a BNN model. The BNN model is then used to make predictions on new data.\n\nThe BNN accelerator is a hardware accelerator that can be used to speed up the execution of BNN models. The accelerator is designed to be efficient and can be used to process large amounts of data quickly.\n\nThe flowchart for using the BNN accelerator is as follows:\n\n1. Load the model to be quantized into the BNN accelerator.\n2. Quantize the model to a BNN model.\n3. Load the BNN model into the BNN accelerator.\n4. Run the BNN model on new data to make predictions.",
        "reference": "Figure 4. An artificial neural network"
    },
    "1339502-Figure4-1.png": {
        "prediction": " Figure 1 shows an overview of how we construct execution traces and derive temporal relations from them. Given two traces Tr1 and Tr2 of API calls, where each trace is a sequence of API calls, we can derive direct call relations and indirect call relations. A direct call relation (C1, C2) indicates that API C1 directly calls API C2, while an indirect call relation (C1, C2) indicates that API C1 indirectly calls API C2 through a sequence of other API calls. We can also derive indirect call patterns from the indirect call relations. An indirect call pattern is a set of APIs that are indirectly called by a given API. For example, the indirect call pattern {C4, C5, C6} of API C2 indicates that API C2 indirectly calls APIs C4, C5, and C6 through a sequence of other API calls.\n\n\nFigure 7 shows the overall architecture of BERT. The input to BERT is a sequence of tokens, where each token is represented by a word embedding. The word embeddings are then passed through a series of Transformer layers. Each Transformer layer consists of a self-attention mechanism and a feed-forward network. The self-attention mechanism allows the model to attend to different parts of the input sequence, while the feed-forward network allows the model to learn non-linear relationships between the input tokens. The output of the Transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\n\nFigure 8 shows the architecture of the convolutional neural network (CNN) used for hard sample mining. The CNN consists of a series of convolutional layers, each followed by a pooling layer. The convolutional layers learn to extract features from the input image, while the pooling layers reduce the dimensionality of the feature maps. The output of the CNN is a feature vector, which is then used to classify the image.",
        "reference": "Figure 4. Structure of the network. Left: Detailed components of the convolutional feature extraction and multi-level feature fusion. The \u201cConvUnit(w, h, n)\u201d represents a convolutional layer of n w\u00d7 h kernels, connected by a batch normalization layer and a ReLU layer. The \u201cUpSample(n)\u201d represents a deconvolution layer of n\u00d7 n kernels with stride n. Right: The design of multi-task module. \u201cScale&Shift\u201d is used to stretch and translate the values."
    },
    "1639981-Figure3-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces Tr1 and Tr2 of API calls, the direct call relations and indirect call relations can be extracted. The direct call relations are the pairs of APIs that are directly called one after another in the traces. The indirect call relations are the pairs of APIs that are not directly called one after another in the traces, but there is a path between them in the call graph. The indirect call patterns are the sets of APIs that are indirectly called from a specific API. For example, in the figure, the direct call relations are (C1, C2), (C2, C3), (C3, C4), (C4, C5), and (C5, C6). The indirect call relations are (C1, C4), (C1, C5), (C1, C6), (C2, C5), and (C2, C6). The indirect call patterns are {C4, C5, C6} for C1, {C5, C6} for C2, and {C6} for C3.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the hidden state of the corresponding token. The model is trained to predict the masked tokens in the input sequence, as well as to predict the next sentence in a sequence of sentences.\n\nFigure 8 illustrates how the TransformerXL model works. The TransformerXL model is a transformer-based language model that is designed to handle long sequences of text. The model consists of a stack of transformer layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, and the feed-forward layer allows the model to process the information that it has attended to. The model is trained to predict the next token in a sequence of tokens.",
        "reference": "Figure 3: Left: Columnar architecture in a fully connected network, with the path through one column highlighted. Each column corresponds to a different \u03b1j . Right: Columnar architecture in a convolutional network. In this setting the w\u03b1\u2019s take linear combinations of the feature maps obtained by convolving the input with the dictionary. We make the same abuse of notation here as in the main text\u2014the vectorized filter banks must be reshaped before the convolution takes place."
    },
    "2204.05103v1-Figure1-1.png": {
        "prediction": " In Figure 1, given two traces Tr1 and Tr2, the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the caller and the callee, while the indirect call relations are the relations between the caller of the caller and the callee of the callee. The indirect call patterns are the patterns of the indirect call relations. In Figure 7, the input sequence is represented by a sequence of word embeddings, and the output of the final layer is a sequence of vectors, where each vector represents the probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probabilities and the true labels. In Figure 8, the architecture of the proposed emotion recognition model is shown. The model consists of two parts: a signal encoder and a transformer encoder. The signal encoder is used to extract the emotion-related features from the input signal, and the transformer encoder is used to capture the long-term dependencies in the extracted features. The output of the transformer encoder is then fed into a fully connected layer to predict the emotion label.",
        "reference": "Figure 1: Our approach with self supervised learning based on a Transformer (a) and fine-tuning strategy for learning the final emotion predictor (b)."
    },
    "2012.03152v1-Figure2-1.png": {
        "prediction": " In Figure 1, the flowchart of the proposed indirect call relation inference approach is illustrated. Given two traces, the direct call relations and indirect call patterns are extracted. Then, the candidate indirect call relations are generated based on the direct call relations and indirect call patterns. Finally, the candidate indirect call relations are validated by using the temporal relations between the caller and callee methods.\n\nIn Figure 7, we show the overall architecture of BERT. The model consists of a stack of transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the final transformer encoder is a sequence of vectors, each of which represents the meaning of the corresponding token in the input sequence.\n\nIn Figure 2, the pipeline of the proposed building point cloud classification method is illustrated. First, the raw point cloud data is preprocessed to remove noise and outliers. Then, features are extracted from the preprocessed point cloud data, including three-dimensional coordinates, the change of curvature, and the average distance of neighborhood. Next, the extracted features are used to train a support vector machine (SVM) classifier to classify the point cloud data into different building types. Finally, the classification results are visualized and evaluated.",
        "reference": "Fig. 2. Flowchart of experiment."
    },
    "2103.11568v4-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to learn temporal relations between API calls. Given two traces Tr1 and Tr2 of API calls, we can extract the direct call relations and indirect call relations. The direct call relations are the pairs of API calls that are directly called one after another in the traces. The indirect call relations are the pairs of API calls that are not directly called one after another in the traces, but there is a path between them in the call graph. We can then use these relations to learn a model that can predict the probability of an API call being called after another API call.\n\nFigure 7 shows the architecture of BERT, a pre-trained transformer-based language model. BERT is a transformer-based language model that has been pre-trained on a large corpus of text. It can be used for a variety of natural language processing tasks, such as text classification, question answering, and machine translation.\n\nFigure 8 shows the training process of a deep neural network. The training process involves presenting the network with a set of training data, and then adjusting the network's weights so that it makes fewer errors on the training data. This process is repeated until the network reaches a desired level of accuracy.",
        "reference": "Fig. 2: The system pipeline of our unsupervised person re-ID method. The upper branch depicts the memory initialization stage. The training data features are assigned pseudo labels by clustering, where features of the same color belong to the same cluster. The lower branch represents the model training stage. Query features in iterative mini-batch are used to update the memory cluster representations with a momentum. The ClusterNCE loss computes the contrastive loss between query features and all cluster representations."
    },
    "2103.10685v2-Figure3-1.png": {
        "prediction": " In Figure 1, the execution traces of two use cases are shown. Each node in the graph represents a function call, and the edges represent the order of the function calls. The labels on the edges indicate the caller and callee functions. For example, the edge from C1 to C2 indicates that C1 calls C2. The dotted line in the figure separates the direct call relations and indirect call relations. The direct call relations are the relations between the caller and callee functions that are directly connected by an edge. The indirect call relations are the relations between the caller and callee functions that are not directly connected by an edge, but there is a path between them. For example, the indirect call relation between C1 and C4 is C1->C2->C4. \n\nIn Figure 7, the architecture of BERT in pretraining is shown. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the meaning of the corresponding token in the input sequence. The model is trained to predict the masked tokens in the input sequence.\n\nIn Figure 8, the process of using a language model to generate a poem is shown. The language model is first prompted with a title and a genre, and then it generates a poem that matches the given title and genre.",
        "reference": "Fig. 3. Language model generation and language model inverse prompting scoring for generating a poem sentence."
    },
    "2012.07082v1-Figure3-1.png": {
        "prediction": " The image shows how to generate execution traces and extract temporal relations from them. Given two traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations can be extracted. In particular, for each pair of functions (C1, C2), if C1 calls C2 in both traces, then there is a direct call relation from C1 to C2. If C1 calls C2 in one trace and C2 calls C1 in the other trace, then there is an indirect call relation from C1 to C2. The extracted direct call relations and indirect call relations can be further used to generate indirect call patterns. For example, if there is an indirect call relation from C1 to C2 and an indirect call relation from C2 to C3, then there is an indirect call pattern C1->C2->C3.",
        "reference": "Figure 3: Illustration of the sampled games generated by modified SGM during its execution."
    },
    "2107.06179v2-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces can be used to extract temporal relations between API calls. Given two traces of API calls, trace t1 and trace t2, we can extract the direct call relations and indirect call relations. The direct call relations are the pairs of API calls that are directly called from one to another, while the indirect call relations are the pairs of API calls that are called from one to another through a sequence of other API calls. The figure also shows how the indirect call patterns can be used to identify API call patterns. For example, the indirect call pattern (C1, C2) indicates that API call C1 is often followed by API call C2.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the encoder layers is a sequence of vectors, each of which represents the meaning of a word in the input sequence. These vectors are then used to predict the next word in the sequence.\n\nFigure 8 shows how a convolutional neural network (CNN) can be used to classify images. The CNN consists of a stack of convolutional layers, each of which is followed by a pooling layer. The convolutional layers learn to identify features in the images, while the pooling layers reduce the dimensionality of the feature maps. The output of the final convolutional layer is a feature map that represents the most important features in the image. This feature map is then used to predict the class of the image.",
        "reference": "Fig. 1. Illustration of myocardial infarct caused by acute thrombus in the culprit coronary artery (see text)."
    },
    "2102.03347v1-Figure2-1.png": {
        "prediction": " The image shows the off-chain and on-chain components of a typical bot attack. The attacker uses a bot to interact with the blockchain. The bot interacts with the attacker's off-chain infrastructure, which in turn interacts with the blockchain through attacker EOAs (Externally Owned Accounts) and bot contracts.",
        "reference": "Figure 2: Attacker model with on-chain and off-chain parts."
    },
    "2102.06702v1-Figure3-1.png": {
        "prediction": " Figure 1 shows how traces are generated and used to extract direct and indirect relations between functions. Given a set of traces, e.g., T1 and T2, we can extract the direct call relations, e.g., C1\u2192C2, by identifying the caller and callee functions in each trace. We can also extract indirect call relations, e.g., C1\u2192C4, by identifying the caller and callee functions in different traces. In this example, C1\u2192C2 is a direct call relation because C1 calls C2 in both T1 and T2. C1\u2192C4 is an indirect call relation because C1 calls C3 in T1 and C3 calls C4 in T2. The set of direct call relations and indirect call relations can be used to infer the call graph of the program.",
        "reference": "Fig. 3. Schematic illustration of the PCD. The gray box indicates the domain of the unit-cell model. Note that, in practice, the size of the THz antenna attached to the electrodes is much larger than the gap distance between the electrodes."
    },
    "2101.00048v1-Figure1-1.png": {
        "prediction": " Figure 1 shows how traces are generated and how relations are extracted. Given two traces, trace 1 (T1) and trace 2 (T2), the direct call relations and indirect call relations can be extracted. The direct call relations are the relations between the functions that are directly called by each function, while the indirect call relations are the relations between the functions that are called by the functions that are called by each function. For example, in T1, function C1 calls function C2, so there is a direct call relation between C1 and C2. In T2, function C1 calls function C2, and function C2 calls function C3, so there is an indirect call relation between C1 and C3. In addition, the indirect call patterns can be extracted from the indirect call relations. For example, in T1 and T2, there is an indirect call pattern between C1 and C2, and there is another indirect call pattern between C1 and C3.",
        "reference": "Figure 1. Schematic diagram of the Slimfield microscope. a) Laser light vertically polarized at source (blue) passes through a shutter (S) and is expanded 3x by the lens pair L1 and L2, before passing through L3 which forms a telescope with the objective lens to ensure light at the sample is collimated. Finally, in the excitation path the laser light passes through a wave plate (WP) on a rotational mount, either \u03bb/2 for linear polarization or \u03bb/4 for circular polarization. The light is directed to the sample with a dichroic mirror which allows the collected fluorescence (green) to pass through it. The emission then encounters the lens L4 and is focused at the side port of the"
    },
    "2010.09890v1-Figure16-1.png": {
        "prediction": " The image shows an overview of the proposed method. \nThe input to the model is a sequence of RGB frames. Each frame is passed through a series of convolutional layers to extract visual features. The visual features are then flattened and passed through a fully connected layer to produce a fixed-length vector. This vector is used to initialize the hidden state of an LSTM. The LSTM then processes the sequence of visual features, and its output is used to predict the state of each object in the scene.",
        "reference": "Figure 16: Network architecture of the goal inference model, which encodes the symbolic state sequence in demonstrations and infers the count for each predicate."
    },
    "2110.07150v3-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces (Trace 1 and Trace 2) of direct call relations (C1, C2), the indirect call relations (C1, C4) and (C2, C6) are inferred. In addition, the direct call patterns (C1, C2) and (C2, C1) are also extracted.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by a word embedding layer. The output of the encoders is a sequence of vectors, each of which represents the meaning of the corresponding token in the input sequence.\n\nThe flowchart in Figure 8 illustrates the training procedure of a neural machine translation (NMT) model. The model consists of an encoder and a decoder. The encoder takes a source language sentence as input and produces a vector representation of the sentence. The decoder takes the vector representation of the source language sentence as input and generates a target language sentence. The model is trained by minimizing the loss between the generated target language sentence and the correct target language sentence.",
        "reference": "Figure 1: Illustration of our proposed Cross-Lingual, Retrieval-based GENQA pipeline."
    },
    "2101.11878v3-Figure1-1.png": {
        "prediction": " The image shows how to use a frozen detector to extract object bounding boxes and then extract image features. The frozen detector takes an image as input and outputs a list of bounding boxes and a list of class labels. The image features are extracted from the bounding boxes using a feature extractor. The feature extractor takes a list of bounding boxes and an image as input and outputs a list of image features.",
        "reference": "Figure 1: Intuitive illustration of how our model acquires knowledge during meta-learning. In particular, it learns a dictionary of component representations, which resemble individual object components. Some of these can be shared across different classes, e.g., car tires. In addition, it learns a map dictionary that contains common spatial activation patterns of components. During meta-testing, the knowledge base facilitates the learning of novel classes by re-using the already learned components and spatial activation patterns."
    },
    "2102.04335v1-Figure1-1.png": {
        "prediction": " * Figure 1: Illustration of execution traces and temporal relations.\n* Figure 7. Pretraining BERT. Architecture of BERT in pretraining. Assume that in the lowercased example sequence consisting of the segment pair \u2018he starts to speak. the nervous crowd is watch-ing him.\u2019 the tokens \u2018speak\u2019 and \u2018nervous\u2019 were sampled to be masked. \u2018speak\u2019 is replaced by the \u2018[MASK]\u2019 token and \u2018nervous\u2019 is replaced by the random token \u2018that\u2019. The model\u2019s task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.\n* Figure: Structural equation model of the hypothesized relationships between social determinants of health and COVID-19 disease outcomes among adults in the United States. All paths were significant at p < 0.05. The model accounted for 45% of the variance in COVID-19 disease and 38% of the variance in deaths due to COVID-19. CCVI-20: COVID Community Vulnerability Index; POP-20: Population Density; POWER-20: Socioeconomic Power Index; UNEMP-20: Unemployment Rate; UNINSU-20: Health Insurance Coverage; CS-COVID-20: Cumulative Incidence of COVID-19 Cases per 100,000; DE-COVID-20: Deaths Due to COVID-19 per 100,000.",
        "reference": "Figure 1. Research empirical model"
    },
    "2011.08706v1-Figure2-1.png": {
        "prediction": " In Figure 1, given two traces (Trace 1 and Trace 2) of a program execution, we aim to learn the direct and indirect relations between the functions. \n\nFor direct relations, we observe that functions C1 and C2 are directly called by C3 in both traces. Hence, we have the direct call relations {(C3, C1), (C3, C2)}. For indirect relations, we observe that C4 is called by C1 in Trace 1 and C6 is called by C2 in Trace 2. Hence, we have the indirect call patterns {(C1, C4), (C2, C6)}. Note that the indirect call relations are not guaranteed to hold in all executions. For example, if the condition in the if statement in Trace 1 is changed, C4 will not be called.\n\nIn Figure 7, the input to the Transformer-based language model is a sequence of tokens, where each token is represented by a word embedding. The model consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The output of the Transformer encoders is a sequence of hidden vectors, which are then used to predict the next token in the sequence.\n\nIn Figure 8, the input to the FPN is a feature map from the previous layer. The FPN consists of a stack of feature enhancement modules (FEMs). Each FEM consists of a convolutional layer, a batch normalization layer, and a ReLU activation function. The convolutional layer is used to extract features from the input feature map, while the batch normalization layer and the ReLU activation function are used to normalize the output of the convolutional layer and introduce non-linearity, respectively. The output of the FPN is a feature map that is used for object detection.",
        "reference": "Fig. 2. The framework of our proposed FPAENet method. ResNet-50 as the backbone to extract features. Two top-down channels are added in the FPN, and feature enhancement with attention is placed on the horizontal connection to enhance the effective information. Next, two parallel Fully Convolutional Networks to classify whether the candidate area is a lesion and locate the lesion."
    },
    "2012.13965v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted. Given two traces, denoted as Trace 1 and Trace 2, we can extract direct call relations and indirect call relations. The direct call relations are represented as a directed graph, where each node represents a function and each edge represents a direct call from the source function to the target function. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the calling functions and the other set of nodes represents the callee functions. An edge between a calling function and a callee function indicates that the calling function can call the callee function. We can also extract indirect call patterns from the indirect call relations. An indirect call pattern is a sequence of functions that are called one after another. For example, in the figure, there is an indirect call pattern C1->C4->C6. This pattern indicates that C1 calls C4, and C4 calls C6. On the right of the figure, we show how direct call patterns are extracted from the direct call relations. A direct call pattern is a sequence of functions that are called directly from one function to another. For example, in the figure, there is a direct call pattern C1->C2->C3. This pattern indicates that C1 calls C2, and C2 calls C3.",
        "reference": "Fig. 1: Pipeline of our method for computing IK on general soft robots by using three learned networks \u2013 1) forward kinematics Nfk, 2) Jacobian NJ and 3) sim-to-real mapping Ns2r . According to the current configurations of robot\u2019s actuation ci, the actuation ci+1 for reaching a target position (or shape) pi+1 is computed by the Jacobian-based iteration."
    },
    "1165810-Figure3-1.png": {
        "prediction": " The image shows the call graph of a program. The nodes in the graph represent functions, and the edges represent the call relations between functions. The direct call relations are represented by solid edges, and the indirect call relations are represented by dashed edges. The indirect call patterns are represented by dotted edges. The use cases of the program are represented by the boxes on the left side of the graph. The traces of the program are represented by the boxes on the right side of the graph.",
        "reference": "Figure 3: Eccentricity-dependent model: Inverted pyramid with sampling points. Each circle represents a filter with its respective receptive field. For simplicity, the model is shown with 3 scales."
    },
    "2011.07995v3-Figure1-1.png": {
        "prediction": " Figure 1 shows how traces are generated and used to learn temporal relations. Given two traces, trace 1 (C1) and trace 2 (C2), the direct call relations and indirect call relations are extracted. The direct call relations are used to learn the direct call patterns, and the indirect call relations are used to learn the indirect call patterns. The learned call patterns are then used to predict the execution time of a new trace.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nFigure 1 shows the study selection process for a study on the use of machine learning to identify diabetic retinopathy in fundus photographs. A total of 16,802 patients were identified as having at least one reconstruction view available in the health system during the study period. Of these, 5,151 were excluded due to missing data or other ineligibility criteria. The remaining 11,651 patients were included in the study.",
        "reference": "Figure 1: Patient flowchart. BI-RADS = Breast Imaging-Reporting and Data System; AD = architectural distortion; LCC = left craniocaudal; RCC = right craniocaudal; LMLO = left mediolateral oblique; RMLO = right mediolateral oblique."
    },
    "2103.09966v1-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. Then, the indirect call patterns are generated based on the indirect call relations. Finally, the direct call patterns are generated based on the direct call relations.\n\nThe architecture of BERT in pretraining is shown in Figure 7. The model consists of a stack of encoders, each of which is composed of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through the encoders, which learn to extract contextual information from the sequence. The output of the encoders is a sequence of vectors, each of which represents the contextualized representation of the corresponding token.\n\nFigure 7 shows a simplified diagram of a power system with a single generator, a single load, and a transmission line connecting them. The generator is represented by a voltage source Vg, and the load is represented by a constant impedance ZL. The transmission line is represented by a series impedance jXline. The power flow in the system is determined by the voltage phasors Vg and VL, the impedance ZL, and the line impedance jXline.",
        "reference": "Fig. 3: Single-line diagram of the test system."
    },
    "2102.10535v1-Figure4-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values represent the presence or absence of an indirect call relation between the functions. Additionally, indirect call patterns are extracted, which capture the patterns of indirect call relations between functions. Finally, the direct call patterns are extracted, which capture the patterns of direct call relations between functions.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of transformer encoders, which are used to learn the representations of the words in the text. The input to the model is a sequence of tokens, which are converted into vectors using word embeddings. The output of the model is a sequence of vectors, which represent the representations of the words in the text.\n\nFigure 8 shows the architecture of a code search system. The system consists of two main components: a code encoder and a query encoder. The code encoder is used to learn the representations of the code snippets, while the query encoder is used to learn the representations of the queries. The representations of the code snippets and queries are then compared to find the most relevant code snippets for the query.",
        "reference": "Figure 4. General CodeSearchNet architecture for all of our baselines. Each language is processed through different encoder mechanisms. The query encoder is shared (an NLP encoder), and the purpose of the CodeSearchNet tasks is to retrieve the most relevant code snippets subject to the natural language query."
    },
    "1087044-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. The process starts with parsing the program code to identify the direct call relations between functions. Then, the indirect call relations are identified by analyzing the control flow graph of the program. Finally, the temporal relations between the functions are extracted by analyzing the execution traces of the program.\n\nThe flowchart in Figure 7 illustrates the pre-training process of BERT. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.\n\nThe flowchart in Figure 8 illustrates the process of using a pre-trained BERT model to perform a relation prediction task. The input to the model is a pair of sentences, which are embedded into vectors by the word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sentences. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the relation labels. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.",
        "reference": "Figure 1: An illustration of our geometric scene parsing. Our task aims to predict the pixel-wise geometric surface labeling (first column) and the interaction relations between main regions (second column). Then the parsing result is applied to reconstruct a 3D model (third column)."
    },
    "2210.00705v2-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe image in Figure 7 shows the architecture of BERT in pretraining. The model consists of a stack of transformer encoders. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The embedded tokens are then passed through the transformer encoders, which learn to attend to each other and generate a representation of the input sequence. The final representation is used to predict the next sentence and the masked tokens.\n\nThe image in Figure 8 shows the architectures of (a) Parallel SpeechCLIP and (b) Cascaded SpeechCLIP. In both architectures, a Transformer encoder is used to extract audio features from the input audio. In Parallel SpeechCLIP, the audio features are then passed through a CLIP image encoder and a contrastive loss is computed between the output of the CLIP image encoder and the visual features extracted from the input image. In Cascaded SpeechCLIP, the audio features are first passed through a transformer encoder to generate a text embedding. The text embedding is then passed through a CLIP text encoder and a contrastive loss is computed between the output of the CLIP text encoder and the text features extracted from the input text.",
        "reference": "Fig. 2: An illustration of SpeechCLIP models. (a) A pre-trained HuBERT [12] extracts audio features. The features are concatenated with a learnable CLS token and fed into a transformer encoder layer to obtain a single vector representing the information of the entire sequence. The vector is then used to compute contrastive loss with the CLIP image encoder\u2019s output [23]. (b) Cascaded SpeechCLIP uses K CLS tokens to capture a small sequence of keywords from the audio signal. The keywords are batch-normalized and vector-quantized before passing to the CLIP text encoder. BN and VQ respectively denote batch normalization and vector quantization."
    },
    "2107.12847v1-Figure3-1.png": {
        "prediction": " The image shows a flowchart that is used to generate a human body model from a video. The flowchart starts with a video of a person walking. The video is then processed to extract the person's body shape and pose. The body shape is represented as a set of 3D points, and the pose is represented as a set of angles that describe the orientation of the person's body parts. The body shape and pose are then used to create a 3D model of the person. The model is then rendered to create a realistic image of the person.",
        "reference": "Figure 3: The proposed local recurrent modeling approach to human mesh recovery."
    },
    "2101.06808v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given a pair of traces, T1 and T2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the elements of the matrix represent the indirect call relations between the functions. The indirect call patterns are represented as a set of triples, where each triple represents an indirect call pattern. The flowchart in Figure 7 illustrates the pre-training process of BERT. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence. The flowchart in Figure 8 illustrates the training process of ERNIE. The input to the model is a pair of text sequences, which are embedded into vectors using a word embedding layer. The embedded sequences are then passed through a series of convolutional layers, which learn to extract features from the sequences. The output of the convolutional layers is then passed through a pooling layer to reduce the dimensionality of the features. The pooled features are then passed through a fully connected layer to predict the label of the text pair.",
        "reference": "Figure 1: An overview of the TREGO framework (detailed in Algorithm 1)."
    },
    "2202.00528v2-Figure1-1.png": {
        "prediction": " The image shows how to use a flowchart to represent an algorithm. The flowchart starts with a title, \"Use Case 1\", and then has two inputs, \"Trace t1\" and \"Trace t2\". The next step is to use these inputs to generate \"Direct Call Relations\" and \"Indirect Call Patterns\". These two outputs are then used to generate the final output, \"Indirect Call Patterns (C1, C2)\".",
        "reference": "Figure 1: Illustration for translation-oriented language models. X and Y denote source and target input, respectively. To enable translation, we adapt the LM self-attention mask to either the PrefixLM mask or CausalLM mask (top right), where filled black circles indicate disallowed attention. We also explore top-only encoding (Top Encoding) for PrefixLM which feeds the final-layer source encodings to generation similar to EncDec, rather than layer-wise coordinated encodings (He et al., 2018). Masks of EncDec are shown in the bottom right for comparison."
    },
    "2107.05802v2-Figure4-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect call relations between the functions. The figure also shows how the indirect call patterns are extracted from the indirect call relations. The indirect call patterns are represented as a set of triples, where each triple represents an indirect call pattern.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the encoder layers is a sequence of vectors, each of which represents the hidden state of the model at a particular position in the input sequence. These hidden states are then used to predict the next word in the sequence.\n\nFigure 8 illustrates the concept of a unit hypersphere and its projection onto a lower-dimensional subspace. The figure shows a 2D unit hypersphere (a circle) and its projection onto a 1D subspace (a line). The projection is done by taking the dot product of the vector representing the point on the hypersphere with the vector representing the subspace. The resulting value is a scalar that represents the distance of the point from the subspace.",
        "reference": "Figure 4: Left panel: An illustration of measuring the width of a set S (in green) in a direction g\u0302 by identifying x,y \u2208 S in maxx,y\u2208S g\u0302 \u00b7 (y \u2212 x). The expectation of this width using random vectors g \u223c N (0, ID\u00d7D) instead of g\u0302 is twice the Gaussian width w(S). Intuitively, it is the characteristic extent of the set T over all directions rescaled by a factor between D/ \u221a D + 1 and \u221a D. Right panel: Illustration of projecting manifolds on the unit sphere and Gordon\u2019s escape theorem. The same manifold far from the sphere will have a smaller projection to it than the one that is close, and therefore it will be harder to intersect with an affine subspace."
    },
    "2011.09361v1-Figure3-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted from traces. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect call relations between the functions. The temporal relations between the functions are represented as a sequence of events, where each event represents a function call.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of transformer encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the transformer encoders is a sequence of vectors, which are then used to predict the next token in the sequence.\n\nFigure 8 shows the knowledge distillation process. The teacher model is a large, pre-trained model that is used to train a smaller, student model. The student model is trained to mimic the output of the teacher model on a set of training data. The training data is divided into two sets: a training set and a validation set. The training set is used to train the student model, and the validation set is used to evaluate the performance of the student model. The student model is trained until it reaches a desired level of performance on the validation set.",
        "reference": "Fig. 3: The procedure followed for splitting the datasets into training, validation and testing sets used by KD-OP."
    },
    "2010.04560v3-Figure9-1.png": {
        "prediction": " The flowchart\u4e00\u958b\u59cb\u662f\u300c\u4f7f\u7528\u6848\u4f8b1\u300d\uff0c\u63a5\u8457\u5206\u6210\u300c\u76f4\u63a5\u95dc\u806f\u300d\u548c\u300c\u9593\u63a5\u95dc\u806f\u300d\u3002\n\n\u300c\u76f4\u63a5\u95dc\u806f\u300d\u7684\u5de6\u908a\u662f\u300c\u8ffd\u8e641\u300d\uff0c\u53f3\u908a\u662f\u300c\u8ffd\u8e642\u300d\uff0c\u4e2d\u9593\u7684\u5713\u5708\u5206\u5225\u662f\u300cC1\u300d\u3001\u300cC2\u300d\u3001\u300cC3\u300d\u3001\u300cC4\u300d\u3001\u300cC5\u300d\u548c\u300cC6\u300d\u3002\n\n\u300c\u9593\u63a5\u95dc\u806f\u300d\u7684\u5de6\u908a\u662f\u300cC1\u300d\u548c\u300cC2\u300d\uff0c\u53f3\u908a\u662f\u300cC1\u300d\u3001\u300cC4\u300d\u548c\u300cC6\u300d\u3002\n\n\u6700\u4e0b\u65b9\u7684\u300c\u76f4\u63a5\u547c\u53eb\u6a21\u5f0f\u300d\u7684\u5de6\u908a\u662f\u300cC1\u300d\u548c\u300cC2\u300d\uff0c\u53f3\u908a\u662f\u300cC1\u300d\u3001\u300cC3\u300d\u548c\u300cC5\u300d\u3002",
        "reference": "Figure 9: Flowchart of an explainable energy recommender system."
    },
    "2110.03143v1-Figure2-1.png": {
        "prediction": " In Figure 1, we show how traces are generated and how temporal relations are extracted. Given two traces Tr1 and Tr2, which are sequences of API calls, we first identify the direct call relations and indirect call relations. The direct call relations are the API calls that are directly called by other API calls. The indirect call relations are the API calls that are called by other API calls indirectly. For example, in Tr1, C1 directly calls C2, and C2 indirectly calls C3. We then use the direct call relations and indirect call relations to generate the indirect call patterns. The indirect call patterns are the sequences of API calls that are indirectly called by other API calls. For example, in Tr1, the indirect call pattern of C1 is (C2, C3). \n\nIn Figure 7, we show the Transformer model used in BERT. The Transformer model consists of a stack of encoder layers and a stack of decoder layers. Each encoder layer consists of a self-attention layer and a feed-forward layer. Each decoder layer consists of a self-attention layer, an encoder-decoder attention layer, and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence. The encoder-decoder attention layer allows the model to attend to different parts of the output sequence. The feed-forward layer is a fully connected layer that is used to project the output of the attention layer to a different dimension.\n\nIn Figure 8, we show the overall architecture of our approach. Our approach consists of two main components: a meta-update module and a domain adaptation module. The meta-update module is used to update the meta-parameters of the model. The domain adaptation module is used to adapt the model to the target domain. The meta-update module consists of a Faster R-CNN model and a meta-loss function. The Faster R-CNN model is used to generate the initial bounding boxes and the meta-loss function is used to update the meta-parameters of the model. The domain adaptation module consists of a feature extractor and a classifier. The feature extractor is used to extract features from the images and the classifier is used to classify the images.",
        "reference": "Figure 2. Overview of meta-learning strategy is presented on the left side and adaptation framework is presented on the right side. The meta-learning performs bi-level optimization, where the inner loop performs domain adaptation and the outer loop performs meta-update with respect to meta-loss. The updated meta-parameters are loaded as the initial condition of Faster-RCNN and this process is repeated. The adaptation framework performs adversarial feature alignment between source and target domain at the image and instance level."
    },
    "1292676-Figure4-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a matrix, where the rows and columns represent the functions and the values in the matrix represent the probability of an indirect call relation between the two functions.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through the encoders, which learn to extract features from the sequence. The output of the encoders is a sequence of vectors, each of which represents the features of a token in the sequence.\n\nFigure 8 illustrates the Long Short-Term Memory (LSTM) unit. It is a type of recurrent neural network (RNN) which is used in a variety of applications, such as natural language processing and speech recognition. The LSTM unit has a cell state, which is a vector that stores information over time. The cell state is updated at each time step based on the input to the unit and the previous hidden state. The output of the unit is a vector that is computed based on the cell state and the previous hidden state.",
        "reference": "Figure 4: An overview of the LS-DQN algorithm."
    },
    "1321977-Figure3-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations can be extracted. The direct call patterns and indirect call patterns can be further extracted from the direct call relations and indirect call relations, respectively.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of Transformer encoders. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through the Transformer encoders, which learn to attend to different parts of the sequence and extract meaningful features. The output of the Transformer encoders is a sequence of vectors, which are then passed through a softmax layer to predict the next token in the sequence.\n\nFigure 8 illustrates the training process of a language model. The model is trained on a large corpus of text. The training data is preprocessed to extract the vocabulary and the word frequencies. The vocabulary is used to create a word embedding matrix. The word frequencies are used to create a probability distribution over the vocabulary. The model is then trained to predict the next word in a sequence of words. The model is trained using a cross-entropy loss function. The loss function is calculated by comparing the model's predictions to the correct next words in the sequence. The model is trained using a gradient-based optimization algorithm, such as Adam. The optimization algorithm updates the model's parameters so as to minimize the loss function.",
        "reference": "Fig. 3. The architecture of JamBot. Chords and piano roll representations are extracted from the MIDI files in the training data (in black). The extracted chords and piano rolls are then used to train the chord and polyphonic LSTMs (in red). During music generation (in blue), the chord LSTM generates a chord progression that is used as input to the polyphonic LSTM which generates new music in MIDI format. When listening to the music, one can freely vary tempo and instrumentation."
    },
    "2010.14795v2-Figure3-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations can be inferred. For example, if C1 calls C2 in Tr1, and C2 calls C3 in Tr2, then an indirect call relation from C1 to C3 can be inferred. In addition, if C1 calls C4 in Tr1, and C4 calls C5 in Tr2, then an indirect call relation from C1 to C5 can be inferred. In this example, the direct call relations are C1->C2, C2->C3, C1->C4, and C4->C5. The indirect call relations are C1->C3, C1->C5, and C2->C5.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between different parts of the sequence. The output of the encoder layers is then fed to a softmax layer to produce a probability distribution over the vocabulary.\n\nFigure 8 illustrates the randomness-utilizing process. The process starts with a random variable $\\rho$, which is then passed through a function $\\Phi$ to produce a new random variable $\\Phi(\\rho)$. The random variable $\\Phi(\\rho)$ is then passed through a function $\\Psi$ to produce a new random variable $\\Psi(\\Phi(\\rho))$. The random variable $\\Psi(\\Phi(\\rho))$ is then passed through a function $\\tau$ to produce a new random variable $\\tau(\\Psi(\\Phi(\\rho)))$. The process is repeated until the desired output is obtained.",
        "reference": "FIG. 3: A generalized randomness-utilizing process \u03a6. If one intends to implement a certain quantum map \u03a8 utilizing a randomness source \u03c3 which has large enough min-entropy but is not a uniform random state, it could be implemented if one employs a broader notion of catalycity and allows the uncorrelated leftover randomness in the output state."
    },
    "2011.13183v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns, while the indirect call relations are used to generate the indirect call patterns. The direct call patterns and indirect call patterns are then used to generate the temporal relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to each other and generate a contextualized representation of the input sequence. The final output of the model is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence.\n\nThe flowchart in Figure 8 shows the architecture of a Faster R-CNN object detector. The detector consists of two main components: a region proposal network (RPN) and a Fast R-CNN detector. The RPN takes an image as input and outputs a set of rectangular object proposals. Each proposal is a bounding box that is likely to contain an object. The Fast R-CNN detector then takes the object proposals from the RPN and classifies each proposal into one of the foreground classes (e.g., person, car, bicycle) or the background class.",
        "reference": "Figure 1: The model architecture of TinaFace. (a) Feature Extractor: ResNet-50 [11] and 6 level Feature Pyramid Network [18] to extract the multi-scale features of input image. (b) Inception block to enhance receptive field. (c) Classification Head: 5 layers FCN for classification of anchors. (d) Regression Head: 5 layers FCN for regression of anchors to ground-truth objects boxes. (e) IoU Aware Head: a single convolutional layer for IoU prediction."
    },
    "2103.00815v1-Figure1-1.png": {
        "prediction": " In Figure 1, given two traces Tr1 and Tr2 of direct call relations, we aim to extract indirect call relations between functions. First, we identify the common functions between the two traces, i.e., C1 and C2. Then, for each common function, we extract the functions that are directly called by C1 and C2, respectively. We denote the sets of functions as C1\u2032 and C2\u2032. Finally, we compute the Cartesian product of C1\u2032 and C2\u2032 to obtain all possible indirect call relations between functions. \n\nIn Figure 7, the input sequence is converted into a sequence of embeddings through the word, segment, and positional embeddings. The embeddings are then fed into the Transformer encoder layers, which consist of self-attention and feed-forward layers. The output of the Transformer encoder is then fed into a softmax layer to predict the masked tokens.\n\nIn Figure 7, a feedforward neural network (FNN) is used to predict the next sentence. The input to the FNN is the concatenation of the representations of the [CLS] token and the last token of the input sequence. The output of the FNN is a two-dimensional vector, where the first element is the probability that the next sentence is the second sentence in the input sequence, and the second element is the probability that the next sentence is not the second sentence in the input sequence.",
        "reference": "Figure 1: The graph associated to a deep neural network with input dimension 3 and 5 layers"
    },
    "2202.06743v1-Figure4-1.png": {
        "prediction": " The figure shows how traces are generated and used to learn temporal relations. Given a set of traces, each trace is first processed to extract the direct call relations and indirect call relations. The direct call relations are used to construct a direct call graph, and the indirect call relations are used to construct an indirect call graph. Then, the direct call graph and the indirect call graph are used to learn temporal relations between the functions. Finally, the learned temporal relations are used to predict the execution time of a new function.",
        "reference": "Figure 4: Schematic representing the finite element discretization used for the numerical implementation of the proposed continuum formulation."
    },
    "2206.12772v1-Figure2-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of traces, we first extract direct call relations and indirect call relations between functions. Then, we use these relations to construct a call graph. Finally, we use the call graph to identify indirect call patterns.\n\nFigure 7 shows the overall architecture of BERT. \n\nFigure 8 shows the architecture of our proposed method.",
        "reference": "Figure 2: Framework Overview. We exploit a Siamese network, with two identical branches, each branch consists of an image encoder and an audio encoder. For the one branch, we perform transformations T 1vis + T 1 aud, while for the other branch, we use transformations T 2vis + T 2 aud. In this figure, T 1 vis only includes appearance transformation Tapp, while T 2 vis includes both appearance and geometric transformations Tapp + Tgeo. Both audio transformations are T\ud835\udc4e\ud835\udc62\ud835\udc51 . The framework is optimised by encouraging the audio-visual representation to be invariant to T\ud835\udc4e\ud835\udc5d\ud835\udc5d and T\ud835\udc54\ud835\udc52\ud835\udc5c , while being equivalent to T\ud835\udc54\ud835\udc52\ud835\udc5c ."
    },
    "2109.09113v1-Figure2-1.png": {
        "prediction": " The flowchart describes the training and evaluation of a deep learning model. The model is trained on a dataset of labeled data. The training process involves feeding the data into the model and adjusting the model's parameters to minimize the loss function. The evaluation process involves feeding the test data into the model and measuring the model's performance on the test data.",
        "reference": "Figure 2: The HPTQ framework. Dashed lines represent statistical information passing, which include also their updates, dotted lines represent data passing and solid lines represent an updated network."
    },
    "2011.13733v1-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe architecture of BERT in pretraining is shown in Figure 7. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The output of the encoders is a sequence of vectors, which are then used by the output layer to predict the next token in the sequence.\n\nFigure 8 shows the Inception module. It consists of four parallel convolutional layers with different kernel sizes. The output of each convolutional layer is then concatenated together and passed through a pooling layer. The pooling layer reduces the dimensionality of the output of the convolutional layers. The output of the pooling layer is then passed through a fully connected layer to produce the final output of the Inception module.",
        "reference": "FIG. 3: The overall schema of the Mini Inception-Resnet network. For the detailed modules, please refer to Figs. 4, 5 and 6."
    },
    "2101.10804v2-Figure1-1.png": {
        "prediction": " The image shows an overview of the Swin Transformer model. \nOn the left, there is an image divided into patches. Each patch is flattened and reshaped into a sequence of tokens. These tokens are then embedded using a patch embedding layer. \nIn the middle, there are several Swin Transformer blocks. Each block consists of a self-attention layer, a cross-attention layer, and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the image, while the cross-attention layer allows the model to attend to the text tokens. The feed-forward layer is used to add non-linearity to the model.\nOn the right, there is a linear layer and a softmax layer. The linear layer is used to project the output of the Swin Transformer blocks to the number of classes. The softmax layer is used to compute the probability of each class.",
        "reference": "Fig. 1. The overall architecture of proposed CPTR model."
    },
    "2205.14647v1-Figure2-1.png": {
        "prediction": " This is a flowchart that shows the steps of the SIMDRAM framework. The steps are as follows:\n\n1. The user inputs the desired operation, which is then converted into a MAJ/NOT logic operation.\n2. The MAJ/NOT logic operation is converted into a row allocation and \u03bcProgram generation, which is then stored in the main memory.\n3. The row allocation and \u03bcProgram are executed, and the result is stored in the memory controller.\n\nThe flowchart also shows the inputs and outputs of the SIMDRAM framework. The inputs are the user's desired operation and the output is the result of the \u03bcProgram execution.",
        "reference": "Figure 2: Overview of the SIMDRAM framework."
    },
    "2101.02550v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated. Given two traces Tr1 and Tr2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a graph where each node represents a function and each edge represents a direct call from the source function to the target function. The indirect call relations are represented as a bipartite graph where one set of nodes represents the functions and the other set of nodes represents the indirect call patterns. An edge between a function node and an indirect call pattern node indicates that the function has the corresponding indirect call pattern. \n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract meaningful features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nFigure 8 shows the SE module. Given an input feature map zl \u2208 RC\u00d7H\u00d7W, the SE module first uses a global average pooling layer to squeeze the feature map into a channel descriptor zl \u2208 RC\u00d71\u00d71. Then, two fully connected (FC) layers with ReLU activation are applied to zl to generate a scale vector \u03c9 \u2208 RC\u00d71\u00d71. Finally, the scale vector \u03c9 is multiplied with the input feature map zl element-wise to obtain the output feature map \u02dczl \u2208 RC\u00d7H\u00d7W.",
        "reference": "Fig. 2. The architecture of the first proposed ATM model, which is denoted as \u201cATMbef \u201d. From the figure, the output of the L-th LSTM layer is used to perform \u03c9, which is then used to extract the representative features at (L\u2212 1)-th LSTM layer in the SE model."
    },
    "2206.09770v1-Figure2-1.png": {
        "prediction": " The image shows the components of our proposed roadside camera system for traffic monitoring. The system consists of four main components: (1) roadside cameras, (2) camera calibration, (3) object detection and localization, and (4) information fusion and object tracking. The cameras capture video frames of the traffic scene, which are then processed by the object detection and localization component to detect and localize objects of interest, such as vehicles and pedestrians. The camera calibration component is used to undistort the images and transform them into a common coordinate system. The information fusion and object tracking component then fuses the detections from multiple cameras and tracks the objects over time. Finally, the system outputs the object ID, location, category, speed, heading, and other attributes of the detected objects.",
        "reference": "Fig. 2: An overview of the proposed framework for roadside vision-based traffic scene perception."
    },
    "1068967-Figure7-1.png": {
        "prediction": " The image shows how TransformerXL processes an input sequence. The input sequence is first embedded into a sequence of vectors. These vectors are then processed by a series of TransformerXL layers. Each TransformerXL layer consists of a self-attention mechanism and a feed-forward network. The self-attention mechanism allows the model to attend to different parts of the input sequence. The feed-forward network then processes the output of the self-attention mechanism. The output of the TransformerXL layers is then processed by a fully connected layer to produce the final output.",
        "reference": "Fig. 7: The architecture of face classifier on the shape-indexed local patches. \u201cConv\u201d means convolution, \u201cMP\u201d means max pooling, and N is the landmark number. The step size in convolution and pooling is 1 and 2, respectively."
    },
    "2012.14142v1-Figure2-1.png": {
        "prediction": " Figure 1 shows how to extract indirect relations from execution traces. Given two traces Tr1 and Tr2, we can extract direct call relations and indirect call relations. The direct call relations are the relations between the functions that are directly called by each function. The indirect call relations are the relations between the functions that are not directly called by each function, but are called by the functions that are called by each function. \n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model is composed of a stack of encoder layers, each of which consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn relationships between the different parts of the sequence. The output of the encoder layers is then fed to a softmax layer to produce a probability distribution over the vocabulary.\n\nFigure 8 shows the architecture of a convolutional neural network (CNN) for image classification. The CNN consists of a stack of convolutional layers, each of which is followed by a pooling layer. The convolutional layers learn to identify features in the input image, while the pooling layers reduce the dimensionality of the feature maps. The output of the final convolutional layer is then fed to a fully connected layer to produce a probability distribution over the classes.",
        "reference": "Fig. 2: The detailed structure of our HR-to-LR ultrasound image generation network."
    },
    "2109.00038v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations can be extracted. The direct call patterns and indirect call patterns can be further extracted based on the direct call relations and indirect call relations, respectively.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features from them. The output of the transformer layers is then passed through a softmax layer, which predicts the probability of each token in the sequence.\n\nThe left panel of Figure 2 shows the compartments in the SEIR model and the transitions between them. The compartments are: S (susceptible), E (exposed), I (infectious), and R (recovered). The transitions between the compartments are: \\(\\beta\\) (infection rate), \\(\\sigma\\) (incubation rate), \\(\\gamma\\) (recovery rate), and \\(\\alpha(t)\\) (\u51fa\u751f\u7387). The right panel of Figure 2 shows the compartments in the SIR model and the transitions between them. The compartments are: S (susceptible), I (infectious), and R (recovered). The transitions between the compartments are: \\(\\beta\\) (infection rate), \\(\\gamma\\) (recovery rate), and \\(m\\) (\u6b7b\u4ea1\u7387).",
        "reference": "Figure 1. Deriving the severity measure m/r from the epidemics compartmental model. SPEIRD model is schematically shown. Transitions between the compartments are denoted by solid arrows, with the transition rates indicated above arrows. The dashed arrow from I to S indicates the interaction of I and S (infections) leading to the transition to E. The dashed arrow from P to S indicates the potential (reverse) transition from P to S due to the easing of measures. The dashed rectangles indicate parts of the model corresponding to the disease transmission (the left rectangle) and the disease outcome for the detected cases (the right rectangle). The single arrows indicate parts of the model from which the reproduction number R(t) and the severity measure (m/r) are, respectively, inferred. The total number of detected cases (D) corresponds to the sum of A, H and F and is denoted by a double arrow. Compartments are S \u2013 susceptible, P \u2013protected, E \u2013 exposed, I \u2013infected, R \u2013 recovered, A \u2013 active, H \u2013 healed, F \u2013 fatalities, D \u2013 total number of detected cases. r and m represent recovery and mortality rates of active (detected) cases."
    },
    "2202.07728v2-Figure3-1.png": {
        "prediction": " The image shows an overview of the approach. Given an input point x, we first sample N random directions and compute the corresponding perturbed points xi = x + ei. Then, we compute the intermediate activations hi = f(xi) and the output activation h = g(h'). Finally, we use these values to compute the empirical estimate of the Lipschitz constant.",
        "reference": "Figure 3. Scaling strategy. In order to scale to very large models, we propose to estimate the bounds of an intermediate layer\u2019s activations empirically by (1) Sampling N input perturbations and (2) calculating empirical bounds on the resulting activations for the layer h(\u00b7). We can then form the set PxB which is a subset of the true boundsHxB since the sampling is never exhaustive. We can then plug this set into a verified perturbation analysis method (3) and continue the forward propagation of the inputs through the rest of the network."
    },
    "2204.11695v1-Figure2-1.png": {
        "prediction": " ## Indirect Call Patterns\nFigure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given two traces Tr1 and Tr2, the direct call relations and indirect call relations are extracted. The direct call relations are represented by the directed edges between the caller and callee functions, and the indirect call relations are represented by the dashed edges between the caller and callee functions. The indirect call patterns are then generated based on the indirect call relations. For example, the indirect call pattern (C1, C2) indicates that C1 calls C2 indirectly.",
        "reference": "Figure 2: Illustration of the proposed BREM. Untrimmed videos are first fed into the backbone to generate the 1D temporal feature, which is used to construct the feature pyramid and frame-level feature. REM adopts each pyramid feature as input and generates coarse proposals and scores. Then the aligned feature is used for refinement of action location and scores. In parallel, BEM acquires the frame-level feature as input and produces the multi-scale boundary quality map for localization quality prediction."
    },
    "2207.11906v2-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces (Trace 1 and Trace 2) with their corresponding CFGs, we can extract direct call relations and indirect call relations. For example, in the figure, C1 directly calls C2, and C1 indirectly calls C4 via C2. We can also derive indirect call patterns, such as (C1, C2) in this example. In addition, we can identify direct call patterns, such as (C1, C3) and (C2, C5), which are not shown in the figure.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, followed by a softmax layer. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer adds non-linearity to the model. The softmax layer is used to predict the next token in the sequence.\n\nFigure 8 shows the streaming architecture for neural speech recognition. The model consists of an encoder and a decoder. The encoder is a convolutional neural network (CNN) that extracts features from the input speech signal. The decoder is a recurrent neural network (RNN) that generates the output text. The model is trained end-to-end using a connectionist temporal classification (CTC) loss function.",
        "reference": "Fig. 1. Illustration of the proposed dual-mode ASR supernet training. When the encoder operates in the streaming mode, it is pruned by the binary mask (marked in purple). The predictor is pruned during streaming mode in the similar way, while intact during the nonstreaming mode."
    },
    "2010.06256v1-Figure3-1.png": {
        "prediction": " The image shows a flowchart that describes how to use a behavior tree. The flowchart starts with the root node, which is the main behavior tree. The root node has a reference to a subtree, which is the behavior tree that will be executed. The subtree can be a simple node, a composite node, or a decorator node. A simple node is a node that executes an action. A composite node is a node that contains a sequence of nodes that are executed in order. A decorator node is a node that modifies the behavior of another node. The flowchart shows how the behavior tree is executed by following the arrows from the root node to the leaf nodes. The leaf nodes are the actions that are executed.",
        "reference": "Figure 3. A meta-model for BehaviorTree.CPP (reverseengineered from its XML format)"
    },
    "2206.00770v2-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns. The direct call patterns and indirect call patterns are then used to generate the temporal relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embedded tokens are then passed through a series of transformer layers, which learn to attend to each other and generate a contextualized representation of the input sequence. The final output of the model is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence.\n\nThe flowchart in Figure 8 shows the components of an autonomous vehicle. The perception component is responsible for detecting and tracking objects in the environment. The planning component is responsible for generating a path for the vehicle to follow. The control component is responsible for executing the plan and controlling the vehicle's actuators. The localization component is responsible for determining the vehicle's position and orientation. The sensing component is responsible for collecting data from the environment. The vehicle interface component is responsible for communicating with the vehicle's sensors and actuators.",
        "reference": "Fig. 2. Overview of the Autoware.Auto software stack components"
    },
    "2012.06782v1-Figure3-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces (denoted as t1 and t2) of direct call relations (left), the indirect call relations (middle) and indirect call patterns (right) can be derived. A direct call relation from C1 to C2 indicates that C1 calls C2. An indirect call relation from C1 to C2 indicates that there is a path from C1 to C2 in the call graph. An indirect call pattern (C1, C2) indicates that there is a path from C1 to C2 in the call graph and C1 and C2 are in different loops. \n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence. The feed-forward layer is a fully connected layer that is used to transform the output of the self-attention layer. The output of the final Transformer encoder is used to predict the masked tokens in the input sequence.\n\nFigure 8 shows how to use an RNN to classify sentiment of a given sentence. The input to the RNN is a sequence of word embeddings. The output of the RNN is a vector of probabilities, where each probability corresponds to a sentiment class. The sentiment class with the highest probability is the predicted sentiment of the sentence.",
        "reference": "Fig. 3: The proposed CNN architecture for classification of different mental tasks. Note: M denotes the number of classes."
    },
    "1026139-Figure2-1.png": {
        "prediction": " The image shows an overview of the approach. Given a video, we first extract C3D features. Then, we use a proposal module to generate proposals. Next, we extract features from each proposal and use a Transformer to encode the video. We then use a decoder to generate captions for each proposal. Finally, we use a ranking module to rank the captions and select the best one.",
        "reference": "Figure 2: Complete pipeline for dense-captioning events in videos with descriptions. We first extract C3D features from the input video. These features are fed into our proposal module at varying stride to predict both short as well as long events. Each proposal, which consists of a unique start and end time and a hidden representation, is then used as input into the captioning module. Finally, this captioning model leverages context from neighboring events to generate each event description."
    },
    "2207.06011v1-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of traces, we first extract direct call relations and indirect call relations. Then, we use the direct call relations to build a direct call graph. For each indirect call relation, we find all possible indirect call patterns. Finally, we use the direct call graph and the indirect call patterns to generate the system dependence graph.\n\nFigure 7: Pretraining BERT. Architecture of BERT in pretraining. Assume that in the lowercased example sequence consisting of the segment pair \u2018he starts to speak. the nervous crowd is watch-ing him.\u2019 the tokens \u2018speak\u2019 and \u2018nervous\u2019 were sampled to be masked. \u2018speak\u2019 is replaced by the \u2018[MASK]\u2019 token and \u2018nervous\u2019 is replaced by the random token \u2018that\u2019. The model\u2019s task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.\n\nFigure 8: Fine-tuning BERT. Architecture of BERT in fine-tuning. The input representations are the same as in pretraining.",
        "reference": "Figure 1: System overview of SATTS during training and inference."
    },
    "2206.04962v1-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of traces, we first extract direct call relations and indirect call relations from them. Then, we use the direct call relations to construct a direct call graph, and use the indirect call relations to construct indirect call graphs. Finally, we use the direct call graph and the indirect call graphs to identify the indirect call patterns.\n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of Transformer encoders with masked language modeling (MLM) and next sentence prediction (NSP) objectives. The input to BERT is a sequence of tokens, each of which is represented by a word embedding. The word embeddings are then passed through the Transformer encoders, which learn to attend to each other and to the input embeddings. The output of the Transformer encoders is a sequence of hidden states, which are then used to predict the masked tokens and the next sentence.\n\nFigure 8 shows the architecture of the speech enhancement model. The model consists of two parts: an encoder and a decoder. The encoder is a stack of convolutional layers that extract features from the input speech. The decoder is a stack of convolutional layers that reconstruct the speech from the encoder's features. The model is trained to minimize the mean squared error between the input speech and the reconstructed speech.",
        "reference": "Fig. 1. The overall architecture of the proposed method. The clean speech S and interference I are fed into the ECAE . The interference consists of background noises, reverberation of both speech and noise signals. After the feature combination is extracted, as the first pre-task, the latent representation of the clean speech signal is learned via ECAE . As the second pre-task, the DM and ERM are estimated in the masking module. Besides, the proposed method utilizes the speech reconstruction losses of each pre-task to train the other pre-task. After the feature maps are recovered in the decoder, the reconstructed clean spectra are obtained as the output by using DCAE . By using the learned speech representation into the mixture representation, the estimated mixtures are produced from the mixture autoencoder (MAE) with unpaired and unseen training mixture spectra Y."
    },
    "2110.05666v1-Figure3-1.png": {
        "prediction": " The figure shows the measurement geometry and the HOPG sample mounted on a copper plate. The HOPG sample has a diameter of 1 mm and a thickness of 50 \u03bcm. The X-ray beam is focused to a spot size of 0.3 mm in diameter on the sample surface. The HOPG sample is heated by the X-ray beam and the temperature of the copper plate is kept constant at 300 K. The transmitted beam is measured by a detector placed behind the sample.",
        "reference": "Fig. 3 (Color online) Thermal model for estimation of temperature rise at at the graphite surface. See the text for details."
    },
    "2207.00422v2-Figure5-1.png": {
        "prediction": " Figure 1 illustrates how execution traces and temporal relations are extracted from the execution of a program. Given two traces of API calls, the direct call relations and indirect call relations are extracted. The direct call relations are the pairs of APIs that are called one after another. The indirect call relations are the pairs of APIs that are not called one after another but have a data dependency. For example, in the trace t1, the API C1 calls the API C2, and the API C2 calls the API C3. Therefore, the direct call relations are (C1, C2) and (C2, C3). In addition, the API C3 calls the API C6, and the API C6 calls the API C4. Therefore, the indirect call relations are (C3, C6) and (C6, C4).\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the meaning of the corresponding token in the context of the sequence.\n\nFigure 8 illustrates the contrastive learning framework for training a visually-aware review generation model. The model consists of a multi-modal encoder, a selection model, and a multi-modal decoder. The multi-modal encoder encodes the user's historical images and reviews into a joint embedding space. The selection model then selects a subset of the historical images that are most relevant to the user's current query. The multi-modal decoder then uses the selected images and the query to generate a personalized review.",
        "reference": "Figure 5: Illustration of our personalized showcases framework for the given business. We take user historical images and textual reviews as inputs. First, we select an image set that is most relevant to a user\u2019s interest. Thenwe generate natural language explanations accordingly with a multi-modal decoder. A cross-modal contrastive loss and a personalized contrastive loss are applied between each input modality and the explanations. Last, the selected images and generated textual explanations will be organized as multi-modal explanations to users."
    },
    "2103.11913v1-Figure8-1.png": {
        "prediction": " The figure shows how TransformerXL processes a sequence of tokens. Each token is represented by a vector (a blue dot in the figure). The vectors are processed by a series of layers (the red and blue blocks in the figure). The output of each layer is a new sequence of vectors. The final output of the TransformerXL model is a sequence of vectors, which can be used to predict the next token in the sequence.",
        "reference": "Figure 8: Illustration of the stencil that refers to the divergence matrix Dn,n+1."
    },
    "2210.07587v2-Figure1-1.png": {
        "prediction": " The image shows an overview of the approach. Given a premise and a hypothesis, the Few-shot QA model first converts the premise and hypothesis into their embeddings. Then, it computes the similarity between the premise and hypothesis embeddings using a bilinear function. Finally, the model predicts the label of the hypothesis by comparing the similarity score with a threshold.",
        "reference": "Figure 1: The overview of the CONENTAIL framework. By casting the classification as a nested entailment task, the model performs classification by telling if a query sentence q entails [premise example p entails hypothesis label h]. In a few-shot setting, the premise is an example sentence; in a zero-shot setting, the premise is a \u201cNULL\u201d placeholder."
    },
    "2204.00172v3-Figure2-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of traces, we first extract the direct call relations and indirect call relations. Then, we use the direct call relations to build a direct call graph, and use the indirect call relations to build an indirect call graph. Finally, we use the direct call graph and the indirect call graph to generate the indirect call patterns.\n\nFigure 7 shows the overall architecture of BERT. BERT consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, and the feed-forward layer allows the model to learn non-linear relationships between the input and output.\n\nFigure 8 shows the training procedure of BERT. First, the input sequence is tokenized and converted into a sequence of word embeddings. Then, the word embeddings are passed through the Transformer encoders. The output of the Transformer encoders is then used to predict the masked tokens in the input sequence.",
        "reference": "Fig. 2: An overview of our unified framework comprising a supervised branch that learns from source domain data with corresponding annotation, as well as an unsupervised branch that learns from unlabeled target domain data. We perform domain alignment both in the input-level via style-transfer with style references from the opposite domain, and the output-level of the model that guides the training on the target domain with more reliable pseudo-labels. The student model is trained by the combination of two losses, while the teacher model is updated with the exponential moving average weights of the student"
    },
    "2012.05858v3-Figure3-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given an input image \\(I_s\\), we first predict its shading and apply it to the warped image \\(I_w\\) to obtain the lit image \\(I_l\\). Then, we use a pre-trained CNN model to extract features from \\(I_l\\) and \\(I_s\\). Finally, we feed the features into a shading network to predict the shading of the input image.",
        "reference": "Figure 3: PCNet \u03c0\u0302 architecture and training. PCNet approximates the real project-and-capture process \u03c0 using a deep neural network (WarpingNet + ShadingNet). The inputs are a projector input image x, a camera-captured scene image (under normal light) Is, and a projector direct light mask Im. The output I\u0302x is an inferred camera-captured scene (under superimposed projection). WarpingNet consists of a learnable affine matrix \u03b8aff, thin-plate-spline (TPS) parameters \u03b8TPS and a grid refinement networkW\u03b8r . This coarse-to-fine pipeline allows WarpingNet to learn a fine-grained image sampling grid \u2126 to warp the projector input image x to the camera\u2019s canonical frontal view by \u03c6(x,\u2126), where \u03c6(\u00b7; \u00b7) is a differentiable image interpolator [16] denoted as \u2297. Then, we use the input projector direct light mask Im to exclude occluded pixels by \u03c6(x,\u2126) Im, where is element-wise multiplication. Afterwards, this warped projector image is further used to compute an intermediate rough shading image \u03c6(x,\u2126) Im Is to enforce the occlusion constraint. ShadingNet has a two-branch encoder-decoder structure to capture complex photometric transformations. In particular, it concatenates Is and \u03c6(x,\u2126) Im Is and feeds them to the middle encoder branch. Similarly, \u03c6(x,\u2126) Im is fed to the backbone encoder branch. The skip connections between the two branches model photometric interactions between the three inputs at different levels. In addition, we pass Is to the output layer through three convolutional layers. Finally, the feature maps are fused into one inferred camera-captured scene (under superimposed projection) I\u0302x by the backbone decoder."
    },
    "2102.09652v2-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct call relations between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions in Trace 1, the other set of nodes represents the functions in Trace 2, and the edges represent the indirect call relations between the functions in the two traces. Additionally, the indirect call patterns are extracted, which capture the patterns of indirect call relations between the functions in the two traces. The direct call patterns are represented as a bipartite graph, where one set of nodes represents the functions in Trace 1, the other set of nodes represents the functions in Trace 2, and the edges represent the direct call relations between the functions in the two traces.",
        "reference": "Figure 1: Illustration of the staggered meshes Kocc and Kvir for a quasi-2D system."
    },
    "2207.14568v1-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given an input utterance, we first extract a sequence of acoustic features. These features are then used to generate a sequence of pseudo-phone labels using a phone segmentation model. The pseudo-phone labels are used to train a GAN-based speech enhancement model. During inference, the GAN-based model is used to enhance the input speech, and the enhanced speech is used to generate a sequence of pseudo-phone labels. Finally, the pseudo-phone labels are used to train an HMM-based speech recognition model.",
        "reference": "Fig. 1: Framework overview: blue lines denote GAN training; yellow lines denote self re-training; black lines denote the paths which are not trainable."
    },
    "2107.02271v1-Figure7-1.png": {
        "prediction": " The flowchart\u4e00\u958b\u59cb\u662f\u90e8\u7f72\uff0c\u7136\u5f8c\u662f\u7db2\u7d61\u521d\u59cb\u5316\uff0c\u63a5\u7740\u662f\u6a21\u578b\u9078\u64c7\uff0c\u7136\u5f8c\u662f\u8003\u616e\u63a5\u6536\u8005\u611f\u77e5\u7684\u901a\u4fe1\uff0c\u6700\u5f8c\u662f\u57fa\u65bc\u6a21\u578b\u7684\u6578\u64da\u901a\u4fe1\u76e3\u6e2c\u3002",
        "reference": "FIGURE 7: Overview of the model-based data communication."
    },
    "2107.05792v4-Figure3-1.png": {
        "prediction": " An MLR study involves gathering requirements from multiple stakeholders and using them to create a model that can be used to classify issues. The model is then used to classify new issues as they are reported.\n\nThe flowchart in Figure 1 illustrates the process of using MLR to classify issues. The process starts with the collection of requirements from multiple stakeholders. These requirements are then used to create a model that can be used to classify issues. The model is then used to classify new issues as they are reported.\n\nThe flowchart in Figure 7 illustrates the process of using BERT for pre-training. The process starts with the input of a sequence of tokens. These tokens are then embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers. The output of the transformer layers is then used to predict the next token in the sequence.\n\nFigure 2 shows an example of how MLR can be used to classify issues. In this example, the model is used to classify a new issue as either a bug or a feature request. The model makes this decision based on the requirements that have been gathered from the stakeholders.\n\nFigure 3 shows an example of how BERT can be used for pre-training. In this example, the model is used to predict the next word in a sequence of words. The model makes this prediction based on the words that have already been seen in the sequence.",
        "reference": "Figure 3: Example of classification process used to analyze the contributions by GL use."
    },
    "2010.07804v1-Figure2-1.png": {
        "prediction": " Figure 1 shows an overview of how execution traces are generated and how temporal relations are extracted. Given two traces, denoted as Trace 1 and Trace 2, which consist of the execution of a set of system calls, we aim to extract the direct and indirect relations between system calls. First, we construct a pseudo-graph \\(G\\) based on the traces, where each node in \\(G\\) represents a system call and an edge between two nodes indicates that the corresponding system calls are executed sequentially in at least one of the traces. We then refine the pseudo-graph \\(G\\) by removing the edges that are not supported by both traces. \n\nNext, we extract direct call relations from the refined pseudo-graph. For each node \\(C_i\\) in the refined pseudo-graph, we check whether there is an edge from \\(C_i\\) to \\(C_j\\) and an edge from \\(C_j\\) to \\(C_i\\). If both edges exist, we add a direct call relation between \\(C_i\\) and \\(C_j\\). \n\nFinally, we extract indirect call relations from the refined pseudo-graph. For each pair of nodes \\(C_i\\) and \\(C_j\\) in the refined pseudo-graph, we check whether there is a path from \\(C_i\\) to \\(C_j\\) and a path from \\(C_j\\) to \\(C_i\\). If both paths exist, we add an indirect call relation between \\(C_i\\) and \\(C_j\\).",
        "reference": "Figure 2: Overview of CIMON. (a) CIMON generates semantic information including refined pseudo-graph and confidence matrix for reliable guidance. (b) With data augmentation, CIMON generates two groups of semantic information. The parallel and cross semantic consistency is constructed between features and hash codes under the same group and crossing the different groups respectively. The contrastive consistency is based on two groups of hash codes."
    },
    "215238689-Figure3-1.png": {
        "prediction": " The figure shows an overview of the neural network model for slot filling. Given the adjacency matrix \\(A\\), the model first constructs a schema graph \\(G\\) and obtains the node embedding, token embedding, segment embedding, and position embedding. Then, it uses a slot-gate classifier to determine whether each node is a slot node. If a node is a slot node, the model uses an RNN-based value decoder to predict the corresponding slot value.",
        "reference": "Figure 3: The overview of the proposed CSFN-DST. It takes the current dialogue utterance, the previous dialogue state and the schema graph as the input and predicts the current dialogue state. It consists of an embedding layer, context and schema fusion networks, a slot-gate classifier and an RNN-based value decoder."
    },
    "1016639-Figure2-1.png": {
        "prediction": " The image shows a flowchart of a machine translation system. The flowchart has three main stages: training, decoding, and evaluation.\n\nThe training stage takes as input a parallel corpus of source and target language sentences. The corpus is first preprocessed to tokenize the sentences and learn a vocabulary. The preprocessed corpus is then used to train a neural network model. The model is trained to maximize the probability of the correct target language sentence given a source language sentence.\n\nThe decoding stage takes as input a source language sentence. The sentence is first preprocessed to tokenize it and convert it to a vector of word indices. The vector of word indices is then passed through the neural network model. The model outputs a vector of probabilities over the target language vocabulary. The vector of probabilities is then used to generate a target language sentence.\n\nThe evaluation stage takes as input a set of source language sentences and their corresponding target language translations. The translations are generated by the decoding stage. The evaluation stage compares the generated translations to the human-generated translations. The evaluation stage outputs a score that measures the quality of the generated translations.",
        "reference": "Figure 2. Our multi-task convolutional neural network (CNN). The model is based on CNN with two task branches for classification and caption embedding respectively."
    },
    "2010.07860v3-Figure1-1.png": {
        "prediction": " In Figure 1, we show how execution traces are generated and how temporal relations are extracted from them. Given two traces Tr1 and Tr2, the direct call relations R1 and R2 are extracted. Note that a direct call relation (e.g., C1 \u2192 C2) indicates that C1 directly calls C2. We then identify the indirect call relations between the functions in F by analyzing the direct call relations. For example, if C1 \u2192 C2 and C2 \u2192 C3, then we can infer that there is an indirect call relation from C1 to C3 (denoted as C1 \u2192 C3). In this example, C1 and C3 are said to be indirectly related. We denote the set of all indirect call relations as R. \n\nIn Figure 7, we illustrate the architecture of BERT in pretraining. BERT consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens. The Transformer encoders are stacked together, and the output of the final Transformer encoder is used to predict the masked tokens in the input sequence.\n\nIn Figure 8, we show the overall architecture of our approach. Given a set of traces, we first extract the direct call relations and the indirect call relations. We then use the direct call relations and the indirect call relations to construct a call graph. Finally, we use the call graph to predict the fault-proneness of the functions in the system.",
        "reference": "Figure 1. Architecture of a deep conditional transformation model. Both the shift and interaction predictor can potentially be defined by a structured network including linear terms, (penalized) splines or other structured additive regression terms and deep neural network defined by an arbitrary network structure. While the shift predictor (C\u03a8) is a sum of both subnetwork predictions, the interaction predictor (A B) is only multiplied with a final 1-hidden unit fully-connected layer (network head, vec(\u0393)) after the structured predictors and latent features of the deep neural network are combined with the basis evaluated outcome. The shift and interaction network part together define the transformation function, which transforms the error distribution and yields the final likelihood used as loss function."
    },
    "2011.08010v1-Figure1-1.png": {
        "prediction": " The flowchart\u4e00\u958b\u59cb\u662f\u5169\u500b\u4e26\u3093\u3067\u7684\u77e9\u5f62\uff0c\u5206\u5225\u662f\u300cuse case 1\u300d\u548c\u300cuse case 2\u300d\uff0c\u7bad\u982d\u7531\u300cuse case 1\u300d\u6d41\u5411\u300cuse case 2\u300d\u3002\n\n\u300cuse case 1\u300d\u7684\u5de6\u908a\u6709\u5169\u500b\u5713\u89d2\u77e9\u5f62\uff0c\u5206\u5225\u662f\u300cTrace t1\u300d\u548c\u300cTrace t2\u300d\uff0c\u7bad\u982d\u7531\u300cTrace t1\u300d\u6d41\u5411\u300cTrace t2\u300d\u3002\n\n\u300cuse case 2\u300d\u7684\u5de6\u908a\u6709\u56db\u500b\u5713\u89d2\u77e9\u5f62\uff0c\u5206\u5225\u662f\u300cdirect call relations\u300d\u3001\u300cindirect relations\u300d\u3001\u300cindirect call patterns (C1, C2)\u300d\u548c\u300cdirect call patterns (C1, C2)\u300d\uff0c\u7bad\u982d\u7531\u300cdirect call relations\u300d\u6d41\u5411\u300cindirect relations\u300d\uff0c\u7531\u300cindirect relations\u300d\u6d41\u5411\u300cindirect call patterns (C1, C2)\u300d\u548c\u300cdirect call patterns (C1, C2)\u300d\n\n\u300cindirect call patterns (C1, C2)\u300d\u548c\u300cdirect call patterns (C1, C2)\u300d\u7684\u53f3\u908a\u6709\u4e00\u500b\u5927\u77e9\u5f62\uff0c\u7bad\u982d\u7531\u300cindirect call patterns (C1, C2)\u300d\u548c\u300cdirect call patterns (C1, C2)\u300d\u6d41\u5411\u8a72\u77e9\u5f62\uff0c\u77e9\u5f62\u5167\u6587\u5b57\u70ba\u300ctemporal relations\u300d\u3002",
        "reference": "Figure 1: The inference pipeline of our model. The two-stage model first generates a segmentation mask from Sentinel-2 imagery in Stage 1, and then combines Sentinel-2 imagery, initial coarse output, and crowdsourced points in Stage 2 to generate the final segmentation mask. Points collected from either a Trained Collector or Social Media model can be used interchangeably in this model."
    },
    "2210.00486v1-Figure2-1.png": {
        "prediction": " The flowchart describes the process of using a joint model to perform a task. The process starts with the privileged party sending a message to the assistant party. The assistant party then sends a message to the joint model. The joint model then sends a message back to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends a message to the joint model. The joint model then sends a message back to the privileged party. The privileged party then sends a message to the assistant party. The assistant party then sends a message back to the privileged party. The privileged party then sends",
        "reference": "Figure 2: Overview of pMPL"
    },
    "2107.10538v1-Figure4-1.png": {
        "prediction": " Figure 1 shows the call graph of a program with two functions, C1 and C2. C1 calls C2 directly, and C2 calls C1 indirectly through C3. The arrows between the functions represent the call relations. The dashed arrows represent the indirect call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that learns representations of words in a sentence. The model is composed of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the representation of a token in the input sequence.\n\nFigure 8 shows the architecture of MAGS. MAGS is a multi-agent system for voice-based communication. The system consists of four agents: a voice record agent, a voice recognition agent, a translation agent, and a storage agent. The voice record agent records the user's voice. The voice recognition agent recognizes the user's speech. The translation agent translates the user's speech into text. The storage agent stores the user's voice and text data. The agents communicate with each other using messages. The arrows between the agents represent the message flow.",
        "reference": "Fig. 4: MAGS model of MCCOMP+DIV."
    },
    "2202.05132v2-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns. The direct call patterns and indirect call patterns are then used to generate the temporal relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The embedded tokens are then passed through the encoders, which learn to extract contextual information from the sequence. The output of the encoders is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence.\n\nFigure 8 shows how BERT can be used for text classification. The input to the model is a sequence of tokens, which are embedded into vectors by the word embedding layer. The embedded tokens are then passed through the encoders, which learn to extract contextual information from the sequence. The output of the encoders is a sequence of vectors, each of which represents the contextualized representation of a token in the input sequence. The final representation of the sequence is obtained by pooling the vectors in the sequence. The pooled representation is then passed through a feed-forward layer to produce the output of the model.",
        "reference": "FIG. 1. (a) Representation of the operator state \u03c1op(t) [Eq. (1)]. Each qubit in Qout is prepared in a maximally entangled state (black dots) with the corresponding qubit Qin, before being time evolved under the channel Nt. (b) Illustration of the Hayden-Preskill protocol [5]. An unknown quantum state |\u03c8\u3009 is used as an input to a small subregion A, while the remaining qubits (B) are prepared in a maximally entangled state with a set of ancillas B\u2032 (circled). If the channel is perfectly scrambling then |\u03c8\u3009 can be reconstructed using the ancillas combined with a subset of output qubits C of the same size as A, regardless of which qubits are in C (qubits in D are discarded). Formally, the final state of the ancillas combined with the outputs C depends on the input state to A through the channel NA\u2192B \u2032C"
    },
    "2202.06453v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 (C1) and trace 2 (C2), the direct call relations and indirect call relations are extracted. The direct call relations are represented by the directed edges between the nodes in the graph, where each node represents a function. The indirect call relations are represented by the dashed edges between the nodes, where each node represents a function. The indirect call patterns are then generated based on the indirect call relations. Finally, the direct call patterns are generated based on the direct call relations.",
        "reference": "Figure 1: Simulation diagram for an interconnection between a circuit of interest and a load."
    },
    "2010.09602v1-Figure5-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given an input utterance x, the acoustic model first computes a sequence of hidden states h. These hidden states are then used to compute a sequence of attention weights over the input tokens. The attention weights are then used to compute a weighted sum of the token embeddings, which is then used to compute a probability distribution over the output tokens.\n\nFigure 7 shows the architecture of the Transformer model used in our experiments. The Transformer model consists of a stack of Transformer layers. Each Transformer layer consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output tokens.\n\nFigure 8 shows the training procedure for our model. The model is trained to minimize the cross-entropy loss between the predicted output tokens and the target output tokens. The model is trained using a batch size of 16 and a learning rate of 0.001. The model is trained for a total of 100 epochs.",
        "reference": "Fig. 5: Detailed architecture of proposed TTS model. Dashed line denotes feedback loop. Number in bracket denotes neural layer size. FC denotes a fully connected layer. During inference, only prior and decoder are used."
    },
    "2204.10523v1-Figure1-1.png": {
        "prediction": " The figure shows a graphical model representing a Bayesian network. The nodes in the graph represent random variables, and the edges represent conditional dependencies between the variables. The nodes B, $\\mu$, and W represent the bias, mean, and weight of a multivariate Gaussian distribution, respectively. The node y_n represents the label of the n-th data point, and the node x_\\{m,n} represents the m-th feature of the n-th data point. The plate notation indicates that the model is replicated for each data point n and each feature m. The arrows indicate that the distribution of each random variable is conditioned on the values of its parent variables.",
        "reference": "Figure 1: The probabilistic graphical model of two-covariance PLDA"
    },
    "2011.12854v6-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the execution of a program. The process starts with the execution of the program, which results in a trace of function calls. This trace is then analyzed to identify direct and indirect call relations between functions. Finally, the temporal relations between these functions are extracted.\n\nThe flowchart in Figure 7 illustrates the architecture of BERT, a pre-training model for natural language processing. The model consists of a stack of transformer encoders, which are used to learn representations of the input text. The output of the encoders is then used to predict the masked words in the input text.\n\nThe flowchart in Figure 8 illustrates the architecture of a neuro-symbolic model for visual question answering. The model consists of a visual encoder, a language encoder, and a reasoning module. The visual encoder is used to learn a representation of the input image, while the language encoder is used to learn a representation of the input question. The reasoning module then uses these representations to answer the question.",
        "reference": "Figure 3: Neuro-Symbolic XIL for improved explanations and interaction. (Top) Neuro-Symbolic Concept Learner with Slot-Attention and Set Transformer. (Bottom) Neuro-Symbolic revision pipeline with explanations of the model before (gray) and after applying the feedback (orange)."
    },
    "2207.04317v1-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect calls. The edges between the two sets of nodes represent the indirect call relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors. The embeddings are then passed through the encoders, which learn to extract features from the sequence. The output of the encoders is a sequence of vectors, which are then used to predict the next token in the sequence.\n\nThe flowchart in Figure 8 illustrates the process of collaborative filtering. The model takes as input a user-item interaction matrix, which represents the interactions between users and items. The model then learns to predict the missing interactions. The predicted interactions can then be used to recommend items to users.",
        "reference": "Figure 1: Illustration of counterfactual explanation in neural recommender systems."
    },
    "2202.02646v1-Figure1-1.png": {
        "prediction": " The flowchart describes the process of using a pre-trained language model (PLM) to perform QA on a document. The input to the process is a question and a document. The first step is to use the PLM to generate an embedding for the question. The next step is to use the embedding to retrieve relevant passages from the document. The final step is to use the PLM to answer the question using the retrieved passages.",
        "reference": "Figure 1: Architectural illustration of the RERRFACT scientific claim verification pipeline."
    },
    "117714-Figure2-1.png": {
        "prediction": " An illustration of execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations (left), we can derive indirect relations (middle) and indirect call patterns (right). For example, given the direct call relations C1->C2 and C2->C3 in Tr1 and C4->C5 and C5->C6 in Tr2, we can derive indirect relations C1->C3, C1->C5, C1->C6, C2->C5, and C2->C6. Furthermore, we can derive indirect call patterns (C1, C2)->(C5, C6) and (C2, C3)->(C5, C6). Note that the indirect call patterns capture the fact that C1 and C2 are both callers of C5 and C6, and C2 and C3 are both callees of C5 and C6.",
        "reference": "Figure 2. Visualization of our motion-triggered model. Our model consists of an RNN with LSTM cell encoder (blue block) and a Policy Network (yellow block). At each frame, RNN will generate an anticipated intention according to a new embedded representation g and the previous hidden state h of the RNN. The policy will generate the motion-trigger decision a for next frame, based on motion representation fm and the hidden state h of the RNN. The orange circle represents the fusion operation (details in Sec. 3.2). The red and black circles represent a trigger and non-trigger decision of policy network, respectively (details in Sec. 3.3). When a = 0, fo is empty since it is not processed."
    },
    "2107.02290v1-Figure8-1.png": {
        "prediction": " Figure 1 shows the traces of API calls. Each node in the graph represents an API call, and the edges represent the order of the API calls. The leftmost node is the first API call in the trace, and the rightmost node is the last API call in the trace. The labels on the edges represent the time difference between the two API calls.\n\nThe figure also shows the direct and indirect relationships between the API calls. The direct relationships are represented by the solid edges, and the indirect relationships are represented by the dotted edges. The direct relationships represent the API calls that are directly related to each other, and the indirect relationships represent the API calls that are related to each other through other API calls.\n\nThe figure shows two use cases. The first use case is a simple API call that does not have any indirect relationships. The second use case is a more complex API call that has several indirect relationships.",
        "reference": "FIGURE 8 Coaxial cable model and its cross section. The inner, black region \u03a90 models the copper wire, the white region \u03a91 the air insulator and the outer, gray region \u03a92 the conducting shield35."
    },
    "2206.13082v1-Figure2-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given a set of execution traces, we first extract the direct call relations and indirect call relations from each trace. Then, we construct the indirect call patterns based on the indirect call relations. Finally, we use the direct call relations and indirect call patterns to predict the vulnerable functions.\n\nFigure 7 shows the overall architecture of BERT. It consists of a stack of transformer encoders, with each encoder consisting of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the final encoder is a sequence of vectors, which are then used to predict the next word in the sequence.\n\nFigure 8 shows the Transformer-based encoder layer used in BERT. The encoder layer consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input tokens.",
        "reference": "Fig. 2. Illustration of the PST architecture. (a) is the main component of PST, which follows an encoder-decoder pipeline. (b) is a demonstration of dynamic voxelization used in the encoding part."
    },
    "2107.08402v1-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent the functions and the edges represent the direct calls between the functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents the functions and the other set of nodes represents the indirect calls. The edges between the two sets of nodes represent the indirect call relations.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The vectors are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe flowchart in Figure 8 shows the federated learning process. In this process, multiple clients collaboratively train a global model. Each client has its own local data and local model. The clients first train their local models on their own data. Then, they upload their local model updates to the server. The server aggregates the local model updates and updates the global model. The updated global model is then downloaded by the clients, who use it to train their local models again. This process is repeated until the global model converges.",
        "reference": "Fig. 2: Overview of Proposed Methods"
    },
    "2102.04593v1-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. First, the direct call relations are extracted from the code, which are represented as a graph where each node represents a function and each edge represents a call relation between two functions. Then, the indirect call relations are extracted, which are represented as a graph where each node represents a function and each edge represents a call relation between two functions that are not directly called by each other but through a chain of other functions. Finally, the direct and indirect call patterns are extracted, which are represented as a graph where each node represents a pair of functions and each edge represents a call relation between the two functions. The extracted temporal relations can be used to analyze the program's behavior and identify potential vulnerabilities.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is trained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence. The model is trained to predict the next word in a sequence given the previous words in the sequence. The loss function is the cross-entropy loss between the predicted next word and the actual next word.\n\nFigure 8 shows the architecture of a GAN. A GAN consists of two neural networks: a generator and a discriminator. The generator takes a random noise vector as input and generates an image. The discriminator takes an image as input and outputs a probability that the image is real or fake. The generator and discriminator are trained together in an adversarial game. The generator tries to generate images that the discriminator cannot distinguish from real images, while the discriminator tries to distinguish between real images and fake images.",
        "reference": "Figure 3. DCGAN architecture"
    },
    "2012.09727v2-Figure1-1.png": {
        "prediction": " Figure 1 shows an overview of our approach. Given an input trace, we first identify the direct call relations and indirect call relations. Then, we extract the indirect call patterns from the indirect call relations. Finally, we use the direct call relations and indirect call patterns to generate the system call graph.\n\nFigure 7 shows the overall architecture of BERT. The model consists of a stack of transformer encoders. Each transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The model is trained using a masked language modeling objective. During training, the model is given a sequence of words, and it is asked to predict the masked words. The model is trained to minimize the cross-entropy loss between the predicted words and the true words.\n\nFigure 8 shows the architecture of our proposed streaming end-to-end speech separation model. Given a multi-channel mixture speech as input, the model first performs speech separation to separate the speech of each speaker. Then, the separated speech is processed by the speaker profile module to extract the speaker\u2019s embedding. Finally, the embedding is used to select the corresponding speech enhancement module for each speaker to enhance the separated speech.",
        "reference": "Fig. 1. (A) The architecture of the proposed continuous speech separation using speaker inventory. The Speaker inventory construction module forms the speaker inventory from the long mixture by using Kmeans clustering; the long mixture is split into small segments, and the speaker profile selection module selects two relevant profiles from the inventory for each segment; the speech separation module fuses the selected speaker profiles into the system for source separation. (B) Multiplicative adaptation of the selected profiles ep1 and ep2 . (C) Stitching procedure of adjacent segment outputs in a long recording."
    },
    "2101.05804v3-Figure1-1.png": {
        "prediction": " The figure shows how the number of independent samples \\(n\\) relates to the information gain \\(\\Phi\\) in different regimes of the Thouless-Anderson-Palmer (TAP) free energy. a TAP free energy landscape with many metastable states, each of which has a free energy \\(\\approx m_0\\). The states are separated by free energy barriers of height \\(\\approx T\\), where \\(T\\) is the temperature. In this regime, \\(n \\gg m_0\\) independent samples are needed to reconstruct the free energy landscape. b TAP free energy landscape with a single metastable state of free energy \\(m_0\\). The free energy barriers separating this state from the other states are of height \\(\\approx T\\). In this regime, \\(n \\approx m_0\\) independent samples are needed to reconstruct the free energy landscape. c TAP free energy landscape with a single metastable state of free energy \\(m_0\\). The free energy barriers separating this state from the other states are of height \\(\\approx T\\). In this regime, \\(n \\ll m_0\\) independent samples are needed to reconstruct the free energy landscape.",
        "reference": "Figure 1: Illustration of the three relevant physical regimes for a system of massive fermions [purple dots] with a long range interaction mediated by a scalar field [blue]. T and n are the characteristic fermion kinetic energy and number density, respectively. For ultrarelativistic fermions, in panel (a); and interparticle distances above the interaction range \u223c M\u22121\u03c6 , in panel (c); there are no long range effects."
    },
    "2107.02955v1-Figure2-1.png": {
        "prediction": " Figure 1: A prismatic joint.",
        "reference": "Fig. 2. Our model for the flat elastic terrain. Each tile is connected to the floor by a spring-loaded prismatic joint."
    },
    "2207.04913v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces Tr1 and Tr2 of direct call relations, the indirect call relations can be inferred. For example, if C1 calls C2 in Tr1, and C2 calls C3 in Tr2, then an indirect call relation from C1 to C3 can be inferred. \n\nFigure 7 shows the architecture of BERT in pretraining. BERT consists of a stack of encoders. Each encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The input to the first encoder is a sequence of word embeddings. The output of the last encoder is a sequence of hidden vectors. These hidden vectors are then used to predict the masked tokens in the input sequence.\n\nFigure 8 shows the training procedure of contrastive learning. In contrastive learning, the model is trained to distinguish between positive and negative pairs of data points. A positive pair is a pair of data points that are similar to each other, while a negative pair is a pair of data points that are dissimilar to each other. The model is trained to maximize the similarity between positive pairs and minimize the similarity between negative pairs.",
        "reference": "Fig. 1. An overview of our WDRDG framework, consisting of three components: (a) Wasserstein uncertainty set construction for each class based on the empirical Wasserstein barycenters and radius obtained from given source domains. One constraint is added to control the discriminability of LFDs; (b) distributionally robust optimization to solve for the least favorable distributions; (c) adaptive inference for target testing samples based on probability mass on LFDs and coupling matrix from optimal transportation between barycenter samples and target samples."
    },
    "2202.13626v1-Figure3-1.png": {
        "prediction": " Figure 3: Sequence diagram of the proposed approach. The user sends a request to the CL server, which then captures an image and transfers it to the CL server. The CL server authenticates the user and transfers the image to the remote IoT cloud for activity detection. The remote IoT cloud processes the image and issues an action command to the local IoT device. The local IoT device executes the action and sends a notification to the user.",
        "reference": "Figure 3: Remote control (IFTTT) of home IoT service with the centralized learning model."
    },
    "19488885-Figure4-1.png": {
        "prediction": " The figure shows the architecture of a neural network model for natural language processing. The model consists of two main components: a shared BiLSTM encoder and a task-specific MLP classifier. The encoder takes as input a sequence of words and produces a sequence of hidden states. The classifier takes as input the sequence of hidden states and produces a probability distribution over a set of labels.\n\nThe shared BiLSTM encoder consists of two BiLSTM layers. The first layer takes as input a sequence of word embeddings and produces a sequence of hidden states. The second layer takes as input the sequence of hidden states from the first layer and produces a sequence of hidden states.\n\nThe task-specific MLP classifier consists of a single fully connected layer. The layer takes as input the sequence of hidden states from the encoder and produces a probability distribution over a set of labels.",
        "reference": "Figure 4: MTL model. Token representations are computed both by a task-specific and a shared BiLSTM. Their outputs are concatenated with the parser state embedding, identical to Figure 2, and fed into the task-specific MLP for selecting the next transition. Shared parameters are shown in blue."
    },
    "2011.10683v1-Figure5-1.png": {
        "prediction": " The image shows how indirect call relations are extracted from traces. Given two traces (left), the direct call relations (C1, C2) and indirect call relations (C1, C4) are extracted (middle). Note that the indirect call relations are extracted by observing the execution order of the functions in the traces. For example, in the first trace, C1 calls C2, and C2 calls C3. In the second trace, C1 calls C4, and C4 calls C6. Therefore, we can infer that there is an indirect call relation between C1 and C4. Finally, the direct call patterns (C1, C2) and indirect call patterns (C1, C4) are used to generate the indirect call graph (right).",
        "reference": "Figure 5: Illustration of response composition in Flow-RG."
    },
    "2103.15997v2-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, denoted as Trace 1 and Trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a directed graph, where the nodes represent functions and the edges represent the direct call relations between functions. The indirect call relations are represented as a bipartite graph, where one set of nodes represents functions and the other set of nodes represents indirect call patterns. The edges between the two sets of nodes represent the indirect call relations between functions and indirect call patterns. \n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The output of the encoders is a sequence of vectors, each of which represents the hidden state of the corresponding token. The final output of the model is a sequence of probabilities, which represent the probability of each token in the input sequence being the next token in the sequence.\n\nFigure 8 illustrates the criss-cross attention block. The block consists of two self-attention layers, each of which is followed by a feed-forward layer. The first self-attention layer attends to the local context, while the second self-attention layer attends to the global context. The output of the criss-cross attention block is a sequence of vectors, each of which represents the hidden state of the corresponding token after attending to both the local and global context.",
        "reference": "Fig. 1. Diagram of the criss-cross attention module. For each position in the input feature map, the attention module generates a sparse attention map by aggregating information in the horizontal and vertical axes. After the second operation, each pixel has extracted context from all the pixels."
    },
    "2102.03921v1-Figure3-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces t1 and t2 of direct call relations, the indirect call relations can be inferred. For example, if C1 calls C2 in t1 and C2 calls C3 in t2, an indirect relation from C1 to C3 can be inferred. In addition, if C1 calls C4 in t1 and C4 calls C5 in t2, an indirect relation from C1 to C5 can be inferred. In this way, all indirect call relations can be inferred from the direct call relations in the two traces.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The encoders are followed by an output layer that is used to predict the next word in a sequence.\n\nThe input to the model is a sequence of tokens, each of which is represented by a word embedding. The word embeddings are passed through the encoders, which learn to attend to the relationships between the tokens in the sequence. The output of the encoders is a sequence of hidden states, each of which represents the meaning of a token in the sequence. The hidden states are then passed through the output layer, which predicts the next word in the sequence.\n\nBERT is trained on a masked language modeling task, in which the model is given a sequence of tokens with some of the tokens masked. The model is then trained to predict the masked tokens. This task helps the model to learn the relationships between the tokens in a sequence and to predict the next word in a sequence.\n\nFigure 8 shows the architecture of a reinforcement learning agent. The agent consists of an action generator, a response-action encoder, a state refresher, and a decision maker. The action generator takes as input the current state of the environment and generates an action. The response-action encoder takes as input the action and the current state of the environment and generates a response. The state refresher takes as input the response and the current state of the environment and generates a new state. The decision maker takes as input the new state and generates a decision. The decision is then used to update the environment.\n\nThe agent is trained by interacting with the environment. The agent receives a reward from the environment for each action it takes. The agent then uses the reward to update its parameters. The goal of the agent is to learn to take actions that maximize the reward.",
        "reference": "Figure 3: Least action classifier network architecture. Blocks of solid lines are differentiable. Sample images are from dataset CIFAR-10, which is collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton[MIT License](https://www.cs.toronto.edu/ kriz/cifar.html)"
    },
    "2102.03921v1-Figure2-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces, trace 1 and trace 2, on the left, the direct call relations and indirect call relations are extracted. On the right, indirect call patterns are extracted from the indirect call relations. Moreover, direct call patterns are extracted from the direct call relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors using a word embedding layer. The embeddings are then passed through a series of transformer layers, which learn to attend to different parts of the sequence and extract features. The output of the transformer layers is then passed through a softmax layer to predict the next token in the sequence.\n\nThe image shows a flowchart of a recurrent neural network (RNN) language model. The model takes a sequence of words as input and predicts the next word in the sequence. The model is composed of an embedding layer, an RNN layer, and an output layer. The embedding layer converts the words into vectors. The RNN layer then processes the sequence of vectors and learns to predict the next word in the sequence. The output layer then converts the output of the RNN layer into a probability distribution over the next word in the sequence.",
        "reference": "Figure 2: Markov decision process diagram [Malashin, 2019]"
    },
    "2210.05232v1-Figure4-1.png": {
        "prediction": " ## Confidence-based Pose Refinement\nConfidence-based pose refinement. Given the predicted confidence score map \\(s_{N_x}\\) and the estimated pose \\(R_{k-1}, t_{k-1}\\) from the previous frame, we aim to refine the pose by minimizing the reprojection error between the transformed model points \\(PX_c^{'}\\) and the observed 2D keypoints \\(P_c^o\\). To this end, we first transform the model points \\(P_c\\) conditioned on the predicted confidence map using a multi-layer perceptron (MLP) with two hidden layers, followed by a rigid transformation. We then compute the reprojection error and update the pose parameters by minimizing the error. Specifically, the confidence-aware transformation is formulated as:\n$$F_\\theta^c(s_{N_x}) = MLP(s_{N_x}) \\odot P_c + b,$$\n\nwhere \\(F_\\theta^c(\\cdot)\\) is the confidence-aware transformation, \\(\\theta\\) represents the learnable parameters, \\(MLP(\\cdot)\\) is a multi-layer perceptron, \\(b\\) is the bias term, and \\(\\odot\\) denotes element-wise multiplication. The rigid transformation is defined as:\n$$T(R, t) = RX_c + t,$$\n\nwhere \\(R \\in SO(3)\\) is a rotation matrix and \\(t \\in R^3\\) is a translation vector. The reprojection error is computed as:\n$$L_{reproj} = \\sum_c^C\\|P_c^o - T(R, t)\\odot F_\\theta^c(s_{N_x})\\|^2,$$\n\nwhere \\(C\\) is the number of 2D keypoints. We update the pose parameters by minimizing the reprojection error using an Adam optimizer.",
        "reference": "Fig. 4. An illustration of the iterative confidence-based pose estimation network."
    },
    "2107.00679v1-Figure1-1.png": {
        "prediction": " In the figure, a blockchain network is shown. It consists of three layers:\nLayer 1: Validators\nLayer 2: Non-validators\nLayer 3: Devices\n\nThe blockchain network is used to validate transactions between the devices. The validators are responsible for validating the transactions and adding them to the blockchain. The non-validators are responsible for relaying the transactions to the validators. The devices are responsible for generating the transactions.\n\nThe figure also shows the normal network communication between the devices. This communication is used to send data between the devices. The blockchain communication is used to send transactions between the devices.",
        "reference": "Fig. 1: Three-layer system architecture [7]"
    },
    "2110.00808v2-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and used to extract temporal relations. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the caller and the callee methods in the same trace. The indirect call relations are the relations between the caller methods in one trace and the callee methods in the other trace. The extracted relations are then used to generate the indirect call patterns.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoder layers and a stack of decoder layers. The encoder layers are used to encode the input text into a sequence of vectors. The decoder layers are used to decode the encoded vectors into a sequence of output tokens. The model is trained to predict the next token in a sequence of text, given the previous tokens.\n\nFigure 8 shows the architecture of a CycleGAN. CycleGAN is a generative adversarial network (GAN) that is used to translate images from one domain to another. The model consists of two generators and two discriminators. The generators are used to translate images from one domain to another. The discriminators are used to distinguish between real images and translated images. The model is trained to minimize the adversarial loss and the cycle consistency loss.",
        "reference": "Figure 1: Cycle-consistent world model. In the pictured situation, a sequence of top camera images is used a the input. The images are encoded frame-wise into latent states and forward predicted by the transition model. From these latent codes, reconstructed top camera images and images translated to semantic top camera images are calculated. From the translated images, cyclic latent codes are calculated. Finally, the four losses can be calculated, which enforce equations (2) and (3)."
    },
    "1245438-Figure1-1.png": {
        "prediction": " Figure 8. Image denoising. An illustration of an image denoising model. The model consists of a series of convolutional layers, each followed by an ELU activation function and a batch normalization layer. The output of the last convolutional layer is passed through a final convolutional layer to produce the denoised image.",
        "reference": "Fig. 1: The network architecture with pipe-lined components."
    },
    "407011-Figure3-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are extracted from the program code. Given a pair of traces (Trace 1 and Trace 2), the direct call relations and indirect call relations are extracted. The direct call relations are represented by the directed edges between the caller and callee functions, while the indirect call relations are represented by the dashed edges between the caller and callee functions. The indirect call patterns are also extracted, which represent the patterns of indirect calls between functions. Finally, the direct call patterns are extracted, which represent the patterns of direct calls between functions.",
        "reference": "Fig. 3. Illustration of a triplet network by incorporating intra-class variance into triplet embedding, in which the joint learning objective is to minimize the combination of softmax loss and triplet loss (consisting of inter-class and intra-class triplet loss)."
    },
    "2204.04591v2-Figure3-1.png": {
        "prediction": " In Figure 1, we illustrate how execution traces and temporal relations are extracted. Given two traces Tr1 and Tr2 of API calls, we can extract the direct call relations and indirect call relations. The direct call relations are the pairs of APIs that are directly called one after another in the traces. The indirect call relations are the pairs of APIs that are not directly called one after another in the traces, but there is a path between them in the call graph. \n\nIn Figure 7, we show the overall architecture of BERT. BERT consists of a stack of Transformer encoders. Each Transformer encoder consists of a self-attention layer and a feed-forward layer. The self-attention layer allows the model to attend to different parts of the input sequence, while the feed-forward layer allows the model to learn non-linear relationships between the input and output. The Transformer encoders are stacked together to form a deep neural network.\n\nIn Figure 2, the flowchart of the proposed CP-ABE scheme is illustrated. The detailed steps are as follows:\n1. The master key ABE and public key are generated by the NOC.\n2. The publisher encrypts the secret data with the public key and sends it to the NDN repository.\n3. The publisher publishes the ciphertext, and the public key needed to encrypt and decrypt to the remote ledger.\n4. The publisher encrypts all segments with initial key and periodically re-encrypts metadata and random segments.\n5. The publisher publishes the data and key to the NDN repository.\n6. The user retrieves data over NDN.\n7. The user reads ledger namespace from data, requests decryption key.\n8. The NOC sends user attributes, public key name, and time attributes to the local ledger.\n9. The NOC generates and publishes ABE key with user attributes.\n10. The user retrieves decryption key over NDN.",
        "reference": "Fig. 3. Encryption and Decryption Model"
    },
    "2107.04229v1-Figure3-1.png": {
        "prediction": " The image shows the architecture of a two-stream end-to-end DNN-HMM hybrid system for cough detection. The left part shows the acoustic feature extraction and selection, and the right part shows the DNN-HMM hybrid system. The acoustic features are first extracted from the speech signal using a gammatone filterbank. Then, a two-stage labeling process is applied to the acoustic features to generate the ground-truth labels. In the first stage, two independent labelers label the cough sounds in the audio files. In the second stage, an inspector inspects the labels generated by the two labelers and makes the final decision. The ground-truth labels are then used to train the DNN-HMM hybrid system. The DNN-HMM hybrid system consists of a DNN and a HMM. The DNN is used to extract high-level features from the acoustic features, and the HMM is used to model the temporal dynamics of the cough sounds. The output of the DNN-HMM hybrid system is a sequence of state labels, which can be used to detect the cough sounds in the speech signal.",
        "reference": "Fig. 3. Flowchart of ground-truth labels establishment."
    },
    "2107.04229v1-Figure4-1.png": {
        "prediction": " The flowchart describes the process of how a neural network model is trained. The model is first trained on a dataset of labeled data. The model is then evaluated on a test set of labeled data to see how well it performs. If the model performs well, it can be used to make predictions on new data.\n\nThe flowchart also describes how the model can be used to generate new data. The model is first trained on a dataset of labeled data. The model is then used to generate new data by sampling from the probability distribution of the model. The generated data can be used for a variety of purposes, such as creating new training data or testing the model's performance.",
        "reference": "Fig. 4. Architecture of the CNN-BiGRU model."
    },
    "2101.10102v2-Figure3-1.png": {
        "prediction": " Figure 1 illustrates execution traces and temporal relations. Given two traces (Trace 1 and Trace 2) of direct call relations (left), indirect call relations (middle), and indirect call patterns (right) are constructed. The direct call relations are extracted by identifying the caller and callee functions in the traces. The indirect call relations are constructed by identifying the caller and callee functions in the traces and then identifying the indirect callers and callees of the caller and callee functions. The indirect call patterns are constructed by identifying the caller and callee functions in the traces and then identifying the indirect callers and callees of the caller and callee functions.\n\nFigure 7 shows the architecture of BERT in pretraining. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, each of which is represented by a vector. The output of the model is a sequence of vectors, each of which represents the probability distribution over the vocabulary.\n\nFigure 8 illustrates the process of contrastive learning for representation learning. Given an input data point, we first apply a transformation (e.g., a neural network) to obtain a representation. We then compute the similarity between the representation and a set of other representations (e.g., by computing the dot product). Finally, we use the similarity scores to update the representation such that it becomes more similar to the representations of similar data points and less similar to the representations of dissimilar data points.",
        "reference": "Figure 3: Framework of PAC-model robustness analysis base on model learning"
    },
    "2010.13007v1-Figure1-1.png": {
        "prediction": " Figure 1 illustrates how execution traces are generated and how temporal relations are extracted. Given two traces, trace t1 and trace t2, the direct call relations and indirect call relations are extracted. The direct call relations are the relations between the caller and the callee, and the indirect call relations are the relations between the caller of the caller and the callee. The indirect call patterns are the patterns of indirect call relations. For example, the indirect call pattern (C1, C2) means that C1 calls C2, and C2 calls C3.\n\nFigure 7 shows the architecture of BERT in pretraining. BERT is a transformer-based language model that is pretrained on a large corpus of text. The model consists of a stack of encoders, each of which consists of a self-attention layer and a feed-forward layer. The input to the model is a sequence of tokens, and the output is a sequence of vectors, where each vector represents the meaning of the corresponding token in the input sequence.\n\nFigure 8 shows the architecture of a transformer-based speech recognition system. The system consists of an acoustic encoder, a pre-trained encoder, and a probing classifier. The acoustic encoder converts the input audio signal into a sequence of acoustic representations. The pre-trained encoder is a transformer-based language model that is pretrained on a large corpus of text. The probing classifier is a linear layer that is used to classify the acoustic representations into a sequence of predicted labels.",
        "reference": "Fig. 1. An illustration of the model architecture used in probing experiments."
    },
    "2012.05892v2-Figure2-1.png": {
        "prediction": " Figure 1: Illustration of execution traces and temporal relations. A trace is a sequence of events, where each event is represented by a node in the graph. The arrows between the nodes represent the temporal relations between the events. In this example, there are two traces, each of which consists of six events. The first trace (labeled \"Trace 1\") has the following events: A, B, C, D, E, and F. The second trace (labeled \"Trace 2\") has the following events: A, C, B, E, D, and F. The temporal relations between the events in each trace are as follows: A precedes B, B precedes C, C precedes D, D precedes E, and E precedes F. In addition, there is a temporal relation between the two traces: Trace 1 precedes Trace 2. This is indicated by the arrow from the last event in Trace 1 (event F) to the first event in Trace 2 (event A).\n\nFigure 7: Pretraining BERT. Architecture of BERT in pretraining. Assume that in the lowercased example sequence consisting of the segment pair \u2018he starts to speak. the nervous crowd is watch-ing him.\u2019 the tokens \u2018speak\u2019 and \u2018nervous\u2019 were sampled to be masked. \u2018speak\u2019 is replaced by the \u2018[MASK]\u2019 token and \u2018nervous\u2019 is replaced by the random token \u2018that\u2019. The model\u2019s task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. P (B follows A) is the next sentence prediction task. FNN stands for feedforward neural network.\n\nFigure 1: (a) Schematic of the experimental setup. The infrared light from a blackbody source is collimated by an off-axis parabolic mirror and then focused onto the sample by a lens with a numerical aperture (NA) of 0.55. The sample is mounted on a temperature-controlled stage. The emitted light from the sample is collected by the same lens and then dispersed by a spectrometer. (b) Photograph of the experimental setup. (c) Measured (symbols) and simulated (lines) emissivity spectra of a sapphire wafer at different temperatures. The emissivity of the sapphire wafer is calculated using Planck\u2019s law. (d) Measured (symbols) and simulated (lines) emissivity spectra of a fused-silica wafer at different temperatures. (e) Measured (symbols) and simulated (lines) emissivity spectra of a 1019 cm\u22123 n-doped silicon wafer at different temperatures. (f) Measured (symbols) and simulated (lines) emissivity spectra of a 1016 cm\u22123 n-doped silicon wafer at different temperatures. (g) Comparison of the measured and simulated emissivity spectra of the sapphire wafer, fused-silica wafer, 1019 cm\u22123 n-doped silicon wafer, and 1016 cm\u22123 n-doped silicon wafer at room temperature. The emissivity of the 1019 cm\u22123 n-doped silicon wafer and 1016 cm\u22123 n-doped silicon wafer is calculated using Equation 3.",
        "reference": "Fig. 2. Experimental demonstration of Planck spectroscopy. (A) Schematic of the experimental setup. Thermal emission from samples on a temperature stage was collected using a lens and focused onto an infrared detector. (B) Normalized voltage vs. temperature for five different samples: a laboratory blackbody, a sapphire wafer, a fused-silica wafer, and two n-doped silicon wafers with doping levels of 1020 and 1021 cm-3. (C) The extracted response function of the measurement setup, obtained by using the blackbody reference (dotted circle), is in good agreement with the expected response function based on vendor-provided properties of the detector, heater window, and lens. (D-G) Measured emissivity of the samples using Planck spectroscopy (circles), and the corresponding spectra measured using a Fouriertransform spectrometer (solid line)."
    },
    "1446589-Figure1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces (Trace 1 and Trace 2) with their corresponding CFGs, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns, while the indirect call relations are used to generate the indirect call patterns. Finally, the direct call patterns and indirect call patterns are combined to generate the temporal relations.\n\nFigure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are first embedded into vectors using a word embedding layer. The token embeddings are then passed through a series of transformer layers, which learn to attend to each other and to the positional embeddings of the tokens. The output of the transformer layers is then passed through a softmax layer to produce a probability distribution over the vocabulary.\n\nThe model is trained to predict the masked tokens in the input sequence. The masked tokens are replaced with a special [MASK] token, and the model is trained to predict the correct token from the context. The model is also trained to predict the next sentence in a pair of sentences. The next sentence prediction task is used to help the model learn to understand the relationships between sentences.",
        "reference": "Figure 1: Illustration of the proposed method. The manifold structure (the straight lines) in the label embedding space is transferred to the image feature space for synthesizing the virtual signature (the purple star) of an unseen class. The purple arrow points to a refined signature, which demonstrates that the synthesized virtual signature is optimized after running the Expectation-Maximization algorithm so that unseen data are assigned to labels according to the data distribution."
    },
    "2205.03991v2-Figure4.1-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are used to generate the direct call patterns. The indirect call relations are used to generate the indirect call patterns.\n\nThe flowchart in Figure 7 shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, which are embedded into vectors. The vectors are then passed through a series of transformer layers, which learn to attend to each other and to the input sequence. The output of the transformer layers is then passed through a softmax layer, which produces a probability distribution over the vocabulary. The model is trained to minimize the cross-entropy loss between the predicted probability distribution and the correct distribution.\n\nThe flowchart in Figure 3 shows the architecture of a Gated Recurrent Unit (GRU). The GRU is a type of recurrent neural network (RNN) that is used to process sequential data. The GRU has a recurrent structure, which means that it can learn from past information. The GRU also has a gating mechanism, which allows it to control the flow of information through the network. The GRU is a powerful RNN that can be used to solve a variety of tasks, such as natural language processing and machine translation.",
        "reference": "FIGURE 4.1. Overview of nonlocal diffusion processes proposed in related work [ETT15, GO09, AVMRTM10] and their interrelations to the nonlocal G-PDE (4.7). The approaches highlighted by the blue region only model the image labeling problem. Edge labels refer to the corresponding sections of the analytical ablation study."
    },
    "2012.12554v1-Figure1-1.png": {
        "prediction": " The image shows the architecture of a video object segmentation system. The system consists of two main components: a frame selection module and an interpolation module. The frame selection module selects a subset of frames from the video to be annotated. The interpolation module then interpolates the annotations from the selected frames to generate annotations for the remaining frames.",
        "reference": "Figure 1: Overview of our video annotation process. A human annotator draws a box on the first frame of the video; then our guiding frame selection mechanism predicts the next frame to annotate and the process iterates. Our method automatically and accurately interpolates bounding boxes for all frames that were not directly annotated by the human. Hence, at the end of the process object annotations are generated for all frames."
    },
    "216738-Figure2-1.png": {
        "prediction": " The flowchart in Figure 1 illustrates how execution traces and temporal relations are generated. Given two traces, trace 1 and trace 2, the direct call relations and indirect call relations are extracted. The direct call relations are represented as a matrix, where each row corresponds to a caller and each column corresponds to a callee. The indirect call relations are represented as a matrix, where each row corresponds to a caller and each column corresponds to a callee. The matrices are then used to generate the indirect call patterns. Finally, the direct call patterns and indirect call patterns are used to generate the temporal relations.",
        "reference": "Figure 2. Architecture of proposed image fusion CNN illustrated for input exposure stack with images of size h\u00d7w. The pre-fusion layers C1 and C2 that share same weights, extract low-level features from input images. The feature pairs of input images are fused into a single feature by merge layer. The fused features are input to reconstruction layers to generate fused image Yfused."
    },
    "2210.01442v1-Figure3-1.png": {
        "prediction": " A flowchart of the experimental setup. The experiment control system consists of three parts: HV control, motion control, and monitoring. HV control consists of two Keithley 2410 source measure units (SMUs) for applying high voltage to the thin and thick planes. Motion control consists of a Physik Instrumente (PI) M-403.42S linear stage for the X axis and a Festo EMMS-AS-55-S-LS-TSB linear stage for the Y axis. Monitoring consists of a HIH4000 humidity sensor, and a Pt100 temperature sensor. All three parts are connected to the experiment control computer through a USB connection. The Keithley SMUs are controlled through a Python script using the `keithley2400` package. The PI stage is controlled through a Python script using the `pimot` package. The Festo stage is controlled through a Python script using the `festoemms` package. The humidity and temperature sensors are controlled through a Python script using the `Adafruit_ADS1x15` package. The log file is a text file that contains all the data from the experiment.",
        "reference": "Figure 3: Schematic overview of the experiment control system."
    },
    "2103.04138v2-Figure2-1.png": {
        "prediction": " The image shows how to compute \\(D_q^{(*)}\\) in the large \\(N\\) limit using the soft-collinear effective theory (SCET). A light-like vector \\(n\\) is introduced which defines the direction of the hard jet. The incoming parton with momentum \\(p\\) is split into a collinear parton with momentum \\(q\\) and a soft parton with momentum \\(k\\). The collinear parton then splits into two partons with momenta \\(q_1\\) and \\(q_2\\). The momenta of the partons are related by \\(p=q+k\\), \\(q=q_1+q_2\\) and \\(q_1^2=q_2^2=0\\). The label \\((*) \\) on \\(D_q^{(*)}\\) indicates that the function has not been convoluted with the Wilson line.",
        "reference": "Figure 2. Leading-order Feynman diagram contributing to the hard kernels Tij(u), where the local four-quark operators are represented by the black square."
    }
}