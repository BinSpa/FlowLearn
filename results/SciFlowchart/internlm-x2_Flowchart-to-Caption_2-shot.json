{
    "2204.00424v1-Figure6-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes a section on Loss/Metrics, which indicates how the loss is calculated and what metrics are used to evaluate the performance of the model during training.",
        "reference": "Figure 6. MSOPunet network. (S1t\u22121, S2t\u22121), (S1t, S2t) and (S1t+1, S2t+1) denotes the input pairs of optical and SAR images. S2t\u2032 denotes the reference image used to compute the loss (during training) and the metrics (at inference time from test data) from the reconstructed optical image S\u03022t."
    },
    "2202.03651v2-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of using a case. The flowchart has several nodes and arrows that represent different steps in the process. The nodes are labeled \"use case u1\" and \"indirect call patterns (C1, C2)\". The arrows show the flow of information between these nodes.",
        "reference": "Figure 1: Instead of retrospectively discovering individual failure cases for perception, we actively search for causal interventions (edits) to existing scenes that consistently result in perception failures. The middle shows an example of a single intervention causing perception failure, which we attribute to the intervention, as opposed to the left where a combinatorial set of factors could explain the error. Consistent failures through this type of intervention constitute a challenging group for the perception model as seen on the right."
    },
    "2011.15102v2-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is trained on a large corpus of text, and the output layer is a stack of encoders. The encoders are used to represent the input sequence as a fixed-size vector. The model is pre-trained using two tasks: next sentence prediction and masked language modeling. The pre-training process is followed by fine-tuning the model for specific natural language processing tasks such as question answering, text classification, and named entity recognition. The model's performance is evaluated using harder tests, which helps to improve the quality of its solutions.",
        "reference": "Figure 1: Learning by passing tests. A tester model creates tests with increasing levels of difficulty from a test bank to evaluate a learner model. The learner continuously improves its learning ability to deliver better solutions for passing those difficult tests."
    },
    "14078997-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the CNN architecture. The input frames are processed through the convolutional layer, which applies a set of filters to extract features from the image. These features are then passed through the pooling layer, which reduces the dimensionality of the feature maps by taking the maximum or average value in each region. The output of the pooling layer is then fed into the fully connected layers, where the final classification is performed. The diagram also shows the use of the MSE loss function for training the model and the use of the Reconstruction method for generating the final output representation.",
        "reference": "Figure 2: Outline of the transformation-based model. The model is a CNN that takes as input a sequence of consecutive affine transforms between pairs of adjacent video frames. It predicts the affine transform between the last input frame and the next one in the sequence. We compute affine transforms (6 parameters per patch) for overlapping patches of size 8 \u00d7 8 in each video frame. Learning operates in the space of transformations as shown inside the dashed box. The front-end on the left is a module that estimates the affine transforms between pairs of consecutive input frames. The post-processor on the right reconstructs a frame from the predicted set of affine transforms and it is only used at test time."
    },
    "14078997-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into three main sections: use case 1, indirect call patterns, and direct call patterns. In the use case 1 section, there are two traces (Trace 1 and Trace 2) that follow each other. These traces are followed by a Probability Distribution over Vocabulary block. The indirect call patterns section contains two sub-sections, C1 and C2, which are followed by a Probability Distribution over Vocabulary block. The direct call patterns section also has two sub-sections, C1 and C2, which are followed by a Probability Distribution over Vocabulary block as well. Finally, the output layer is shown at the bottom of the diagram.",
        "reference": "Figure 3: Outline of the system predicting 4 frames ahead in time. Only affine transforms A1, A2 and A3 are provided, and the model predicts A\u03034, A\u03035, A\u03036 and A\u03037, which are used to reconstruct the next 4 frames. Since affine parameters are continuous values and the whole chain of CNNs is differentiable, the whole unrolled system can be trained by back-propagation of the error. Note that CNNs all share the same parameters"
    },
    "2204.10509v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of the PEGE Model Architecture. The model architecture is designed to be helpful, honest, and harmless. It consists of several components, including the Transformer, Emotion Classifier, and Emotional Loss. The Transformer is used for pretraining BERT, which is an architecture that learns contextualized word embeddings. The Emotion Classifier is responsible for predicting the emotional state of a given text. The Emotional Loss is a loss function that measures the difference between the predicted and actual emotional states. The model also includes a stack of encoders, which are used to process the input data. The output layer represents the final representation of the input. The model is trained on a dataset consisting of positive and negative examples. During training, the model learns to predict the emotional state of a given text based on the input. Once the model is trained, it can be used to classify the emotional state of new texts.",
        "reference": "Figure 2: Illustration of our PEGE Model"
    },
    "2202.05735v2-Figure1-1.png": {
        "prediction": "The diagram depicts a complex system with several components and their interactions. The system is designed to learn from the data it receives, which is processed by a neural network. The input layer of the neural network receives information from various sources, such as a waveform, a photoplethysmography (PPG) signal, and a deep learning signal. The output layer of the neural network provides the final representation of the input data.\n\nThe system also has a feature engineering component that processes the input data to improve its quality before being fed into the neural network. This feature engineering process includes pulse rate variability (PRV) analysis, morphological measures, and instantaneous pulse rate (IPR) analysis. These techniques are used to extract meaningful features from the input data for better performance of the neural network.\n\nThe BM-DS component of the system is responsible for deep learning, which helps the system learn from the data and improve its performance over time. The BM-FE component, on the other hand, is responsible for the neural network's feature extraction, which is crucial for the system's ability to learn and make accurate predictions.\n\nOverall, the system depicted in the diagram is a complex machine learning system that processes data from multiple sources, learns from the data, and improves its performance over time through feature engineering and deep learning.",
        "reference": "Fig. 1: Three approaches to ML for sleep staging from PPG. Our new model, SleepPPG-Net takes as input the PPG waveform (WAV).The derived time series (DTS) and feature engineering (FE) approaches allow comparison with SOTA algorithms described in the literature."
    },
    "2101.02429v1-Figure6-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to learn the representations of words, sentences, and paragraphs from text data. The architecture of the model is based on the Transformer model, which uses self-attention mechanisms to learn contextualized word embeddings. The model is pre-trained on large amounts of unlabeled text data using two tasks: masked language modeling and next sentence prediction. The pre-training process involves randomly masking some tokens in the input text and predicting their values. The model is also fine-tuned on specific NLP tasks such as question answering, natural language inference, and sentiment analysis.",
        "reference": "Figure 6: Illustration of the neural decomposition procedure for d = 2 (i.e. Z\u2217 \u2208 R3\u00d73). The dashed red arrows correspond to the eigenvectors of Z\u2217 (q1, q2, q3) and the solid blue arrows show the decomposed vectors p1 and p2. In this example, the rank of Z \u2217 is 2 where q1 and q2 are its two principal eigenvectors. The eigenvalue corresponding to the eigenvector q1 is zero. The light blue colored surface shows the Lorentz cones z = \u221a x2 + y2 and z = \u2212 \u221a x2 + y2. We observe that the decomposed vectors p1 and p2 lie on the boundary of Lorentz cones."
    },
    "1246860-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that processes text. The system begins with a document, which is then passed through a Document Sentence model. The Document Sentence model outputs a sentence pair, which is then passed to the Sentence Sentence model. The Sentence Sentence model outputs another sentence pair, which is then passed to the Word Embedding model. The Word Embedding model outputs a word embedding, which is then passed to the Sentence Model. The Sentence Model outputs a sentence representation, which is then passed to the Document Model. The Document Model outputs a document representation, which is then passed to the Word Embedding model. The Word Embedding model outputs a word embedding, which is then passed to the Word Sentence model. The Word Sentence model outputs a word sentence, which is then passed to the Word Embedding model. The Word Embedding model outputs a word embedding, which is then passed to the Word Sentence model. The Word Sentence model outputs a word sentence, which is then passed to the Word Embedding model. The Word Embedding model outputs a word embedding, which is then passed to the Word Sentence model. The Word Sentence model outputs a word sentence, which is then passed to the Word Embedding model. The Word Embedding model outputs a word embedding, which is then passed to the Word Sentence model. The Word Sentence model outputs a word sentence, which is then passed to the Word Embedding model. The Word Embedding model outputs a word embedding, which is then passed to the Word Sentence model. The Word Sentence model outputs a word sentence, which is then passed to the Word Embedding model. The Word Embedding model outputs a word embedding, which is then passed to the Word Sentence model. The Word Sentence model outputs a word sentence, which is then passed to the Word Embedding model. The Word Embedding model outputs a word embedding, which is then passed to the Word Sentence model. The Word Sentence model outputs a word sentence, which is then passed to the Word Embedding model. The Word Embedding model outputs a word embedding, which is then passed to the Word Sentence model. The Word Sentence model outputs a word sentence, which is then passed to the Word Embedding model. The Word Embedding model outputs a word embedding, which is then passed to the Word Sentence model. The Word Sentence model outputs a word sentence,",
        "reference": "Figure 1: Deep multi-instance transfer learning approach for review data."
    },
    "2102.10828v2-Figure7-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model, specifically BERT (Bidirectional Encoder Representations from Transformers). BERT is a pre-trained language model that uses transformer-based deep bidirectional contextualized representations. The architecture of BERT in pre-training is shown in the figure.\n\nThe pre-training process involves two main tasks: masked language modeling and next sentence prediction. In the masked language modeling task, some words are randomly selected to be replaced with a special token [MASK]. The model's goal is to predict the original word from the representation it learns at the input embeddings position of the [MASK] token. In the next sentence prediction task, the model is trained to predict whether two sentences follow each other or not.\n\nThe figure also includes an illustration of a geometric construction involving triangles, lines, and points. This construction is used to explain the concept of degeneracy, which occurs when a line segment is parallel to one of its endpoints. The construction demonstrates how the degeneracy of a triangle can be determined by examining the angles formed by the line segments and their endpoints.",
        "reference": "Figure 7. Illustration of Equation (4.1)."
    },
    "1022460-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a program. The program starts with the \"road bike\" object, and then it goes to the \"tandem bike\" object. After that, the program goes through a loop where it checks for the \"wheel\", \"vessel\", and \"car\" objects. If any of these objects are found, the program will go back to the start of the loop. If none of these objects are found, the program will end. This flowchart is an example of a simple program that checks for the presence of specific objects in a given context.",
        "reference": "Figure 1. A feedback based learning model. The basic idea is to make predictions in an iterative manner based on a notion of the thus-far outcome. This provides several core advantages: I. enabling early predictions (given total inference time T , early predictions are made in fractions of T ); II. naturally conforming to a taxonomy in the output space; and III. better grounds for curriculum learning."
    },
    "2110.03031v3-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the architecture of BERT in pretraining. The flowchart shows the input sequence, which is segmented into two parts: the first part is the masked token, and the second part is the random token. The model's task is to predict the masked and random tokens from the representation vectors it learns at the positions of the input embeddings. Additionally, the next sentence prediction task is also shown in the diagram. The flowchart also includes various other elements such as the stack of encoders, the final input representation, and the output layer. It is a complex and detailed flowchart that provides a comprehensive view of the BERT pretraining process.",
        "reference": "Figure 1. RieszNet architecture."
    },
    "2101.11189v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented by the top left corner, which has two options: direct call relations and indirect call relations. The direct call patterns are shown in the bottom right corner. The diagram also includes various nodes such as trace 1, trace 2, and output layer. The stack of encoders and final input representation are also depicted. The diagram is quite complex and seems to be related to some form of data processing or machine learning.",
        "reference": "Fig. 2: The overall framework of our arbitrary-oriented ship detection method. Feature maps are first generated by using a fully convolutional network. Afterward, the peaks of the center feature map are selected as center points. Then, the center points offsets, object sizes and head regression locations are regressed on the corresponding feature maps on the position of each center point. The potential head points are collected by extracting peaks with confidence scores larger than 0.1 on the head feature map. The final head location is obtained by assigning each regressed location to its nearest potential head points."
    },
    "2204.01715v1-Figure7-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of executing traces and temporal relations. The flowchart has several components, including data ingestion, transformation, model/app, user app, and infrastructure. The data ingestion component includes various elements such as TensorFlow dataset, PyTorch DataLoader, XShards, Apache Spark, and Ray. The transformation component consists of elements like TensorFlow, PyTorch, and AutoEstimator. The model/app section includes TensorFlow, PyTorch, and Ray. The user app section contains elements like TensorFlow, PyTorch, and Ray. Finally, the infrastructure section includes K8s cluster, Hadoop/yarn cluster, Ray cluster, AWS, etc.\n\nThe flowchart also shows the relationship between these components. For example, there is an arrow pointing from the data ingestion component to the transformation component, indicating that the data is transformed after being ingested. Similarly, there are arrows pointing from the transformation component to the model/app component and from the user app component to the infrastructure component, showing the flow of information or data between these components. Overall, the flowchart provides a visual representation of the process of executing traces and temporal relations in a system.",
        "reference": "Figure 7. BigDL-Orca architecture"
    },
    "2202.07508v3-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of the execution traces and temporal relations. The flowchart is divided into different sections, such as \"use case u1\", \"direct call relations\", \"indirect call patterns (C1, C2)\", and \"direct call patterns (C1, C2)\". The diagram also shows the \"ResBlocks\" and the \"Global Pooling\" process. It also includes a section for \"Convolution filters process\" and \"Reshape\" process. The diagram also has a section for \"Masked language modeling\" and \"Next sentence prediction\". The diagram is quite complex, and it seems to be related to machine learning or deep learning.",
        "reference": "Figure 3. Architecture of the dynamic deep linear kernel."
    },
    "2202.11824v1-Figure1-1.png": {
        "prediction": "The diagram shows a DNA sequencing service. The process starts with a biochemical hacking alert, and the hospital is notified. The hospital then sends shotgun sequencing to the DNA assembler. The DNA assembler sends the input control to the CNN. The CNN sends the activate signal to the Trojan SW. The Trojan SW sends the final input representation to the attacker.",
        "reference": "Fig. 1: Hybrid Trojan Bio-Cyber Hacking Attack. Steps 1 - 3 indicate a typical genetic sequencing operation for patients. Steps 4 - 6 indicate a situation where a hacker has embedded their IP address and Port number into a DNA that will trigger a remote connection from a Trojan-horse infected software tool leading to a connection to the attacker in Step 8. Our proposed approach utilizes Deep-Learning to detect Trojan payload in digital data using encoded into DNA strands that can prevent the attack."
    },
    "2011.04105v1-Figure3.5-1.png": {
        "prediction": "The diagram shows a data plane, a discovery plane, a dissemination plane, and a decision plane. The data plane is the first step in the process, where data is collected from various sources. The discovery plane is the next step, where the data is analyzed to identify patterns or anomalies. The dissemination plane is the third step, where the findings from the discovery plane are shared with other parts of the system. Finally, the decision plane is the last step, where decisions are made based on the information gathered in the previous steps.",
        "reference": "Figure 3.5: Network-level objectives."
    },
    "2202.01326v1-Figure5-1.png": {
        "prediction": "The diagram depicts a flowchart for a machine learning model. The model is designed to predict whether a given sequence of words is a valid sentence or not. The first step in the process is to check if the input is a valid sequence of words. If it is, the model proceeds to the next step, which is to determine if the sequence contains any masked tokens. If it does, the model uses a pre-trained BERT model to predict the masked tokens. If the sequence does not contain any masked tokens, the model uses a different pre-trained BERT model to classify the sequence as either a valid sentence or not. Finally, the output layer of the model provides the final prediction.",
        "reference": "Figure 5: Flowchart describing the process for determining if SF is improving photocurrent in a device."
    },
    "2204.08696v2-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT (Bidirectional Encoder Representations from Transformers). The model is designed for natural language processing tasks and consists of an input layer, an output layer, and several intermediate layers. The architecture of the BERT model includes pretraining, which involves masking some of the input tokens and predicting their values. The model also includes fine-tuning, where it is trained on specific NLP tasks such as question answering or sentiment analysis.\n\nThe diagram shows the flow of data through the BERT model. The input layer takes in a sequence of text, represented by word embeddings. These embeddings are then passed through multiple transformer blocks, which are composed of self-attention mechanisms and feedforward neural networks. The output of these transformer blocks is then passed through a pooling layer, which produces a fixed-length vector representation of the input text. This vector is used as the final output of the model, which can be used for various NLP tasks.\n\nThe diagram also highlights the use of LCM (Local Global Feature Cooperative Model) and FSAU (Facial Structure Attention Unit) in the model. LCM is a feature enhancement unit that helps to improve the performance of the model by incorporating local and global features. FSAU is a facial structure attention unit that focuses on the attention mechanism, allowing the model to pay more attention to specific parts of the image.",
        "reference": "Fig. 1: The complete structure of the proposed CNN-Transformer Cooperation Network (CTCNet). CTCNet is a U-shaped symmetrical hierarchical network with three stages: encoding stag, bottleneck stage, and decoding stage. Among them, the encoding stage is designed to extract local and global features with different scales, and the decoding stage is designed for feature fusion and image reconstruction."
    },
    "2101.08366v2-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a quadratic residual network (QRes) layer. The QRes layer is composed of two sub-layers, W1 and W2, which are connected by an XOR gate. The input to the QRes layer is represented by x, and the output is represented by y. The W1 and W2 sub-layers are responsible for processing the input data and producing the final output. The XOR gate plays a crucial role in determining the output based on the input from the W1 and W2 sub-layers. This architecture is designed to learn the relationship between the input and the output, allowing it to make predictions or classifications based on the given data.",
        "reference": "Figure 2: Overview of our proposed Quadratic Residual Network (QRes) layer in comparison with plain DNN layer. Blue rectangular boxes represent trainable parameters and round boxes represent operations (purple \u201c\u00d7\u201d: multiplication, orange \u201c+\u201d: addition, green \u201d\u00b7\u201d: Hadamard product, and cyan \u201d\u03c3\u201d: activation operator)."
    },
    "2205.09510v2-Figure6.4-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is pre-trained on a large corpus of text, and the pre-training task involves predicting masked words in the input sequence. The pre-training data is then used to fine-tune the model for specific NLP tasks such as question answering and sentiment analysis. The model architecture consists of an encoder that processes the input text and generates contextualized embeddings. These embeddings are then passed through a stack of encoders, which are used to extract features from the input. The final output layer represents the predicted word embeddings. The model is optimized using classical optimization techniques.",
        "reference": "Figure 6.4: An illustration of the \u201cQC\u201d setting of quantum machine learning, in which data are quantum and processing is classical."
    },
    "2205.09510v2-Figure6.11-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is pre-trained using the BERT architecture, which consists of a stack of encoders and a final input representation layer. During pre-training, the model learns to predict masked tokens in a given sequence. This is done by replacing certain tokens with a special [MASK] token and training the model to predict the original token. The model also learns to predict the next sentence in a given pair of sentences.\n\nIn the fine-tuning stage, the pre-trained BERT model is adapted for specific NLP tasks such as question answering, text classification, and named entity recognition. The model is trained on labeled data for these tasks while keeping the pre-trained weights fixed. The fine-tuning process involves updating the model's parameters to minimize the loss function for the specific task at hand.\n\nThe diagram shows that the output of the BERT model is passed through an average function, which calculates the average value of the output. This average value is then used as the input to a classical optimizer, which updates the model's parameters during the training process.",
        "reference": "Figure 6.11: Illustration of the operation of a VQE. The classical optimizer aims at minimizing the expected value \u3008F \u3009\u03c8(\u03b8)\u3009 = \u3008\u03c8(\u03b8)|F |\u03c8(\u03b8)\u3009 of the observable F ."
    },
    "2206.05240v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses a reward-based learning approach. The system consists of a history, uncertainty, and action components. The history component stores the past actions and their outcomes, which are used to update the reward function. The uncertainty component calculates the expected reward for each action based on the current state and the history. The action component selects the action with the highest expected reward.\n\nThe reward function is updated based on the outcome of the selected action. If the outcome is positive, the reward is increased. If the outcome is negative, the reward is decreased. This process continues until the system converges to an optimal policy.\n\nThe system also has a curriculum learning component, which allows it to learn in a more structured manner by starting with simple tasks and gradually increasing the complexity. The system is trained using a combination of supervised and reinforcement learning techniques.",
        "reference": "Figure 2: Model Overview. We propose CBRL for RCB. Guided by a curriculum sequence, the agent learns to adaptively trade-off constraints and the objective in non-stationarymarkets. During deployment, the agent updates its belief over the market dynamics based on its past experience, and acts through posterior sampling."
    },
    "2210.02172v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that uses a network to learn from data. The system is designed to approximate the target Q-value, which is an important aspect of reinforcement learning. It uses a network update and evolution neural network to improve its performance over time.\n\nThe architecture of the system includes an input layer, hidden layers, and an output layer. The input layer receives data, which is then processed through the hidden layers before being sent to the output layer for a final decision or action.\n\nThe system also has a reward mechanism in place, which provides feedback to the network after each action. This feedback helps the network to learn and improve its performance over time.\n\nOverall, the system is designed to learn from data and improve its performance through a combination of network updates, evolution neural networks, and a reward mechanism.",
        "reference": "Fig. 2: An illustration of deep Q-learning model for the ISRE-based 6G vehicular aided HetNet environment. The model uses a deep neural network to find the approximate Q-values. The model includes experience replay mechanism (to remove the correlation between different observations), a feature set (given as input to the deep neural network), a target Q-network for updating the primary Q-network and the simulation environment for extracting different parameters."
    },
    "14844989-Figure2-1.png": {
        "prediction": "The image is a diagram of a multi-task network architecture. The network consists of an input layer, a stack of encoders, and a stack of decoders. The encoders and decoders are connected in a cross-domain fashion. The encoders and decoders are also connected in a cross-domain task manner. The encoders and decoders are trained using a ranking task and a classification task. The encoders and decoders are also trained using a pretraining task. The encoders and decoders are also trained using a pretraining BERT task.",
        "reference": "Figure 2: The framework of the proposed multi-task deep network and the cross-domain architecture. The crossdomain architecture is only used when an auxiliary dataset is needed for training."
    },
    "2205.03661v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of designing and implementing deep neural networks. The flowchart has several key components, including the design of basic blocks, determining network depth, replacing sign(x) with the piecewise function F(x), and enhancing computing speed.\n\nThe process starts with the design of basic blocks, which are the building blocks of the neural network. This step involves determining the number of basic blocks to be used in the network. Once the basic blocks are designed, the network depth is determined. This step involves deciding on the number of layers in the network.\n\nAfter the network depth is determined, the next step is to replace the sign(x) function with the piecewise function F(x). This step is important for improving the accuracy of the network.\n\nFinally, the process concludes with enhancing the computing speed and reducing memory overhead. This step is crucial for ensuring that the network can process data efficiently and effectively.\n\nOverall, the flowchart provides a clear and concise overview of the process of designing and implementing deep neural networks. It highlights the importance of each step in the process and emphasizes the need for efficient and accurate networks.",
        "reference": "Fig. 1. Overview of methods adopted"
    },
    "162169-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into different sections, including \"use case u1\", \"direct call relations\", \"indirect call patterns (C1, C2)\", and \"direct call patterns (C1, C2)\". The diagram also includes various nodes such as \"Trace 1\" and \"Trace 2\" and arrows connecting them. The flowchart appears to be a complex system with multiple interconnected components.",
        "reference": "Figure 3 Partial illustration of frames, roles, and lexical units related to the CAUSE TO MAKE NOISE frame, from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are unfilled bars. No particular significance is ascribed to the ordering of a frame\u2019s roles in its lexicon entry (the selection and ordering of roles above is for illustrative convenience). CAUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here."
    },
    "2202.03344v2-Figure2-1.png": {
        "prediction": "The image is a flowchart that depicts the process of training a language model. The flowchart begins with the \"Start\" node, which represents the beginning of the process. From there, the process moves to the \"Input: Candidate degrees\" node, where candidate degrees are input into the system.\n\nNext, the process moves to the \"Initialize\" node, where the system initializes the variables. The flowchart then moves on to the \"Set latent variable\" node, where the system sets the latent variable. This is followed by the \"Set truncated set\" node, where the system sets the truncated set.\n\nThe process then moves to the \"q-norm adaptivity\" node, where the system adapts the q-norm based on the input data. Following this, the system moves to the \"Degree adaptivity\" node, where it adapts the degree based on the input data.\n\nThe flowchart then moves to the \"Cross-validation to tune if\" node, where the system uses cross-validation to tune the parameters. Finally, the process ends at the \"Stop\" node, which represents the end of the training process.",
        "reference": "Figure 2: Flow chart of the procedure to adaptively build a stochastic PCE"
    },
    "2207.12764v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of manufacturing a complex object using a manufacturing-centric process model. The process starts with the manufacturing event date, which includes the complex object and the manufacturing process. The next step is to select the desired objects and graphs, which are then typecasted to the cluster. The cluster is then used to determine the existence of the cluster, and if it exists, the cluster is used to the event. Finally, all the objects that belong to the event are included in the cluster. This process is repeated until all the objects have been included in the cluster.",
        "reference": "Fig. 1: Overview of the proposed framework."
    },
    "2103.13446v3-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system, with the main components being the input, node input, and concatenate. The input is received from neighbors, and the node input is used to process the data. The concatenate function combines the results of the node input, which are then passed on to the next step in the process. The overall system appears to be designed for processing and transmitting information between different nodes or components.",
        "reference": "Fig. 2: ModGNN\u2019s message aggregation module. In this diagram, the system is shown from a centralized point of view. First, the raw observation or output from the last layer is transformed by finput. Then, for each transmission up to K hops, the data from the neighboring agents is passed through fcom and then aggregated. The output is the set of the data from each k-hop neighborhood up to K."
    },
    "2107.06007v1-Figure14-1.png": {
        "prediction": "The image is a flowchart that depicts the use of a case. The flowchart is divided into two main sections: the top section, which shows the direct call relations and indirect call patterns, and the bottom section, which shows the direct call patterns.\n\nIn the top section, there are two main sections labeled \"Direct Call Relations\" and \"Indirect Call Patterns (C1, C2)\". The Direct Call Relations section has two sub-sections labeled \"C1\" and \"C2\", with each sub-section containing several blue circles representing different types of calls. The Indirect Call Patterns (C1, C2) section also contains similar blue circles, but they are arranged in a more complex pattern.\n\nIn the bottom section, there is a single section labeled \"Direct Call Patterns (C1, C2)\". This section is similar to the top section's Direct Call Relations section, with two sub-sections labeled \"C1\" and \"C2\", and each sub-section containing several blue circles representing different types of calls.\n\nOverall, the flowchart provides a visual representation of the different types of calls and their relationships within a system. It is likely used for understanding and analyzing the behavior of a computer system or software application.",
        "reference": "Figure 14. The EXO-200 slow control architecture consisted of three major parts- the MySQL backbone, the embedded system, and the distributed system."
    },
    "2107.12859v2-Figure3-1.png": {
        "prediction": "The diagram depicts a complex system with several components and their interactions. The system is divided into three main sections, labeled (a), (b), and (c). \n\nIn section (a), there are two main components: P and P'. These components are connected by an arrow, indicating that they have a direct relationship or interaction. There is also a dotted line connecting P to P', which may represent a more indirect or weak connection.\n\nSection (b) features a series of components arranged in a row. These components are connected by arrows, suggesting that they are part of a sequential process or workflow. The components are labeled with numbers, such as 1, 2, 3, and so on, which may indicate their order or position within the process.\n\nSection (c) contains a single component, labeled \"f\". This component is connected to the other parts of the system through dotted lines, possibly indicating its role in mediating or coordinating the interactions between the other components.\n\nOverall, the diagram seems to represent a complex system with multiple interacting components, some of which are part of a sequential process. The use of arrows, dotted lines, and numbered labels helps to clarify the relationships and connections between these components.",
        "reference": "Figure 3: One iteration of our Recurrent Graph Learning framework. (a) We process part features and compute a graph message. (b) The message is encoded sequentially in our bidirectional GRU framework. (c) The features generated by the forward and reverse GRU are used to regress part-pose. We use three such iterations in our framework."
    },
    "2012.00248v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case \"use case u1\" is at the top, followed by the \"direct call relations\" and \"indirect call relations\". There are also \"indirect call patterns\" and \"direct call patterns\" shown in the diagram. The diagram also includes various objects such as \"Objects\", \"Fourier lens\", and \"Hologram plane\". The diagram seems to be related to some form of data processing or analysis.",
        "reference": "Fig. 1. Schematic diagram for synthesizing the digital Fourier hologram. The letter objects are placed at a distance d in front of the Fourier lens in the direction of propagation of light. The real-valued hologram are drawn in the logarithmic scale."
    },
    "2010.06310v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that depicts the execution traces and temporal relations of a program. The flowchart has several nodes, including \"use case u1\", \"direct call relations\", \"indirect call patterns (C1, C2)\", and \"direct call patterns (C1, C2)\". The arrows in the flowchart represent the direction of the program's execution. The flowchart also includes various components such as \"annotated tag (entity + trigger) dist.\", \"annotated entity dist.\", \"annotated trigger dist.\", and \"annotated entity dist.\". These components are likely part of the program's logic and functionality. Overall, the flowchart provides a visual representation of the program's structure and how it processes information.",
        "reference": "Fig. 2: The framework of the joint-event-extraction model with our proposed cross-supervised mechanism."
    },
    "2202.05336v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that processes input images to generate background information. The system begins with fixed image input, which is then passed through an input image i. From there, the system uses motion estimation and warping of input images to generate post-processing. The post-processing step is followed by binary segmentation, which separates the image into foreground and background regions. The background information is then used for inverse warping, dynamic entrophy map computation, and pixel-wise distance threshold computation. Finally, the system outputs the final representation of the input image.",
        "reference": "Fig. 1. Block diagram of DBSGen"
    },
    "2107.03026v1-Figure7-1.png": {
        "prediction": "The diagram depicts a cycle of influence between ecological systems and social economic systems. The ecological systems are influenced by the social economic systems, which in turn are influenced by the ecological systems. This cycle is represented by the arrows connecting the two systems.",
        "reference": "Figure 7: Influence matrix schematic graph, based on [5, Figure 5]"
    },
    "1381187-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The architecture of BERT in pretraining is shown, where the model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes the encoders and decoders, as well as the convolutional layers, batch normalization, and ReLU activation functions. The final output layer represents the positional embeddings and word embeddings.",
        "reference": "Fig. 1: Illustration of the SegNet architecture applied to EO data."
    },
    "13567899-Figure3-1.png": {
        "prediction": "The image is a flowchart that depicts the execution traces and temporal relations of a program. The program appears to be a machine learning model, as it involves pretraining BERT. The flowchart shows the architecture of BERT in pretraining, with masked language modeling and next sentence prediction tasks. It also includes various components such as Conv3, Conv4, and Conv5, which are likely part of the neural network architecture. Additionally, there are several layers, including input layer, output layer, and representation layer. The flowchart provides a visual representation of the program's structure and the sequence of operations it performs during execution.",
        "reference": "Figure 3. V2V Architecture for Voxel Prediction. The lower part (below dashed line) consists of layers from C3D [28]. Connected to these layers we have three 3D convolution layers: Conv3c,Conv4c,Conv-pre use filters of size 3\u00d7 3\u00d7 3 with stride 1\u00d7 1\u00d7 1. Both Deconv5 and Deconv4 are deconvolutional layers employing kernels of size 4 \u00d7 4 \u00d7 4 with output stride of 2 \u00d7 2 \u00d7 2. Deconv3 has kernel size 8 \u00d7 4 \u00d7 4 and output stride of 4 \u00d7 2 \u00d7 2. The numbers inside the boxes represent the number of learning filters in that layer, while the numbers near the boxes (above or below) represent the size of output signals produced by that layer. The part inside the thick-dashed box is application-dependent."
    },
    "2204.10374v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of using a mobile app. The app uses AndroidEnv action and AndroidEnv action to perform gestures, which are then used by the gesture class selection. The gesture class selection is based on the reward from the gesture class. The app also uses RL agent 1 and RL agent 2 to make decisions based on the reward from the gesture class. The final output representation is based on the positional embeddings of the word embeddings. The diagram also includes a pink smartphone in the top left corner.",
        "reference": "Figure 1 | Gesture Hierarchy. The architecture used for the Android applications is based on a 3-layer hierarchy: (1) The lowest level operates over GVFs corresponding to all supported gestures; (2) The middle layer selects a gesture GVF given the latest pixel image in AndroidEnv and its agent is trained to maximize the return associated with the task that the agent is trained on; and (3) The top layer selects a single gesture class for the task and the agent is trained to maximize the average per step reward. All levels are operated by distributed DQN agents."
    },
    "2010.00041v1-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a process. The process starts with the random creation of the initial population. This population is then evaluated based on fitness, and if it meets the stopping criteria, the process ends. If the stopping criteria are not met, the process continues to the next step.\n\nIn the next step, passing elites are selected to move to the next population. These passing elites are chosen based on their fitness. If there are no passing elites, the process moves to the mutation step. In the mutation step, a new population is created by randomly mutating some individuals in the current population. The process then returns to the evaluation step, where the fitness of the new population is evaluated. If the stopping criteria are met, the process ends. If not, the process continues to the selection step, where passing elites are selected to move to the next population.",
        "reference": "Fig. 3. The optimization flowchart for the inverse material design."
    },
    "2103.14005v2-Figure9-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. There are two ResNet blocks, each with 50 layers. The first block has 64 hidden layers and the second block has 128 hidden layers. The model is trained on a dataset consisting of 1024 sentences. The output layer represents the final input representation. The diagram also includes a class prediction layer and a fusion layer prediction.",
        "reference": "Figure 9. The Multi Input Fusion Classifier end task architecture. The orange box shows the frozen encoder."
    },
    "2110.14764v2-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the use case of a system. The flowchart is divided into two main sections: Direct Call Relations and Indirect Call Patterns. The Direct Call Relations section has three sub-sections: Trace 1, Trace 2, and Direct Call Patterns. The Indirect Call Patterns section also has three sub-sections: C1, C2, and Direct Call Patterns. The flowchart shows the relationship between these different sections, as well as the specific elements within each section. It provides a visual representation of the system's functionality and how it operates.",
        "reference": "Fig. 1. The Fun architecture, exemplified with |L|=3 languages (Chinese, Italian, English). Note that the different term-document matrices in the 1st-tier may contain different numbers of documents and/or different numbers of terms. The three grey diamonds on the left represent calibrated classifiers that map the original vectors (e.g., TFIDF vectors) into |Y|-dimensional spaces. The resulting vectors are thus aligned and can all be used for training the meta-classifier, which is represented by the grey diamond on the right."
    },
    "1422748-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the process of text processing. The flowchart starts with the \"Document\" and moves through several stages, including \"Language Detection,\" \"Text Preprocessing,\" \"Tokenization,\" \"Sentence Breaking,\" \"Tokenization,\" \"Entity Extraction,\" and finally ends at the \"Entity Extraction.\" The flowchart also shows the relationship between these stages, indicating how they are connected and how information flows from one stage to another.",
        "reference": "Figure 1: Overview of the Lithium NLP pipeline"
    },
    "2101.07004v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the use case for a system that involves direct and indirect call relations. The system is designed to process data transmissions, with an uplink pilot and a downlink data transmission. The processing involves tracing t1 and t2, as well as probabilistic distribution over vocabulary. The system also includes a stack of encoders and a final input representation. The output layer consists of positional embeddings, segment embeddings, and word embeddings. The mask sequence starts at the beginning, and the SEP token marks the end of a sentence. The system is trained on a large corpus of text, and it can be fine-tuned for specific tasks such as question answering or language modeling.",
        "reference": "Figure 2. Block diagram of one transmission block."
    },
    "1061647-Figure5-1.png": {
        "prediction": "The diagram shows a flowchart of the use case for a system that involves direct and indirect call relations. The image, which is 256x256 pixels in size, is processed to generate a nose heat map. The system then uses this information to perform deconvolution and convolution. The output layer of the system is shown at the bottom of the diagram.",
        "reference": "Fig. 5. Architecture of the pose estimation network. First 5 layers copied from VGGM, followed by a deconvolution layer. A 1x1 convolution layer is then used to predict each output heat map."
    },
    "2103.06081v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT (Bidirectional Encoder Representations from Transformers). The objective of the model is to learn contextualized representations of words and their relationships. The architecture of BERT in pretraining is shown, with the input sequence being masked and replaced by special tokens like [MASK] or [CLS]. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the sample plane, objective, and other related components of the BERT model.",
        "reference": "Figure 2. Illustration of the illumination scheme of double SIM using far-field optics. (a) Schematic of one possible approach to generate the illumination patterns with structured electric energy density and OC simultaneously. Structured OC patterns formed by the interference of (b) two L-CPL beams ( )C x+ and (c) two R-CPL beams ( )C x\u2212 . (d, e) Structured patterns of ( )eU x . The OC patterns in (b) and (c) possess the opposite handedness. The eU patterns in (d) and (e) are the same. Line-cut profile of the (f) OC and (g) eU along the dashed lines in (b-e). Scale bar in (b) is 02\u03c0/k and applicable for (d-e)."
    },
    "2202.03373v1-Figure5-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is designed to learn the contextualized word embeddings and segment embeddings from the input text. The architecture consists of several components, including an input layer, a stack of encoders, a final input representation, and a positional embeddings layer. The encoders are responsible for processing the input and extracting features. The final input representation layer combines the features extracted by the encoders with the positional embeddings. The output layer then uses this information to generate the final output. The entire process is supervised by a curve estimation method, which helps in optimizing the model's performance. Overall, the model is designed to learn the contextualized representations of words and segments, which can be used for various natural language processing tasks such as language translation, sentiment analysis, and text classification.",
        "reference": "Figure 5. An illustration of Curve Non-Linear Unit. This layer can be seen as a learnable non-linear activation function between 0 and 1. Based on Eq. 3, the learned function always follows concave down increasing curves to increase feature intensities."
    },
    "2207.07828v1-Figure2-1.png": {
        "prediction": "The diagram depicts a complex system with various components and connections. The main components are labeled \"use case u1\", \"direct call relations\", \"indirect call relations (C1, C2)\", and \"direct call patterns (C1, C2)\". There are also several smaller components such as \"Discriminator-D\", \"Structural Prior Estimator\", and \"Discriminator-Da\". The connections between the components are represented by arrows. The diagram is quite detailed, with many different elements and relationships between them. It seems to be a flowchart or a similar type of diagram that is used to illustrate a process or a system.",
        "reference": "Fig. 2. Overview of the proposed Structural Prior guided Generative Adversarial Transformers (SPGAT). The SPGAT contains one generator, two discriminators, and one structural prior estimator. The generator is used to explore non-local information with the guidance of a structural prior estimator (SPE) for better clear image restoration. We build the skip connections between the generator and discriminators so that the discriminators can better discriminate between real and fake features in the generator for generating more natural images. Meanwhile, SPE is also utilized to guide the discriminators for better estimations. The generator takes a low-light image L as the input and outputs an enhanced image E\u0302. The SPE is input the corresponding structure S of L by gradient operation and outputs the estimated structural prior P\u0302. Both the two discriminators take the concatenation of the enhanced image E\u0302 and estimated structural P\u0302 as the input, which is regarded as the fake label. However, there is no real label feature. To that aim, we generate real label features by inputting the normal-light image to the generator and its corresponding structure to SPE. PW-STB illustrated in Fig. 3 is the basic block of the generator, discriminators, and SPE."
    },
    "2102.08827v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that illustrates the scene and skill ontology. It shows the relationships between different elements, such as objects, environment, and action skill. The flowchart also includes various nodes like use case, direct call relations, indirect call patterns, and output layer. The scene ontology and skill ontology are depicted in the diagram, with arrows indicating the flow of information and relationships between the different components. Overall, the flowchart provides a visual representation of how these elements interact within the scene and skill ontology.",
        "reference": "Fig. 3. Class diagram of the connections between skills and scene elements."
    },
    "2204.08805v1-Figure5-1.png": {
        "prediction": "The image is a flowchart that shows the process of video processing. The flowchart has different sections, such as user interface, feedback, timeline overview generation, body model augmentation, and suggestive viewpoint. It also includes various elements like pose analysis, sequence alignment, and temporal segmentation. The diagram also shows the interaction between these elements, such as the user interface and the feedback. Additionally, it depicts the process of 3D pose reconstruction and the role of attributes in the comparison.",
        "reference": "Fig. 5. The architecture of VCoach, which comprises five main modules."
    },
    "2207.13865v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of pretraining BERT. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The architecture consists of a ground set of domains, a randomly sampled mask, and a randomly sampled token. The model is trained on a level-one sampling and a level-two sampling. The final output of the model is a representation of the input sequence.",
        "reference": "Figure 3: Illustration of the sampling framework of DOMI. The solid arrow indicates the actual sampling flow, while the dotted arrow only indicates the difference between randomly sampling and DOMI."
    },
    "1828713-Figure2-1.png": {
        "prediction": "The image is a flowchart that depicts the architecture of BERT, a pre-trained contextual language model. The model is designed to be used for various natural language processing tasks such as question answering and text classification.\n\nThe flowchart shows the different components of the BERT model, including the input layer, transformer layers, and output layer. The input layer consists of word embeddings, which are represented by green rectangles. These embeddings are then passed through multiple transformer layers, which are represented by blue rectangles. The transformer layers are responsible for capturing the contextual information in the text.\n\nAfter passing through the transformer layers, the output layer is used to perform specific NLP tasks. The output layer is shown in orange and is connected to the transformer layers with red arrows. This indicates that the output layer receives the information from the transformer layers to perform the desired task.\n\nOverall, the flowchart provides a clear visual representation of how the BERT model works and how it can be used for various NLP tasks.",
        "reference": "Figure 2: An illustration of the neural network approach for sentiment classification. wi means the i-th word of a review text. uk and pj are continuous vector representations of user k and product j for capturing user-sentiment and product-sentiment consistencies. Uk and Pj are continuous matrix representations of user k and product j for capturing user-text and product-text consistencies."
    },
    "53776814-Figure4-1.png": {
        "prediction": "The image is a diagram of the architecture of BERT, a pre-trained language model. The diagram shows two stacks of encoders, each with 12 layers. Each layer consists of self-attention and feedforward neural network (FNN) sub-layers. The encoders are followed by a pooler and a classification head. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes a section on the STOGN layer, convolution, and deconvolution.",
        "reference": "Figure 4. Illustration of stacked hourglass STGCN with two levels."
    },
    "2107.01209v1-Figure5-1.png": {
        "prediction": "The image depicts a diagram of the Earth's shadow on the Moon during a lunar eclipse. The blue circle represents the Earth, and the yellow circle represents the Moon. The orange dot in the center of the blue circle is the Sun. The Earth's shadow, which is cast onto the Moon, is shown as a black area surrounding the blue circle. The diagram also includes labels for the Earth, the Moon, the Sun, and the shadow.",
        "reference": "Figure 5. Left: The DM particle enters the bulk of the sun with an angle \u03b8, collides with solar electrons and finally escapes from the Sun. For the purpose of illustration, the DM trajectory is projected onto a single plane. Right: The flux of reflected DM particles can be approximated as propagating along the radial direction from the center of the Sun. See text for details."
    },
    "2110.14160v2-Figure2-1.png": {
        "prediction": "The image is a flowchart that shows the process of training a BERT model. The flowchart starts with the \"Training data\" and \"Testing data\" inputs, which are then processed by the \"Preprocessing\" step. The preprocessed data is then used in the \"ResNet-50\" model for feature extraction. The extracted features are then passed through the \"Ensemble method selection\" and \"Sampling strategy selection\" steps to create a final representation of the input. This final representation is then used in the \"Prediction\" step, which outputs the predicted result. Additionally, the flowchart also shows the \"Objective function selection\" and \"LR schedule selection\" steps, which are used to optimize the model's performance during the training process.",
        "reference": "Figure 2: Components analyzed in our deep learning-based DR grading framework. The evaluation process of a framework can be divided into two parts: training (top) and testing (bottom). In the training phase, we first fix the architecture of the selected network (ResNet-50). Then we examine a collection of designs with respect to the training setting including preprocessing (image resizing and enhancement), training strategies (compositions of data augmentation (DA) and sampling strategies) and optimization configurations (objective functions and learning rate (LR) schedules). In the testing phase, we apply the same preprocessing as in the training phase and employ paired feature fusion to make use of the correlation between the two eyes (the training step of the fusion network is omitted in this figure). Then, we select the best ensemble method for the final prediction."
    },
    "2205.12022v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of an image generator with Res FFT-Conv block. The image generator consists of an encoder, a per-region encoding, and a per-region normalization. The Res FFT-Conv block is used to extract features from the input images. The output layer represents the final input representation. The diagram also includes a parsing generator with Res FFT-Conv block, which is used to parse the input sequence and generate the corresponding output sequence. Additionally, there are two encoders, one for the input image and another for the input sequence. A spatial-aware normalization is applied after the Res FFT-Conv block. The diagram also shows a mask sequence, which is used in the pretraining of BERT.",
        "reference": "Fig. 1. Overview of our model. Our model consists of parsing generator and image generator, training parsing generator requires a pair of source-target images IS , IT , then obtaining human keypoints KS ,KT and human parsing map PS , PT respectively by using openpose and PGN framework. We concatenate KS , PS ,KT as the input of parsing generator, then the input is fed into an Unet-type network that generates a target parsing map with the same size of PT , which contains body shape information. To get the vivid image with detailed texture (e.g.style of clothing), we extract the Per-region feature information PSj from the source image via VGG type network, then we concatenate the parsing map to the above KS , PS ,KT and normalize it along with the region information PSj to combine the information of source texture and target parsing map. Finally, the target image can be generated by spatial normalization and decoder."
    },
    "2207.06080v2-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system, with various components and their interactions. The system is designed to process text data and generate output in the form of an image.\n\n1. The input layer receives the text data, which is then passed through a feature extraction layer. This layer extracts features from the text data, such as word embeddings, segment embeddings, and position embeddings.\n2. The extracted features are then passed through a classification layer, where the text data is classified into one of two categories: \"use case 1\" or \"use case 2\".\n3. If the text data belongs to \"use case 1\", it is passed through a pre-training layer, where the model learns to predict masked tokens in the text data. This is done using a BERT-like architecture, which consists of a stack of encoders and a final input representation layer.\n4. If the text data belongs to \"use case 2\", it is passed through a dense layer, which performs further processing on the text data.\n5. The output layer generates the final image based on the processed text data.",
        "reference": "Fig. 2: Illustration of feature and classification embeddings, using the Resnet 32 architecture. Feature embeddigs (FE) are extracted by the CNN\u2019s non-linear layers and are combined with the final classification layer weights to form classification layer embeddings (CLE), before they are summed."
    },
    "2204.09225v1-Figure1-1.png": {
        "prediction": "The diagram shows a model and its corresponding twin transformer. The model consists of a space, time, and FMRI data section. The twin transformer is composed of a spatial transformer, a matrix multiplication layer, a recovery layer, and an FMRI data2 section. The spatial transformer has a spatial position, a temporal position, and a linear projection. The matrix multiplication layer has a matrix multiplication, a recovery layer, and an FMRI data2 section.",
        "reference": "Fig. 1. Illustration of the proposed Twin-Transformers framework. (a) shows the overall picture of the proposed model, and (b) shows the details of the Twin-Transformers. The Twin-Transformers take the identical brain signal matrix as input. The spatial Transformer focuses on the space dimension and takes non-overlapping spatial patches as tokens to build attention in the spatial dimension and generate spatial features. Conversely, the temporal Transformer focuses on the temporal dimension and the non-overlapping temporal patches are used as tokens. Correspondingly, the temporal Transformer is designed to build attention in the time dimension and generate temporal features. The twin Transformers are designed for disentangling temporal and spatial features of the input signal matrix."
    },
    "2102.09837v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that involves input, calibration, and output. The system is designed to calibrate s\\_calibrate, which is the calibration of the system. The input and output are represented by Init and Calibrated, respectively.\n\nThe diagram also shows a section called \"Calibrating\" with a label \"tp = 5\", indicating that the system will calibrate for a period of time equal to 5 units. This suggests that the system will calibrate for a specific amount of time before producing an output.\n\nOverall, the diagram represents a system that takes an input, calibrates it, and then produces an output after a certain amount of time.",
        "reference": "Figure 2: The platform model of a robot arm."
    },
    "2202.01032v2-Figure5-1.png": {
        "prediction": "The image is a flowchart that depicts the execution traces and temporal relations of a program. The flowchart is divided into three main sections: the upper section, the middle section, and the lower section.\n\nThe upper section shows the use case \"use case u1\" and the \"indirect call patterns (C1, C2)\". The middle section displays the \"direct call patterns (C1, C2)\" and the \"indirect call relations\". The lower section represents the \"direct call relations\".\n\nThe flowchart also includes several other elements such as \"Trace 1\", \"Trace 2\", \"Prob. Distribution over Vocabulary\", \"Prob. Distribution over Vocabulary\", \"Output Layer\", \"Stack of Encoders\", \"Final Input Representation\", \"Positional Embeddings\", \"Word Embeddings\", and \"Masked [CLS] he starts to [MASK] SEP the that crowd is watch-ing him. SEP\". These elements are interconnected with arrows, indicating the flow of information or execution in the program.",
        "reference": "Fig. 5: Near-RT RIC architecture. The near-RT RIC connects to the RAN through the E2 interface, at the bottom of the figure (yellow), and to the non-RT RIC/SMO through the A1 and O1 interfaces, at the top of the figures (orange and green, respectively). The communication among the RIC components (in light blue) is mediated by an internal messaging infrastructure. The near-RT RIC can onboard custom logic as xApps (dark blue)."
    },
    "2202.01032v2-Figure6-1.png": {
        "prediction": "The diagram depicts a service management and orchestration (SMO) framework. The SMO/Non-RT RIC framework functions are shown, as well as the data management and exposure functions. There is an internal messaging infrastructure, and the external interfaces are also shown. The diagram includes terms such as \"rApp 1\", \"rApp 2\", \"rApp N\", \"A1 termination\", \"R1 termination\", \"P (B follows A)\", \"FNN\", \"PB follows A\", \"Output Layer\", \"Stack of Encoders\", \"Final Input Representation\", \"Positional Embeddings\", \"Segment Embeddings\", \"Masked [CLS] he starts to [MASK]\", \"he starts to speak. the nervous crowd is watch-ing him.\", \"that\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\", \"nervous\", \"speak\",",
        "reference": "Fig. 6: Non-RT RIC and SMO architecture. The SMO functionalities (in green) enable connectivity to the O-Cloud (through the O2 interface) and the other RAN components (through O1) for management and orchestration. The non-RT RIC features custom logic (rApps, in red), and a termination of the A1 interface to the near-RT RIC (orange). Shared functionalities between the non-RT RIC and the SMO are in yellow."
    },
    "2202.02070v2-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is designed to pretrain the BERT model, which is a transformer-based language model developed by Google. The architecture of the BERT model is shown in the top left corner of the diagram. The model is trained on two tasks: next sentence prediction and masked language modeling. The pretraining process involves inputting a sequence of words, masking some of them, and predicting their values based on the context. The model is also trained to predict the next sentence in a given context. The diagram also shows the different components of the BERT model, such as the input embeddings, position embeddings, segment embeddings, and word embeddings. Additionally, there are several layers, including the transformer layer, the feedforward neural network (FNN), and the output layer. The diagram provides a detailed view of how the BERT model works and how it is pretrained for various natural language processing tasks.",
        "reference": "Fig. 2. The architecture of the proposed CGiS-Net."
    },
    "1319230-Figure1-1.png": {
        "prediction": "The diagram depicts a system for processing natural language queries. The input to the system is a query represented as a sequence of tokens, which are then passed through a sentence selection process. The selected sentence is then passed to a document summary module, which generates an answer in the form of a sequence of tokens. The answer is then passed to an answer generation module, which generates a final answer representation. The system also includes a query and document representation layer, a query and document summary layer, and a sentence selection layer.",
        "reference": "Figure 1: Hierarchical question answering: the model first selects relevant sentences that produce a document summary (d\u0302) for the given query (x), and then generates an answer (y) based on the summary (d\u0302) and the query x."
    },
    "210713891-Figure1-1.png": {
        "prediction": "The image depicts a flowchart of a task, with various elements and their corresponding functions. The task is to predict the masked tokens in a given sequence using BERT. The architecture of BERT in pretraining is shown, where the model's task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. Additionally, there is a section for the dialogue history, which contains the answer and the last question. The task also includes elements such as the i3d-flow, i3d-rgb, vggish, and AVSD Task.",
        "reference": "Figure 1: An illustration of DSTC8 AVSD task. The goal of the task is generating accurate answer based on multiple modalities."
    },
    "2206.00726v1-Figure1-1.png": {
        "prediction": "The diagram depicts a surrogate model and an update model. The surrogate model is used to predict the output of a function, while the update model is used to improve the surrogate model by incorporating new data. The surrogate model is based on a neural network, and the update model is based on a random sampling method. The diagram also shows that the surrogate model can be updated with new data, which will help improve its accuracy in predicting the output of the function.",
        "reference": "Fig. 1: Overview of our proposed algorithm. The BayesOpt model is composed of multiple Gaussian process models, which represent dynamic feasibility and collision avoidance constraints."
    },
    "2101.09353v2-Figure1-1.png": {
        "prediction": "The diagram shows a system for processing data. The data is collected by base stations, which are connected to a radio link. The processed data is then sent to a base station, which is connected to a processing machine. The data is also sent to a database. The system is used for cases where the data needs to be processed quickly and efficiently.",
        "reference": "Figure 1. Working principle and radioprobe system architecture."
    },
    "2101.01418v1-Figure2-1.png": {
        "prediction": "The diagram shows a high-level overview of the BERT pretraining process. The architecture is shown in two parts: the lower part represents the input to the model, and the upper part shows the output. The input consists of a sequence of words (tokens) that are passed through an embedding layer, which converts them into dense vector representations. These embeddings are then fed into a transformer-based architecture, which consists of several layers of self-attention and feedforward neural networks. The output of the transformer is a set of contextualized embeddings, which capture the meaning of each word in the context of the entire sentence. These embeddings are then used for various natural language processing tasks, such as classification, segmentation, and representation.",
        "reference": "Figure 2: Different levels in image processing process."
    },
    "2101.05151v2-Figure5-1.png": {
        "prediction": "The diagram depicts a step of prediction in a neural network. The input is processed through an inference, which is then passed to the output. The inference is represented by three pink circles, and the input and output are represented by two red circles. The inference process takes place between the input and output, and the output is generated after the inference has been completed. This process is repeated for multiple steps of prediction, as indicated by the arrows connecting the pink and red circles.",
        "reference": "Figure 5: Graphical illustration of long horizontal link forecasting. Given a sequence of graph snapshots G = {G(t\u2212tk), ...,G(t)}, whose length is k, test quadruples at t+ \u2206T are to be predicted."
    },
    "1482745-Figure1-1.png": {
        "prediction": "The diagram shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the input embeddings positions. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Figure 1: PacGAN(m) augments the input layer by a factor of m. The number of edges between the first two layers are increased accordingly to preserve the connectivity of the mother architecture (typically fully-connected). Packed samples are fed to the input layer in a concatenated fashion; the grid-patterned nodes represent input nodes for the second input sample."
    },
    "2210.08812v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT (Bidirectional Encoder Representations from Transformers). The model is pre-trained on large-scale unlabeled text data, and it consists of an encoder, transformer layers, and convolutional layers. The pre-training process involves masking some of the input tokens and predicting their values using the context. This allows the model to learn contextualized representations of words. The architecture of the BERT model is designed to capture both the left and right context of a given word, which helps in improving its performance on various natural language processing tasks such as question-answering, sentiment analysis, and text classification.",
        "reference": "Fig. 2. Illustration of aggregation based explicit transformer and modulation based implicit transformer."
    },
    "2101.07910v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that illustrates a process of generating code snippets. The process starts with an input, which could be either a literation or a sample elite. If the input is a literation, the process checks if it is less than N. If it is, the process generates a code snippet and adds it to the next generation. If the input is a sample elite, the process checks if it is a sample elite. If it is, the process generates a code snippet and adds it to the next generation.\n\nIf the input is not a literation or a sample elite, the process reflects the input with a mutation rate. This means that the process modifies the input slightly to create a new input for the next iteration. The process then continues to check if the new input is a literation or a sample elite, and repeats the process until a final generated code snippet is produced.",
        "reference": "Fig. 2. The guided mutation (GM) process for code adversarial generation."
    },
    "2012.06182v1-Figure1-1.png": {
        "prediction": "The diagram shows a complex system of communication and data transfer. The system is composed of different layers, each with its own set of protocols and methods for transmitting information. The layers are connected by inter-layer links, which allow data to be passed between them.\n\nAt the top of the system is the terrestrial layer, which is where people interact with the system through their devices such as laptops, smartphones, and tablets. These devices communicate with the next layer down, the astronomical layer, using radio waves or other forms of electromagnetic radiation.\n\nThe astronomical layer is responsible for transmitting data between Earth and space-based objects like satellites and telescopes. This layer uses a variety of technologies, including lasers, microwaves, and optical signals, to communicate with these objects.\n\nThe next layer down is the stratospheric layer, which is responsible for relaying data between the astronomical layer and the deep space layer. This layer uses high-altitude balloons to float in the stratosphere, where they can communicate with both ground-based and space-based objects.\n\nThe deep space layer is the final destination for data transmitted from the stratospheric layer. This layer contains a network of space-based objects, such as satellites and telescopes, that are used to collect and transmit data back to Earth.\n\nOverall, this system allows for the collection, processing, and transmission of data from space to Earth, and back again. It is a complex and sophisticated system that relies on a wide range of technologies and protocols to function properly.",
        "reference": "Fig. 1: Illustration of a multilayered SN with satellites, HAPs, and UAVs."
    },
    "2012.06182v1-Figure8-1.png": {
        "prediction": "The diagram shows a swarm of UAVs (Unmanned Aerial Vehicles) flying over a city. The UAVs are connected to a network of HAPS (High Altitude Platform Stations) that provide internet and communication services. The HAPS are connected to a ground station, which is in turn connected to a population area through an RFLink (Remote Area Link). The UAVs are also connected to a base station, which is connected to a tower. The tower is connected to a building, which is part of the population area. The UAVs are also connected to a router, which is connected to a computer. The computer is connected to a server, which is located in a data center.",
        "reference": "Fig. 8: An architecture of HAPs network with P2P HAP-to-HAP and backhauling links."
    },
    "2103.04910v1-Figure6-1.png": {
        "prediction": "The image is a flowchart that depicts the execution of a program. The flowchart has three main sections: the system, the regulator, and the recursive identifier. The system section is at the top, followed by the regulator in the middle, and the recursive identifier at the bottom.\n\nThe system section contains two sub-sections: the input layer and the output layer. The input layer consists of a stack of encoders, while the output layer represents the final input representation. These sections are interconnected with arrows, indicating the flow of data and information within the system.\n\nThe regulator section, located in the middle, is responsible for controlling the flow of data within the system. It includes a control design and a recursive identifier. The control design section is further divided into two parts: the input and the output. The input part is connected to the system's input layer, while the output part is connected to the system's output layer.\n\nThe recursive identifier section, located at the bottom, is connected to both the system's input and output layers. This section is responsible for identifying the input and output of the system, ensuring the proper flow of data and information.\n\nOverall, the flowchart shows a well-structured and organized system, with each component playing a specific role in the execution of the program.",
        "reference": "Figure 6: Model building approach"
    },
    "2202.06208v1-Figure1-1.png": {
        "prediction": "The diagram depicts a dynamic hierarchical tree, which is used for optimal transport of feature alignment and metric learning for prediction boundary. The tree has two main branches, with the left branch representing the optimal transport of feature alignment, and the right branch representing the metric learning for prediction boundary. The left branch contains several nodes, including \"use case u1\", \"direct call relations\", \"indirect call patterns (C1, C2)\", and \"output layer\". The right branch also contains multiple nodes, such as \"use case u1\", \"direct call relations\", \"indirect call patterns (C1, C2)\", and \"output layer\". The tree structure is designed to help in understanding the relationships between different components of the system and their interactions.",
        "reference": "Figure 1. The architecture of our proposed BROT. Sub-figure (a) depicts the OT process to align features from different domains. Sub-figure (b) shows that the metric learning aims to seek better decision boundaries with a dynamic hierarchical tree. The darkness of blue colors reflects the magnitude of molecular properties."
    },
    "2107.01104v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of BERT pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the input embeddings positions. The diagram also shows the architecture of BERT in pretraining, and the process of pretraining BERT. It includes the segment pair, input embeddings, word embeddings, positional embeddings, and mask sequence.",
        "reference": "Figure 1: Distribution of real-space data between MPI processes in TurTLE. Fields are split into slabs and distributed between P MPI processes along the x3 direction. The Np particles are also distributed, with each MPI process storing Pp particles on average. Within each MPI process the particle data is sorted according to its x3 location. This leads to a direct association between each of the Ps field slices to contiguous regions of the particle data arrays \u2014 in turn simplifying the interpolation procedure (see text for details). On average, S p particles are held within each such contiguous region."
    },
    "51865-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The architecture of BERT in pretraining is shown, with the model's task to predict the masked tokens from the representation vectors it learns at the input embeddings positions. The diagram also shows the CNN module and the C&E module, which are part of the BERT architecture. The arrows in the diagram indicate the flow of data and information through the different components of the BERT pretraining process.",
        "reference": "Figure 3. Overview of DCEC. The network is built by cascading the CNN module and the CEC."
    },
    "2012.02113v1-Figure8.1-1.png": {
        "prediction": "The image is a diagram that illustrates the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the next sentence prediction task and the feedforward neural network (FNN). It is a Venn diagram with three intersecting circles, representing H(X), H(Y), and H(X|Y).",
        "reference": "Figure 8.1 Venn diagram showing entropic quantities associated with a pair of random variables taking values in different sets: the Shannon entropies H(X) and H(Y), the joint entropy H(X,Y), the conditional entropies H(X | Y) and H(Y | X), and the mutual information I(X; Y)."
    },
    "2012.02113v1-Figure7.4-1.png": {
        "prediction": "The image is a flowchart that shows the use case of a system. The system has two main cases, \"use case 1\" and \"use case 2\". The flowchart also includes several nodes, such as \"direct call relations\", \"indirect call relations\", and \"direct call patterns\". These nodes are interconnected by arrows, indicating the flow of information or data between them.\n\nThe flowchart appears to be designed to help users understand the different components and their relationships within the system. It may be used in software development, system design, or other technical fields where it is important to visualize the flow of data or information within a system.",
        "reference": "Figure 7.4 Schematic illustration of Lemma 7.4.6."
    },
    "2204.12696v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart with several nodes and arrows. The nodes are labeled \"use case u1\", \"direct call relations\", \"indirect call relations (C1, C2)\", and \"direct call patterns (C1, C2)\". The arrows in the flowchart connect these nodes, indicating a sequence of operations or steps. The flowchart also includes text labels such as \"Strength of Microphone\", \"StyleGan Latent V\", and \"Latent space\". These labels provide additional information about the flowchart's purpose and function. Overall, the flowchart seems to represent a process or system that involves the use of microphones, style generation, and other elements related to audio processing.",
        "reference": "Figure 2: A tensor illustration of our low-rank micromotion subspace hypothesis. In the StyleGAN latent space, we hypothesize the same type of micromotion, at different quantitative levels but for the same identity, can be approximated by a low-rank subspace. We further hypothesize that subspaces for the same type of micromotion found at different identities are extremely similar to each other, and can hence be transferred across identities."
    },
    "206593096-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The model is trained on two tasks: masked language modeling (MLM) and next sentence prediction (NSP). MLM involves randomly masking 15% of the input tokens and predicting their values, while NSP involves predicting whether a pair of sentences are consecutive or not. The architecture of BERT in pretraining includes a stack of encoders, followed by a classification head for MLM and a feedforward neural network for NSP. The diagram also shows the design A and B, which represent the design of the region-wise computation and the design of the scaled input images, respectively.",
        "reference": "Figure 3: Two network designs in this paper. The input image is processed as a whole at the convolutional layers from conv1 to conv5. Segments are exploited at a deeper hierarchy by: (Left) applying CFM on the feature map of conv5, where \u201c b\u201d means for \u201cbounding boxes\u201d and \u201c s\u201d means for segments; (Right) applying CFM on the finest feature map of the spatial pyramid pooling layer."
    },
    "2101.10868v1-Figure1-1.png": {
        "prediction": "The diagram illustrates a system in which data is collected, processed, and shared. The government collects data on its citizens through order and monitoring. This data is then used to change users' behaviors. Social media platforms like Instagram and Twitter are also involved in the data collection process. Data is shared with unknown entities, and mass surveillance is conducted. The system also includes restrictions and surveillance, as well as changing users' behaviors. Multiple information aggregation occurs, and the user is at the center of the system.",
        "reference": "Figure 1: A Systematic Overview of Privacy Threats from Multiple Domains Related to the COVID-19 Pandemic"
    },
    "2102.04539v2-Figure10-1.png": {
        "prediction": "The diagram depicts a complex system with multiple interconnected components. The system is composed of several nodes, represented by circles, and arrows that connect them. There are at least 10 nodes in the system, and the arrows show the direction of information flow between these nodes.\n\nThe diagram can be divided into two main sections: the upper section and the lower section. The upper section has a more complex structure, with many interconnections and loops. In contrast, the lower section appears to be less complex, with fewer connections and a simpler layout.\n\nOverall, the diagram represents a complex network or system, with various components interacting and exchanging information. It could be used to model a real-world process, such as a communication system, a transportation network, or a biological system.",
        "reference": "Figure 10: Illustration for 2-Diam GBP. Here, V q denotes all sets V{q,\u00b7}."
    },
    "2102.04539v2-Figure6-1.png": {
        "prediction": "The diagram shows a flowchart with several nodes and arrows. The nodes are labeled \"use case u1\", \"direct call relations\", \"indirect call relations\", and \"direct call patterns\". The arrows connect the nodes, indicating relationships between them. The flowchart appears to be related to software development or system design.",
        "reference": "Figure 6: Illustration for the construction in the proof of Proposition 8 for 2-Reach GBP with r = 1. In this example, U = {u1, . . . , un} and we have {u1, ui, uj , un} = F \u2208 F ."
    },
    "2102.04539v2-Figure4-1.png": {
        "prediction": "The image is a flowchart that shows the execution traces and temporal relations. The flowchart is divided into different sections, including \"use case u1\", \"direct call relations\", \"indirect call relations\", and \"direct call patterns\". The diagram also includes various nodes such as \"Trace 1\", \"Trace 2\", \"Prob. Distribution over Vocabulary\", \"Prob. Distribution over Vocabulary\", and \"Output Layer\". Additionally, there are arrows connecting these nodes, representing the flow of information or data.",
        "reference": "Figure 4: Illustration to Constructions 3 & 4. Part (a) shows an exemplary directed graph which is a yes-instance for DHP. Applying Construction 3 on (a) yields (b). Applying Construction 4 on (b) yields the instance whose graph is depicted in (c) and two habitats of which are depicted in (d) and (e). Vertices marked yellow in (d) are contained in the habitat Xout. Vertices marked red in (e) are contained in the habitat Yout. The graph induced by Yout contains the red edges."
    },
    "2110.06487v1-Figure5-1.png": {
        "prediction": "The diagram shows a flowchart of a system that involves a victim, government, fund transfer, and online sale. The victim is shown in green, the government in blue, the fund transfer in orange, and the online sale in red. The flowchart starts with the victim, who then goes to the government. From the government, the flowchart branches out to the fund transfer and the online sale. The fund transfer is connected to both the government and the online sale.",
        "reference": "Fig. 5. Covid unemployment threat model"
    },
    "2103.00907v1-Figure4-1.png": {
        "prediction": "The diagram shows a neural network architecture with multiple hidden layers. The input layer is connected to the first hidden layer, which is then connected to the next hidden layer, and so on until the final hidden layer. The output layer is connected to the final hidden layer. The arrows in the diagram represent the flow of information through the network. The hidden layers are represented by circles, while the input and output layers are represented by rectangles. Each circle represents a node in the network, and each arrow represents a connection between two nodes.",
        "reference": "Figure 4. Diagram of the neural network(NN2) for the turbulent channel flow. The inputs are the pressure and velocity gradient and the output is the pressure strain term. The FCFF has 5 layers with 10 neurons in each layer."
    },
    "2206.00162v2-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses BERT for pre-training. The architecture of BERT is shown in the pre-training phase, where the model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The Quality Boosters are used to improve the quality of the embeddings. In the lowercased example sequence consisting of the segment pair \u2018he starts to speak. the nervous crowd is watch-ing him.\u2019 the tokens \u2018speak\u2019 and \u2018nervous\u2019 were sampled to be masked.'speak' is replaced by the \u2018[MASK]\u2019 token and \u2018nervous\u2019 is replaced by the random token \u2018that\u2019. The model's task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Figure 2: Overview of PAGER generation method."
    },
    "2011.03148v2-Figure4-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model, specifically BERT. The model is pre-trained on large amounts of unlabeled text, and then fine-tuned on a specific task such as question-answering or sentiment analysis.\n\nThe first part of the flowchart represents the pre-training phase. In this phase, the model learns to predict masked words in a given sentence. This is done by randomly masking some of the words in a sentence and training the model to predict these missing words based on the context provided by the surrounding words. This process helps the model learn the relationships between different words in a sentence and understand the meaning of the text.\n\nThe second part of the flowchart shows the fine-tuning phase. In this phase, the pre-trained BERT model is fine-tuned on a specific task, such as question-answering or sentiment analysis. During this process, the model's parameters are adjusted to better perform the specific task it has been trained for. This fine-tuning step allows the model to adapt to the specific requirements of the task at hand, improving its performance on that particular task.\n\nOverall, the BERT model is designed to be highly versatile, able to perform well on a wide range of natural language processing tasks with minimal additional training.",
        "reference": "Fig. 4. Diagram of perception consistency loss computation. An EfficientDet object detector predicts boxes and classes. Consistency of predictions between images is captured by losses similar to those in object detection training."
    },
    "2110.09749v2-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the process of executing traces and temporal relations. The flowchart includes several key components, such as the use case, direct call relations, indirect call patterns, and document representation. The diagram also shows how these elements interact with each other in the process. It seems to be a part of a software or system design, possibly related to data processing or information retrieval.",
        "reference": "Figure 1: The KIEMP model architecture."
    },
    "2103.07592v2-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented by the top left corner of the diagram, with the direct call relations and indirect call patterns depicted in the middle and right sections respectively. The diagram also includes information about the output layer, stack of encoders, and final input representation. The architecture of BERT in pretraining is shown, where masked tokens are replaced by random tokens and the model's task is to predict the original tokens from the learned representations.",
        "reference": "Figure 1: Schematic diagram for flux-mediated dark matter."
    },
    "2107.05680v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into two main parts: the upper part represents the use case, and the lower part represents the direct call patterns. The use case consists of two main components: the direct call relations and the indirect call relations. The direct call relations are further divided into two sub-components: the C1 and the C2. The indirect call relations are also divided into two sub-components: the C1 and the C2. The direct call patterns are represented by the C1 and the C2. The diagram also includes several other components such as the P, B, W, ZW, and D.",
        "reference": "Figure 2: A modified architecture for progressive training of convex GANs (ProCoGAN). At each stage i, a linear generator Wi is used to model images at a given resolution Xi, attempting to fool quadratic-activation discriminator Di, for which the optimal solution can be found in closed-form via (15). Once stage i is trained, the input to stage i + 1 is given as the output of the previous stage with learned weights W\u2217i , which is then used to model higher-resolution images Xi+1. The procedure continues until high-resolution images can be generated from successive application of linear generators."
    },
    "2107.05214v2-Figure6-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is trained on a large corpus of text, and the output layer is used to predict the next word in a sequence. The model is pre-trained using a masked language modeling (MLM) task, where it learns to predict the masked words in a given sentence. The pre-training process involves randomly masking some of the words in a sentence and training the model to predict the masked words. This helps the model learn the context and meaning of the words in the sentence.\n\nAfter pre-training, the model is fine-tuned for a specific task such as question-answering or sentiment analysis by training it on a smaller dataset that is relevant to the target task. During fine-tuning, the model learns to perform the specific task by adjusting its parameters based on the new data. The model is then evaluated on a validation set to check its performance before being deployed in a real-world application.\n\nIn summary, the diagram shows a machine learning model that is pre-trained using a MLM task and then fine-tuned for a specific task like question-answering or sentiment analysis.",
        "reference": "Figure 6: The illustration of the attention mechanism. The prediction of current hidden state h\u0302t and the grid-level features E is used as query and key, respectively."
    },
    "2102.12058v2-Figure17-1.png": {
        "prediction": "The image is a flowchart that illustrates the federated voting process. The process starts with an uncommitted node, which then votes \"a\" and is committed. A node in each slice accepts \"a\", and if \"a\" is valid, it is confirmed. The flowchart also shows the accepted and confirmed nodes as part of the process.",
        "reference": "Fig. 17. Federated voting process [46] ."
    },
    "2103.05447v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a data collection location. The data collection location is located in the middle of the flowchart, and it is connected to a pump on the right side with a pipe. There are also two arrows pointing towards the data collection location, indicating that the data is being collected from two different sources. Additionally, there is a flow meter located at the top left corner of the flowchart, which measures the flow rate of the data being collected. Finally, there is a pool at the bottom right corner of the flowchart, where the data is stored after being collected.",
        "reference": "Figure 2. Experiment schematic."
    },
    "2010.00041v3-Figure2-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of evaluating fitness in a population. The process starts with the random creation of an initial population. Then, the fitness of each individual in the population is calculated. If the fitness is satisfactory, the individual passes to the next population. Otherwise, the individual is mutated and returned to the best individual. This process continues until a stopping criterion is met. The flowchart also shows the selection of passing elites to the next population and the mutation of individuals.",
        "reference": "Fig. 2. The optimization flowchart for the inverse material design."
    },
    "2202.00846v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that involves user interaction, call relations, and call patterns. The system begins with a user purchasing a log, which is then used to display the information to the user. The system then estimates the CVR for each group, and if the CVR is high, the system proceeds to Group 2. If the CVR is low, the system stops and declares the results. The system also involves computing an assignment probability, which is used to determine the next step in the process. The flowchart shows a complex system with multiple conditions and decision points based on the CVR estimation.",
        "reference": "Figure 2: Method Overview"
    },
    "2101.07424v2-Figure1-1.png": {
        "prediction": "The image depicts a flowchart with various nodes and arrows. The flowchart is divided into different sections, including \"use case u1\", \"direct call relations\", \"indirect call relations\", and \"direct call patterns\". The diagram also shows the use of \"PB follows A\", \"Prob. Distribution over Vocabulary\", and \"FNN+softmax\" in the context of \"Output Layer\". Additionally, there are sections for \"Encoded scene\", \"Encoded scene\", \"Prism\", and \"Shifted encoded scene\". The flowchart seems to be related to a specific type of software or system architecture.",
        "reference": "Fig. 1. Physical sensing phenomena in CASSI, which is the CSI prototype used to validate the proposed approach."
    },
    "2204.03873v1-Figure2-1.png": {
        "prediction": "The image depicts a flowchart of a machine learning model. The model is designed to pretrain the BERT model, which is a transformer-based language model. The flowchart shows the input and output layers, as well as the different components of the model such as the Transformer, Softmax, and Mish. The model also includes various operations like batch normalization, dropout, and input embeddings. The architecture of the BERT model in pretraining is also shown, where masked tokens are replaced with special tokens, and the model's task is to predict the original tokens from their representations. The diagram provides a detailed view of the BERT model's architecture and its pretraining process.",
        "reference": "Figure 2: Structure of gait-TR. TCN is the temporal convolutional network module, and ST is the spatial transformer module. FC denotes full connect layer. Batch-norm is BatchNorm2D for input Xtv \u2208 \u211d C\u00d7T\u00d7V , while Batch-norm* denotes BatchNorm1D for input Xtv \u2208 \u211d C\u2217V \u00d7T ."
    },
    "2204.10426v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the use case for a system that uses BERT for natural language understanding. The system is designed to process text, and it is trained on two main tasks: next sentence prediction and masked language modeling. The input to the system consists of an embedding layer, followed by a stack of encoders. The encoders are responsible for processing the input and extracting features from the text. These features are then passed through a feedforward neural network (FNN) to make predictions about the next sentence or to predict missing words in a masked text sequence. The output of the system is a representation of the input text, which can be used for various natural language understanding tasks such as question answering, sentiment analysis, and text classification.",
        "reference": "Figure 1: Three-state illness-death model"
    },
    "2204.09537v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that uses a cryogenic shield to detect and excite specific fields. The system is designed to read-out the output layer representation of the input layer, which is achieved by using a wave meter. The system also includes a fiber coupler, a diole detector, and an excitation light source. The diagram shows the various components and their connections within the system, as well as the direction of data flow through the system.",
        "reference": "Figure 1: Top: Schematic of the hydrogen beamline including the electric field ionizer (FI) and the microchannel plate (MCP) particle detectors. The electric and magnetic fields required for optical Rydberg excitation (cf. section 2.2.1) yet need to be experimentally implemented. Bottom: Schematic of the injection seeded Ti:Sa Rydberg excitation laser."
    },
    "2107.06268v1-Figure1-1.png": {
        "prediction": "The image depicts a flowchart with several sections. The first section, labeled \"Data cleaning and preprocessing,\" is followed by a holiday adjustment procedure. The next section involves train individual forecasting models. Finally, the last section combines forecasts by smoothed Bernstein Online Aggregation.",
        "reference": "Fig. 1: Structure of forecasting approach used for the forecasting competition."
    },
    "2011.14684v2-Figure4-1.png": {
        "prediction": "The diagram depicts a Residual Reduction Module. The module consists of two blocks, the SE Block and the Reduction Block. The SE Block is connected to the Reduction Block by an addition operation. The SE Block is composed of a global average pooling layer followed by a dense layer. The Reduction Block is composed of a sigmoid layer, a flatten layer, a dropout layer, and a dense layer. The input to the module is passed through convolutional layers, followed by a Relu activation function. The output of the module is obtained from the dense layer of the Reduction Block.",
        "reference": "Fig. 4: Overview of the REMnet architecture. The input of the model is the K\u00d71 tensor representing the CIR of the measurement. The dimensionality is reduced by N subsequent Residual Reduction Modules (RRM) with a feature attention mechanism. Finally, a fully connected layer composes the high-level extracted features and outputs the range error estimation."
    },
    "2206.08316v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to classify images of dogs and cats. The first stage of the model is the training stage, where the raw data is fed into the model. The second stage is the generating stage, where the model generates adversarial examples for the training data. The third stage is the attacking stage, where the model attacks the adversarial examples to generate new adversarial examples. The final stage is the representation stage, where the model represents the final input.",
        "reference": "Fig. 1 An illustration of transfer-based adversarial attack and the proposed method. The two images in Raw Dataset are from ImageNet, labeled as \u201cpersian cat\u201d and \u201cpapillon\u201d respectively. Note that they also have features of other cats and dogs, as well as pillow and car. The normal surrogate model is trained by one-hot labels, and its adversarial transferability is relatively weak. In contrast, the dark surrogate model we proposed is trained with enhanced dark knowledge. Thus, it demonstrates stronger adversarial transferability."
    },
    "2010.03420v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The diagram has an input layer, hidden layers, and an output layer. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the next sentence prediction task. The diagram is labeled with \"hidden layer\", \"hidden layer\", and \"hidden layer\" for each layer. It is a complex network with many interconnected nodes.",
        "reference": "Fig 2: Fully connected convolutional neural network with four hidden layers."
    },
    "2107.00465v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to learn the contextualized word embeddings and segment embeddings from the input sentences. The architecture consists of three hidden layers, with each layer having multiple nodes. The nodes in the hidden layers are interconnected by edges, representing the flow of information through the network. The arrows in the diagram indicate the direction of data flow within the network. The model is trained on a large corpus of text data, which is used to update the weights of the network during training. The final output of the model is a set of word embeddings and segment embeddings that capture the semantic meaning of the input sentences.",
        "reference": "Fig. 1: Illustration of the neural network architecture to predict the optimal generation outputs P\u0302g using the active power demand Pd as input: There are K hidden layers in the neural network withNk neurons each. Where k = 1, ...,K."
    },
    "210701837-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT. The top part of the diagram shows the architecture of BERT in pretraining. The bottom part of the diagram shows the training and prediction process of the BERT model.\n\nIn the pretraining section, the model's task is to predict the masked tokens from the input embeddings. This is done by training the model on a large corpus of text data. The pretraining process helps the model learn the contextual relationships between words in a sentence.\n\nIn the training and prediction sections, the BERT model takes an input sequence and generates a representation of the input. This representation is then used for various natural language processing tasks such as question answering, sentiment analysis, and text classification. The model is trained using supervised learning techniques where it learns to predict the next word in a sequence given the previous words. The model is also fine-tuned for specific tasks by adjusting its parameters based on the task-specific data.\n\nThe diagram also shows the loss function used during training, which measures the difference between the predicted output and the actual output. The model is optimized to minimize this loss function using backpropagation and gradient descent. The final output of the model is a vector representation of the input sequence, which can be used for various NLP tasks.",
        "reference": "Figure 2: The final segmentation volume is generated by taking into account activations from three FCNNs specialized on each 2D orientation. Neighboring slices are taken into account in a multi-channel approach. Full slices are used in prediction time, but training uses patches."
    },
    "2107.09388v2-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that processes audio input. The system consists of an input features section, which includes a convolution block and a fully connected layer. The input features are then passed through a position embedding layer, followed by a masking layer. The masked input is then processed by two MHSAs (Multi-Head Self-Attention) layers, each with add and norm operations. The final output of the system is represented by a fully connected layer.",
        "reference": "Figure 1: MHSA model configuration for SELD task."
    },
    "2103.10114v1-Figure10-1.png": {
        "prediction": "The image depicts a flowchart of a language model, specifically BERT. The diagram shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens represented as vectors (x,y,z). The model first adapts the input for 3M times by accumulating the output of the adaptation process. Then, it accumulates the output of the adaptation process again for 3 times. Finally, the output of the model is represented as a sequence of word embeddings. This process helps the model learn contextualized representations of words and phrases.",
        "reference": "Fig. 10: Iteration procedure in dynamical core."
    },
    "2011.02216v2-Figure10-1.png": {
        "prediction": "The diagram shows a flowchart of a use case for a system. The system has three rounds, each with its own set of direct and indirect call patterns. In the first round, there are two traces, one following the other. The second round has two traces that follow each other in the opposite direction. The third round has a single trace that follows a different path. Each round has a different set of encoders, and the final state is represented by an input representation.",
        "reference": "FIG. 10 Example of entanglement detection with bounded configuration space. In this illustration, every measurement in the first n \u2212 1 rounds is a 4-outcome POVM, where one outcome leads directly to guess SEP, and 3 outcomes continue with the measurement process."
    },
    "2102.12327v1-Figure4-1.png": {
        "prediction": "The image is a flowchart that depicts the use case of a system. The flowchart has three main sections: \"use case 1\", \"direct call relations\", and \"indirect call patterns\". The \"use case 1\" section is further divided into sub-sections, including \"Trace 1\" and \"Trace 2\". The diagram also shows various nodes, such as \"FNN+softmax\" and \"FNN+softmax\", which are likely part of the system's architecture or components. The flowchart seems to be designed to illustrate the relationships between different parts of a system, possibly in a software development context.",
        "reference": "Figure 4: PC recommender knowledge base: result of the diagnosis process presented in WEEVIS."
    },
    "2010.01919v2-Figure6-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of pre-training and fine-tuning BERT. The flowchart has two main sections, \"Pre-training start\" and \"Fine-tuning start\", which are further divided into sub-sections. The \"Pre-training start\" section shows the model's training, evaluation, and pre-training steps. The \"Fine-tuning start\" section displays the model's training, evaluation, and fine-tuning steps. The flowchart also includes a diagram showing the relationship between the \"APmin(AP,i)\" and \"APmin(AP,i-1)\" variables.",
        "reference": "Fig. 6: Training process. Data augmentation in pre-training. No data augmentation in fine-tuning."
    },
    "2107.12156v1-Figure2-1.png": {
        "prediction": "The diagram illustrates a machine learning process, specifically the Bayesian optimization method. The process begins with a recommended thin film fabrication condition. This condition is then used to create a thin film sample. The results of this sample are then evaluated, and the evaluation is used to update the Bayesian optimization process. The updated process then recommends a new thin film fabrication condition, and the cycle continues. The final output of the process is a representation of the input embeddings.",
        "reference": "Figure 2. Schematic illustration of a machine-learning-integrated closed-loop process to optimize thin film fabrication parameters [13]"
    },
    "2107.00710v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The top part of the diagram represents the use case, which is divided into two parts: direct call relations and indirect call relations. The bottom part of the diagram shows the direct call patterns and the indirect call patterns. The diagram also includes a short input example, a mood state classification, a batch norm, a fully connected layer, a dropout, a long interval, and a convolution. This diagram provides a detailed view of the system's operation and how it processes data.",
        "reference": "Figure 1. (A) The network\u2019s architecture employed for Mood-State Bipolar classification using short intervals containing 2830 learnable parameters. In this figure, Ri refers to the ith residual block (i \u2208 {1, 2, 3}), while Bj refers to the jth InceptionTime block (j \u2208 {1, 2, 3}). Conv refers to a convolutional layer and GAP refers to the Global Average Pooling operation. Finally, the plus signs refer to an element-wise summation. (B) Short-Long Network\u2019s architecture using 5689 parameters. The features from the Short Network corresponds to the output of the global average pooling operation in the Short Network."
    },
    "1369182-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a use case for a system. The system has two main components: the master action and the action. The master action is responsible for executing a specific task, while the action is the actual task that the system performs.\n\nThe system starts with an observation, which is then passed through the master action. The output of the master action is then sent to the action, which is the final step in the process. The arrows in the diagram represent the flow of data from one component to another.\n\nThe diagram also includes a section on indirect call patterns, which are used by the system to perform specific tasks. These patterns are designed to be efficient and effective, allowing the system to complete its tasks quickly and accurately.",
        "reference": "Figure 1: Structure of a hierarchical sub-policy agent. \u03b8 represents the master policy, which selects a sub-policy to be active. In the diagram, \u03c63 is the active sub-policy, and actions are taken according to its output."
    },
    "2107.09510v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a self-attention block. The self-attention block is designed to help the model understand the context of the input and generate appropriate responses. It consists of several components, including the stress label, output layer, and self-attention block. The self-attention block contains an add and normalization step, followed by a dense layer. This process is repeated multiple times in the multi-head attention section. The final output representation is generated from the positional encoding. Overall, this flowchart provides a detailed view of how the self-attention mechanism works within a language model.",
        "reference": "Fig. 2. The structure of the self-attention network (SAN) in this study. X is the input sequential physiological data. Four layers including one multi-head attention layer, one dense layer with two add & normalization layers form a self-attention block. After positional encoding, the model passes input data through 3 self-attention blocks and outputs the stress estimation result."
    },
    "2103.10312v2-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that involves the use of case 1 and case 2. The system is designed to handle both direct call relations and indirect call patterns. It consists of several components, including a stack of encoders, a pre-trained descriptor, a future extraction network, a region network, and a phase correlation model. The arrows in the diagram show the flow of information between these components. The system also includes an output layer, which represents the final input representation. The diagram provides a detailed view of the architecture of the BERT model in pre-training.",
        "reference": "Figure 2. The Deep Autofocus architecture for a mini-batch size of one (recall our mini-batch size is thirty-two during training). The network inputs a single-look complex (SLC) image, ge, which is dynamic range compressed (DRC) and passed into a feature extraction network (blue) along with the SLC\u2019s phase. The features are then fed to a regression network (green) which outputs the phase corruption model parameters, which in this case are the coefficients of ten-degree polynomial modeling the phase error. The phase error is then applied in the k-space domain, computed by the fast Fourier transform (FFT), of the SLC and then inverse Fourier transformed back to the spatial domain. Finally, the relative improvement in sharpness between the input and output magnitude images is measured and weights are backpropagated to minimize this quantity (recall minimization of this term equates to maximisation of relative image sharpness, see Eq 8). Our formulation is end-to-end differentiable and trained on a graphics processing unit (GPU). During deployment, only a single forward pass is needed to compute g\u0302 which is represented by the red path."
    },
    "204955470-Figure1-1.png": {
        "prediction": "The diagram shows a system for ranking videos based on their relevance to a given topic. The system consists of a candidate generator, a ranking model, and a value model. The candidate generator takes in videos and generates candidates for the ranking model. The ranking model ranks the candidates based on their relevance to a given topic. The value model then evaluates the ranking model's performance by comparing it to a set of ground truth topics. The output of the system is a ranked list of videos relevant to the given topic.",
        "reference": "Figure 1. Solution overview."
    },
    "204955470-Figure6-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses a NextVLAD model for image classification. The system takes in an input image and frame-level data, which is then passed through a stack of encoders. These encoders are used to extract features from the input image. The extracted features are then passed through a NextVLAD model, which is a neural network-based model for learning visual representations. The output of the NextVLAD model is a set of embeddings, which are then passed through a gate to select the most relevant embeddings. These selected embeddings are then used as input to a prediction module, which predicts the class of the input image based on the embeddings. The system also includes a soft target prediction module, which is used to predict the soft targets for the input image.",
        "reference": "Figure 6. Overview of a mixture of 3 NeXtVLAD models(MixNeXtVLAD) with online knowledge distillation. The orange arrows indicate the distillation of knowledge from the mixture prediction to the predictions of sub-models."
    },
    "12938495-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The model is trained on a large corpus of unlabeled text, and it learns to predict missing words in sentences. The architecture of BERT consists of an embedding layer, a transformer-based encoder, and a pooler layer. The pretraining task involves predicting the masked words in a sentence and predicting the next sentence in a given pair of sentences. The model's task is to predict the masked words from the representation vectors it learns at the positions of the input embeddings. The output layer represents the final input representation.",
        "reference": "Fig. 1. Illustration of the subgraph embedding model scoring a candidate answer: (i) locate entity in the question; (ii) compute path from entity to answer; (iii) represent answer as path plus all connected entities to the answer (the subgraph); (iv) embed both the question and the answer subgraph separately using the learnt embedding vectors, and score the match via their dot product."
    },
    "2202.06988v2-Figure22-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations of a program. The program is divided into two main parts: the pretraining BERT architecture and the supervised part. In the pretraining BERT architecture, there are two main tasks: next sentence prediction and masked language modeling. The masked language modeling task involves replacing some tokens in the input sequence with a special token, which the model has to predict from the representation it learns at the position of the input embeddings. The next sentence prediction task requires the model to predict whether the next sentence follows the current one or not. The supervised part of the program consists of a stack of encoders, followed by a final input representation layer. The encoders are responsible for processing the input data, and the final input representation layer combines the output of the encoders to generate the final input representation. This representation is then used as the input to the downstream task, such as question answering or text classification.",
        "reference": "Figure 22: Visualisation of gradient back-propagation, comparing differentiable and supervised setups; displayed is a 3-step setup; the loss gradients from the last step are propagated through all previous steps and towards all previous network outputs; if the back-propagation is split into subranges, the gradients of the simulation state are set to zero, visualised by \u2019\\\u2019"
    },
    "2102.06744v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The architecture consists of an input embedding layer, a stack of encoders, and a final input representation layer. The encoders are composed of multiple layers, including a transcribed layer, a correction function, a hyperparameter layer, and a mask layer. The output of the encoders is passed through a max pooling layer, which is used to extract the most important information from the input. The final output is then passed through a positional embeddings layer, which adds positional information to the output. This process is designed to help the model learn better representations of the input data by incorporating positional information and using a stack of encoders to extract more complex features.",
        "reference": "Fig. 1. Neural classifier model"
    },
    "2206.05406v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The top section shows the architecture of BERT in pretraining, with a server and an aggregating section. The bottom left section illustrates the normal clients, training, and free-rider client sections. The bottom right section shows the empty train and no train sections. The middle section displays the pretraining BERT architecture.",
        "reference": "Figure 2: Illustration of a free-rider attack. The free-rider does not perform normal training, but transmits fake model updates to the server by adding opportune stochastic perturbations \u03b5 based on Gaussian noise N(0, \u03c3). Finally, the global model issued by the server will be distributed to the free-rider."
    },
    "2101.11685v2-Figure3-4-1.png": {
        "prediction": "The diagram depicts a flowchart of a use case, with the main focus on the product memory pipeline. The flowchart is divided into three sections: the pretraining BERT architecture, the product memory pipeline, and the output layer.\n\n1. Pretraining BERT Architecture: This section shows the architecture of the BERT model in pretraining. The input to the model consists of two segment pairs, which are represented as \"he starts to speak\" and \"the nervous crowd is watching him.\" The model's task is to predict the tokens \"speak\" and \"nervous\" from the representation vectors it learns at the positions of the input embeddings of \"speak\" and \"nervous.\"\n\n2. Product Memory Pipeline: The product memory pipeline is a critical part of the flowchart. It consists of a stack of encoders, followed by a product memory, and then a final input representation. The product memory pipeline is designed to process the input data efficiently and store the results for later use.\n\n3. Output Layer: The output layer of the flowchart represents the final step in the processing of the data. It takes the product memory as input and produces the desired output based on the given use case.\n\nOverall, the flowchart provides a detailed view of the different components involved in the processing of the data, from the pretraining of the BERT model to the final output layer.",
        "reference": "Figure 3-4: The overview of the modified squeeze-and-excitation block augmented with the memory layer. FGB is the function of global pooling which reduces the dimension of 3 dimensional feature map to the signle dimension, and Fadd is channel-wise addition of a resulting vector from memory to the original feature tensor."
    },
    "214727773-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented by the top left corner, which has two traces: Trace 1 and Trace 2. These traces are followed by a stack of encoders, which are represented by the green boxes. The encoders are connected to a stack of decoders, which are shown in orange. The decoders are connected to an output layer, which is depicted in blue. The diagram also shows that the output layer is followed by a final input representation. This flowchart provides a visual representation of the process and the relationships between different components.",
        "reference": "Figure 1. Illustration of LGKD, t(x,\u03b8) (left) and LTKD, t(x,\u03b8) (right)"
    },
    "2202.13121v1-Figure14-1.png": {
        "prediction": "The image depicts a flowchart related to the BERT pretraining process. The diagram is divided into two main sections: the source domain and the target domain. In the source domain, there are several images of people, and in the target domain, there is a textual description of the BERT pretraining architecture.\n\nThe BERT pretraining process involves two main steps: feature extraction and re-ID model. The feature extraction step is shown as a green arrow pointing from the source domain to the target domain. This suggests that the features extracted from the source domain are used as input for the re-ID model in the target domain.\n\nThe re-ID model is represented by a blue box with a white label that reads \"Re-ID Model.\" This indicates that the re-ID model is a key component of the BERT pretraining process. The re-ID model is also connected to the target domain, which further emphasizes its importance in the pretraining process.\n\nOverall, the diagram provides a clear visual representation of the BERT pretraining process, highlighting the role of the source domain, feature extraction, and the re-ID model in this process.",
        "reference": "Figure 14: An approach to achieve generalization."
    },
    "2011.07233v2-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart with various nodes and arrows. The nodes are labeled as \"use case u1\", \"direct call relations\", \"indirect call relations\", and \"direct call patterns\". The arrows indicate the flow of information or data between these nodes. The diagram also includes labels such as \"PB follows A\", \"Prob. Distribution over Vocabulary\", \"FNN+softmax\", and \"Output Layer\". The diagram seems to be related to a machine learning or artificial intelligence model, possibly a neural network architecture.",
        "reference": "Figure 3: On-surface aggregation. A 3D point x on the geometric scaffold \u0393 is seen in a set of source images. Each such image contributes a feature vector fk along a ray vk (green). On-surface aggregation uses a differentiable set network to process this data and produces a feature vector g for the target ray u (red)."
    },
    "2205.08180v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that illustrates the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the Transformer Encoder, Pre-Trained XL-SR Encoder, Attention Pooling, Linear Projection, Tand Act Function, and Cosine Distance Loss. The output layer represents the final input representation.",
        "reference": "Fig. 3: An illustration of the multimodal training framework"
    },
    "2010.03990v1-Figure5-1.png": {
        "prediction": "The diagram shows a flowchart of a deep convolutional neural network. The network is designed to classify images into one of two categories, either \"dog\" or \"cat\". The first step in the process is the input layer, which receives the image data. This data is then passed through a series of convolutional layers, which extract features from the image. These features are then passed through a pooling layer, which reduces the dimensionality of the data and helps to make the network more computationally efficient. Finally, the output layer uses a classifier to determine whether the image is a dog or a cat.",
        "reference": "Fig. 5: Architecture of FRCNN [33]"
    },
    "2010.03990v1-Figure6-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is based on the BERT (Bidirectional Encoder Representations from Transformers) architecture, which is pre-trained using masked language modeling and next sentence prediction tasks. The base network VGG-16 is used as the feature extractor for the input embeddings. The model is trained on a large corpus of text data to learn contextualized word representations. The predicted class scores and coordinates of each default box are used to generate the final output representation. Non-maximum suppression is applied to filter out redundant bounding boxes. The model is designed to be helpful, honest, and harmless in its application.",
        "reference": "Fig. 6: Architecture of SSD [21]"
    },
    "2010.03990v1-Figure7-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT. The model is pre-trained on a large corpus of unlabeled text and then fine-tuned on downstream tasks. The architecture of the BERT model is shown, with input embeddings, position embeddings, segment embeddings, and word embeddings. The pre-training process involves masking some of the tokens in the input sequence and predicting their values from the context. This is done using a feedforward neural network (FNN) and a convolutional module. The model is trained to predict the masked tokens and the next sentence. The diagram also shows the use case of the BERT model for various natural language processing tasks such as question answering, sentiment analysis, and named entity recognition.",
        "reference": "Fig. 7: Architecture of UESegNet-1"
    },
    "2202.07376v1-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is pre-trained using a masked language modeling task and a next sentence prediction task. The input to the model consists of two sentences, which are tokenized and passed through an embedding layer. The embeddings are then fed into a transformer encoder, which consists of multiple layers of self-attention and feedforward neural networks. The output of the transformer encoder is then passed through a pooling layer, which generates a fixed-length vector representation of the input. This vector is then used as the input to a classification or regression task.",
        "reference": "Figure 3: Our proposed end-to-end QPP model comprising a Siamese network of shared parameters of layered convolutional feature extraction, followed by either i) merge (concatenation) and a fully connected (FC) layer with a Sigmoid loss for pairwise testing (Equation 7) yielding a binary comparison indicator between a pair, or ii) a linear activation layer with pairwise hinge loss for pointwise testing yielding a score for a given query (Equation 8). Since the interaction for MDMQ and SDSQ are matrices with a single row only, the two layers of convolution filter sizes for these approaches are 1 \u00d7 5 and 1 \u00d7 3 (see Section 2.4)."
    },
    "2205.02090v1-Figure2-1.png": {
        "prediction": "The diagram depicts a complex system with multiple components and interactions. The system is composed of several modules, including an inter-model, an intr-model, and a discourse dependencies module. There are also several input and output layers, as well as a stack of encoders.\n\nThe inter-model and intr-model are connected to each other through a series of arrows, indicating the flow of information between these components. The discourse dependencies module is also connected to the inter-model and intr-model, suggesting that it plays a role in the overall functioning of the system.\n\nThe system also includes a final relations component, which is connected to the inter-model, intr-model, and discourse dependencies module. This suggests that the final relations component is responsible for processing the information from these components and producing a final output.\n\nOverall, the diagram shows a complex system with multiple interacting components, each playing a specific role in the functioning of the system.",
        "reference": "Figure 2: An overview of our model. Intra-sentential dependencies are discovered first and inter-sentential dependencies are constructed after that to form a complete dependency tree."
    },
    "2102.00713v1-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT. BERT is a pre-trained contextual language model that can be fine-tuned for various natural language processing (NLP) tasks. The architecture of BERT in pre-training is shown, where the model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the convolutional layers and the fully connected layers. The output layer is shown as well, which represents the final input representation. The diagram provides a detailed view of the different components of the BERT model, including the convolutional layers, fully connected layers, and the output layer. It also includes information about the input and output layers, as well as the mask sequence and the original sequence.",
        "reference": "Figure 3: The architecture details of the proposed multi-task CNN. Here n denotes the number of output feature maps."
    },
    "1505637-Figure4-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is trained on a target network, and the output of this network is fed into a perspective projection. This projection is then used to update the target network's parameters. The process is repeated until the desired level of accuracy is reached.\n\nThe model also includes a target kernel, which is used to store the weights of the model. These weights are updated during the training process by adjusting them based on the output of the perspective projection. The model also includes a receptive field, which is used to control the size of the input to the model. The receptive field is adjusted during the training process to ensure that the model is able to learn from the input data effectively.\n\nOverall, the diagram shows a machine learning model that is designed to be able to learn from a large amount of data. The use of a perspective projection and a target network allows the model to be able to make accurate predictions about new data.",
        "reference": "Figure 4: Method to select the kernel height kh. We project the receptive field of the target kernel to equirectangular projection Ie and increase kh until it is taller than the target kernel in Ie. The kernel width kw is determined using the same procedure after kh is set. We restrict the kernel size kw \u00d7 kh by an upper bound Uk."
    },
    "2011.03197v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that shows the execution traces and temporal relations of a program. The flowchart has three main sections: the use case, direct call relations, and indirect call patterns. The use case section is shown in the top left corner, with the \"use case 1\" label. The direct call relations are depicted in the center of the flowchart, while the indirect call patterns are located on the right side.\n\nThe flowchart also includes several boxes, each containing text and arrows. These boxes represent different parts of the program's execution, such as the input layer, output layer, stack of encoders, and final input representation. The arrows between these boxes indicate the flow of information and the sequence of operations within the program.",
        "reference": "Figure 2: Reliability block diagram of series-parallel system."
    },
    "2011.03307v2-Figure11-1.png": {
        "prediction": "The diagram depicts the process of pretraining BERT, a language model. The architecture of BERT in pretraining is shown. In this example sequence, the tokens \u2018speak\u2019 and \u2018nervous\u2019 are masked. The model's task is to predict these tokens from the representation vectors it learns at the positions of the input embeddings. The figure also shows the X-ray Corona, Black Hole, Accretion Disk, and the final input representation.",
        "reference": "Fig. 11. Illustration of the considered scenario. Above a spinning black hole, X-rays are emitted isotropically. Because of the compact corona very close to the black hole, the majority of the photons either hit the accretion disc or fall into the black hole. From the high- to the low-flux state, a partial coverer is obscuring and increasing part of the emitted X-ray radiation."
    },
    "2010.09125v2-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a system for training a neural network. The system is designed to learn from a large number of images, and the architecture of the neural network is shown in the lower part of the diagram. The input layer of the network consists of a stack of encoders, which are used to extract features from the input images. These features are then passed through a series of layers that perform various operations on them, such as convolution, pooling, and activation functions. The final layer of the network is a representation layer, which produces a set of embeddings for each image. These embeddings are then used to train the network using an inverse graphics network.",
        "reference": "Figure 1: We employ two \u201crenderers\u201d: a GAN (StyleGAN in our work), and a differentiable graphics renderer (DIB-R in our work). We exploit StyleGAN as a synthetic data generator, and we label this data extremely efficiently. This \u201cdataset\u201d is used to train an inverse graphics network that predicts 3D properties from images. We use this network to disentangle StyleGAN\u2019s latent code through a carefully designed mapping network."
    },
    "2102.06271v1-Figure5-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to predict the next word in a given sequence, such as a sentence or a paragraph. The model is pre-trained on a large corpus of text data, and it learns to predict the next word based on the context of the previous words. The pre-training process involves two main tasks: masked language modeling and next sentence prediction. In masked language modeling, the model is trained to predict the masked word in a given sentence. In next sentence prediction, the model is trained to predict whether two sentences are consecutive or not. The output layer of the model consists of a softmax function that takes the representation of the input embeddings and outputs a probability distribution over the possible next words. This probability distribution is then used to select the most likely next word in the given context.",
        "reference": "Figure 5. Schematic demonstrating calculation of NCI ."
    },
    "212029-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of an algorithm. The first part of the flowchart, on the left side, shows a series of steps in red boxes with black text. These steps are labeled \"Body Net\" and \"Head\". The second part of the flowchart, on the right side, shows a series of steps in blue boxes with white text. These steps are labeled \"Image Segmentation\", \"Confidence Segments\", and \"Batch Normalization\". The third part of the flowchart, at the bottom center, shows a series of steps in green boxes with gray text. These steps are labeled \"Semantic Feature of Different Scales\". The fourth part of the flowchart, at the top center, shows a series of steps in orange boxes with black text. These steps are labeled \"Use Case 1\" and \"Indirect Call Patterns\".",
        "reference": "Figure 2. An overview of the proposed FastMask architecture."
    },
    "174869-Figure2-1.png": {
        "prediction": "The diagram shows a complex network with several nodes and connections. The nodes are labeled with different types of weights, such as \"Importance Evaluation\", \"Prune lower layer weights\", and \"Modify upper layer weights\". There are also nodes labeled with \"Mask\" and \"Speak\". The connections between the nodes represent the flow of information in the system. This diagram is likely related to a machine learning or artificial intelligence model, where the nodes represent different components of the model, and the connections represent the interactions between them.",
        "reference": "Figure 2. Illustration of \u201cSparse Shrink\u201d algorithm. We evaluate the importance factor of each channel of feature maps f `, and prune the least important channels (dashed box). The pruning operation involves removing corresponding channels in W ` (dashed line), and modifying convolutional kernel W ` (blue line)."
    },
    "2103.05900v1-Figure5-1.png": {
        "prediction": "The diagram shows a directed graph and its topology. The graph is composed of nodes and arrows, with the nodes representing different elements or components. The arrows indicate the direction of the relationship between these elements. The graph is divided into two main sections: the upper section and the lower section. In the upper section, there are 10 nodes, while in the lower section, there are 20 nodes. The arrows connect the nodes, creating a network of relationships between them. This type of graph is often used to represent complex systems or processes, as it allows for the visualization of the relationships and interactions between different elements.",
        "reference": "Fig. 5: Overview of the DPN model for diagram classification."
    },
    "2107.03491v1-Figure5-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is designed to learn from a large corpus of text data, and it consists of several key components.\n\n1. **Input**: The input to the model is a sequence of words (tokens) that are represented as embeddings.\n2. **Masked Language Modeling (MLM)**: The model randomly masks some of the input tokens and tries to predict their original values. This task helps the model learn the context in which each word appears.\n3. **Next Sentence Prediction (NSP)**: The model is trained to predict whether two given sentences follow each other in the original text or not. This task helps the model understand the relationships between sentences.\n4. **Stack of Encoders**: The model has multiple layers of encoders, which process the input embeddings and generate contextualized representations for each token.\n5. **Semi-coherent R-M Search**: The model uses a semi-coherent R-M search algorithm to find the most coherent representation of the input. This step helps the model generate more meaningful and contextually appropriate responses.\n6. **Coherent Rotation**: The model performs coherent rotations on the R-M search results to improve the coherence of the generated text.\n7. **Output Layer**: The final output layer generates the text response based on the contextualized embeddings produced by the model.\n\nThis model is designed to learn the context in which words appear, understand the relationships between sentences, and generate coherent and meaningful text responses.",
        "reference": "Figure 5. A schematic of the semi-coherent RM search routine that offsets the deleterious effects of intra-channel depolarization by coherently rotating to discrete trial RMs and applying RM-synthesis. The separation between trial values, \u03b4RM \u223c 700 rad m\u22122, ensures that no more than 10% depolarization can occur across the semi-coherent search range. This amounts to several thousand coherent de-rotation operations to search out to RM amplitudes as large as 106 radm\u22122."
    },
    "2011.09318v2-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of cryptocurrency transaction network analysis. The process starts with a network modeling step, which involves creating a model of the network. This is followed by a network profiling step, where the properties of the network are analyzed. The next step is a network-based detection step, where transactions are detected in the network. The process then moves on to a transaction recognition step, where the transactions are recognized. Finally, the process ends with a transaction tracing step, where the transactions are traced.",
        "reference": "Fig. 1: The outline of cryptocurrency transaction network analysis methods."
    },
    "1294630-Figure2-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes the correct and incorrect link, the softmax function, and the output layer. The diagram is designed to be helpful, honest, and harmless.",
        "reference": "Figure 2: Architecture of our neural EL system. The input to the system are: a document D containing the query mention m and the corresponding Wikipedia candidate link li \u2208 L, where L is the set of all possible links extracted from the fast match step described in Section ."
    },
    "2107.03234v5-Figure5-1.png": {
        "prediction": "The diagram depicts a complex system with multiple components, including platforms, stations, and tracks. The platforms are numbered 1, 2, and 3, while the tracks are numbered 1 and 2. There are also two stations, S1 and S2. The arrows in the diagram indicate the flow of information or resources between these components. The diagram is quite detailed, with many different elements and connections, which may make it difficult to understand at first glance. It would be helpful to have a key or legend to explain the symbols used in the diagram.",
        "reference": "Figure 5: The demonstrative model."
    },
    "2011.06150v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the use case of a system. The flowchart has three main sections: Schedule S, Schedule S', and Schedule S\". \n\nSchedule S contains a series of steps, including m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11, m12, m13, m14, m15, m16, m17, m18, m19, m20, m21, m22, m23, m24, m25, m26, m27, m28, m29, m30, m31, m32, m33, m34, m35, m36, m37, m38, m39, m40, m41, m42, m43, m44, m45, m46, m47, m48, m49, m50, m51, m52, m53, m54, m55, m56, m57, m58, m59, m60, m61, m62, m63, m64, m65, m66, m67, m68, m69, m70, m71, m72, m73, m74, m75, m76, m77, m78, m79, m80, m81, m82, m83, m84, m85, m86, m87, m88, m89, m90, m91, m92, m93, m94, m95, m96, m97, m98, m99, m100, m101, m102, m103, m104, m105, m106, m107, m108, m109, m110, m111, m112, m113, m114, m115, m116, m117, m118, m119, m120, m121, m122, m123, m124, m125, m126, m127, m128, m129, m130, m131, m132, m133, m134, m135, m136, m137, m138, m139, m140, m141, m142, m143, m144, m145, m146, m147, m148, m149, m150, m151, m152,",
        "reference": "Figure 1 An illustration of an application of Algorithm 1. Let the set of cliques be given by"
    },
    "2207.10397v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a programming problem. The problem involves a pre-trained language model, which is used to generate text and then tested in two different test cases. The best code solution is selected based on the results of these test cases. The flowchart also includes instructions for the use case, direct call relations, indirect call patterns, and the output layer. It seems to be a complex problem that requires careful consideration of the generated text and the results of the test cases.",
        "reference": "Figure 1: The illustration of CODET. Both the code solutions and the test cases are generated by the pre-trained language model. The best code solution is then selected by a dual execution agreement."
    },
    "2110.08422v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a system, with several key components. The system is designed to provide content to the consumer and monitor the integrity of the content. The censored region represents the restricted access area, where certain content may be blocked or filtered. The free world represents the unrestricted part of the system.\n\nThe system also includes a censor, which is responsible for monitoring the content and blocking any inappropriate or harmful material. The integrity attacks are represented by the arrows pointing towards the censored region, indicating that the system is designed to protect against such attacks.\n\nThe UWeb represents the user interface, which is the point of interaction between the user and the system. The Satoshi blockchain is an integral part of the system, providing a secure and decentralized platform for the storage and distribution of content.\n\nOverall, the system is designed to provide a secure and efficient way to deliver content to the consumer while maintaining the integrity of the data.",
        "reference": "Fig. 1. System and adversary model. A client in the censored region cannot directly access sensitive services and news. Instead, content publishers embed content on commercial blockchains; clients access data with privacy and anonymity through standard Satoshi clients. The adversary can censor select client communications, monitor communications and perform integrity attacks."
    },
    "2011.01713v2-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses a neural network for language processing. The system is designed to process text, and it has an input layer, several hidden layers, and an output layer. The input layer receives the text as input, and the output layer produces the processed text. The hidden layers are where the neural network processes the text.\n\nThe system also includes a weight buffer, which stores the weights used by the neural network during processing. The weight buffer is connected to the input layer, allowing the weights to be loaded into the system when it is started up. The system also has a weight memory, which stores the weights used by the neural network during processing. The weight memory is connected to the output layer, allowing the weights to be saved after the system has processed the text.\n\nThe system also has a central control logic, which controls the flow of data through the system. The central control logic is connected to the input layer, the weight buffer, the weight memory, and the output layer, allowing it to control the flow of data through the system.\n\nOverall, the system is designed to process text using a neural network, with the input layer receiving the text, the hidden layers processing the text, and the output layer producing the processed text. The system also includes a weight buffer and a weight memory, which store the weights used by the neural network during processing.",
        "reference": "Fig. 2. Data-path schematic view of the accelerator core and its embedding into an SoC-level system. The diagram shows the unrolled compute architecture and encoding/decoding blocks, as well as the weight and feature map memories and tile buffer module. The dataflow of the accelerator is scheduled to first buffer full feature map windows in the tilebuffer and then compute the convolution result with pre-loaded weights in the compute units after which they are saved back to the feature map memory."
    },
    "2010.15032v3-Figure1-1.png": {
        "prediction": "The diagram shows a system for generating and removing instances. The system consists of an Invocation Controller, a Function Instance, and a Scale Controller. The Invocation Controller receives invocations and distributes them to the Function Instance. The Function Instance then generates or removes instances based on the input it receives. The Scale Controller controls the size of the Function Instance. The diagram also includes a section on Invocation Sources, which shows how the system can be used to create and remove instances. Overall, the system is designed to be efficient and effective in managing instances.",
        "reference": "Figure 1: Abstract FaaS architecture."
    },
    "2010.09258v1-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a data processing system. The data is input into the system, and then it is processed by the data manager. The data is then passed through a data partition strategy, which separates the data into two groups: direct call relations and indirect call relations. The data is then passed through a data allocation process, which distributes the data over the vocabulary. The data is then passed through a model partition process, which separates the data into sub-models. The data is then passed through a model allocation process, which updates the model return. Finally, the data is output from the system.",
        "reference": "FIGURE 3 The architecture and data flow of distributed machine learning"
    },
    "1090603-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that involves several components and processes. The system begins with a Substitute Training Dataset Collection, which is followed by Substitute DNN F Labeling. This process is then followed by Substitute DNN F Training, which leads to the final output representation. The architecture selection process is also shown in the diagram, which includes the Substitute DNN and the Oracle DNN O. The diagram also shows the use case of the system, which is to use the case 1 and case 2 for indirect call patterns. The system is designed to be helpful, honest, and harmless. It is capable of understanding and communicating fluently in English and Chinese. The system is also capable of comprehending and articulating responses effectively based on the provided image.",
        "reference": "Figure 3: Training of the substitute DNN F : the attacker (1) collects an initial substitute training set S0 and (2) selects an architecture F . Using oracle O\u0303, the attacker (3) labels S0 and (4) trains substitute F . After (5) Jacobian-based dataset augmentation, steps (3) through (5) are repeated for several substitute epochs \u03c1."
    },
    "2210.08554v1-Figure8-1.png": {
        "prediction": "The image is a flowchart that shows the process of pretraining BERT. The flowchart has several nodes, including \"Candidate Entities\" and \"Subway Scene-Test\". The arrows in the flowchart show the direction of the process. The flowchart also contains images of Lionel Messi, Adam Young, and Taj Mahal. It seems to be related to language processing or machine learning.",
        "reference": "Figure 8: Overview of Image Wikification (visual entity linking) method in KRAMT. To recognize named visual entities in images, we use available methods such as CRAFT+CRNN, VGG-Face, and Landmark ArcFace for brands, celebrities, and landmarks respectively. Using these experts, we measure similarity against several thousands of reference entities to obtain a set of high ranking candidates. This open-set recognition approaches allow for addition or removal of any number of reference entities without a need to re-train."
    },
    "2107.06007v2-Figure13-1.png": {
        "prediction": "The diagram depicts a system with several components, including a ballast bottle, inner cryostat, lead wall, isolation valves, and a standpipe. The system is designed to store and transport liquids and gases. The ballast bottle is used to control the pressure within the system, while the inner cryostat is responsible for maintaining a low temperature environment. The lead wall provides structural support and protection, while the isolation valves are used to control the flow of liquids and gases. The standpipe serves as a conduit for the liquids and gases to flow through. Overall, this system appears to be a complex setup for handling liquids and gases, likely used in scientific or industrial applications.",
        "reference": "Figure 13. A simplified schematic of the HFE system. The major system components are the storage dewar, ballast bottle, and inner cryostat vessel (IV)."
    },
    "2107.05307v2-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is pre-trained using the BERT architecture. The input to the model consists of segment pairs, and the task is to predict the masked tokens. The model's architecture includes an FNN (Feedforward Neural Network) and a residual connection. The output layer represents the final input representation. The model is trained using a stack of encoders, which are used to extract features from the input. The model is trained using a combination of word embeddings, positional embeddings, and segment embeddings. The pre-training process involves masking some of the input tokens and predicting their values. The model is then fine-tuned for specific tasks such as question answering and sentiment analysis.",
        "reference": "Fig. 1. Overview of EGVSR network."
    },
    "2103.07052v1-Figure4-1.png": {
        "prediction": "The image depicts a flowchart with two main sections labeled \"At Known Doc. Token W\" and \"At Unknown Doc. Token W\". The flowchart shows the process of pretraining BERT, a language model used for natural language processing tasks.\n\nIn the \"At Known Doc. Token W\" section, the input sequence is first tokenized into word embeddings. These embeddings are then passed through a feedforward neural network (FNN) to obtain a representation of the input sequence. This representation is then passed through an average pooling layer, which outputs a fixed-length vector representation of the entire input sequence.\n\nIn the \"At Unknown Doc. Token W\" section, the input sequence undergoes a similar process. However, instead of using a fixed-length vector representation, this section uses a masking technique where some tokens are replaced with a special token called [MASK]. The masked tokens are then replaced with random tokens during training. The output of this section is also a fixed-length vector representation of the input sequence.\n\nOverall, the flowchart illustrates the architecture of BERT in pretraining, which involves tokenizing the input sequence, passing it through an FNN, and using an average pooling layer to obtain a fixed-length vector representation. In the case of unknown documents, a masking technique is used to train the model on the masked tokens.",
        "reference": "Figure 4: Network architecture of the DV-Projection method. Vectors \ud835\udc38\ud835\udc40\ud835\udc35, \ud835\udc3f\ud835\udc40 and \ud835\udc37\ud835\udc49 are represented using rounded rectangle shape. Fully connected layers are represented using trapezoid shape. Element-wise math operations are represented using circles."
    },
    "2107.03564v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that shows the process of executing traces and temporal relations. The flowchart has different sections, including \"use case u1\", \"direct call relations\", \"indirect call patterns (C1, C2)\", and \"direct call patterns (C1, C2)\". It also includes various elements such as \"Proxy embedding set\", \"Softmax with temperature\", \"Projection\", \"Item embedding set\", \"Target item embedding\", \"Short-term interest encoder\", and \"Self-attention network\". The flowchart is designed to help understand the process of executing traces and temporal relations in a system.",
        "reference": "Figure 2: The overall architecture of ProxySR."
    },
    "2110.03090v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of pretraining BERT. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The architecture consists of a stack of encoders, followed by a softmax layer and an input layer. The ResNet block is used for the encoder. The diagram also shows the sequence of operations, including masking, resnet, and softmax. The final output is represented by the positional embeddings.",
        "reference": "Fig. 3: Network architecture for the player identification model. The networks accepts a player tracklet as input. Each tracklet image is passed through a ResNet18 to obtain time ordered features F . The features F are input into three 1D convolutional blocks, each consisting of a 1D convolutional layer, batch normalization, and ReLU activation. In this figure, k and s are the kernel size and stride of convolution operation. The activations obtained from the convolutions blocks are mean-pooled and passed through a fully connected layer and a softmax layer to output the probability distribution of jersey number pjn."
    },
    "2204.04370v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the process of executing traces and temporal relations. The flowchart consists of three main sections: Preprocessing, Quantum Circuit, and Beat Construction.\n\n1. Preprocessing: In this section, the input audio is filtered through a Filter Bank. The Filter Bank then passes the audio to the Feature Extraction module, which extracts features from the audio signal. These extracted features are then passed on to the Encoder module, which encodes the features into a format suitable for processing in the quantum circuit.\n2. Quantum Circuit: This section represents the quantum circuit, which is used to perform computations on the encoded features. The encoded features are passed to the Execute Quantum Circuit module, which executes the quantum circuit. The output of the Execute Quantum Circuit module is the result of the computation performed by the quantum circuit.\n3. Beat Construction: In this section, the results of the quantum circuit are passed to the Decode module, which decodes the results into a format suitable for processing in the beat construction. The decoded results are then passed to the Beat Construction/ Music Gen App module, which constructs the beat based on the decoded results. The final output of the Beat Construction/ Music Gen App module is the generated beat.\n\nOverall, the flowchart illustrates the process of executing traces and temporal relations in a system that involves audio processing, quantum computing, and music generation.",
        "reference": "Figure 1: QuiKo Architecture"
    },
    "2103.01209v3-Figure3-1.png": {
        "prediction": "The diagram shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Figure 3. Model Overview. Left: The GANformer layer is composed of a bipartite attention operation to propagate information from the latents to the image grid, followed by convolution and upsampling. These are stacked multiple times starting from a 4\u00d74 grid and up to producing a final high-resolution image. Right: The latents and image features attend to each other to capture the scene structure. The GANformer\u2019s compositional latent space contrasts with the StyleGAN monolithic one (where a single latent modulates the whole scene uniformly)."
    },
    "2102.05956v1-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a program. The program begins with the original network, which is followed by softmax. This process is repeated three more times. After the final input representation, there are two options: either to run the mask or the sequence embeddings. If the mask is chosen, the program will run the original network and softmax again. If the sequence embeddings are chosen, the program will run the original network, softmax, and then the final input representation.",
        "reference": "Fig. 3: Monte Carlo dropout performed in fully-connected and convolutional neural networks. Keeping dropout during inference creates an implicit ensemble of models. As shown it requires running the same network with different dropout masks in order to provide uncertainty estimations."
    },
    "2205.10889v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to learn from a large amount of data and can be used for various tasks, such as text classification, question answering, and language translation. The architecture of the model includes an input layer, a stack of encoders, a transformer-based encoder-decoder, and a final output layer. The encoders are responsible for processing the input data and extracting features. The transformer-based encoder-decoder is used to generate a sequence of words that represent the input. The final output layer produces the predicted output based on the input. The model is pre-trained using a technique called masked language modeling, which involves randomly masking some of the input words and training the model to predict them. This helps the model learn the context of the sentences and improve its performance on various tasks.",
        "reference": "Fig. 1: Overview of the proposed many-core wireless-enabled IMC platform. Orange encoders map to our wireless TX, while green IMCs map to our wireless-augmented IMCs. Bit-wise majority operation maps to the wireless OTA computation."
    },
    "2202.10590v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a system, with the use case \"use case u1\" at the top. The flowchart includes several nodes and arrows, representing different parts of the system. The nodes are labeled with numbers and letters, such as \"1\", \"2\", \"C1\", \"C2\", and \"C3\". The arrows connect the nodes, indicating the flow of data or information through the system. The diagram also includes labels for the root server, edge caching, routing, and network. Overall, the flowchart provides a visual representation of the system's architecture and how it functions.",
        "reference": "Fig. 1. System Model. A network of J caches serves file requests from a set I of user locations. Unserved requests are routed to the Root Server. Caching decisions are aided via the recommendations provided by the rec-sys."
    },
    "17589422-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into two main sections, \"use case u1\" and \"indirect call patterns (C1, C2)\". The \"use case u1\" section has three sub-sections: \"direct call relations\", \"indirect call relations\", and \"direct call patterns (C1, C2)\". Each of these sub-sections has its own set of nodes, which are connected by arrows. These arrows represent the flow of information or data between the different nodes in the flowchart. Overall, the flowchart provides a visual representation of the process or system being described.",
        "reference": "Figure 2: Description of a parallel document-level compositional vector model (DOC). The model recursively computes semantic representations for each sentence of a document and then for the document itself, treating the sentence vectors as inputs for a second CVM."
    },
    "2103.03875v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the input, trainable layers, and output. The arrows represent the direction of information flow through the network. The diagram provides a clear view of how the BERT model works during the pretraining phase.",
        "reference": "Figure 1. Transfer CNN model in sandwich-shape encoding, Ls and Le are tunable parameters to determine the boundary of trainable layers"
    },
    "2012.14111v1-Figure6-1.png": {
        "prediction": "The diagram depicts a system architecture for a web server. The system consists of multiple components, including a router, a proxy server, a squid as a mitm, and a c-cap server. The router is responsible for directing the data to the appropriate destination. The proxy server acts as an intermediary between the client and the web server. The squid, which functions as a mitm, is used to inspect and modify the data being transmitted between the client and the web server. Finally, the c-cap server is used to enforce access control policies.\n\nThe system also includes a device with an std Gateway, which is connected to a Mitmproxy CA Certificate. This device is used to establish a secure connection between the client and the web server by providing a certificate that verifies the identity of the web server. The certificate is signed by the Mitmproxy CA, which ensures that the web server is legitimate and not a malicious entity.\n\nOverall, the system architecture depicted in the diagram is designed to provide a secure and efficient way to access web content while enforcing access control policies.",
        "reference": "Figure 6: Logical Diagram showing how DLP solution is deployed within the organization"
    },
    "2204.05751v2-Figure2-1.png": {
        "prediction": "The image is a flowchart that depicts the execution traces and temporal relations. The flowchart has three main sections: the pretraining of BERT, the training of BERT, and the fine-tuning of BERT.\n\n1. Pretraining of BERT: In this section, the architecture of BERT in pretraining is shown. The model's task is to predict the masked tokens from the representation vectors it learns at the input embeddings positions.\n2. Training of BERT: This part shows the training process of BERT. It includes the Sup-Span and Sup-Span-fat sub-sections, which are used for the training of BERT.\n3. Fine-tuning of BERT: The final part of the flowchart represents the fine-tuning of BERT. It consists of the MAML-Span-fat and Sup-Span-fat sub-sections, which are used for the fine-tuning of BERT.\n\nOverall, the flowchart provides a detailed view of the BERT pretraining, training, and fine-tuning processes.",
        "reference": "Figure 2: Case study of span detection. Sup-Span: train a span detector in the fully supervised manner on available data from all training episodes, and then directly use it for span detection. Sup-Span-f.t.: further fine-tune the model learned by Sup-Span as in the proposed approach."
    },
    "2107.04367v2-Figure4-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into three main sections: the layout clip data, the spectral domain data, and the feature channels of the first convolution layer. The layout clip data is shown in the top left corner of the diagram, while the spectral domain data is located in the bottom right corner. The feature channels of the first convolution layer are displayed in the middle of the diagram. The arrows in the diagram indicate the direction of the data flow, with the blue arrows representing the data flow from the layout clip data to the spectral domain data, and the orange arrows indicating the data flow from the spectral domain data to the feature channels of the first convolution layer.",
        "reference": "Fig. 4: Procedure of the proposed feature selection."
    },
    "2107.04367v2-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into three main sections: use case 1, indirect call patterns (C1, C2), and direct call patterns (C1, C2). The first section, use case 1, has two sub-sections, trace 1 and trace 2. Trace 1 includes the following elements: P (B follows A), FNN+softmax, and FNN+softmax. Trace 2 consists of the following elements: P (B follows A), FNN+softmax, and FNN+softmax. The second section, indirect call patterns (C1, C2), includes the following elements: P (B follows A), FNN+softmax, and FNN+softmax. The third section, direct call patterns (C1, C2), contains the following elements: P (B follows A), FNN+softmax, and FNN+softmax.",
        "reference": "Fig. 3: Overview of the proposed LHD framework using heterogeneous federated learning with local adaptation."
    },
    "2107.04367v2-Figure5-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The input layer is shown on the left side, and the output layer is located on the right side. The layers in between are fully connected layers. The Convolution + RelU layer is shown in blue, and the Max-Pooling layer is shown in gray. The Hotspot and Non-Hotspot are shown as scattered points in the diagram. The Flatten Layer is shown in orange. The diagram also shows the Fully Connected Layers, which are the layers that connect all the nodes in the previous layer to every node in the next layer.",
        "reference": "Fig. 5: Neural network architecture example at the client."
    },
    "2010.06536v1-Figure5-1.png": {
        "prediction": "The diagram shows a flowchart of the process of creating 3D renderings. The first step is to use an annotation tool to mark important areas in a photograph or image. This is followed by a Facade Parsing and rectification step, where the image is analyzed to identify the different parts of a building's facade. Next, a 3D mesh generation step creates a digital model of the building. The model is then refined using an inverse procedural modeling technique. Finally, the 3D rendering is created using a 3D repository.",
        "reference": "Figure 5. System diagram for the 3D models module."
    },
    "2107.02220v2-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is designed to learn from a large amount of data, which is then used to make predictions on new data. The process starts with feature extraction, where the input data is transformed into a set of features that can be used by the model. The original features are then updated and re-identified in order to improve the accuracy of the model. The updated features are then used to train the model. During training, the model learns to recognize patterns in the data and makes predictions based on these patterns.\n\nAfter training, the model is tested on a set of validation data to evaluate its performance. If the model's performance is not satisfactory, it is re-trained using different hyperparameters or a different architecture. This process is repeated until the model achieves the desired level of accuracy.\n\nOnce the model is trained and tested, it is ready to be deployed for making predictions on new data. The final output of the model is a representation of the input data, which can be used for various applications such as image recognition, natural language processing, and more.",
        "reference": "Fig. 2: The pipeline of the proposed graph convolution based re-ranking (GCR) method."
    },
    "2103.10630v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to learn the representations of words and sentences from a large corpus of text. The architecture of the model includes an input layer, a stack of encoders, a projection layer, a propagation layer, and a final projection layer. The arrows in the diagram show the direction of data flow through the model. The model is trained on a large dataset using a pre-training method called BERT. BERT stands for Bidirectional Encoder Representations from Transformers. The pre-training process involves masking some of the words in a sentence and predicting their values based on the context. This helps the model to learn the relationships between words and their meanings. After pre-training, the model is fine-tuned on specific tasks such as question-answering, sentiment analysis, and natural language inference. Fine-tuning involves training the model on a smaller dataset that is specific to the task at hand. The output of the model is a set of embeddings that represent the meaning of the input text. These embeddings can be used to perform various natural language processing tasks such as text classification, named entity recognition, and text generation.",
        "reference": "Fig. 2. Illustration of the forward model used for the cryo-EM MBIR method. It involves a 3D projection at a fixed orientation (with appropriate offset for the center of rotation) followed by a propagation operator that depends on the contrast transfer function (CTF) of the system. The figure shows an example of the magnitude of the Fourier transform of a typical CTF, illustrating that the CTF typically zeros out several frequency components of the projection data. While this can pose challenges for typical pre-process and reconstruct approaches, we use this model in order to perform the reconstruction."
    },
    "2101.03561v3-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into three sections: Proposition 12, Proposition 15, and Proposition 3. \n\nProposition 12 includes the use case \"use case u1\" and the indirect call patterns \"C1, C2\". It also has the direct call relations and the output layer. \n\nProposition 15 includes the use case \"use case u1\" and the indirect call patterns \"C1, C2\". It also has the direct call relations and the output layer. \n\nProposition 3 includes the use case \"use case u1\" and the indirect call patterns \"C1, C2\". It also has the direct call relations and the output layer.\n\nOverall, the flowchart depicts the different components of a system, their interactions, and the flow of information between them.",
        "reference": "Figure 1. Main Lemmas and Theorems diagram."
    },
    "2210.01191v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that involves several components, such as BERT, LISTEN, MAC-X, and WID. The arrows in the flowchart represent the relationships between these components.\n\nBERT is a pre-trained language model that is used to perform various NLP tasks. It is connected to the LISTEN component, which is responsible for processing audio data. The audio data is then passed to the MAC-X component, which is likely a machine learning model designed to perform a specific task on the audio data. Finally, the output of MAC-X is processed by the WID component, which may be a database or a storage system for the results of the task performed by MAC-X.\n\nThe flowchart also shows that there are two types of relationships between the components: direct and indirect call relations. Direct call relations are represented by blue arrows, while indirect call relations are represented by red arrows. This indicates that the system is designed to handle different types of tasks, and the flow of data and information is managed through these relationships.",
        "reference": "Figure 2: Overview of the proposed end-to-end architecture, centered around the MAC-X Network: On the left, the question (Q), visual frames (V ), dialogue transcript (T ), acoustic input (Ac) as well as correct (A1) and incorrect (A2) answers are shown for the binary task. Their features are encoded with LSTMs, before use in MAC-X or in final classification along with last memory mp. Two identical classifiers make the predictions y1, y2 which are then used to calculate the loss in equation (7)."
    },
    "2101.08621v1-Figure3-1.png": {
        "prediction": "The diagram depicts a system for face detection, face alignment, and head pose estimation. The participant interacts with the system through a PC. The system uses an experimental setup to detect faces, align faces, and estimate the head pose. The output layer of the system is the head pose estimation. The system also includes a control server that activates or deactivates interventions based on attentive or not attentive conditions. Additionally, the system uses a GPU server to process video streams from the participant's PC.",
        "reference": "Figure 3: Architecture of the entire system we implemented for the second experiment."
    },
    "2205.10635v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The original neural network is used to learn the input embeddings, which are then passed through a feedforward neural network. The output layer represents the positional embeddings and word embeddings. The higher response time, higher accuracy, and lower accuracy are all part of the layer-wise splitting process. The semantic splitting process involves inputting the input sequence, followed by the output sequence. The input and output layers are also shown in the diagram.",
        "reference": "Figure 1: Overview of layer and semantic splitting strategies"
    },
    "2103.03189v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses a laser to create a transient signal. The transient signal is then sent through a photo diode, which converts the light into an electrical signal. This electrical signal is then sent to a computer (PC) through a DAQ (data acquisition) controller. The PC processes the data and sends it back to the system through a diaphragm, which controls the amount of light that enters the system. The system can be configured in different ways depending on the order of the components. The system can also be configured in different ways depending on the order of the components.",
        "reference": "Fig. 1. Schematic sketch of the experimental setup."
    },
    "2101.07327v1-Figure5-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case \"use case u1\" is depicted, along with the \"direct call relations\" and \"indirect call relations.\" The diagram also includes the \"direct call patterns\" and \"indirect call patterns.\" The architecture of BERT in pretraining is shown, with the model's task to predict the tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also features a system on a chip (SoC) architecture, which includes a host, main CPU, CPU, memory controller, and I/O interface. Additionally, the diagram displays a wireless communication system, including a wireless channel, a wireless device, a baseband processor, a modem, and a transport layer.",
        "reference": "Fig. 5. The baseline architecture in a modern UVR system"
    },
    "2101.07327v1-Figure7-1.png": {
        "prediction": "The image is a flowchart that depicts the architecture of BERT in pretraining. The diagram shows the host PC, kernel space, user space, and mobile user device. The host PC has a VR engine/graphics library, VR application, OpenVR host API, and OpenVR content capturing module. The mobile user device has an OpenVR MUD program, OpenVR MUD API, and OpenVR content presentation/user input module. The wireless network connects the host PC and the mobile user device. The diagram also includes a kernel space, user space, and OpenVR network module.",
        "reference": "Fig. 7. The OpenUVR system architecture"
    },
    "2110.08043v1-Figure14-1.png": {
        "prediction": "The image is a flowchart that depicts the architecture of BERT, a pre-trained contextual language model. The flowchart shows the pre-training process of BERT, which involves masking some tokens in the input sequence and predicting their values from the representation vectors. The diagram also includes mathematical equations and notations related to the BERT model's architecture.",
        "reference": "Figure 14: Mode I (left) and Mode I+II (right) for the study of the crack path under thermal expansion and the loading process. Here, the initial damage z\u2217(x) is illustrated by the red initial crack in the figures."
    },
    "1125974-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of an RNN cell, which is a type of recurrent neural network. The RNN cell has three main components: the input layer, the hidden layer, and the output layer.\n\n1. The input layer receives the input data, which is then passed on to the hidden layer.\n2. The hidden layer processes the input data and produces a hidden state, which is then passed on to the next RNN cell in the sequence.\n3. The output layer takes the hidden state as input and produces the output of the RNN cell.\n\nThe diagram also shows the flow of information through the RNN cell. The input data flows from the input layer to the hidden layer, and the hidden state flows from the hidden layer to the output layer. This process is repeated for each RNN cell in the sequence, with the output of one RNN cell serving as the input to the next RNN cell.",
        "reference": "Figure 2: Model architecture of RNN."
    },
    "812092-Figure7-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The input layer is connected to the softmax layer, which is then connected to the output layer. The arrows in the diagram represent the direction of data flow through the network. The model is designed to learn and predict the next word in a given sequence.",
        "reference": "Fig. 7. Illustration of the decoder model. \u201c<bos>\u201d is a tag denoting the beginning of a sentence, and \u201c<eos>\u201d is the end of a sentence."
    },
    "2101.11032v5-Figure1-1.png": {
        "prediction": "The image depicts a flowchart with text nodes and arrows. The flowchart is divided into two main sections: \"use case u1\" and \"indirect call patterns (C1, C2)\". The use case section contains three sub-sections: \"direct call relations\", \"indirect call relations\", and \"direct call patterns (C1, C2)\". The indirect call patterns section also has three sub-sections: \"Trace 1\", \"Trace 2\", and \"Output Layer\". The flowchart shows the relationships between these different sections and their corresponding sub-sections. It appears to be a diagram of a system or process that involves direct and indirect call patterns.",
        "reference": "FIG. 1. Top: A physicist with limited control of a gas in a box predicts and observes an irreversible (to her) increase of entropy in a spontaneous equilibration process. A cheeky demon with full knowledge of positions and momenta of all particles can reverse the process bringing the gas (and in principle her too) back to the original state. Bottom: Wigner\u2019s friend predicts and observes an irreversible (to her) collapse of the quantum state during a measurement process. A cheeky demon named Wigner with full knowledge of the global quantum state can reverse the process bringing system, measuring device and her back to the original state. In this note we highlight and discuss the analogies between these two setups and their significance for the debate surrounding the original Wigner\u2019s friend gedankenexperiment."
    },
    "2110.13367v1-Figure4-1.png": {
        "prediction": "The image is a diagram that shows the architecture of BERT in pretraining. The diagram has three main sections, each with its own set of components and functions. The first section, labeled \"VOLTA-BELMAP\", contains a graph that represents the architecture of BERT. The second section, labeled \"Training and Tuning\", displays a graph that shows the training and tuning process for the BERT model. The third section, labeled \"Detection and Result Output\", shows a graph that demonstrates how the detection and result output are achieved using the BERT model. Overall, the diagram provides a detailed view of the BERT model's architecture and its application in natural language processing tasks.",
        "reference": "Figure 4: Workflow of the detection (in the third step, red is the output of model and yellow cube is the final output of the method)"
    },
    "2011.06192v3-Figure5-1.png": {
        "prediction": "The diagram shows a flowchart of an AI system. The top part of the flowchart is labeled \"use case u1\" and has two sections: \"direct call relations\" and \"indirect call relations.\" The bottom part of the flowchart is labeled \"direct call patterns (C1, C2)\" and has two sections: \"direct call patterns (C1, C2)\" and \"indirect call patterns (C1, C2).\"\n\nThe flowchart also includes several nodes, which are represented by blue circles with text inside them. These nodes include \"use case u1,\" \"direct call relations,\" \"indirect call relations,\" \"C1,\" \"C2,\" and \"C4.\" The arrows in the flowchart connect these nodes, showing the relationships between them.\n\nOverall, the flowchart depicts a complex system with many interconnected parts. It appears to be designed for use in a real-world application, as it includes specific terms like \"direct call relations\" and \"indirect call patterns.\"",
        "reference": "Fig. 5. Overview of general IL and our bilateral control-based IL. In general, the delays caused during the demonstration and autonomous operation are different. Therefore, a general IL can realize only slow motion, which can ignore delays. In the bilateral control-based IL, the delays caused during the demonstration and autonomous operation are the same. Thus, in our bilateral control-based IL, fast motion with delays can be achieved."
    },
    "2107.07277v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the use of case 1 and 2 in indirect call relations. The flowchart has two main sections, with the left side showing the use of case 1 and the right side showing the use of case 2. The top section of the flowchart shows the direct call relations, which are represented by blue circles. The bottom section of the flowchart shows the indirect call patterns, which are represented by green circles. The flowchart also includes arrows and text nodes that help to explain the flow of information through the system. Overall, the flowchart provides a visual representation of how the system processes and routes information based on the use of case 1 and 2 in indirect call relations.",
        "reference": "Figure 1: Electric circuit representing the averaged model of a DC/DC buck converter connected to the microgrid."
    },
    "2011.08946v1-Figure8-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of using BERT, a pre-trained language model. The flowchart begins with the input network, which takes in text data. The data is then passed through a diffusion process, where the model learns to predict missing words or phrases in a given context. This is done by randomly masking some of the words and training the model to predict their original values.\n\nNext, the model uses a seed size function to determine the number of random seeds to be used in the diffusion process. These seeds are then used to generate a small seed group, which is a set of random values that will be used to perturb the input data during the diffusion process.\n\nAfter the seed group is generated, the model applies a scaling function to adjust the size of the perturbations. This helps to control the level of noise introduced into the data, ensuring that the model can still learn from it.\n\nThe perturbed data is then passed through a target ratio function, which determines the proportion of the input data that will be used for training. This helps to balance the amount of data used for training and testing, ensuring that the model does not overfit to the training data.\n\nFinally, the output layer of the model is used to generate a representation of the input data. This representation is then passed through a ranking users function, which ranks the users based on their performance on the task. The top-ranked users are then selected to provide feedback on the quality of the model's predictions.\n\nOverall, this flowchart provides a detailed overview of how BERT works, from the input network to the output layer. It also highlights the importance of various functions such as the seed size function, scaling function, and target ratio function in controlling the learning process.",
        "reference": "Figure 8: The framework illustration of Disparity Seeding."
    },
    "999120-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to learn from a large amount of data, and it is trained on two main tasks: pretraining and fine-tuning. In the pretraining stage, the model learns to predict masked words in a given text. It also learns to predict the next sentence in a given sequence. This is done by using a language modeling task. The model is trained on a large corpus of text, which includes a mix of English and Chinese texts. The training process involves several steps, including input embedding, transformer blocks, and output layer. The model architecture consists of an encoder-decoder structure with attention mechanisms. The pretraining stage helps the model to learn general language patterns and relationships between words. In the fine-tuning stage, the model is trained on a specific task, such as question-answering or natural language inference. The fine-tuning process involves adjusting the weights of the pretrained model to better fit the new task. The model is then evaluated on a test set to measure its performance. Overall, the diagram illustrates the process of training a machine learning model for natural language processing tasks.",
        "reference": "Figure 2. Overview of part loss networks (PL-Net), which is composed of a baseline network and a part loss computation extension. \u201cGAP\u201d denotes the Global Average Pooling. Given an input image, we firstly extract its feature maps X , then compute the global loss and person part loss based on X . The person part loss is computed on K parts generated with an unsupervised method."
    },
    "2012.05825v3-Figure1-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of real-time diagnosis prediction. The process begins with doctors labeling novel diseases, which are then added to the training set. The training set is used to train an expert-labeled training data model. The model is then used to make real-time diagnosis predictions. If the prediction is correct, the patient is given healthy flu. If the prediction is wrong, the process is repeated until the correct diagnosis is made. The flowchart also shows a legend indicating the known diseases and novel diseases.",
        "reference": "Figure 1. Novelty detection is challenging since X-rays of novel diseases are remarkably similar to known conditions. The unlabeled batch of inference-time data can be used to adapt a semi-supervised novelty detection approach to emerging novel diseases."
    },
    "200059-Figure4-1.png": {
        "prediction": "The diagram shows a flowchart with the title \"use case u1\" at the top. The flowchart is divided into three sections: Near-Frontal, Non-Frontal, and Output Layer. The Near-Frontal section has two sub-sections: \"Near-Frontal\" and \"X1\". The Non-Frontal section also has two sub-sections: \"Non-Frontal\" and \"X2\". The Output Layer section contains a single sub-section: \"Output Layer\". The arrows in the flowchart indicate the direction of data flow from one section to another.",
        "reference": "Figure 4. A genuine pair {x1,x2} that share the same identity but different pose is fed into the recognition network \u03b8r to obtain the rich embedding er1 and er2. By regularizing the self and cross reconstruction, er11 and er21, the identity and non-identity features are eventually disentangled to make the non-frontal peer ei2 to be similar to its near-frontal reference ei1."
    },
    "2207.10758v1-Figure7-1.png": {
        "prediction": "The image depicts a flowchart of a machine learning model, specifically BERT (Bidirectional Encoder Representations from Transformers). The diagram shows the architecture of BERT in pretraining. In this pretraining process, the input consists of two sentences, and the task is to predict the masked tokens. The model learns a representation of the input by predicting the masked tokens. The diagram also includes a section on the continuous world and the discrete world, which are part of the model's architecture. Additionally, there is a section on the 3D point on the plane, which is an important component of the model's structure. Overall, the image provides a detailed view of the BERT pretraining process and its architecture.",
        "reference": "Fig. 7: Overview of Theorem 1 (Example 13.2 from [30]), which links the planarity and projective transformations in the continuous world."
    },
    "2202.04333v1-Figure7-1.png": {
        "prediction": "The diagram depicts a high-level architecture of a real-time prediction system. The system consists of several components, including a real-time prediction server, a computation node, and an anchor broadcast tree. The real-time prediction server is responsible for processing the data and making predictions in real-time. The computation node is used to perform computations on the data before it is sent to the real-time prediction server. The anchor broadcast tree is a tree-like structure that broadcasts data from the root to all the leaves. The data flows through the system as follows: the user browsing tree sends the data to the real-time user behavior event, which then sends the data to the anchor demography dataset. The anchor demography dataset sends the data to the real-time anchor behavior event, which then sends the data to the search index. The search index sends the data to the real-time prediction server, which makes a prediction based on the data. The prediction is then sent back to the user through the anchor retrieved items.",
        "reference": "Figure 7: Online live broadcast recommender system with proposed TWINS model that partly shares similar idea with [20]. The new system lightens the TWINSmodel, and builds tree structures for user browsing histories and anchor histories in a offline fashion to save computation and latency costs for online serving."
    },
    "1358694-Figure1-1.png": {
        "prediction": "The diagram depicts a process in which an input image is transformed into a geometric predictor, which then warps the output image. The input and output images are both of a cat. The geometric predictor is a yellow block that takes the input image and transforms it into a new form. This transformation is represented by the \"warp\" function. The output image, which is also a picture of a cat, is then generated from the warped version of the input image.",
        "reference": "Figure 1: Network module of Spatial Transformers [7]. The blue arrows indicate information passing of appearance, and the purple one indicate that of geometry. The yellow 3D trapezoid denotes the geometric predictor, which contains the learnable parameters."
    },
    "2202.06670v2-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of pretraining BERT. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The architecture of BERT in pretraining is also shown, where the model is trained on a large corpus of unlabeled text.",
        "reference": "Figure 1: Left: Self-supervision. Self-supervised learning (SSL) uses self-supervision (the supervision from the data itself) for learning representations. An example of self-supervision is the augmented variant of the original data. Middle: Auxiliary Information. This paper aims to leverage auxiliary information of data for weakly-supervised representation learning. We consider data attributes (e.g., binary indicators of attributes) as auxiliary information. Right: Our Weakly-supervised Contrastive Learning Method. We first construct data clusters according to auxiliary information. We argue the formed clusters can provide valuable structural information of data for learning better representations. Second, we present a contrastive learning approach - the clustering InfoNCE (Cl-InfoNCE) objective to leverage the constructed clusters."
    },
    "2012.14294v1-Figure1-1.png": {
        "prediction": "The diagram shows a network of interconnected entities, with the Blockchain Network at the center. The Local Network and Local Network (Private Clinic) are connected to the Blockchain Network through the Hospital (LHSP). The Insurance Company is connected to the Local Network, while the Pharmacy and Doctor are connected to the Local Network (Private Clinic). The Ministry of Public Health and National Institute of Health are also connected to the Blockchain Network. The flow of information between these entities is represented by arrows in the diagram.",
        "reference": "Fig. 1. The proposed I-Health system architecture."
    },
    "2110.06875v2-Figure3-1.png": {
        "prediction": "The diagram shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Fig. 3. Illustration of the possible steps performed during the iteration by HM-Improve. The edges of the current sub-allocation Y are depicted using bold, green lines, while edges of the envy graph are shown by dashed, red lines. Source and sink vertices of Y are depicted with a white black diamond, respectively. Vertices of R as well as all edges incident to them are shown in grey."
    },
    "210911552-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of an AI assistant. The AI assistant is designed to understand the user's request, generate a response, and then output the result. The process starts with the user inputting a question or command into the system. The system then uses a representation module to convert the user's words into a format that can be understood by the system. The system then uses a moderation module to check if the user's input is appropriate and safe for the system to process. If the user's input passes this check, it is sent to a question generator which generates a response to the user's input. Finally, the system outputs the response to the user.",
        "reference": "Figure 3. Multi-Cue Bayesian Moderator Network. We first use a Bayesian CNN/LSTM to obtain the embeddings gi, gp, gc, gt and then fuse those using the Fusion Module to get \u00b5p, \u00b5c, \u00b5t. These embeddings are then passed to the Moderator network. These are then fed to the decoder to get the questions for each image."
    },
    "2205.03371v1-Figure5-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is designed to learn the representations of words and sentences from a large corpus of text. The architecture of the model includes an input layer, a stack of encoders, a softmax function, and a final output representation. The model is pre-trained on a masked language modeling task, where it learns to predict the masked words in a given sentence. This is done by randomly masking some of the words in the input sequence and training the model to predict the masked words based on the context provided by the surrounding words. The model is also trained on a next sentence prediction task, where it learns to predict whether two given sentences are consecutive or not. Overall, the model is designed to learn the relationships between words and sentences, which can be used for various natural language processing tasks such as question answering, sentiment analysis, and text classification.",
        "reference": "Fig. 5: Illustration on the instance representation and the generation of bag probability distribution."
    },
    "1189091-Figure3-1.png": {
        "prediction": "The diagram depicts a region detection network and a localization and captioning network. The region detection network consists of a region proposal network, proposals, and detection scores. The localization and captioning network includes bounding boxes, captions, and context features. The network is designed to detect objects in an image and provide their locations and descriptions.",
        "reference": "Figure 3: Our framework consists of two stages: a region detection network and a localization and captioning network."
    },
    "2101.04989v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that shows the execution traces and temporal relations of a program. The flowchart is divided into two sections, with the left section showing the use case and the right section showing the indirect call patterns. The flowchart also includes various nodes such as trace 1, trace 2, direct call relations, and indirect call relations. These nodes are interconnected by arrows, which represent the flow of execution. Additionally, the flowchart includes labels such as \"use case u1\" and \"indirect call patterns (C1, C2)\". Overall, the flowchart provides a visual representation of the program's execution process and the relationships between different parts of the code.",
        "reference": "Fig. 2. Steps in processing esophageal biopsy images to produce patches. (A) A typical image of a hemotoxylin and eosin (H&E)-stained esophageal biopsy section obtained from an individual with active EoE. The image was taken at 80X magnification. (B) The same image after background removal with an illustration of tissue coverage criteria per patch size to meet the threshold for inclusion in training or validation sets. Box 1 (red): patch of 224X224 pixels with less than 10% tissue coverage. Box 2 (yellow): patch of 224X224 pixels with greater than 10% tissue coverage. Box 3 (red): patch of 448X448 pixels with less than 10% tissue coverage. Box 4 (yellow): patch of 448X448 pixels with greater than 10% tissue coverage."
    },
    "2011.05643v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a neural network. The input stage is on the left, and the output layer is on the right. The input stage consists of an input stage, an input gain selection, and an input stage. The output layer has an output layer representation, a final input representation, and a positional embeddings word embeddings. The middle part of the diagram shows the architecture of BERT in pretraining. The model's task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Figure 2: Block level schematic diagram of an ASIC channel [8]."
    },
    "2012.06186v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that illustrates the architecture of BERT, a pre-trained contextual language model. The flowchart shows the input to the model, which includes a stack of encoders and a final input representation. The encoders are responsible for processing the input text, and the final input representation is then used to train the model. The model's task is to predict the masked tokens in the input sequence. The flowchart also shows the output layer, which represents the positional embeddings, word embeddings, and segment embeddings. Additionally, there is a section that describes the pre-training process of BERT, which involves masking some of the input tokens and training the model to predict them. This pre-training process allows the model to learn general language representations that can be fine-tuned for specific natural language processing tasks.",
        "reference": "Fig. 3: CNN architecture composed of the ResNet-20 followed by the NetVLAD layer. Numbers in each rectangle denote kernel size, number of output filters, padding, and size of stride, respectively. The N -way fully connected is dropped and instead the 1\u00d7 1\u00d7 64 output vector is passed to the NetVLAD layer."
    },
    "113704-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT (Bidirectional Encoder Representations from Transformers). The model is pre-trained on large-scale unlabeled text data. The architecture of BERT in pre-training is shown, where the input to the model consists of two sentences. The tokens \u2018speak\u2019 and \u2018nervous\u2019 are masked, and the task of the model is to predict these tokens from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. The model is trained with a combination of Convolution + Max Pooling, WEs Retrival + Projection, and Sentence s.",
        "reference": "Figure 2: DAN Architecture for Text Classification"
    },
    "2102.07774v2-Figure4-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT. The model is pre-trained on a large corpus of unlabeled text using two tasks: masked language modeling and next sentence prediction. In the masked language modeling task, some tokens in the input sequence are randomly selected and replaced with a special [MASK] token. The model's goal is to predict the original tokens from their [MASK] token representations. In the next sentence prediction task, the model is asked to predict whether a given pair of sentences follow each other or not.\n\nThe diagram also shows the architecture of the BERT model. It consists of an encoder that processes the input sequence and produces a set of contextualized word embeddings. These embeddings capture the meaning of words in the context of the entire input sequence. The output layer of the model is a feedforward neural network that takes the contextualized word embeddings as input and produces a fixed-length vector representation of the input sequence. This vector can be used for various natural language processing tasks such as question answering, sentiment analysis, and text classification.",
        "reference": "Figure 4. An illustration of the approach to the limit point Clim, where CA and CB are infinitesimally close to the limiting point."
    },
    "2206.01335v2-Figure1-1.png": {
        "prediction": "The diagram depicts a large-scale training process for a language model. The process begins with source code, documentation, and instant extraction. The extracted instances are then used to pre-train the language model. After pre-training, raw completions are generated from the pre-trained model. Finally, post-processing results in the code generation. The diagram also shows the use case of the trained language model, which is to generate code from a given prompt.",
        "reference": "Figure 1: Overview of a general framework for generating code analysis tools using few-shot, pre-trained language models."
    },
    "2210.01384v1-Figure3-1.png": {
        "prediction": "The diagram depicts a complex system with several components and their interactions. The main components of the system are:\n\n1. Search Algorithm: This component is responsible for searching for information in the system.\n2. Edge-Friendly Search Space: This component provides a search space that is optimized for fast searches.\n3. Edge-Efficient Candidate: This component helps to find the most efficient candidate for a given task.\n4. Proxy Multi-Task Training: This component trains multiple tasks simultaneously, which can improve performance.\n5. Edge Latency Estimator: This component estimates the latency of the system, which is important for real-time applications.\n6. Hardware-Aware Multi-Task Objective: This component optimizes the system's performance by considering the hardware constraints.\n7. Optimal Edge Architecture: This component optimizes the architecture of the system for edge devices.\n\nThe arrows in the diagram show the flow of information between these components. For example, the Search Algorithm sends a request to the Edge-Friendly Search Space, and the Edge-Efficient Candidate sends a sample to the Edge Latency Estimator. The arrows also indicate the direction of the data flow and the type of interaction between the components (e.g., training, hardware).\n\nOverall, this system seems to be designed for real-time applications that require fast and efficient processing on edge devices.",
        "reference": "Figure 3: A system-level overview of our proposed methods. We leverage multi-objective, hardware-aware neural architecture search to discover optimal neural components suitable for multi-task dense predictions, while simultaneously ensuring efficient edge inference."
    },
    "2010.06791v3-Figure4-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that uses a conventional decoder and an output processing block. The input to the system is first passed through a conventional decoder, which then sends the output to the output processing block. The output processing block further processes the output and sends it to the final output representation. The system also includes a degraded message path, which is used for error detection and correction. The overall architecture of the system is designed to handle both direct and indirect call patterns.",
        "reference": "Fig. 4. Comparison of the conventional decoder and the optimal GNNDR, for channels with imperfect receiver CSI. The left block diagram is part of the channel model in Figure 1. The upper right block diagram illustrates the conventional approach for realizing the decoder, where the imperfect receiver CSI is utilized to estimate the true channel state, and the estimated channel state is fed into the NNDR for decoding; the lower right block diagram illustrates the optimal GNNDR, which directly obtains the output processing function and the codeword scaling function, without estimating the channel state. We remark that channel estimation is still a critical module in practical wireless communication systems, and that the optimal GNNDR structure bypassing the channel estimation step is only valid for the specific information transmission system model in this paper."
    },
    "2101.03244v5-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart related to the use of case 1. The flowchart is divided into two main sections: \"Direct Call Relations\" and \"Indirect Call Patterns.\" The diagram also includes several sub-sections, such as \"use case 1,\" \"Trace 1,\" and \"Trace 2.\" The flowchart appears to be designed to illustrate the relationships between different elements in a system or process.",
        "reference": "Fig. 2. Proposed end-to-end framework for computing voxel-level detections of csPCa in validation/test samples of prostate bpMRI. The model center-crops two ROIs from the multi-channel concatenation of the patient\u2019s T2W, DWI and ADC scans for the input of its detection and classification 3D CNN sub-models (M1, M2). M1 leverages an anatomical prior P in its input x1 to synthesize spatial priori and generate a preliminary detection y1. M2 infers on a set of overlapping patches x2 and maps them to a set of probabilistic malignancy scores y2. Decision fusion node NDF aggregates y1, y2 to produce the model output yDF in the form of a post-processed csPCa detection map with high sensitivity and reduced false positives."
    },
    "2012.06168v4-Figure4-1.png": {
        "prediction": "The diagram shows a K-Best Self-Play procedure for model evaluation and generation. The state representation, action information representation, and card information representation are all processed by ConvNets. The ConvNets then pass the information to the FFCN, which is followed by the FCN. The value prediction and action probability are calculated and used to train the model. The policy loss, trinal-clip PPO, and value loss are also shown in the diagram. This process helps improve the performance of the model through training and evaluation.",
        "reference": "Fig. 4. End-to-end learning architecture of our deep RL based AI ARL."
    },
    "2204.10485v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that involves several components, including a feature extraction module, a feature fusion module, and a patch prediction module. The feature extraction module is responsible for extracting features from the input data, which are then fused in the feature fusion module to create a more comprehensive representation. This fused feature is then used as input to the patch prediction module, which predicts the patch locations within the image. The system also includes a distortion image, reference image, and offset, which are used to improve the accuracy of the predictions. Additionally, there is a prediction score and a ground truth score, which are used to evaluate the performance of the system. Overall, the system seems to be designed for image processing or computer vision tasks.",
        "reference": "Figure 2. Overview of AHIQ. The proposed model takes a pair of the reference image and distortion image as input and then obtains feature maps through ViT [11] and CNN, respectively. The feature maps of reference image from ViT are used as global information to obtain the offset map of the deformable convolution [8]. After the feature fusion module which fuses the feature maps, we use a patch-wise prediction module to predict a score for each image patch. The final output is the weighted sum of the scores."
    },
    "2102.08921v2-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of an evaluation pipeline and an auditing pipeline. The evaluation pipeline consists of a generative model, an evaluation embedding, and an evaluation precision. The auditing pipeline includes a sample-level metrics, a sample filter, a check precision, and a check authenticity. The diagram also illustrates the use case of the pipelines, which includes direct call relations, indirect call patterns, and direct call patterns. Additionally, there is a section for accepting or rejecting Xg based on Pn and E.",
        "reference": "Figure 2. Illustration for the evaluation and auditing pipelines."
    },
    "2010.11991v2-Figure8-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is designed to pretrain the BERT language model. The architecture of BERT in pretraining is shown, where the tokens \u2018speak\u2019 and \u2018nervous\u2019 are replaced by the \u2018[MASK]\u2019 token and the random token \u2018that\u2019 respectively. The model's task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. The model is trained using a pretraining data set, which consists of a sequence of sentences. The model is then fine-tuned on a downstream task, such as question-answering or sentiment analysis.",
        "reference": "Fig. 8. Schematic of the point cloud aggregation pipeline. The input data are filtered from redundant points, downsampled to reduce the computational complexity. At the same time, the current agent\u2019s position, and the position for the previous scan is taken from history. The PointCloudExtrapolator splits the entire scan into the smaller batches, and for every batch is calculates linearly interpolated transformation that is proportional to the when the point has been scanned. Finally, all the bathes are aggregated in the PointCloudAggregator"
    },
    "2204.05928v2-Figure3-1.png": {
        "prediction": "The diagram depicts a complex system with several components, such as \"frozen RoBERTa model\", \"domain\", \"sample\", \"probability distribution over domains\", \"probability distribution over intents\", and \"probability distribution over slots\". The flow of information in the system is represented by arrows connecting these components. The system seems to be designed for natural language processing tasks, as evidenced by the presence of the \"frozen RoBERTa model\" and the \"domain\" component. The \"sample\" and \"probability distribution\" components suggest that the system may be trained on a dataset and uses probability distributions to make predictions or decisions. Overall, the diagram provides a high-level overview of a machine learning system, likely used for natural language understanding or generation.",
        "reference": "Figure 3: Proposed action prediction in DDPT using a transformer decoder. In every decoding step, a token embedding for domain, intent or slot informs the model what needs to be predicted and the previous output is fed into the decoder. In case of domain prediction, we propose a domain gate that decides whether to choose a domain that the user currently talks about."
    },
    "2110.02775v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the execution traces and temporal relations. The flowchart has three main sections, each representing a different type of trace. The first section shows the use case, which is represented by a rectangle with rounded corners. The second section represents direct call relations, which are also shown in a rectangle with rounded corners. The third section displays indirect call patterns, which are depicted as rectangles with square corners. The flowchart also includes arrows that connect the different sections, indicating the flow of information or execution.",
        "reference": "Figure 1: An example of a network for the MONK-2 dataset. xi are the inputs, y is the output. The red and blue rectangles represent the plot of functions, with input range on the x-axis and output on the y-axis. The green rectangles contain the aggregation function. The numbers in bold represent the thresholds for the step functions."
    },
    "2010.15764v1-Figure4-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes the pretraining BERT, FNN, and the stack of encoders. The final output representation is shown on the right side of the diagram.",
        "reference": "Figure 4: The causal diagram for mixed-causal-anticausal domain adaptation."
    },
    "2202.09450v1-Figure38-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses case 1 and case 2 to determine the type of call. The system has two main cases, case 1 and case 2, which are represented by blue and green rectangles respectively. Each case has its own set of subcases, which are represented by smaller rectangles within the larger ones.\n\nCase 1 has three subcases: C1, C2, and C3. These are represented by blue rectangles within the blue rectangle representing case 1. Similarly, case 2 has four subcases: C4, C5, C6, and C7. These are represented by green rectangles within the green rectangle representing case 2.\n\nThe arrows in the diagram show the flow of information through the system. The arrows between the cases and their subcases indicate the flow of information from one case to another. The arrows between the subcases indicate the flow of information within each case.\n\nOverall, the diagram shows a system that uses case 1 and case 2 to determine the type of call, with each case having its own set of subcases.",
        "reference": "Fig. 38. Illustration of AttGAN extension for attribute style manipulation. Courtesy of [52]"
    },
    "2202.09450v1-Figure59-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is designed to learn the contextualized representations of words, phrases, and sentences from unlabeled text data. The architecture of the model includes an input frame, depth map, depth net, rigid flow, and final flow. The pretraining process involves masking some tokens in the example sequence and predicting them from the representation vectors. The consistency check ensures that the learned representations are consistent with the original input. The model is trained using a combination of supervised and unsupervised learning techniques.",
        "reference": "Fig. 59. The overview of GeoNet Framework. It consists of rigid structure reconstructor for estimating static scene geometry and non-rigid motion localizer for capturing dynamic objects. Courtesy of [89]"
    },
    "2202.09450v1-Figure31-1.png": {
        "prediction": "The diagram shows a system design for a face recognition system. The system takes source image and source texture as input, and outputs texture and image. It uses UV position map, color transfer, and pattern mask in the process. The system also uses reference image, reference texture, and pixelwise multiplication. The output of the system is an output texture and an output image.",
        "reference": "Fig. 31. The high-level architecture of . Courtesy of [41]"
    },
    "2202.09450v1-Figure32-1.png": {
        "prediction": "The diagram depicts a flowchart of the execution traces and temporal relations. The use case is represented by the top left corner, followed by the direct call relations and indirect call patterns. The diagram also shows the output layer, stack of encoders, and final input representation. The architecture of BERT in pretraining is also shown, with the masked tokens'speak' and 'nervous' being replaced by the '[MASK]' and 'that' tokens respectively. The model's task is to predict the original tokens from the learned representations at the positions of the input embeddings. The diagram also includes the cycle-consistency loss, which is used to train the model.",
        "reference": "Fig. 32. CycleGAN model contains two mapping functions G : X \u2192 Y and F : Y \u2192 X , and associated adversarial discriminators DY and DX . DY encourages G to translate X into outputs indistinguishable from domain Y , and vice versa for DX , F, and X. To further regularize the mappings, they introduced two \u201ccycle consistency losses\u201d that capture the intuition that if they translate from one domain to the other and back again we should arrive where they started. Courtesy of [47]"
    },
    "2202.09450v1-Figure21-1.png": {
        "prediction": "The image is a flowchart that depicts the execution traces and temporal relations. It shows the use case \"use case u1\" with two different call relations, \"direct call relations\" and \"indirect call relations\". The diagram also includes various elements such as \"Trace 1\", \"Trace 2\", \"Prob. Distribution over Vocabulary\", \"Prob. Distribution over Vocabulary\", \"Output Layer\", \"Stack of Encoders\", \"Final Input Representation\", \"Positional Embeddings\", \"Word Embeddings\", and \"Masked Sequence\". The flowchart seems to be related to machine learning or natural language processing tasks.",
        "reference": "Fig. 21. Overview of M3D-VTON architecture. Courtesy of [20]"
    },
    "2202.09450v1-Figure47-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The diagram is divided into three main sections: the upper part, the middle part, and the lower part. The upper part contains the use case, the direct call relations, and the indirect call patterns. The middle part shows the pretraining BERT architecture in pretraining. The lower part depicts the depth estimation network, the 3D gaze estimation, the head pose extractor, the eye feature extractor, the head position, the Fov attention module, the binary classification head, the hearing regression head, and the backbone. There are also some images and text nodes that provide more information about the different parts of the flowchart.",
        "reference": "Fig. 47. The Architecture of Dual Attention Guided Gaze Target Detection. Courtesy of [62]"
    },
    "2102.10313v1-Figure1-1.png": {
        "prediction": "The left image is a flowchart that shows the use case, direct call relations, and indirect call patterns. The right image appears to be a 3D rendering of a drone with a camera attached to it.",
        "reference": "Fig. 1. Left: Illustration of a typical planning run with all planners on the hilo scenario. Red is the proposed, green the DGEO, and brown the RRT*Con planner. Blue shades correspond to the RRT*-Sam, yellow shades to the RRT*-Pro, and pink to CHOMP. Right: Example of an OMAV inspecting a surface using the proposed planner."
    },
    "2207.10392v2-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of a system, with various components and their connections. The system is designed to process data and generate output based on the input. The flowchart has different sections, including an overview of the FDE, dynamic feature pre-up sampling, and dynamic feature pre-up sampling. The system consists of several encoders, decoders, and gates. The encoders are responsible for encoding the input data, while the decoders decode the output. The gates control the flow of information between the encoders and decoders. The system also includes a semi-shift convolutional layer, which is used to perform convolution operations on the data. The system is designed to be dynamic, meaning that it can adapt to changes in the input data. Overall, the system appears to be a complex machine learning model, designed to process large amounts of data and generate accurate output.",
        "reference": "Figure 3: Technical pipeline of FADE. From (a) the overview of FADE, feature upsampling is executed by jointly exploiting the encoder and decoder feature with two key modules. In (b) dynamic feature pre-upsampling, they are used to generate upsampling kernels using a semi-shift convolutional operator (Fig. 5). The kernels are then used to reassemble the decoder feature into pre-upsampled feature. In (c) gated feature refinement, the encoder and pre-upsampled features are modulated by a decoder-dependent gating mechanism to enhance detail delineation before generating the final upsampled feature."
    },
    "2011.07831v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a language model. The model is designed to predict the next word in a given sequence. The input to the model is a sequence of words, and the output is a probability distribution over the vocabulary for the next word. The model consists of an encoder, a transformer-based language model, and a decoder. The encoder takes the input sequence and produces a fixed-length context vector. The transformer-based language model takes the context vector as input and outputs a probability distribution over the vocabulary for the next word. The decoder takes the output of the language model and generates the final output representation. The architecture of the BERT model is also shown in the diagram.",
        "reference": "Figure 1: A simplified illustration of our proposed method where \u039b refers to the write mechanism described in section 3.1.1. Ft are the recurrent weights of the FWM which have been generated by the LSTM. The LSTM is a regular slow RNN. The residual connection between the FWM and the LSTM is not depicted."
    },
    "1262057-Figure5-1.png": {
        "prediction": "The diagram depicts a complex system with several components. The system is designed to recognize and predict human motion, using spatial and temporal cues. It consists of various elements such as a network, a matrix, a lobby, an office, a terminal, and a lobby. The system also includes a place recognition system, which uses a network, a matrix, and a lobby. Additionally, the system has a place labeling and probabilities for each action. The diagram also shows that the abnormal human behavior is detected.",
        "reference": "Figure 5. Overview of our approach for unbiased human action recognition on a sample of the Guiabot robot dataset."
    },
    "2107.00638v1-Figure5-1.png": {
        "prediction": "The diagram depicts a flowchart of execution traces and temporal relations. The flowchart is divided into three sections, with the top section showing the use case, the middle section showing direct call relations, and the bottom section showing indirect call patterns. The flowchart also includes various nodes such as Trace 1, Trace 2, and Trace t, as well as arrows that connect these nodes to represent the flow of information. The diagram provides a visual representation of the relationships between different elements in a system, allowing for better understanding of the system's operation.",
        "reference": "Figure 5: Charge transfer (CT) due to relative orbital positions in metal-organic interfaces. Panel a) shows the situation before contact. The Fermi-level of the metal lies above the LUMO of the adsorbate. Panel b) intermediate stage (purely for illustration, this does not exist in reality). Adsorbate and substrate are in contact, adsorbate orbitals broaden IP and EA are changed due to band gap renormalization, charge is transferred to enforce a common Fermi level. Panel c) Final situation. Charge transfer is finished, result is a common Fermi level and a dipole that shifts the adsorbate levels by \ud835\udee5\ud835\udef7. Reproduced with permission from [62]."
    },
    "2205.15979v1-Figure7-1.png": {
        "prediction": "The diagram depicts a machine learning model, specifically a transformer-based language model. The model is designed to perform two tasks: short-term planning (STPS) and long-term planning (LTPS). In the STPS task, the model is trained to predict the next word in a given sequence of words. This is achieved by masking some of the input tokens and training the model to predict them. The model is also trained to predict the next sentence in a given sequence of sentences.\n\nIn the LTPS task, the model is trained to generate text that follows a given prompt. It is trained to generate text that is coherent and relevant to the given prompt. The model is also trained to generate text that is grammatically correct and semantically meaningful.\n\nThe model consists of an encoder-decoder architecture with attention mechanisms. The encoder processes the input sequence and generates a set of contextualized embeddings for each token. These embeddings are then used by the decoder to generate the output sequence. The attention mechanism allows the model to focus on different parts of the input sequence when generating the output sequence.\n\nThe model is trained using a combination of supervised and unsupervised learning techniques. The supervised learning component involves training the model to predict the next word or sentence in a given sequence. The unsupervised learning component involves pre-training the model on large amounts of unlabeled text data to learn general language patterns.",
        "reference": "Figure 7: Overview of the short-term and long-term planning steps for the local trajectory planning."
    },
    "2206.00991v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings of the masked tokens. The diagram also includes an occupancy grid decoder, which is used to predict the occupancy of vehicles and pedestrians. There are several other components such as the convolutional layer, the transformer layer, and the predicted occupancy grid. The diagram also shows the predicted occupancy grid for vehicles and pedestrians.",
        "reference": "Fig. 3. An overview of the StopNet architecture. The encoder processes the input point set P and produces a feature map, which is used to predict both per-agent trajectories and whole-scene occupancy grids for each agent type. Input agent boxes at t = 0 are also rendered in BEV as binary features and fed to the trajectory and occupancy grid decoders."
    },
    "1356654-Figure2-1.png": {
        "prediction": "The image depicts a flowchart that shows the levels of supervision in a deep learning model. The original image is at the top, followed by the segmentation and the point-level. The full image level, object prior, and image-level are also shown. The diagram also includes a horse person and a full image.",
        "reference": "Fig. 2. (Top): Overview of our semantic segmentation training framework. (Bottom): Different levels of training supervision. For full supervision, the class of every pixel is provided. For image-level supervision, the class labels are known but their locations are not. We introduce point-level supervision, where each class is only associated with one or a few pixels, corresponding to humans pointing to objects of that class. We include an objectness prior in our training loss function to accurately infer the object extent."
    },
    "980236-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses a neural network to predict the next word in a given text. The system is designed to be able to predict the next word based on the context of the previous words. The system takes an input image, which is transformed into a sequence of features. These features are then passed through a loss network, which is used to learn the content and style targets. The output layer of the system is a feedforward neural network, which is trained using a style target. The system is able to predict the next word by using a content target, which is a vector that represents the next word in the text.",
        "reference": "Fig. 2. System overview. We train an image transformation network to transform input images into output images. We use a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process."
    },
    "204823930-Figure2-1.png": {
        "prediction": "The diagram depicts a complex system with several components. The evidence reasoning section is shown in the top left corner, and it contains an evidence selection, node kernel, and edge kernel. The evidence selection is further divided into two parts: the node kernel and the edge kernel. The node kernel is shown in the bottom left corner of the diagram, while the edge kernel is located in the lower right corner.\n\nIn the middle of the diagram, there is a claim label that connects the evidence selection to the node kernel. This label is a key element in the system, as it links the evidence to the node.\n\nThe bottom right corner of the diagram shows a claim label that connects the evidence selection to the edge kernel. This label is also important, as it links the evidence to the edge.\n\nOverall, this diagram represents a complex system with many interconnected components. It is not clear what the specific purpose of this system is, but it appears to be related to some form of evidence-based reasoning or decision-making process.",
        "reference": "Figure 2: KGAT Architecture."
    },
    "336802-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the PHOCNet architecture. The input image is processed by the Deep CNN, which is then passed to the PHOCNet. The PHOCNet consists of an estimated PHOC layer, followed by a deep CNN and a softmax layer. The output of the PHOCNet is used for pretraining BERT. The pretraining task includes masked language modeling and next sentence prediction. The diagram also shows the architecture of BERT in pretraining, where the input sequence is segmented into word embeddings and position embeddings. The masked language modeling task involves predicting the masked tokens from their representation vectors. The next sentence prediction task involves predicting whether the next sentence follows the current one or not.",
        "reference": "Fig. 1. Overview over the proposed PHOCNet approach for Query-byExample (QbE) and Query-by-String (QbS) word spotting."
    },
    "2206.10801v3-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to perform tasks like text classification, question answering, and language generation. The input layer consists of an encoding process that converts the input data into a format suitable for the model. The encoded data is then passed through a series of layers, including the encoder, decoder, and recurrent layers. These layers are responsible for processing the data and extracting features from it.\n\nThe output layer of the model is a representation of the input data. This representation is then used to perform various tasks such as text classification, question answering, and language generation. The model's architecture is designed to be efficient and effective in performing these tasks by utilizing techniques like categorical latency and probabilistic distribution over vocabulary.",
        "reference": "Fig. 1: Overview of the proposed system. D denotes the discriminator, G denotes the generator."
    },
    "2101.04223v2-Figure2-1.png": {
        "prediction": "The diagram shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the input embeddings positions. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Figure 2: Single and hierarchical echo-state network (ESN) architectures. A: A single ESN with internally connected nodes with a single set of hyper-parameters \u03b1 and \u03c1. B: A hierarchical ESN composed of 2 connected reservoirs where the input is fed into reservoir 1 only and the connection is unidirectional from R1 to R2, which is identical to the 2 layer DeepESN of [27]. C: A parallel (or unconnected hierarchical) ESN where the network is partitioned into 2 reservoirs, R1 and R2, which each receive the input and provide output but have distinct hyper-parameters."
    },
    "2101.11508v8-Figure2-1.png": {
        "prediction": "The image depicts a flowchart of a machine learning model, specifically BERT. BERT is a transformer-based pre-training model for natural language processing developed by Google. The flowchart shows the architecture of the BERT model in pre-training mode.\n\nThe diagram begins with an \"Image Input Layer\" that takes in the input data. This data is then passed through an \"Encoder Stage\" and a \"Decoder Stage\" to extract features from the input. These features are then passed through a \"Bridge\" layer, which is a special type of layer that connects the encoder and decoder stages. The output of the bridge layer is then passed through a \"Final Convolution Layer\" to generate the final representation of the input.\n\nThe diagram also includes information about the \"Segmentation Layer\" and the \"Softmax Layer\". The segmentation layer is used to segment the input into different parts or segments, while the softmax layer is used to convert the output of the final convolution layer into a probability distribution over the possible classes.\n\nOverall, the flowchart provides a detailed view of the BERT model's architecture, highlighting its key components and their functions.",
        "reference": "Figure 2. U-net architecture. Conv means convolution. ReLU is rectified linear unit. DepthConv is depth concatenation. UpConv means up-convolution or transposed convolution. MaxPool is Max Pooling."
    },
    "2210.09757v1-Figure4-1.png": {
        "prediction": "The diagram depicts a sequence of operations, represented by green and yellow arrows. The green arrows represent VLS observation t, while the yellow arrows represent VIO observation t. The sequence starts with VLS observation t1, followed by VIO observation t1. This pattern continues with VLS observation t2, VIO observation t2, and so on. The diagram also shows the use case u1, which is connected to the VLS observation t1 and VIO observation t1. Additionally, there are indirect call patterns (C1, C2) and direct call patterns (C1, C2) shown in the diagram.",
        "reference": "Fig. 4. Illustration of drift RANSAC. Drift RANSAC randomly samples local key frames and calculate errors of each set. Then we estimates T lo \u2032 between VIO and VLS coordinates of least error among T lo,k . Finally we calculate error and weight of each VLS edge."
    },
    "2102.03117v2-Figure2-1.png": {
        "prediction": "The image is a flowchart that shows the execution traces and temporal relations of a program. The flowchart has two main sections, one for the use case \"use case u1\" and another for the indirect call patterns. The use case section contains information about the direct call relations, indirect call relations, and the output layer. The indirect call patterns section also includes information about the direct call patterns. There are several nodes in the flowchart, including \"Trace 1\", \"Trace 2\", \"Prob. Distribution over Vocabulary\", and \"Prob. Distribution over Vocabulary\". These nodes are connected by arrows, which represent the flow of the program's execution. Overall, the flowchart provides a visual representation of the program's structure and the order in which it executes its various components.",
        "reference": "Figure 2 A more detailed proof diagram."
    },
    "2010.11078v2-Figure2-1.png": {
        "prediction": "The diagram shows a high-level task planner, which is used to control the behavior of a neural network. The task planner has two levels: a high-level task planner and a lower-level trajectory optimization. The high-level task planner is responsible for planning the overall movement of the system, while the lower-level trajectory optimization is responsible for optimizing the specific movements within each level.\n\nThe high-level task planner is designed to be flexible, allowing it to handle a wide range of tasks. It uses a set of rules to determine when to move, push, grasp, or throw objects. These rules are based on the current state of the environment and the goals that the system is trying to achieve.\n\nThe lower-level trajectory optimization is used to plan the specific movements required to achieve the goals set by the high-level task planner. It uses a set of constraints, such as velocity, dynamic constraints, and contact constraints, to ensure that the system moves smoothly and safely.\n\nOverall, this high-level task planner and lower-level trajectory optimization system can be used to control a wide range of robotic systems, from industrial robots to humanoid robots. By using these tools, developers can create intelligent systems that can perform complex tasks in a safe and efficient manner.",
        "reference": "Fig. 2. A conceptual illustration of the natural match between the discrete task planner and the low-level distributed trajectory optimization. In the move action without an object in hand, velocity and dynamics constraints are enforced. When the gripper is in the push action, the optimization naturally adds a contact constraint. Note that qgoal,move = qinit,push."
    },
    "2204.06433v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that depicts the use case of a system. The system consists of several components, including a simulator, simulation software, robot hand controller, and a movit and RVIZ. The flowchart also shows the relationships between these components, with arrows indicating the direction of data flow or control flow. The use case is related to the interaction of the system with objects, as indicated by the presence of an \"Objects\" component in the flowchart. Overall, the flowchart provides a visual representation of the system's architecture and its interactions with various objects.",
        "reference": "Fig. 3. Implementation Diagram"
    },
    "2204.12346v1-Figure2-1.png": {
        "prediction": "The diagram shows the process of training a language model, specifically BERT. The architecture of BERT is shown in pretraining. The task of the model is to predict the masked tokens from the representation vectors it learns at the input embeddings positions. The pretraining process involves masking some of the input tokens and predicting their values. This helps the model learn the context in which the masked tokens are used. The model is then fine-tuned for specific NLP tasks such as question-answering and sentiment analysis. The pretraining and fine-tuning process helps the model learn better representations of the input data, leading to improved performance on various NLP tasks.",
        "reference": "Figure 2. A schematic representation of the SIR and the SIRD model."
    },
    "2011.12745v4-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is pre-trained on large-scale unlabeled text data, and the pre-training process involves two tasks: masked language modeling and next sentence prediction. In the masked language modeling task, some words in the input sequence are randomly selected and replaced with a special token [MASK]. The model's objective is to predict the original word from its representation vector. In the next sentence prediction task, the model is trained to predict whether a given pair of sentences follow each other or not.\n\nAfter pre-training, the model is fine-tuned on a specific task, such as question-answering or sentiment analysis. During fine-tuning, the pre-trained model is used as a feature extractor, and the extracted features are passed through an Interpolation Matrix, which is then connected to a select top R matrix. The output of the select top R matrix is passed through an Interpolation Matrix, and the final output is generated by adding the generated dense point cloud. The model is trained using a coordinate descent method, and the training process is monitored by an Interpolation Matrix.",
        "reference": "Fig. 3. The flowchart of the proposed method. Given a sparse point cloud with N points, it first learns a U -dimensional feature for each point (i.e., the feature learning module) and also embeds the relative position information of K nearest neighbouring (KNN) points into the U -dimensional features (i.e., the distance encoder module). Then the two types of high-dimensional features are concatenated to regress unified and sorted interpolation weights (i.e., the weight learning module), which coarsely interpolate the input sparse point cloud into a dense one. The coarse point cloud is finally refined via a self-attention-based refinement module, which regresses an offset for each point from the corresponding feature (see Fig. 4 for the detailed network architecture). After one-time end-to-end training, the proposed method is capable of handling flexible upsampling factors not greater than the maximum factor Rmax."
    },
    "2210.06361v3-Figure2-1.png": {
        "prediction": "The diagram shows a multi-view architecture for a neural network. The architecture consists of an input layer, a feature pyramid encoder, a view combining layer, and a hierarchical channel fusion decoder. The feature pyramid encoder is designed to extract features from the input at multiple scales. The view combining layer combines the features from different views to create a single representation. Finally, the hierarchical channel fusion decoder fuses the features from the view combining layer to generate the final output. This architecture is used for tasks such as object detection, segmentation, and image classification.",
        "reference": "Figure 2: Overview of our model structure. We generate multiple views (Diagonally and Vertically flipped views, Close looking views) of the data by different transformation methods. The shared pyramid feature encoder is applied to extract hierarchical features of different scales corresponding to different view choices. The view combining layer concatenates features of same level from different views (fDi , f V i , f O i , f C1 i , f C2 i ) channel-wisely and output multi-view feature tensors (mvtensors). The model feeds mv-tensors into CAMVs and obtain multi-view enhanced feature tensor (en-tensor) fi. CAMV is adopted to fuse features and aggregate vital clues between different views by a two-stage co-attention mechanism. The entensors are further decoded and the contextual correlation are exploited by hierarchical channel fusion unit simultaneously. In the end, a probability map of camouflaged object in the input image is computed by several convolutional blocks."
    },
    "2011.09340v3-Figure9-1.png": {
        "prediction": "The image is a flowchart that shows the process of pretraining BERT. The architecture of BERT in pretraining is depicted, with a segment pair being used as an example sequence. The tokens \"speak\" and \"nervous\" are masked, and the model's task is to predict these tokens from the representation vectors it learns at the positions of the input embeddings. The next sentence prediction task is also shown. The diagram includes various elements such as input embeddings, positional embeddings, word embeddings, and segment embeddings. The final output of the model is represented as well.",
        "reference": "Figure 9: Process with an entanglement breaking map on at least one of its spaces. If the circuit of a process can be represented with an entanglement breaking (EB) channel on one of its wires, then the resulting comb \u03a5ABC is separable in the corresponding cut. For example, an entanglement breaking channel on the environment R implies that \u03a5ABC is separable in the splitting A : BC. If there are two entanglement breaking channels (independent of what two wires they act on), then the resulting comb is fully separable. For better tracking of the involved spaces, the input and output spaces of the EB channels are labelled differently."
    },
    "2011.03178v1-Figure1-1.png": {
        "prediction": "The image depicts a flowchart, which is a type of diagram that represents a sequence of events. The flowchart in the image is divided into two main sections: the selection model and the prediction model.\n\nThe selection model consists of three main components: queries, select, and join. The queries are used to retrieve data from a train dataset. The select component is responsible for selecting the appropriate data to be joined with other datasets. Finally, the join operation combines the selected data with other datasets.\n\nOn the other hand, the prediction model consists of four main components: transductive information, test dataset, report, and prediction. The transductive information is used to train a machine learning model. The test dataset is used to evaluate the performance of the trained model. The report component provides feedback on the model's performance. Finally, the prediction component makes predictions based on the trained model.\n\nOverall, the flowchart shows how data is processed through a selection model and then used to train a machine learning model, which is then evaluated using a test dataset before making predictions.",
        "reference": "Figure 1: A diagram for the pipeline of (transductive) active learning."
    },
    "2101.07079v1-Figure5-1.png": {
        "prediction": "The image is a flowchart that illustrates the use case of a system. The system has two main input cases, \"use case 1\" and \"use case 2\". These inputs are connected to several nodes, which represent different call relations and patterns. The call relations include \"direct call relations\" and \"indirect call relations\", while the call patterns consist of \"direct call patterns\" and \"indirect call patterns\". The flowchart also shows the output layer, where the final representation of the input is generated. Additionally, there is a stack of encoders, which are used to process the input data. The overall structure of the flowchart represents a complex system with multiple input cases, call relations, and patterns, as well as an output layer for generating the final representation.",
        "reference": "Figure 5: Illustration for the notations in the beginning of Section 5.1."
    },
    "1580300-Figure2.2-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of executing traces and temporal relations in a computer system. The flowchart is divided into two main sections: the Classification Network and the Dynamic Parameter Layer.\n\nIn the Classification Network, there are several nodes representing different types of parameters such as GRU, GRU, GRU, GRU, GRU, and GRU. These parameters are connected to each other with arrows, indicating the flow of information or data between them.\n\nThe Dynamic Parameter Layer also has several nodes, but they represent different types of parameters such as Hashing, Hashing, Hashing, Hashing, Hashing, and Hashing. These parameters are also connected to each other with arrows, showing the flow of information within this layer.\n\nOverall, the flowchart provides a visual representation of the complex interactions and relationships between various parameters in a computer system's execution process. It shows how these parameters work together to process and execute traces and temporal relations efficiently.",
        "reference": "Figure 2.2: DPPnet, the question is used to learn how to predict parameters for a dynamic parameter layer in the classification network."
    },
    "2207.14299v1-Figure2-1.png": {
        "prediction": "The diagram shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the bounding boxes, graph abstraction, encode graph abstraction, temporal alignment, and embedding video sequence. It is a complex system that involves multiple components and processes.",
        "reference": "Figure 2: Overview. We extract object bounding boxes from video sequences using an off-the-shelf detector, and construct a graph abstraction of the scene. We model graph-abstracted object interactions using Interaction Networks [5], and learn a reward function by aligning video embeddings temporally. We then train image-based RL policies using our learned reward function, and deploy on a real robot."
    },
    "2107.04225v2-Figure1-1.png": {
        "prediction": "The diagram depicts a system for processing data. The system consists of two networks: a student network and a teacher network. The student network is connected to labeled and unlabeled data, while the teacher network is connected to the student network. The teacher network also has connections to an expression module, an AU module, and a VA module.\n\nThe labeled and unlabeled data are processed by the student network, which then sends the output to the teacher network. The teacher network uses this output to generate expressions, AUs, and VAs. These generated expressions, AUs, and VAs are then used in a multi-task supervised loss function to train the system. This process helps improve the accuracy of the system over time.",
        "reference": "Fig. 1. Framework for multi-task affective behavior analysis model"
    },
    "2012.04925v1-Figure2-1.png": {
        "prediction": "The image is a diagram of a machine learning model, specifically BERT (Bidirectional Encoder Representations from Transformers). The diagram shows the architecture of BERT in pretraining. In this pretraining stage, the model is trained to predict masked words in a given text sequence. The diagram also shows the visual feature space and the machine translation process. The model is trained on a dataset that consists of sentences with masked words, and it learns to predict the masked words based on the context provided by the surrounding words. This pretraining step helps the model learn better representations of words and their relationships within a sentence.",
        "reference": "Figure 2. Conceptual illustration of the three proposed metrics. Given a caption \ud835\udc66\ud835\udc61 generated by a crosslingual image captioning model, we propose WMDRel and CLinRel to measure the semantic relevance between \ud835\udc66\ud835\udc61 and \ud835\udc66\ud835\udc60 , the reference in a source language (English here), and CMedRel to measure the semantic relevance between \ud835\udc66\ud835\udc61 and the visual content. Different from previous works, no reference caption in the target language (Chinese here) is needed."
    },
    "1592947-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a program. The program starts with the \"hello\" and \"hello\" cases, which are represented by the \"Xin\" and \"chao\" nodes. The \"cau\" node is the next case, followed by the \"khoe you\" and \"chu\" cases. The final case is the \"fine\" case. The program has a total of 5 cases, and it uses a stack of encoders to process the input.",
        "reference": "Figure 2. A BI-LSTM-CRF model for identifying FS."
    },
    "2011.14903v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that depicts the process of asset management and battery degradation. The flowchart begins with a parallel asset management model, which is followed by a model for minimizing the cost of replacement schedule. This is then followed by a nonlinear capacity fade model based on experimental data. The final step in the process is an incorporate battery degradation in asset management model.",
        "reference": "Fig. 3. Battery Degradation Model and Asset Management Model Framework"
    },
    "2110.15553v2-Figure4-1.png": {
        "prediction": "The image is a flowchart that depicts the data-driven generation of realizations. The flowchart is divided into two main sections: the upper section shows the architecture of BERT in pretraining, and the lower section displays Grassmann manifold projections. The upper section of the flowchart illustrates how the model's task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. The lower section of the flowchart shows the data-driven generation of realizations using the Grassmann manifold projections.",
        "reference": "Figure 4. A schematic illustration of the proposed data-driven surrogate modeling framework for UQ of computational head models: in the first stage (see Section 3.2), the available material properties of each of the four substructures, \ud835\udcb3M \ud835\udc56 \u2208 \u211d4\u00d7\ud835\udc41\ud835\udc56 (\ud835\udc41\ud835\udc56 denotes number of voxels for substructure \ud835\udc56) are used to generate 300 realizations of the input random vector of material properties for the 2D head model (i.e., \ud835\udcb3M). Simulations of these realizations yields input-output (\ud835\udcb3M \u2212 \ud835\udcb4M MAS) data sets for training the surrogate model in the second stage. The surrogate model is developed in three steps (see Section 3.3): 1. perform nonlinear dimensionality reduction on the output via"
    },
    "214611580-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of pretraining BERT. The architecture of BERT in pretraining is shown, with the representation learning network at the top. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes hard n-tuplets, robust anchor generation, and safety-aware sampling. The raw examples are shown at the bottom.",
        "reference": "Figure 1. The overview of the NeuCrowd framework. The dash line indicates that the SRL network passes its training loss to the n-tuplet sampling network. The blue dots represent original crowdsourced examples and red dots represent the robust anchors."
    },
    "1047598-Figure4-1.png": {
        "prediction": "The diagram shows a flowchart of a system that includes an active base model, a fuzzy product inference system, and a quantum particle swarm optimization. The active base model is connected to the fuzzy product inference system through an optical flow and a motion pathway. The fuzzy product inference system is also connected to a synergistic neural network. The system uses a form pathway and an output layer. The diagram also includes a section on pretraining BERT with a description of the architecture of BERT in pretraining.",
        "reference": "Figure 4. Schematic of recognition in proposed model."
    },
    "2107.07001v1-Figure13-1.png": {
        "prediction": "The diagram shows a solution method that is applied to different phases. The phases are represented by arrows and boxes. The arrows indicate the direction of the flow, while the boxes represent the different steps in the process.",
        "reference": "Fig. 13 Illustration of the Apollo CSM Transposition and Docking maneuver with the LM housed inside the Saturn S-IVB third stage [35, Figure 2-11]."
    },
    "2204.03065v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to learn the contextualized representations of words and sentences. The first part of the model, \"input items x\", is used to extract features from the input data. These features are then passed through a task processing G, which is responsible for transforming the input into a more meaningful representation. This transformed representation is then passed through a transformer T, which further refines the representation by applying a task processing G. The final output of the model is a representation of the input, which can be used for various natural language processing tasks such as language translation, text summarization, and sentiment analysis.",
        "reference": "Fig. 2: Generic designs of networks that act on sets of inputs. These cover relevant architectures, e.g. for few-shot-classification and clustering. Left: A generic network for processing a set of input items typically follows the depicted structure: (i) Each item separately goes through a common feature extractor F . (ii) The set of extracted features is the input to a downstream task processing module G. ; Right: A more general structure in which the extracted features undergo a joint processing by a transform T . Our SOT transform (as well as other attention mechanisms) is of this type and its high-level design (within the \u2018green\u2019 module) is detailed in Fig. 1."
    },
    "137199-Figure3-1.png": {
        "prediction": "The image depicts a flowchart with three main sections. The top section shows the architecture of BERT in pretraining, which includes masked language modeling and next sentence prediction tasks. The middle section presents the structured kernel, which is a combination of kernels (a1), (a2), and (a3). The bottom section displays the output layer, which consists of a random forest, neural networks, and MICE. The diagram also includes various components such as P, FNN, and PCA. It seems to be related to machine learning and natural language processing.",
        "reference": "Figure 3. Illustration for a exemplary subspace decomposition {\u039b(m)}3m=1."
    },
    "2012.01644v2-Figure5-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The diagram is divided into three main parts: the input layer, the output layer, and the hidden layers. The input layer contains the input volume, which is then passed through the positive child and negative child. The hidden layers are represented by the hierarchical triplet loss, convolution encoder, and upsampling. The output layer contains the final input representation, positional embeddings, word embeddings, and mask sequence. The diagram also includes the use case, direct call relations, indirect call patterns, and probabilistic distribution over vocabulary.",
        "reference": "Figure 5: Example of multi-patch sampling procedure with sampled anchor patch, positive child, and negative child."
    },
    "2012.01249v2-Figure16-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to learn from a large amount of data, and it is trained on two tasks: pretraining and fine-tuning. In the pretraining stage, the model learns to predict masked words in a sentence. It also learns to predict the next sentence in a pair of sentences. This stage helps the model understand the context of the text.\n\nIn the fine-tuning stage, the model is trained on a specific task, such as question-answering or sentiment analysis. During this stage, the model's pretrained weights are used as a starting point for further training on the specific task. The model is then fine-tuned by adjusting its weights based on the task-specific data.\n\nThe diagram also shows the different components of the model, including the input layer, attention features, graph features, and output layer. These components work together to process the input data and generate the output predictions. Additionally, the model has several fully connected layers that help to extract meaningful features from the input data. Finally, the model uses an aggregation layer to combine the information from all the layers and generate the final output.",
        "reference": "Fig. 16. ABCNet architecture used for pileup identification in Ref. 16. FC layer and encoding node sizes are denoted inside curly brackets. For each graph attention pooling layer (GAPLayer), the number of k-nearest neighbors and attention heads (h) are given."
    },
    "2012.01249v2-Figure12-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The architecture of BERT in pretraining is shown, where the model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The pretraining process includes the following steps: \n\n1. Encoder: The input sequence is passed through the encoder, which consists of multiple layers of Transformer blocks.\n2. Graph Module: The graph module is used to generate a graph based on the input sequence. This graph is then used to train the model.\n3. Graph Module: Another graph module is used to generate another graph based on the input sequence.\n4. Graph Module: A third graph module is used to generate yet another graph based on the input sequence.\n5. Output Module: The output module takes the representation vectors from the last layer of the encoder and generates the final output representation.\n\nOverall, the BERT pretraining process involves training the model to predict the masked tokens in the input sequence by using multiple graph modules to generate different graphs based on the input sequence.",
        "reference": "Fig. 12. Graph neural network architecture for particle tracking [21]. The initial latent features of the nodes and edges after the encoder network are named H0. The graph module is applied repeatedly to the latent features. For the ith iteration, the initial features H0 are concatenated with the current features Hi\u22121. After 8 iterations, the output network takes the last latent features H8 to produce classification scores for every edge."
    },
    "2012.01249v2-Figure14-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT. The model is pre-trained and then fine-tuned for specific tasks. The architecture of the BERT model is shown in the lower part of the diagram. The input to the model consists of a sequence of tokens, with some of them being masked. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The output layer represents the final input representation. The diagram also shows the edge classification, which is used to classify the edges of the graph. There are several hidden representations, track pairs, and n(x)(x-1)2 edge scores that are involved in the process.",
        "reference": "Fig. 14. The Set2Graph [36, 37] model architecture (top) consisting of a set-to-set component \u03c6, which creates a hidden representation of each track, a broadcasting layer \u03b2, which creates a representation for each directed edge (ordered pair of tracks in the jet), and an edge classifier \u03c8. Edges whose symmetrized edge score is over a certain threshold are connected, resulting in the set partition."
    },
    "140529-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented by the top left corner of the diagram, with the direct call relations and indirect call patterns shown in the middle and right sections respectively. The diagram also includes information about the output layer, stack of encoders, and final input representation. The diagram is designed to be helpful, honest, and harmless, and it provides a clear and concise representation of the flowchart's content.",
        "reference": "Figure 1: (Top) Model architecture: the network is split into two branches after the shared feature extraction layers. The top branch predicts a segmentation mask for the the object located at the center while the bottom branch predicts an object score for the input patch. (Bottom) Examples of training triplets: input patch x, mask m and label y. Green patches contain objects that satisfy the specified constraints and therefore are assigned the label y = 1. Note that masks for negative examples (shown in red) are not used and are shown for illustrative purposes only."
    },
    "2204.08626v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that processes and filters data. The system begins with an input layer, where data is received. This data is then passed through a series of spatial filters, which are used to extract specific features from the data. These features are then concatenated together and passed through a convolutional layer, which further processes the data.\n\nNext, the data is passed through a series of spatial filters again, before being passed through another convolutional layer. This process is repeated several times, with the data being passed through more spatial filters and convolutional layers each time.\n\nFinally, the data is passed through a final input representation layer, which represents the data in a format that can be processed by a neural network. The output of this layer is then fed into a neural network, which is used to make predictions or classifications based on the data.",
        "reference": "Fig. 1: The procedure of extracting features. The EEG signals are bandpass filtered using K different frequency ranges represented in F . The CSP algorithm is applied to the filtered signals to generate a feature vector. These vectors are then fused to feed a supervised autoencoder."
    },
    "2101.08520v3-Figure1-1.png": {
        "prediction": "The diagram depicts a complex system with various components and their interactions. The system is composed of several interconnected nodes, represented by blue circles. These nodes are connected to each other through lines, indicating the flow of information or data between them.\n\nThere are several key elements in the diagram:\n1. LossGE: This element appears multiple times in the diagram, suggesting that it is a significant part of the system.\n2. LossLimit: Another recurring element, this component may be used to control the flow of data or information within the system.\n3. LossTotal: This element is also present multiple times, indicating its importance in the system.\n4. Losspc: This element is found only once, suggesting that it might have a specific function or role in the overall system.\n\nThe arrows in the diagram represent the direction of the data or information flow. They connect the nodes, showing how the data moves from one component to another.\n\nOverall, the diagram represents a complex system with multiple interconnected components, where data or information flows from one node to another, following a specific path defined by the arrows.",
        "reference": "Figure 1. Pictorial description of the overall architecture."
    },
    "2012.14758v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a user interacting with a system. The user's face is recognized by the system, and then the user is authenticated. After authentication, the user can access the system. The system has an encryption key, which is used to encrypt data. The encrypted data is stored in a database. When the user wants to access the data, they need to decrypt it using the encryption key. The system also has a secure sketch template module, which is used to generate a hash value for the user's face. This hash value is compared to a hash value stored in the database to verify the user's identity. If the hash values match, the user is granted access to the system.",
        "reference": "Fig. 1: Block diagram of the proposed system."
    },
    "2012.14758v1-Figure2-1.png": {
        "prediction": "The image is a diagram of a neural network architecture. The diagram is divided into two main parts: the left side, which shows the input and output layers, and the right side, which shows the hidden layers. The input layer consists of 512 nodes, while the output layer has 2 nodes. The hidden layers are composed of 512 nodes, with each layer having a different number of nodes. The arrows in the diagram represent the connections between the nodes. This diagram provides a visual representation of how information flows through the network during the training process.",
        "reference": "Fig. 2: Proposed deep feature extraction and binarization (DFB) model for the fully concatenated architecture (FCA)."
    },
    "2012.05647v1-Figure2-1.png": {
        "prediction": "The diagram shows a neural network architecture. The input is fed into a Spatial Phase Modulation layer, which is followed by a Scattering layer. The output of the Scattering layer is then passed through a reward function and a phase correction layer. The final output is then fed into a neural network for further processing. The diagram also includes a section on \"Quasi Reinforcement Learning Loop\" and a section on \"Sparse Intensity Measurements.\"",
        "reference": "Figure 2: Schematic of the specific reinforcement learning loop used for each phase pattern of the training data set, with a particular example of desired phase pattern."
    },
    "2101.00341v1-Figure1.1-1.png": {
        "prediction": "Line 1 and Line 2 are similar, with Line 1 being slightly above Line 2. Line 3 is below Line 1 and Line 2. The lines all have a lot of noise and fluctuations.",
        "reference": "Figure 1.1: An illustration of a UDCN and its intrinsic spatio-temporal popularity dynamics. (a) Spatially dynamics of popularity (b) Temporal dynamics where the content popularity changes for long-term and short-term duration. The long-term dynamics are captured by the Chinese restaurant process, which determines the mean popularity for a certain time period of T . During this period, the instantaneous popularity is captured by the mean reversion model following the OU process [43]."
    },
    "2205.13948v1-Figure4-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is designed to learn from a dataset and then make predictions on new, unseen data. The dataset is split into two parts: the training set and the validation set. The training set is used to train the model, while the validation set is used to evaluate the model's performance.\n\nThe model consists of several components, including an input layer, a stack of encoders, a selection mechanism, a crossover, and a mutation. The input layer receives the data and passes it through the encoders. The encoders are responsible for extracting features from the data. The selection mechanism selects the best individuals from the population based on their fitness. The crossover combines the selected individuals to create new offspring. Finally, the mutation introduces random changes to the offspring, which helps to maintain genetic diversity in the population.\n\nThe model is trained using a genetic programming approach. During training, the model generates a population of individuals (i.e., potential solutions) and evaluates their fitness. The fittest individuals are selected to reproduce, creating a new generation of offspring. This process is repeated until a stopping criterion is met, such as a maximum number of generations or a minimum fitness threshold.\n\nOnce the model is trained, it can be used to make predictions on new, unseen data. The input data is passed through the same sequence of layers and operations as during training. The final output represents the predicted value for the input data.",
        "reference": "Fig. 4. Overview of PEGA."
    },
    "2210.01528v1-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a text processing system. The system starts with reading the input text, which is then passed through a thermal noise removal process. After that, the text is sent to a speckle-filling process, followed by a terrain-correction process. The processed text is then passed through a linear-to-from-RDB process. The output layer represents the final input representation. The system also includes an apply-orbit-file, calibration, and subset processes.",
        "reference": "Fig. 3: Illustration of one of the back-scattering SNAP pipeline we have integrated in the system."
    },
    "913274-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a use case for a system. The system has two main components: a direct call relations component and an indirect call relations component. The direct call relations component has three sub-components: trace 1, trace 2, and output layer. The indirect call relations component also has three sub-components: direct call patterns, indirect call patterns, and output layer. The use case is represented by the letters \"C\" and \"2\". The diagram also includes a stack of encoders and a final input representation.",
        "reference": "Figure 1. An illustration of the Bi-LSTM-RNN model. The example is \u201cHe had headachese1 from molde2 in the bedrooms.\u201d. e1 and e2 denote two target entities."
    },
    "914562-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a feature network (CNN) with several components. The first component is the input layer, which receives the data and passes it to the next layer. The second component is the metric learning layer, which learns the distance or similarity between the features of the data. The third component is the push layer, which pushes the data forward in the network. The fourth component is the pull layer, which pulls the data back in the network. The fifth component is the output layer, which produces the final result of the network. The arrows in the diagram show the direction of the data flow through the network.",
        "reference": "Figure 2: Illustration of our learning formulation. It tends to maximize the distance disparity between the matched pair and mismatched pair for each triplet, in terms of optimization."
    },
    "2101.02047v2-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The model is trained on a large corpus of unlabeled text. During training, some of the input tokens are randomly masked and replaced with a special [MASK] token. The model's task is to predict the masked tokens from their context. This process helps the model learn the relationships between words in a sentence. After pretraining, the model is fine-tuned on a specific task, such as question-answering or sentiment analysis. The diagram also shows the architecture of the BERT model, which consists of an input layer, a stack of encoder layers, and a final output layer. The input layer takes the masked input tokens and passes them through the encoder layers. The output layer then processes the encoded representations to make predictions for the task at hand.",
        "reference": "Figure 2: A block diagram of the unified gesture recognition and fingertip detection algorithm depicting the CNN architecture with input and output."
    },
    "2202.06065v1-Figure2-1.png": {
        "prediction": "The image depicts a flowchart that shows the use of case 1 and case 2. The flowchart is divided into four sections, each representing a different stage in the process. The first section, \"use case 1\", is shown in green and has a dotted line leading to it from the top left corner. The second section, \"direct call relations\", is also in green and has a dotted line leading to it from the bottom left corner. The third section, \"indirect call patterns\", is in pink and has a dotted line leading to it from the top right corner. The fourth section, \"direct call patterns\", is in blue and has a dotted line leading to it from the bottom right corner. Each section contains various elements, such as circles, rectangles, and lines, which represent different parts of the process. Overall, the flowchart provides a visual representation of the steps involved in using cases 1 and 2.",
        "reference": "Fig. 2. Illustration of the construction of (B , B ) . The straight edges are the five possible types for edges of % . The curvy edge corresponds to an edge of Alice, and the do ed edge to an edge of Bob."
    },
    "2012.15175v3-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is trained on two types of data: HPE models and weight-adaptive loss. The HPE models are used to generate heatmaps, which are then re-scaled to create scale-adaptive GT. The weight-adaptive loss is used to push and pull the loss in the embedding space. The model is then tested using the original GT and the generated GT. The output layer represents the final input representation. The diagram also includes images of people sitting on benches, which are not part of the model's function.",
        "reference": "Figure 2. During training, the ground-truth heatmaps are firstly scaled according to predicted scale maps and then are used to supervise the whole model via weight-adaptive loss. During testing, the predicted heatmaps and associative embeddings are used for grouping of individual persons."
    },
    "2103.06446v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a data analysis process. The process starts with input data, which is then screened to remove any irrelevant or redundant information. The remaining data is then clustered into time series, which are further analyzed for any causal inference. Finally, the hypothesis is tested to determine the impact on future trends across other subjects.",
        "reference": "Figure 1: The procedures of our approach"
    },
    "18233504-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a system that encodes and decodes with LSTM (Long Short-Term Memory) and DQN (Deep Q-Network). The arrows in the flowchart represent the direction of information flow. The system is designed to process input data, which is then encoded by the LSTM layer. The encoded data is then passed through a DQN layer for further processing. The output of the DQN layer is then decoded back into its original form using another LSTM layer. The system is designed to be efficient and effective at processing large amounts of data.",
        "reference": "Figure 1: Iteratively decoding with DQN and LSTM; the encoder- coder LSTM network is depicted as gray-filled rectangles on the bottom; the top-leftis the graphical illustration of the DQN with bidirectional LSTMs; the dash arrow line on the right indicates the iteration loop."
    },
    "2010.13114v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that depicts the process of executing traces and temporal relations. The flowchart has several components, including input layer, output layer, stack of encoders, and a teacher network. The teacher network is connected to the input layer, which is then followed by softmax layer. The output layer is also connected to the stack of encoders. The flowchart also shows the use case of using the case 1 and case 2 for indirect call patterns. The diagram provides a clear visual representation of the process and its components.",
        "reference": "Figure 2: A schematic of the proposed methodology for joint knowledge distillation and open set recognition. The outputs from teacher and student models are their respective representations, T and S, which are used for calculating CRD loss term. Next, we zS and zT represent student and teacher logits, which are used for calculating the KD loss term. Finally, we calculate class probabilities \u03c3S(zS), which are used for computing of cross-entropy loss term."
    },
    "988721-Figure1-1.png": {
        "prediction": "The diagram depicts a complex system with multiple components and connections. The image is processed to extract features, which are then passed through a shared convolutional layer. These features are then sent to a fully connected CRF, which outputs a semantic representation. The output is then passed through a depthwise separable convolutional layer, followed by a multi-scale semantic and depth unaries layer. The final output is a representation of the input image.",
        "reference": "Figure 1. Overview of the proposed method. Multi-scale fully convolutional network is used for image representation. The network consists of 5 different paths and each path extracts feature at a difference scale. At the end of each path, two convolutional layers extract feature for semantic segmentation and depth estimation. These feature maps are scaled and aggregated to form a comprehensive feature map for semantic segmentation and depth estimation. Depth values are estimated using Eq. 3. Estimated depth values along with semantic unaries and the image are passed through the fully connected CRF to get the final probabilities of semantic labels."
    },
    "2202.07568v4-Figure2-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of using a machine learning model to make predictions. The flowchart begins with the \"Offline\" section, which includes the pre-training of a BERT model. This involves masking some of the input tokens and predicting their values.\n\nNext, the \"Online\" section shows how the trained BERT model is used to make predictions. The user provides input for the prediction, and the model generates an output based on the input. The output is then passed through a strategy vector, which is used to evaluate the model's performance and adjust its parameters accordingly.\n\nThe \"How to move\" section outlines the different ways the model can be moved between phases or models. It suggests adding new models to those already considered by the defender, choosing optimizers, and setting all models considered by the defender. The diagram also includes a section on \"Prediction,\" which shows how a single model is chosen strategically from a set of available models, taking into account the available information on the attacker and the attack intensity.\n\nOverall, the flowchart provides a detailed overview of the process of using a machine learning model to make predictions, including the pre-training, online use, and movement of the model.",
        "reference": "Fig. 2: Overview of StratDef."
    },
    "201666925-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to predict the masked tokens in a given text sequence. The architecture of the model is shown, and it includes an input layer, a stack of encoders, a re-constructor, and an output layer. The model is pre-trained on a large corpus of text data, and then fine-tuned for specific tasks such as next sentence prediction and masked token prediction. The model's performance is evaluated using various metrics, including accuracy, precision, and recall. The diagram also shows the use of case 1 and case 2, which are used to determine the type of input data that the model will process.",
        "reference": "Fig. 1. Illustration depicting the comparison between the baseline GAN model and our Recon-GLGAN model. In the training phase, the ZF image and the ROI are fed in as inputs to the Recon-GLGAN model, while the baseline GAN only takes the ZF image as input. In the testing stage, the ZF image is fed as input to either model to produce the reconstruction (Note: ROI is not used during testing stage). The reconstruction error of the Recon-GLGAN model is lesser than the baseline GAN model in the ROI"
    },
    "201666925-Figure2-1.png": {
        "prediction": "The diagram depicts a complex system with various components and their interactions. The system is divided into two main sections: the upper section, which represents the use case \"use case u1\", and the lower section, which represents the \"direct call relations\" and \"indirect call patterns\". The upper section contains several sub-components, including \"Trace 1\" and \"Trace 2\", which are connected to the \"use case u1\" by arrows. The lower section consists of multiple sub-components such as \"Direct call relations\" and \"Indirect call patterns (C1, C2)\". These sub-components are interconnected through arrows, indicating the flow of information or data between them. Overall, the diagram seems to represent a detailed view of a specific system or process, with various components and their relationships clearly defined.",
        "reference": "Fig. 2. Recon-GLGAN architecture"
    },
    "2107.05180v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that shows the process of using a case. The flowchart has several text nodes and arrows, which are used to represent different steps in the process. The text nodes include \"use case u1\", \"direct call relations\", \"indirect call relations\", \"C1, C2\", and \"direct call patterns (C1, C2)\". The arrows show the flow of information between these text nodes, indicating how the different steps are connected. Overall, the flowchart provides a visual representation of a specific process or system.",
        "reference": "Figure 2: The framework overview of MugRep."
    },
    "2202.10337v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of knowledge discovery in machine learning. The process starts with semantic space domain knowledge, which includes quantitative knowledge and governing equations. The process then moves to the vector space, where knowledge embedding models are used. The next step is knowledge discovery based on machine learning, which involves sparse regression-based methods and genetic algorithms. Finally, the process ends with applications such as simulation, inverse modeling, and interpretation.",
        "reference": "Figure 1: Schematic diagram of the relationship between knowledge embedding and knowledge discovery"
    },
    "30595348-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart for training a Convolutional Neural Network (CNN) to recognize cats and dogs. The first step is to pretrain the BERT model, which consists of an input layer, a stack of encoders, a final input representation layer, and a positional embeddings layer. The next step is to use the BERT model to predict masked tokens in a sequence. This is done by masking some of the tokens in the input sequence and then using the BERT model to predict the masked tokens. The output of this step is a feature map that is used to train a CNN. The CNN is trained on a dataset consisting of images of cats and dogs. The final step is to use the trained CNN to classify new images as either cats or dogs.",
        "reference": "Fig. 1. An illustration of the proposed reversed attention network (RAN), where the lower and upper branches learn features and predictions that are and are not associated with a target class, respectively. The mid-branch focuses on local regions with complicated spatial patterns whose object responses are weaker and provide a mechanism to amplify the response. The predictions of all three branches are fused to yield the final prediction for the segmentation task."
    },
    "1233699-Figure4-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The top part of the flowchart is labeled \"use case u1\" and has two sections, \"direct call relations\" and \"indirect call relations.\" The bottom part of the flowchart is labeled \"direct call patterns (C1, C2)\" and has two sections, \"direct call patterns\" and \"indirect call patterns.\" The flowchart also includes several boxes with arrows connecting them, representing different stages of the process.",
        "reference": "Fig. 4. The proposed network architecture for skeleton extraction, which is converted from VGG 16-layer net [36]. (a) Multi-task Scale-associated side outputs (SSOs) learning. Our network has 4 stages with SSO layers connected to the convolutional layers. Each stage branches into two sibling SSO layers, one for skeleton localization and the other for scale prediction, denoted by Loc-SSO (the left multi-color blocks) and ScalePred-SSO (the right blue block), respectively. The SSOs in each stage are guided by a scale-associated groundtruth skeleton map (The skeleton pixels with different quantized scales are in different colors. Each block in a Loc-SSO is the activation map for one quantized scale, marked by the corresponding color). (b) Scale-specific fusion. Each Loc-SSO provides a certain number of scale-specific skeleton score maps (identified by stage number-quantized scale value pairs). The score maps of the same scales from different stages will be sliced and concatenated. Five scale-specific weighted-fusion layers are added to automatically fuse outputs from multiple stages."
    },
    "2012.09688v3-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of a system that includes several components such as input, output, attention map, value, attention feature, and linear. The system is designed to process data and provide an output based on the input provided. The flowchart also includes various operations like transpose, scale + softmax, softmax + norm, matrix multiplication, and addition/subtraction. These operations are used to manipulate the data and generate the final output.",
        "reference": "Figure 3. Architecture of Offset-Attention. Numbers above tensors are numbers of dimensions N and feature channels D/Da, with switches showing alternatives of Self-Attention or Offset-Attention: dotted lines indicate Self-Attention branches."
    },
    "2110.10072v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses a flat Brazilian disc to record sound. The system is designed to capture sound and then play it back through an amplifier. The flat Brazilian disc is used as the storage medium for the recorded sound. The system also features a pulse shaper, which helps to shape the pulses of the recorded sound, and a pulse shaper, which further refines the pulses before they are played back through the amplifier. The system also includes a transmission bar, which is used to transmit the recorded sound from one part of the system to another. Additionally, the system has a strain gauge, which measures the strain in the system, and a strain gauge, which measures the strain in the system. Finally, the system has a momentum trap, which is used to capture any momentum that may be present in the system. Overall, this system appears to be a complex and sophisticated method of recording and playing back sound.",
        "reference": "Fig. 1 Schematic diagram of a Split Hopkinson Pressure bar and top view of the flattened Brazilian disc sample before mounting"
    },
    "688013-Figure7-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses a neural network. The system has an input layer, a region proposal network, and a classification box refinement step. The input layer takes in data, which is then passed through the region proposal network to generate proposals for object locations. These proposals are then refined by the classification box refinement step. The output of the system is a set of bounding boxes around objects in the input image.",
        "reference": "Figure 7. Illustration of our approach for spatio-temporal action localization. Region proposals are detected and regressed with Faster-RCNN on RGB keyframes. Spatio-temporal tubes are classified with two-stream I3D convolutions."
    },
    "1379674-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The model is trained on two tasks: masked language modeling (MLM) and next sentence prediction (NSP). In MLM, some tokens in the input sequence are randomly selected and replaced with the [MASK] token. The model's task is to predict the original tokens from their [MASK] token representations. In NSP, the model is asked to predict whether two input sentences follow each other or not. The architecture of BERT consists of an encoder and a stack of encoders. The input embeddings are processed by the encoder, which then passes the information to the stack of encoders. The final output representation is obtained from the last layer of the stack of encoders.",
        "reference": "Figure 1. Inference in a 3-Level Pyramid Network [15]: The network G0 computes the residual flow v0 at the highest level of the pyramid (smallest image) using the low resolution images {I10 , I20}. At each pyramid level, the network Gk computes a residual flow vk which propagates to each of the next lower levels of the pyramid in turn, to finally obtain the flow V2 at the highest resolution."
    },
    "2011.05655v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a process. The process begins with raw water being sent to the raw lab, where it is purified and then sent to the distillation unit. From there, the distilled water is sent to the PPO-Bis-MSB mixing unit. The wasted liquid from this step is sent to the floors for dissolution. Gas stripping is also performed on the distilled water before it is sent to the function manifold. The function manifold sends the water to the KCL mixing unit, which then sends the water to the DYB water. Finally, the DYB water is sent to the Hall #5, and the gas stripping is performed one more time before reaching the final input representation.",
        "reference": "Figure 1: Schematic of the pilot plant experiment"
    },
    "2101.08003v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented by the top left corner, and the direct call relations are shown in the middle. The indirect call patterns are displayed in the bottom right corner. The diagram also includes information about the output layer, stack of encoders, and final input representation.",
        "reference": "FIG. 1. A schematic of (a) simulation setup showing the major components as depicted. The plane of polarization is XY. The angle of incidence of the monochromatic light (L) is \u03b8i, thickness of each material and detector (D) are shown and (b) a single layer of shape anisotropic molecules (rod shaped) tilted with respect to X-axis along Y-axis on the YZ plane. The projection of the molecules is shown in black. Such projection resembles nematic ordering on 2D plane with optic axis along Y-axis."
    },
    "2202.03587v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a transformer model. The transformer model is a type of neural network that is used for various natural language processing tasks, such as machine translation and text summarization.\n\nThe diagram shows the architecture of the transformer model, which consists of an encoder and a decoder. The encoder takes in input sequences and produces a set of hidden states, while the decoder uses these hidden states to generate output sequences.\n\nThe transformer model uses a self-attention mechanism, which allows it to weigh the importance of different words in the input sequence when generating the output sequence. This mechanism helps the model capture long-range dependencies between words, making it more effective at tasks like machine translation.\n\nThe diagram also includes information about the input and output layers of the transformer model. The input layer is where the input sequence is fed into the model, and the output layer is where the final output sequence is generated. The diagram also shows the positional embeddings, which are used to represent the position of each word in the input sequence during training.\n\nOverall, the diagram provides a detailed view of the transformer model's architecture and how it works to process and generate text.",
        "reference": "Figure 2: The acoustic tokens, as output by the SpecTran network pretrained via CALP, are employed together with (text) words in a multimodal transformer. The multimodal transformer encodes the set of tokens with modality and positional information."
    },
    "2010.13130v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the process of executing traces and temporal relations. The flowchart consists of several nodes and arrows, with the nodes representing different tasks and the arrows indicating the flow of information between them.\n\nStarting from the top left corner, there is a node labeled \"use case u1\" which represents the use case for the system. This is followed by a node labeled \"direct call relations\" and another node labeled \"indirect call relations.\" These nodes represent the different types of call relations that can be executed in the system.\n\nMoving to the right side of the flowchart, there are two more nodes labeled \"C1\" and \"C2,\" which represent the different patterns of direct calls. Below these nodes, there is a node labeled \"direct call patterns (C1, C2),\" which represents the specific patterns of direct calls that can be executed in the system.\n\nThe flowchart also includes a node labeled \"algorithm\" at the bottom, which represents the algorithm used to execute the traces and temporal relations. Additionally, there is a node labeled \"prediction\" and a node labeled \"prediction y,\" which indicate the prediction of the system based on the input data.\n\nOverall, the flowchart provides a visual representation of the process of executing traces and temporal relations within a system, including the use case, call relations, direct call patterns, and the algorithm used for prediction.",
        "reference": "Figure 1: AutoSpeech Challenge\u2019s evaluation process for one task defined by the 5-tuple: Dtr, D \u2205 te, L,BT , BS . Participants need to submit a strategy implemented by Python scripts which must contain a file named \u201dmodel.py\u201d. forgiving In this file, the two functions named train and test implement the logic of AutoSpeech algorithm. These two functions are called by the ingestion program (defined in ingestion.py) orderly, to train on Dtr and produce a prediction Y t pred on D \u2205 te at the timestamp t respectively. The prediction Y tpred is then compared to true labels Yte in scoring program (defined by score.py and produces a score st. This ingestion/scoring loop is controlled by time budget BT and a flag in model.py, to determine whether the whole program ends. At any time,the score sequence st0, st1, ... is visualized as a learning curve and the area under learning curve is used as the evaluation for this task. Note that only the run time of ingestion program is accumulated, but the time of scoring is not counted for running in parallel."
    },
    "2103.14969v2-Figure7-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model, specifically BERT. The model is pre-trained and then fine-tuned for specific tasks. The pre-training process involves masking some of the input tokens and predicting their values. The architecture of the BERT model is shown in the diagram, with different components such as the input embeddings, transformer blocks, and output layers. The diagram also includes information about the group and norm, relu, conv3x3 stride 2, and conv1x1, which are part of the model's architecture. Additionally, there is a section on the stack of encoders, which is an important component of the BERT model. Overall, the diagram provides a detailed view of the BERT model and its various components.",
        "reference": "Fig. 7. V-Net with autoencoder regularization: \u201cSchematic visualization of the network architecture. Input is a four channel 3D MRI crop, followed by initial 3x3x3 3D convolution with 32 filters. Each green block is a ResNet-like block with the GroupNorm normalization. The output of the segmentation decoder has three channels (with the same spatial size as the input) followed by a sigmoid for segmentation maps of the three tumor subregions (WT, TC, ET). The VAE branch reconstructs the input image into itself, and is used only during training to regularize the shared encoder.\u201d [33]"
    },
    "2010.00702v1-Figure4-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is pre-trained on large-scale unlabeled text data, and then fine-tuned on a labeled set of text data for a specific task. The architecture of the BERT model is shown, with its input layer, transformer layers, and output layer. The pre-training process involves masking some of the input tokens and predicting their values, as well as predicting the next sentence in a given sequence. The fine-tuning process involves training the model on a specific task, such as question-answering or sentiment analysis. The arrows in the diagram represent the flow of information through the model, from the input layer to the output layer.",
        "reference": "Figure 4: Our dual-view reflection removal. Given images I1 and I2, our reflection-invariant optical flow network \u03c6flow estimates the motion F1 2 between the unknown transmissive layers of the inputs, which is then used to warp I2 towards I1 to \u201cundo\u201d that motion. Our synthesis network \u03c6syn can then use these aligned images to leverage the parallax between the reflective layers (and the lack thereof between the transmissive layers) to synthesize I1\u2019s transmissive layer T1."
    },
    "2205.04319v1-Figure1-1.png": {
        "prediction": "The image depicts a flowchart of an AI assistant system. The system is designed to help customers by providing information and assistance. The main components of the system include an operator, a legend, a broker, a customer, a request, a collection of requests, an offer, and a vehicle.\n\nThe operator is responsible for processing the customer's request and providing the necessary information or assistance. The legend represents the knowledge base of the system, which contains all the information that the system can access to answer the customer's queries. The broker is responsible for managing the communication between the customer and the system. The customer initiates the request, and the system processes it through the collection of requests. The offer component provides additional information or options to the customer based on their request. Finally, the vehicle component represents the physical device or platform on which the system runs, such as a website or mobile app.\n\nOverall, the diagram shows how the different components of the AI assistant system work together to provide assistance to customers.",
        "reference": "Figure 1: Illustration of an AMOD Broker System"
    },
    "2205.11782v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of launching an IPA or OPA. The flowchart shows the different components and their interactions in the system, such as the input, output, local user end, and remote server. It also includes the target values, mean, and variance for each component. The diagram provides a clear understanding of how the system works, from the launch of IPA or OPA to the output from OPA or IPA.",
        "reference": "Figure 1: Illustration of our fine-grained data poisoning attacks on LDP-based mean/variance estimation."
    },
    "2107.09786v1-Figure1-1.png": {
        "prediction": "The diagram illustrates a data processing system, with the server and client components. The server is responsible for sending activations to the cloud server, which then sends gradients back to the client. The client, in turn, passes the model to the next client. This process continues until the final input representation is reached. The architecture of BERT is also shown, where the masked tokens are replaced by the '[MASK]' token, and the random token is used to replace the 'nervous' token. The model's task is to predict the tokens'speak' and 'nervous' from the representation vectors it learns at the positions of the input embeddings of '[MASK]' and 'that'.",
        "reference": "Fig. 1. Overview of split learning scheme. (a) Split learning with K clients. (b) Communication and computation at the client level."
    },
    "2210.09964v2-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a system, with several components and their connections. The main components are RC (range controller), SRNF (safe-range neural field), RANF (range-aware neural field), and SQL (query language). \n\nThe flowchart begins with the RC component, which is connected to the SRNF and RANF components. The SRNF component is then connected to the RANF component, which is in turn connected to the SQL component. The SQL component is also connected back to the RC component, creating a loop.\n\nOverall, this flowchart represents a system where data is processed by the RC component, which then passes it on to the SRNF and RANF components for further processing. The final output is generated by the SQL component, and the result is fed back into the system through the RC component.",
        "reference": "Figure 1: Overview of our translation."
    },
    "2010.06417v2-Figure2.12-1.png": {
        "prediction": "The image is a flowchart that shows the process of executing traces and temporal relations. The flowchart has several nodes and arrows that represent different parts of the process. The nodes are labeled with letters such as \"h\", \"f\", \"U\", and numbers like \"1\", \"2\", and \"3\". The arrows connect the nodes, showing the flow of information or data through the system. The flowchart also includes text labels that provide additional information about each part of the process.\n\nThe flowchart appears to be related to a software development or programming context, as it includes terms like \"Unfold\" and \"Masked Sequence\". The process seems to involve executing traces, which could refer to the execution of code or algorithms. The temporal relations between the different parts of the process may indicate how the different components of the system interact with each other over time. Overall, the flowchart provides a visual representation of a complex system or process, making it easier to understand the various steps involved and how they relate to each other.",
        "reference": "Figure 2.12: A recurrent network with no outputs. This recurrent network just processes information from the input x by incorporating it into the state h that is passed forward through time. (Left) Circuit diagram. The black square indicates a delay of a single time-step. Right The same network seen as an unfolded computational graph, where each node is now associated with one particular time instance."
    },
    "2110.02729v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that depicts the execution traces and temporal relations. The flowchart is divided into three main sections, with each section representing a different aspect of the process.\n\nThe first section shows the use case \"use case u1\" and includes the \"direct call relations\" and \"indirect call relations.\" This section also has a sub-section called \"Trace 1\" and \"Trace 2,\" which are used to represent the different paths that the process can take during execution.\n\nThe second section of the flowchart represents the \"direct call patterns (C1, C2)\" and the \"indirect call patterns (C1, C2).\" These sections show the different types of calls that can be made during the process, including direct and indirect calls.\n\nThe third section of the flowchart displays the \"direct call patterns (C1, C2)\" and the \"indirect call patterns (C1, C2).\" This section provides more detailed information about the specific types of calls that can be made during the process, such as direct and indirect calls.\n\nOverall, the flowchart provides a comprehensive view of the process, including the different paths that the process can take, the types of calls that can be made, and the specific types of calls that can be made. It is a useful tool for understanding the process and its various components.",
        "reference": "Figure 3. The block diagram of the proposed time domain offset cancelation technique."
    },
    "2103.04234v2-Figure6-1.png": {
        "prediction": "The diagram depicts a complex system with various components and their interactions. The system is divided into an upper layer, a lower layer, and a message layer. The upper layer contains the interface node, which connects to the consistency checker, HTTP server, configuration, and fault injection. The lower layer has the data store, state machine, and node ID. The message layer includes the client, benchmarker, and calls. The arrows in the diagram represent the flow of information between these components. The system seems to be designed for handling a large number of clients and providing a consistent and reliable service.",
        "reference": "Fig. 6: The Paxi architecture"
    },
    "2206.10375v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The architecture of BERT is shown in the lower part of the diagram. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the input and output layers, as well as the stack of encoders. The final input representation is shown in the rightmost part of the diagram.",
        "reference": "Fig. 1. Overview of proposed ME2Stereo-Du2CNN architecture: The input consists of three stereo pair of the same scene captured at different camera exposure level and the output is a refined disparity map (R\u0302). The input is passed through novel Dual CNN architecture to get the disparity estimates at three exposure levels. The estimates are fused using two quality measures: well-exposedness and contrast, which in-turn are obtained respectively from the input left-view image and the disparity estimate of the corresponding stereo pair. The predicted refined disparity map (R\u0302) is a better estimate compared to three intermediate disparity estimates."
    },
    "2101.08386v4-Figure5-1.png": {
        "prediction": "The image depicts a flowchart of a machine learning model. The model is pre-trained on a large corpus of text using the masked language modeling objective and next sentence prediction objective. The input to the model consists of an embedding layer, followed by a stack of encoders. The output of the model is a representation of the input. The model is then fine-tuned for specific tasks such as question answering and natural language inference. The architecture of the BERT model is also shown in the diagram.",
        "reference": "Figure 5: (Handwritten digits) Diagram of the learning model used in the handwritten digits experiment. The model is obtained by concatenating a computer vision (CV) model and an identity effect (IE) model. From right to left: The model takes a pair of images as input (right); then, the CV model classifies them or, equivalently, encodes them as two 10-dimensional probability vectors (center); finally, the IE model assigns a rating from 0 to 1 to the pair of probability (or encoding) vectors to identify whether the images represent identical digits or not (left)."
    },
    "2204.07837v2-Figure3-1.png": {
        "prediction": "The diagram depicts a system that involves several components, including tokens, token classifiers, positions, and encoders. The tokens are represented as circles, and the token classifiers are represented as rectangles. The encoders are represented as yellow rectangles.\n\nThe arrows in the diagram indicate the flow of information between the different components. For example, there is an arrow pointing from the tokens to the token classifiers, which suggests that the classifiers process the tokens. There is also an arrow pointing from the token classifiers to the encoders, indicating that the encoders receive the output of the classifiers.\n\nThe diagram also includes labels such as \"use case u1\" and \"direct call relations\", which provide additional context for the system's operation. Overall, the diagram provides a visual representation of a complex system with multiple interacting components.",
        "reference": "Fig. 3. The illustration of our proposed self-supervised input representation (Section III-C) in sequence-to-sequence learning framework. We add two classifier to predict the token and position of perturbed tokens synthesized by the smooth augmented data generator in Section III-B. The meaning of blue rectangle and green rectangle is the same as in Figure 2. The red rectangles represent disturbed tokens\u2019 intermediate representation produced by the top layer of encoder."
    },
    "11277821-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes equations and text that describe the process of pretraining BERT.",
        "reference": "Figure 1: Architecture of the Wasserstein Deep Learning: two samples are drawn from the data distribution and set as input of the same network (\u03c6) that computes the embedding. The embedding is learnt such that the squared Euclidean distance in the embedding mimics the Wasserstein distance. The embedded representation of the data is then decoded with a different network (\u03c8), trained with a Kullback-Leibler divergence loss."
    },
    "2010.10246v2-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is composed of several components, including a dataset repository, library repository, and pipeline repository. The dataset repository contains the data that will be used to train the model. The library repository holds the pre-trained BERT models. The pipeline repository contains the predefined pipelines for the model.\n\nThe model architecture consists of a stack of encoders, followed by a final input representation layer. The encoders are responsible for processing the input data and extracting features from it. The final input representation layer combines the extracted features into a single vector representation of the input.\n\nThe model is trained using supervised learning, with the output layer being a softmax function. This function is used to classify the input data into one of several possible categories. The model is also trained using unsupervised learning, with the output layer being a feature extraction function. This function is used to extract relevant features from the input data without any specific labels or categories.\n\nThe model is pre-trained on a large corpus of text, which allows it to learn general language patterns and relationships. This pre-training process helps the model to better understand the context of the input data and make more accurate predictions.",
        "reference": "Fig. 1. The architecture of MLCask for supporting collaborative pipeline development with version control semantics."
    },
    "2202.07638v2-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a use case for a system. The system is designed to be used in two different ways, as indicated by the two paths that start from the top of the diagram. The first path represents a direct call relation, which is shown in green. The second path represents an indirect call relation, which is shown in red. The two paths are connected at the bottom of the diagram, where they both end in a stack of encoders. This suggests that the system can handle both direct and indirect calls, and that it uses a stack of encoders to process the calls.",
        "reference": "Fig. 1. Schematic representation of the multiplex network considered in this paper. Black nodes denote the agents while blue nodes represent the (possible) leaders."
    },
    "2101.05329v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that processes data. The system starts with an uncompressed bit array, which is then transformed into a dump wheel. From the dump wheel, the data is sent to a dynamic slice re-mapping process. After the re-mapping, the data is sent to a medical blue reading process. Following the reading process, the data undergoes a print length encoding step and finally ends at a half-length encoding stage. This flowchart shows the path that data takes as it moves through the system, from its initial state as an uncompressed bit array to its final state after being processed by the various stages in the system.",
        "reference": "Figure 1: Schematic illustration of the proposed algorithm."
    },
    "2110.06972v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system, with various components and their connections. The system is designed to observe encoders, which are connected to an interaction history. The encoders are also connected to a context encoder, which is linked to a dynamics model. Additionally, the encoders are connected to a reward model. The diagram also includes a mask, which is connected to a sequence encoder and an original sequence encoder. The arrows in the diagram represent the flow of data between the different components.",
        "reference": "Figure 2: Proposed ZeUS algorithm. The components shown in green (i.e. observation encoder, context encoder, dynamics model and reward model) are shared across tasks. Components/representations in red or blue belong to separate tasks."
    },
    "2110.14705v2-Figure10-1.png": {
        "prediction": "The diagram depicts a process of pretraining BERT. The architecture of BERT in pretraining is shown. In the lowercased example sequence, the tokens \u2018speak\u2019 and \u2018nervous\u2019 were sampled to be masked.'speak' is replaced by the \u2018[MASK]\u2019 token and \u2018nervous\u2019 is replaced by the random token \u2018that\u2019. The model's task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "FIG. 10. A candidate low-order process leading to the meltdown of the order parameter. In the main text, we prove that energy resonance between the two configurations is not possible under a generic assumption of strong incommensurability of the couplings."
    },
    "2012.06275v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into two main parts: the phonocardiogram and the spectrogram. The phonocardiogram has two main sections: the spectrogram and the phase. The spectrogram is further divided into the encoder, the decoder, and the convolutional layer. The phase consists of the activation function, the deconvolutional layer, and the convolutional layer. The diagram also includes the output layer, which is shown in the lower right corner. Overall, the flowchart provides a detailed view of the execution traces and temporal relations within the system.",
        "reference": "Fig. 1. The convolutional deep autoencoder (DAE(C)) architecture."
    },
    "2205.13113v1-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT. The architecture of BERT in pretraining is shown, with the model's task being to predict masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes an illustration of training and pilot transmission for CE, as well as data transmission. The diagram is designed to be helpful, honest, and harmless.",
        "reference": "Fig. 3. Illustration of the training process."
    },
    "2206.10128v2-Figure1-1.png": {
        "prediction": "The diagram shows a query generation model and an encoder-decoder architecture. The model takes in a query and generates a response based on the input. The encoder takes the input and encodes it into a fixed-length vector, which is then passed to the decoder. The decoder generates the response by predicting the next word in the sequence. The model is trained on a large corpus of text data using supervised learning techniques. During inference, the model takes in a query and generates a response by predicting the next word in the sequence.",
        "reference": "Figure 1: Overview of the proposed DSI-QG indexing framework, where a query generator (yellow box) is used to generate potential queries, which in turn are used to represent documents for indexing. The architecture of the method includes a cross-encoder ranker (gray box) that is used to select only promising queries to be sent for indexing."
    },
    "2202.13078v2-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of the BERT pretraining process. The architecture of BERT in pretraining is shown, with the model's task being to predict masked tokens from the representation vectors it learns at the positions of the input embeddings. The ResNet18 is used as a feature extractor, and the output layer is a GAP (Global Average Pooling) layer followed by a non-linear projection layer. The input consists of an input sequence and a segment pair. The output is a representation of the input sequence. The diagram also shows the input and output layers, as well as the stack of encoders.",
        "reference": "Fig. 1: Model architecture used in the pretraining phase of the proposed method."
    },
    "2102.00719v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into three main sections: use case 1, direct call relations, and indirect call patterns. The first section, use case 1, has two sub-sections, Trace 1 and Trace 2. The second section, direct call relations, contains four sub-sections, C1, C2, C3, and C4. The third section, indirect call patterns, also consists of four sub-sections, C1, C2, C3, and C4. The flowchart is designed to help understand the relationships between different elements in a system or process. It provides a visual representation of the flow of information and the sequence of events that occur within the system.",
        "reference": "Figure 1: Video Transformer Network architecture. Connecting three modules: A 2D spatial backbone (f(x)), used for feature extraction. Followed by a temporal attentionbased encoder (Longformer in this work), that uses the feature vectors (\u03c6i) combined with a position encoding. The [CLS] token is processed by a classification MLP head to get the final class prediction."
    },
    "2012.02670v2-Figure8-1.png": {
        "prediction": "The diagram depicts a flowchart of a server, with the use case \"use case u1\" at the top. The flowchart is divided into two main sections: \"Direct call relations\" and \"Indirect call relations.\" The direct call relations section contains three sub-sections: \"C1,\" \"C2,\" and \"C3.\" The indirect call relations section also has three sub-sections: \"C4,\" \"C5,\" and \"C6.\" The flowchart shows the relationships between different components, such as \"Xpub,\" \"Ypub,\" \"Xprio,\" and \"Yprio.\" The arrows in the flowchart indicate the direction of the flow, connecting the different components and their respective functions within the server.",
        "reference": "Figure 8: Schematic representation of the training process of the server\u2019s networks for the attribute inference attack. In the figure, the network \ud835\udc36\ud835\udc4e\ud835\udc61\ud835\udc61 substitutes \ud835\udc53 \u22121 and \ud835\udc52\ud835\udc5b refers to a suitable entropy measure for the classification task."
    },
    "2205.10688v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of the BERT pretraining process. The architecture of BERT is shown in pretraining, with the assumption that certain tokens in the lowercased example sequence are masked and replaced by the '[MASK]' token or random tokens. The model's task is to predict these masked tokens from the representation vectors it learns at the positions of the input embeddings. Additionally, there is a next sentence prediction task (P (BfollowsA)) where the model predicts if the next sentence follows the current one. The diagram also shows the PPO training, which involves creating a policy network, value network, and optimizer. The policy network is trained using the actor-critic method, while the value network is optimized using the advantage function. Finally, the diagram illustrates the selection of the best policy, which is done by comparing the performance of different policies on a validation set.",
        "reference": "Fig. 2. Overview: The agent is either generated randomly or with user support. The user also defined constraints (yellow arrows) (a). The initial Proximal Policy Optimization (PPO) trains the input agent to provide baseline agent policy (b). The system then creates variants of the initial model (c) and trains them all together with universal PPO (d). Selection (e), crossover, and mutation (f) create a new generation that is trained again. The system outputs the best(s) co-designed agents and their policies (g)."
    },
    "1241810-Figure5-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that involves weight and bias training, weight quantification, and weight freezing and bias learning. The system begins with the deployment of a memristor programming in NCS. This is followed by weight and bias training, which is then quantified to provide weight freezing and bias learning. The final step in the process is the representation of the input embeddings.",
        "reference": "Fig. 5. The framework of proposed bias tuning method."
    },
    "2011.12453v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is designed to learn the contextualized embeddings of words in a sentence, which are then used to predict the next word in a sequence. The architecture of the model is based on a transformer network, and it is pre-trained using masked language modeling and next sentence prediction tasks. The model is trained on a large corpus of text data, and it is fine-tuned on specific NLP tasks such as question answering, sentiment analysis, and text classification. The output layer of the model is designed to represent the final input representation, which is then used for downstream NLP tasks.",
        "reference": "Figure 1: (a) Illustration of graph neural ansatz. (b) Distributed workflow of VMC using ISGO."
    },
    "2202.04161v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that depicts the execution traces and temporal relations of a program. The program appears to be a language model, as indicated by the presence of nodes such as \"use case u1\", \"direct call relations\", and \"indirect call patterns\". The flowchart also shows the input and output layers, as well as the stack of encoders. The arrows in the flowchart represent the direction of the program's execution. Overall, the diagram provides a visual representation of the program's structure and how it processes input to produce output.",
        "reference": "Figure 3: Task structure for the generative model."
    },
    "2107.00466v1-Figure1-1.png": {
        "prediction": "The image is a diagram of the architecture of BERT, a pre-trained contextual language model. The diagram shows three main parts: the input layer, the transformer layers, and the output layer. The input layer consists of word embeddings, position embeddings, and segment embeddings. The transformer layers are made up of self-attention mechanisms and feedforward neural networks. The output layer contains a masking mechanism and a sequence representation. The diagram also includes an arrow that indicates the direction of information flow in the model.",
        "reference": "FIG. 1. Illustration of the inverse renormalization group. Inverse transformations are applied to iteratively increase the size L in each dimension by a factor of b, therefore evading the critical slowing down effect."
    },
    "2204.00034v1-Figure1-1.png": {
        "prediction": "The image depicts a flowchart that shows the use case of a system. The flowchart is divided into two main sections: \"Direct Call Relations\" and \"Indirect Call Relations.\" \n\nIn the \"Direct Call Relations\" section, there are three sub-sections labeled \"C1,\" \"C2,\" and \"C3.\" Each of these sub-sections contains a series of checkboxes with different labels such as \"PB follows A,\" \"Prob. Distribution over Vocabulary,\" and \"FNN+softmax.\" These checkboxes are connected by arrows, indicating a sequence or relationship between them.\n\nIn the \"Indirect Call Relations\" section, there are also three sub-sections labeled \"C1,\" \"C2,\" and \"C3.\" Similar to the \"Direct Call Relations\" section, this section contains a series of checkboxes with different labels. However, the specific labels in this section cannot be determined from the image.\n\nOverall, the flowchart seems to represent a complex system with multiple interconnected components, as indicated by the various checkboxes and their connections through arrows.",
        "reference": "Fig. 1. Schematic comparison of sequential proof-of-work (Bitcoin, left) and parallel proof-of-work (proposed, right). Boxes represent blocks, checkmarks represent proof-of-work solutions, and arrows represent hash-references."
    },
    "1508199-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The first step is to form an unsupervised affinity matrix, which is followed by the input label information. Then, the input is run through a PCA on individual clusters. After that, the spectral clustering is performed. Finally, the output layer representation is selected as the test point.",
        "reference": "Figure 2. Diagram of SUPERPAC algorithm for pairwise constrained clustering."
    },
    "2012.12683v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The diagram is divided into two sections: deployment network and components. The deployment network section contains various blocks such as KSPC, OSFC, and IMS. The components section consists of blocks like TPS, RODS, and QDQ. The arrows in the diagram represent the flow of data and information between these blocks. The diagram also includes labels such as \"use case u1\", \"direct call relations\", and \"indirect call patterns\". Overall, the diagram provides a visual representation of the flow of data and information within a system.",
        "reference": "Figure 1. Left: Network control architecture of SOXS. Right: Components of the SOXS software; red boxes represent software requiring custom configuration or development, green boxes represent VLTSW components that will be used without modifications."
    },
    "2103.07018v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of learning. The flowchart has three main sections: interleaving learning, block learning, and task management.\n\n1. Interleaving learning: In this section, there are three tasks (Task 1, Task 2, and Task 3) that are designed to be completed in a specific order. Each task is followed by a mask, which indicates where the next task should begin. This section emphasizes the importance of completing tasks in a specific sequence.\n2. Block learning: In this section, there are two tasks (Task 1 and Task 2) that are designed to be completed in any order. This section emphasizes the importance of flexibility in learning, as it allows for more control over the pace and order of learning.\n3. Task management: This section includes a list of tasks (Task 1, Task 2, and Task 3) that need to be completed. The tasks are organized in a vertical column, with each task being followed by a mask indicating where the next task should begin. This section emphasizes the importance of managing tasks effectively, ensuring that all tasks are completed in a timely manner.\n\nOverall, the flowchart highlights the importance of both interleaving and block learning, as well as effective task management, in the learning process.",
        "reference": "Figure 1: Comparison between interleaving learning and block learning. In interleaving learning, we perform task 1 for a short while, then move to task 2, then task 3. Afterwards, we move from task 3 back to task 1. This process iterates where each task is performed for a short time period before switching to another task. In contrast, in block learning, we perform task 1 to the very end, then move to task 2, and later task 3. Each task is performed for a long time period until it is completed. Once a task is finished, it will not be performed again."
    },
    "2107.02704v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that depicts the process of training a deep learning model. The flowchart begins with the input layer, which consists of a set of input embeddings. These input embeddings are then passed through a tissue parameter estimation process, which uses a tissue parameter estimation algorithm to estimate the parameters of the tissue. The output of this process is a set of echo time values.\n\nNext, the echo time values are passed through an echo time contrast process, which creates a 12-contrast, multi-echo, multi-echo acquisition from the ex vivo human brain. This process produces a set of flash MRI acquisitions, which are used as the input to a deep learning model.\n\nThe deep learning model is trained using a ground truth tissue parameter estimation process. This process uses a ground truth tissue parameter estimation algorithm to estimate the true parameters of the tissue. The model is trained to predict these true parameters from the input embeddings.\n\nAfter the model is trained, it is evaluated on a set of simulated training data. This evaluation process uses a scanner parameters process to simulate the data that would be produced by a real scanner. The model's performance is then measured using a set of metrics, such as the mean squared error between the predicted and true parameters.\n\nFinally, the trained model is used to predict the parameters of new tissue samples. This process uses a set of tissue parameter estimation processes to estimate the parameters of the new tissue samples. The predicted parameters are then used to generate a final input representation, which can be used for further analysis or processing.",
        "reference": "Figure 3: Training Data simulation steps. We estimated tissue parameters p from a large 12-contrast FLASH MRI acquisition containing multiple flip angles (FA) and echo time (TE) scanner parameters for 22 different ex vivo human brain specimens using a dictionary-based method [20]. Estimated tissue parameters were used as ground truth to simulate training data."
    },
    "2202.06076v1-Figure2-1.png": {
        "prediction": "The diagram shows a multi-layer transformer architecture. The input layer is composed of word tokens and image features, which are then passed through the encoder layers. These layers use self-attention mechanisms to capture long-range dependencies in the input. The output layer represents the final input representation, which is then passed through a feedforward neural network for classification or regression tasks. The diagram also includes an illustration of a chest X-ray, indicating that this model may be used for medical image analysis.",
        "reference": "Fig. 2: The overview of method. We extend a multi-layer transformer pre-trained on textual data with imaging input. The images are provided as features extracted from a ResNet50 network. The features are reshaped to 49 vectors of 2048 dimensions each and combined with two embeddings describing segment (image or text) and position of the token."
    },
    "2010.00150v1-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into two main parts: the upper part represents the use case, while the lower part shows the direct call relations. The diagram also includes an attention layer, which helps in focusing on the most important information. The diagram is designed to be helpful, honest, and harmless. It is a complex and detailed representation of a system's operation.",
        "reference": "Figure 3: Attentional Encoder-Decoder architecture with each supervision method shown."
    },
    "2205.00186v2-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that involves several components, such as CNN, GMM, and H-Aug. The system is designed to perform tasks like image recognition, labels assignment, and image retrieval. The diagram also shows the architecture of BERT in pretraining, which includes masked language modeling and next sentence prediction. Additionally, the diagram illustrates the process of pretraining BERT, where the model learns to predict masked words in a sentence and the next sentence in a pair of sentences.",
        "reference": "Figure 2: An overview of the proposed LC-Booster framework. We first warm up the model for a few iterations, and then fit a GMM to the loss distribution to separate clean or noisy sets. We then adopt H-Aug, which enforces consistency between weak-weak and weak-strong views. At the middle of training, we perform ReCo to revise the labels for all samples. The revised labels are used for GMM and CE Loss in the rest of training."
    },
    "211532678-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses case 1 and case 2. The system has two main input cases, case 1 and case 2. Case 1 is further divided into two subcases, direct call relations and indirect call relations. Case 2 also has two subcases, direct call patterns and indirect call patterns. The system processes the input cases and subcases in a specific order. First, it processes the input case 1 using the direct call relations subcase. If the input is not a direct call relation, the system processes it as an indirect call relation. Then, the system processes the input case 2 using the direct call patterns subcase. If the input is not a direct call pattern, the system processes it as an indirect call pattern. Finally, the system outputs the result based on the input case and subcase.",
        "reference": "Figure 1. Adversarial training framework of image reconstruction network f , jointly with another network G, generating the additive perturbations"
    },
    "14194315-Figure2-1.png": {
        "prediction": "The diagram shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the input and output videos, as well as the predicted labels and labels.",
        "reference": "Figure 2: Schematic representation of deep neural networks for supervised and unsupervised learning. We use pink boxes to denote convolutional layers, yellow boxes denote rectified linear unit layers and green boxes indicate normalization layers. Our technique combines unsupervised learning approaches (a) with labeled prediction (b) to predict gestures using massive amounts of unlabeled data and few labeled samples. Autoencoder (a) is used to initialize weights and then predictor (b) is fine tuned to predict labels."
    },
    "2207.06543v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that shows the process of training BERT. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the memory stability over old tasks, discrepancy between task distributions, and learning plasticity over new tasks.",
        "reference": "Fig. 3. Illustration of simultaneously promoting learning plasticity and memory stability in continual learning, where arrows represent the tightening process."
    },
    "2012.10294v4-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart for a machine learning model. The model is pre-trained using masked language modeling and next sentence prediction tasks. In the pre-training phase, the input to the model consists of a sequence of words (or tokens) and a segment label indicating whether the sequence belongs to the first or second sentence of a pair. The model learns to predict the masked tokens and the next sentence in the sequence. During fine-tuning, the model is trained on a specific task, such as question answering or sentiment analysis. The output layer of the model is a softmax function that maps the input embeddings to class probabilities. The arrows in the diagram represent the flow of data through the model during training and inference.",
        "reference": "Figure 1 Data flow chart and convolutional neural network structure."
    },
    "2103.12279v1-Figure2-1.png": {
        "prediction": "The diagram depicts a complex system with several components and their interactions. The main components are the Local Interpretable Layer (LIL), the Global Interpretable Layer (GIL), the Transformer Encoder, and the Transcoder. The Local Interpretable Layer is responsible for processing the input data, while the Global Interpretable Layer provides a global view of the data. The Transformer Encoder is used to encode the data, and the Transcoder is used to decode the output. The arrows in the diagram represent the flow of information between these components. This system is designed to process large amounts of data efficiently and provide meaningful insights.",
        "reference": "Figure 2: Model Architecture: Our architecture comprises a base encoder that encodes the input and its relative non-terminals. GIL then uses MIPS to retrieve the most influential concepts that globally explain the sample, while LIL computes a relevance score for each ntj that quantifies its relevance to predict the label. The model interpretability is enforced through regularization (example parse tree inspired from Zanzotto et al. (2020))."
    },
    "1189033-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that involves various components and processes. The system starts with the detection of networks, which are then followed by the detection of boxes and scores. The next step is the pretraining of BERT, which is an architecture for language understanding. The model's task in this stage is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings.\n\nAfter the pretraining, the system moves on to the online action stage, where the model performs the actual task. The final step in the process is the generation of the output layer, which represents the final result of the system.\n\nThroughout the system, there are several components and processes involved, such as the appearance of SSD, the fusion of FNN, and the detection of real-time optical flow. These components and processes work together to create a comprehensive and effective system for language understanding.",
        "reference": "Figure 2. At test time, the input to the framework is a sequence of RGB video frames (a). A real-time optical flow (OF) algorithm (b) [16] takes the consecutive RGB frames as input to produce flow images (d). As an option, (c) a more accurate optical flow algorithm [1] can be used (although not in real time). (e) RGB and OF images are fed to two separate SSD detection [22] networks (\u00a7 3.2). (f) Each network outputs a set of detection boxes along with their class-specific confidence scores (\u00a7 3.2). (g) Appearance and flow detections are fused (\u00a7 3.3). Finally (h), multiple action tubes are built up in an online fashion by associating current detections with partial tubes (\u00a7 3.4)."
    },
    "2207.05544v1-Figure5-1.png": {
        "prediction": "The diagram shows a flowchart of a system that includes a user, an Ackermann drive, wheel sensors, a laser sensor, and navigation. The user inputs data into the system through the Ackermann drive. The wheel sensors detect the position of the wheels, while the laser sensor detects the position of the laser. The data from these sensors is then processed by the navigation and mapping systems. The navigation system uses this data to determine the path the vehicle should take, while the mapping system creates a map of the environment. Finally, the system outputs the information to the user.",
        "reference": "Fig. 5: Robot navigation system overview"
    },
    "2204.12527v1-Figure1-1.png": {
        "prediction": "The diagram shows a generative model, which is a type of artificial intelligence. The model has two main components: a generator and a discriminator. The generator takes in a random vector as input and outputs a fake sample. The discriminator takes in both real and fake samples and outputs a probability that the sample is real. The generator and discriminator are trained together in an adversarial process, where the generator tries to generate samples that can fool the discriminator, and the discriminator tries to correctly identify real and fake samples. The generator is trained to minimize the loss function, which is the difference between the predicted probabilities from the discriminator and the true labels (real or fake). The discriminator is trained to maximize the loss function. The training process continues until the generator can generate samples that are indistinguishable from real samples by the discriminator.",
        "reference": "Fig. 1. Overview of our recommender system (CFWGAN-GP)"
    },
    "2204.00352v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a self-supervised learning and meta-learning system. The system consists of an encoder, classifier, and classifier. The encoder takes in the input data and encodes it into a fixed-length vector representation. The classifier then takes this vector and classifies it into one of several possible classes. The output of the classifier is then fed back into the system to be used as a target for the next iteration.\n\nThe system is trained on a set of tasks that are labeled with their corresponding targets. The training process involves optimizing the parameters of the system to minimize the loss function between the predicted targets and the true targets. The system is also trained on a set of meta-tasks, which are tasks that involve learning how to learn from a new task. The meta-tasks are used to fine-tune the system's parameters so that it can quickly adapt to new tasks.\n\nThe system is also equipped with a meta-learning component, which allows it to learn how to learn from a new task. The meta-learning component is used to optimize the system's parameters based on the current task and the available meta-tasks. This allows the system to quickly adapt to new tasks by leveraging the knowledge gained from previous tasks.\n\nOverall, the system is designed to be able to learn from a large number of tasks, and to be able to quickly adapt to new tasks by leveraging the knowledge gained from previous tasks.",
        "reference": "Figure 1: The pipeline of our methods. First, we will decide whether to initial the encoder with the pre-trained self-supervised learning model and fix the parameters or not. Then, we will train our model using meta-learning algorithms. The white module blocks represent the randomly initialized parameters and the blue encoder block refer to the model pre-trained from SSL."
    },
    "2110.08607v1-Figure4-1.png": {
        "prediction": "The image is a flowchart that shows the execution traces and temporal relations. It has two main sections, one for forward RNN and another for backward RNN. The forward RNN section contains a stack of encoders, followed by a forward RNNE, and then a final input representation. The backward RNN section also contains a stack of encoders, but it has a different structure with a backward RNNE and a final input representation. The diagram also includes a section on pretraining BERT, which involves masking certain tokens in the example sequence and predicting them from the representation vectors. This flowchart provides a detailed view of the process involved in executing RNNs and pretraining BERT models.",
        "reference": "Figure 4: Inference neural network for PgDMM: The approximate inference model qi\u03c6(z i 1:T |x1:T ) for zi1:T , where i stands for either phy or NN, since the structure of both inference models is the same, is derived using bidirectional recurrent neural networks (BRNNs). The BRNN admits x1:T as inputs and - through a series of hidden layers of the neural network, denoted by blue arrows - forms two sequences of deterministic hidden states hft and h b t . These express the flow of information in the forward (from left to right) and backward (from right to left) sense, respectively. Then hft and h b t , together with the previous latent state z i t\u22121, are fed into a further neural network NN i\u03c6, denoted by the red arrows. Finally, the inference network outputs two vectors \u00b5i\u03c6(z i t\u22121,x) and \u03a3 i \u03c6(z i t\u22121,x), which parameterize the mean and diagonal covariance of qi\u03c6(z i t|zit\u22121,x), as shown in Eq.(14a)."
    },
    "2011.14771v1-Figure6-1.png": {
        "prediction": "The diagram depicts a flowchart with various nodes and arrows. The flowchart is divided into three main sections: the use case, direct call relations, and indirect call patterns. The use case section has two sub-sections, \"use case u1\" and \"indirect call patterns (C1, C2)\". The direct call relations section also contains two sub-sections, \"direct call relations\" and \"indirect call patterns (C1, C2)\". The diagram shows a sequence of steps, represented by the nodes, and the flow of information or data, represented by the arrows connecting the nodes.",
        "reference": "Fig. 6. Schematic illustration of the silicon master cross-section with \ud835\udeff = 29.5\u25e6 as the blaze angle and \ud835\udf03 \u2248 70.5\u25e6 defined by the crystal structure of silicon. At a groove spacing of \ud835\udc51 / 160 nm, the flat-top regions have widths \ud835\udc64 ' 30 nm as a result of the etch undercut while the groove depth is \u210e \u2248 65 \u2212 70 nm by Eq. (3). Indicated by the circle, the indented portion of the etched topography cannot be described with a functional form for diffraction-efficiency analysis."
    },
    "1023605-Figure10-1.png": {
        "prediction": "The image depicts a flowchart of a relu activation function. The relu activation function is a common element in deep learning models. In this diagram, the relu function is shown to be applied to the input data, which is then passed through a 1x1 convolution layer (32) and a 3x3 convolution layer (32). The output of these layers is then passed through another 1x1 convolution layer (32) and a final 3x3 convolution layer (32). The arrows in the diagram show the flow of data through the different layers of the network.",
        "reference": "Figure 10. The schema for 35 \u00d7 35 grid (Inception-ResNet-A) module of Inception-ResNet-v1 network."
    },
    "1011245-Figure6-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is designed to learn from a large corpus of text, and it consists of several layers. The first layer is the input layer, which takes in the text as input. The second layer is the convolutional layer, where the text is processed to extract features. The third layer is the max-pooling layer, which reduces the dimensionality of the data. The fourth layer is the linear layer, which performs a linear transformation on the data. The fifth layer is the batch normalization layer, which normalizes the data. The sixth layer is the dropout layer, which randomly drops out some of the nodes to prevent overfitting. The seventh layer is the cross-entropy loss layer, which calculates the loss between the predicted and actual output. The eighth layer is the final representation layer, which represents the final output of the model.",
        "reference": "Figure 6: 2D object shape classification network architecture."
    },
    "2102.06872v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a program and test suite. The program and test suite are connected by an arrow, indicating that the test suite is used to test the program. The run program and coverage are also connected by an arrow, showing that the run program is used to execute the coverage. Additionally, the gen new configs and post processing are connected by an arrow, indicating that the post processing is used to generate new configurations. Lastly, the new trees and final interactions are connected by an arrow, showing that the final interactions are used to create new trees.",
        "reference": "Fig. 1: GenTree overview"
    },
    "2107.01496v2-Figure2-1.png": {
        "prediction": "The diagram depicts a complex system with several components and processes. The system is designed to employ the same detect strategy, as indicated by the text \"Employ the same detect strategy\" in the diagram. The system consists of various elements such as a detector, a strategy iterator, a strategy pool, an op agent, and a negotiation trace dataset. These elements are interconnected through arrows, indicating the flow of information or processes between them.\n\nThe system begins with a repeated negotiation process, which is followed by feature engineering and train modeling. The negotiation traces dataset is used to feed the corresponding model, which then feeds the recognition model set. The result of this process is fed back into the negotiation trace, creating a loop that continues throughout the system. This indicates that the system is designed to learn and improve over time based on the feedback it receives from the negotiation traces dataset.",
        "reference": "Fig. 2. The overall approach of strategy recognition."
    },
    "2110.14532v2-Figure2-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of pretraining BERT. The flowchart begins with the \"Fact checkers\" and ends with the \"Natural Language Interference.\" In between, there are three hoaxes: Hoax 1, Hoax 2, and Hoax 3. These hoaxes are sorted into two groups based on their similarity: Hoax 1 and Hoax 2, which have a high level of similarity, and Hoax 3, which has a low level of similarity. The flowchart also shows the semantic similarity and the top n related hoaxes. The final step in the process is the representation of the final input.",
        "reference": "Figure 2: Architecture for the evaluation of information pieces against hoaxes already identified by fact checkers. A first step allows to retrieve hoaxes that are semantically similar to the input text. In the second step, a Natural Language Inference model measures the degree of entailment against each hoax retrieved in step 1."
    },
    "2107.13826v1-Figure6-1.png": {
        "prediction": "The image is a flowchart that shows the process of using a case. The flowchart has two main sections, \"use case u1\" and \"indirect call patterns (C1, C2)\". The use case section contains several sub-sections such as \"direct call relations\", \"indirect call relations\", and \"probability distribution over vocabulary\". The indirect call patterns section also includes sub-sections like \"probability distribution over vocabulary\" and \"date-driven process model\". The flowchart also has various nodes and arrows connecting them, representing different parts of the process.",
        "reference": "Figure 6. Model overview: a) Flowchart of chlor-alkali process model, dashed controllers are removed from model and associated manipulated variables are used as input variables in sampling algorithm. b) Structure of used recurrent neural networks. Each output is modeled separately. Parameters N and O are determined in hyperparameter tuning."
    },
    "2110.11334v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that depicts the execution traces and temporal relations of a program. The flowchart is divided into different sections, each representing a specific aspect of the program's execution. These sections include \"use case u1\", \"direct call relations\", \"indirect call patterns (C1, C2)\", and \"direct call patterns (C1, C2)\". \n\nThe flowchart also includes various nodes, which represent different elements of the program's execution. These elements are represented by blue circles, with some of them labeled with text such as \"FNN+softmax\" and \"FNN+softmax\". The arrows in the flowchart indicate the flow of execution, showing the order in which the different elements are executed.\n\nOverall, the flowchart provides a visual representation of the program's execution, illustrating the sequence of events and the relationships between different parts of the program.",
        "reference": "Fig. 3: Timeline for representative methodologies of (a) anomaly detection & one-class novelty detection, details in Section 3, (b) multi-class novelty detection & open set recognition, details in Section 4, and (c) OOD detection, details in Section 5, under generalized OOD detection framework. Different colors indicate different categories of methodologies. Each method has its corresponding reference (inconspicuous white) in the lower right corner. We do not list outlier detection methods in this figure due to their limited number of works on computer vision in deep learning era."
    },
    "822804-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to learn the contextualized word embeddings, which are used to understand the meaning of words in their specific contexts. The architecture of BERT is shown, where the input layer represents the positional embeddings and the mask sequence embeddings. The output layer represents the final input representation. The model is pre-trained using the masked language modeling task and the next sentence prediction task. The diagram also shows the different types of transformations that can be applied to the input data, such as bilinear transformation, non-linear transformation, and cosine similarity element-wise subtraction. The model is trained on a large dataset, and the training process involves multiple steps, including soft attention, element-wise subtraction, and element-wise multiplication. Finally, the model is fine-tuned for a specific task, such as text classification or question answering.",
        "reference": "Figure 1: The left hand side is an overview of the model. The right hand side shows the details about the different comparison functions. The rectangles in dark represent parameters to be learned. \u00d7 represents matrix multiplication."
    },
    "1576593-Figure4-1.png": {
        "prediction": "The image depicts a flowchart of a machine learning model training process. The model is pre-trained on a large corpus of text data, and then fine-tuned on a smaller set of labeled data. The pre-training step involves masking some of the input tokens and predicting them using a language model. This is done to learn the contextual relationships between words in the text. The fine-tuning step involves training the model to perform a specific task, such as sentiment analysis or question answering. The output layer of the model is used to make predictions based on the input data. The model is trained using a combination of supervised and unsupervised training methods. The diagram also shows the use of an encoder-decoder architecture with an abstract sentence cache. The model is trained to generate an output sequence that represents the input sequence.",
        "reference": "Figure 4: An overview of our approach for utilizing abstract examples for data augmentation and model training."
    },
    "2011.02573v1-Figure8-1.png": {
        "prediction": "The diagram depicts a flowchart with several nodes and arrows. The nodes are labeled \"use case u1\", \"direct call relations\", \"indirect call relations\", and \"direct call patterns\". The arrows indicate the flow of information between these nodes. The flowchart appears to be related to software development, as it shows the relationships between different components of a system.",
        "reference": "Fig. 8 A general appraisal-emotion network with k appraisal variables and l emotion types."
    },
    "2011.02573v1-Figure7-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented by the top left corner, with the direct call relations and indirect call patterns depicted in the center. The bottom right corner shows the direct call patterns (C1, C2). The diagram also includes the appraisal variables, emotions, and other elements such as desire, joy, appreciation, and gratitude.",
        "reference": "Fig. 7 An weighted appraisal-emotion network showing many-to-many relationship between appraisal variables and emotions"
    },
    "213980-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT (Bidirectional Encoder Representations from Transformers). The model is pre-trained on a large corpus of unlabeled text. The architecture of the BERT model is shown in the lower part of the diagram.\n\nThe model's task is to predict the masked tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.\n\nThe diagram also shows the probability and one-hot label associated with each step of the process. The probability is represented by green circles, while the one-hot label is indicated by black circles. The image pool, noise, and pool2 are also shown in the diagram.",
        "reference": "Figure 3. Ladder network architectures Rasmus et al. (2015). :"
    },
    "2102.05963v1-Figure4-1.png": {
        "prediction": "The diagram depicts a flowchart of the BERT pretraining process. The architecture consists of two main parts: the Transformer-based encoder and the pretraining tasks. The Transformer-based encoder is composed of a stack of 12 layers, with each layer containing two sub-layers: a self-attention mechanism and a feedforward neural network (FNN). The pretraining tasks include masked language modeling (MLM) and next sentence prediction (NSP). MLM involves randomly masking some input tokens and predicting their original values from the context, while NSP involves predicting whether a given pair of sentences are consecutive in a document. The model parameters are optimized using the BRDF model, which is trained to predict the masked tokens in the MLM task and the next sentence in the NSP task. The output of the model is then passed through a CDF-1 layer, which is used for fine-tuning the model on specific downstream tasks.",
        "reference": "Figure 4: Scheme for quick computation of inverse CDF from an NBRDF: we train a network to map from latent NBRDF embeddings to importance sampling parameters of a chosen analytic BRDF model."
    },
    "2011.05452v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart with various nodes and arrows. The nodes are labeled with different case names, such as \"use case u1\", \"direct call relations\", \"indirect call relations (C1, C2)\", and \"direct call patterns (C1, C2)\". There are also nodes labeled with \"Hint\" and \"Output Layer\". The arrows in the diagram indicate connections between these nodes.",
        "reference": "Figure 2. Schematic of relation between parts of the AKLT state and the corresponding entanglement Hamiltonian. The entanglement Hamiltonian for a part of the state is similar to the Heisenberg Hamiltonian between the effective spin-1/2 particles at the boundaries of the blocks. The green line in Hint represents couplings between interblock effective spin-1/2 particles (J(2)E ), while the red lines represent the coupling between intra-block spins (J(1)E )."
    },
    "909872-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a use case for a system that involves direct and indirect call relations. The use case is represented by the top left corner of the diagram, with the direct call relations shown in the center and the indirect call relations shown in the right side. The diagram also includes a section on recursion, which is shown at the bottom.",
        "reference": "Fig. 2. (a) The flowchart of the sub-band reconstruction for image super-resolution. (b) A relaxed version of (a). Gi is set as the element-wise summation function. In this framework, only the MSE loss is used to constrain the recovery. (c) The deep network designed with the intuition of (b). Gi is the element-wise summation function and Fi is modeled by two layer convolutions."
    },
    "126819-Figure2-1.png": {
        "prediction": "The diagram depicts the DeepSurvey architecture. The architecture is composed of a stack of encoders, followed by a mask sequence prediction task and an original sequence prediction task. The encoders are designed to learn representations of input embeddings. The mask sequence prediction task involves predicting masked tokens from their representation vectors. The original sequence prediction task involves predicting the original sequence from its representation vector. The diagram also shows the use case of the architecture, which includes direct call relations, indirect call patterns, and call relations.",
        "reference": "Fig. 2 DeepSurvey architecture: ( ) shows the actual number of papers and the number of ideas and implementations."
    },
    "211678216-Figure8-1.png": {
        "prediction": "The image is a diagram that shows the process of training BERT. The first step is to obtain a large amount of unlabeled text data. This data is then used to pretrain the model using two tasks: masked language modeling and next sentence prediction. In the masked language modeling task, some words in the input sequence are randomly selected and replaced with a special token, which the model must predict. In the next sentence prediction task, the model is asked to predict whether two given sentences are consecutive or not.\n\nAfter pretraining, the model is fine-tuned on a specific task, such as question-answering or sentiment analysis. During fine-tuning, the pretrained weights of the model are adjusted to better perform the new task. Finally, the model is evaluated on a test set to measure its performance.",
        "reference": "Fig. 8. Structural design for the model of vanilla CNN"
    },
    "2012.00020v3-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of an algorithm. The flowchart is divided into two main sections: the initial state preparation (A) and the time evolution (B). The initial state preparation section consists of three sub-sections: turn-on interactions, interactions, and measurement (C). The time evolution section also has three sub-sections: turn-on interactions, interactions, and measurement (C). The arrows in the flowchart indicate the direction of the flow, starting from the initial state preparation (A) and moving to the time evolution (B). This flowchart likely represents a specific algorithm or process, with each sub-section containing different steps or operations that contribute to the overall function of the algorithm.",
        "reference": "FIG. 1. Overview of the general algorithm to quantum compute high energy scattering cross-sections, including the values of the bare couplings \u03bb and m for simulation time t. Initial state preparation is discussed in Section IV A, time evolution in Section IV B, and measurement of particle cross-sections in Section IV C. The choice of (renormalized) couplings \u03bb(t), m(t) is discussed in Section IV D."
    },
    "2012.00020v3-Figure3-1.png": {
        "prediction": "The image is a flowchart that illustrates the execution traces and temporal relations. The flowchart consists of several nodes, including \"use case u1\", \"direct call relations\", \"indirect call relations\", and \"direct call patterns\". The diagram also shows the relationship between these nodes through arrows. The flowchart seems to be related to software development or system design, as it depicts the flow of information and processes in a structured manner.",
        "reference": "FIG. 3. Overview of the time evolution scheme for one Trotter-Suzuki step \u03b4. Here S (discussed in Section IV B 2) and qFT (discussed in Section IV B 3) denote the squeezing operation and the quantum Fourier transformation, respectively."
    },
    "2011.06125v3-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The first step in the process is feature extraction, which involves taking a sequence of text and converting it into a set of features that can be used for further processing. This is followed by concatenation, where the extracted features are combined to form a single vector.\n\nNext, the model uses forecasting to predict the next word in a given sentence or the position of a masked token. This helps the model learn the context in which words appear and their relationships with other words in a sentence.\n\nAfter the forecasting step, the model uses the XGBest Model to make predictions about the next word in a given sequence or the position of a masked token. This step is followed by statistical data analysis, which involves analyzing the results of the previous steps to identify patterns and trends in the data.\n\nFinally, the output layer represents the final result of the BERT pretraining process. The model's performance is evaluated using statistical data, and the results are visualized on a chart.",
        "reference": "Fig. 2. Representation of our multimodal machine learning framework using the two data sources: statistical and reanalysis maps. During Step 1, we extract embeddings from the reanalysis maps. In particular, we use encoder-decoder architectures or tensor decomposition to obtain a one-dimensional representation. During Step 2, we concatenate the statistical data with the features extracted from the reanalysis maps. During Step 3, we train one XGBoost model for each of the prediction tasks: intensity in 24 h, latitude displacement in 24 h, and longitude displacement in 24 h."
    },
    "2202.10332v1-Figure2-1.png": {
        "prediction": "The diagram illustrates the process of executing traces and temporal relations. The data source is connected to the Azure Blob Storage, which is then linked to the Azure ML service. The consuming ports are shown on the left side of the diagram, while the Azure Kubernetes Service is located on the right. The Apigee API Gateway is in the center, with the Rest API and the Azure ML service connected to it. The diagram also shows the Azure ML service as a part of the process.",
        "reference": "Figure 2: Azure ML Deployment Solution Architecture"
    },
    "2202.09110v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The user input is processed by the bootstrapping set, which then passes it to the model. The model evaluates the user's input and sends the result back through the bootstrapping set. The inference on training set is also sent to the model for further processing. The new \"ground truth\" is then used to train the model. The iterative loop continues until the model is trained enough.",
        "reference": "Fig. 1. Flowchart outlining the developed framework."
    },
    "2101.08122v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of self-supervised pretraining and change detection. The flowchart has two main sections: \"Self supervised pretraining\" and \"Change detection.\" In the \"Self supervised pretraining\" section, there are three sub-sections: \"Image 1\", \"Image 2\", and \"Set of patches P\". The \"Image 1\" and \"Image 2\" sub-sections have an \"Image\" label, while the \"Set of patches P\" sub-section has a \"Set of patches\" label. The \"Change detection\" section also has three sub-sections: \"Pretrained CNN layers\", \"Classifier\", and \"Change map\". The \"Pretrained CNN layers\" sub-section has a \"Pretrained CNN layers\" label, the \"Classifier\" sub-section has a \"Classifier\" label, and the \"Change map\" sub-section has a \"Change map\" label. There is also a \"Layer selection\" label in the middle of the flowchart.",
        "reference": "Fig. 1: Overview of the methodology."
    },
    "2011.06236v3-Figure4-1.png": {
        "prediction": "The diagram shows a flowchart of a use case for a system that involves direct and indirect call relations. The system is designed to handle two types of calls: direct and indirect. Direct calls are represented by the green boxes, while indirect calls are represented by the pink boxes. The arrows in the flowchart show the flow of information between the different components of the system. The system also has a control input function, which is shown in the lower part of the diagram. Overall, the flowchart provides a visual representation of how the system processes and handles different types of calls.",
        "reference": "Fig. 4: Block diagram of the proposed adaptive force-based controller."
    },
    "2110.07525v2-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart with various nodes and arrows. The nodes are labeled \"use case u1\", \"direct call relations\", \"indirect call relations\", and \"direct call patterns\". The arrows in the diagram represent connections between these nodes, indicating relationships or dependencies between them.",
        "reference": "Fig. 1. ORAN architecture with distributed controllers located at CU and DU/RU, and intelligence controller RIC"
    },
    "2102.03357v1-Figure16-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to process text, and it consists of several components such as the logic library, synthesis configurations, filtered configurations, circuit features, and FUB profiles. The logic library contains the logic for the model, while the synthesis configurations are used to synthesize the model's features. The filtered configurations are used to filter out unnecessary information from the input data. The circuit features are used to extract important information from the input data, and the FUB profiles are used to represent the final output of the model. The model is trained on a dataset that includes both unique and non-unique examples. The training process involves testing the model's performance on a set of test data to ensure its accuracy and reliability. Once the model is trained, it can be used to process new text data and provide accurate predictions or classifications.",
        "reference": "Fig. 16. The flow of proposed method in [97]."
    },
    "2204.07810v1-Figure6-1.png": {
        "prediction": "The diagram depicts a machine learning model. The model is trained on a dataset, and the training process involves several steps. First, the data is preprocessed to create input embeddings. These embeddings are then fed into a Transformer-based architecture, which consists of an encoder and a decoder. The encoder processes the input embeddings and produces a hidden state representation. This hidden state is then passed through a feedforward neural network, which outputs a set of segment embeddings. The segment embeddings are then passed through another feedforward neural network, which outputs a set of word embeddings. These word embeddings are then fed into a language model, which predicts the next word in a sequence based on the previous words. The predicted word is then used to update the hidden state representation. This process is repeated for each input in the dataset until the model has been trained on all the data. Once the model is trained, it can be used to make predictions on new data by passing the input embeddings through the same architecture and using the language model to predict the next word in a sequence.",
        "reference": "Fig. 6 Model training and prediction flow chart"
    },
    "2204.07810v1-Figure3-1.png": {
        "prediction": "Line 1: Mean flow. Line 2: Turb. flow. Line 3: Converged turb. flow. Line 4: Final stress (i, u, w, p). Line 5: Final mean flow. Line 6: Final turb. flow. Line 7: Pretraining BERT. Architecture of BERT in pretraining. Assume that in the lowercased example sequence consisting of the segment pair \u2018he starts to speak. the nervous crowd is watch-ing him.\u2019 the tokens \u2018speak\u2019 and \u2018nervous\u2019 were sampled to be masked. \u2018speak\u2019 is replaced by the \u2018[MASK]\u2019 token and \u2018nervous\u2019 is replaced by the random token \u2018that\u2019. The model\u2019s task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Fig. 3 CFD solution process of the iterative framework"
    },
    "2110.03183v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a use case, with the text nodes and arrows as shown. The flowchart is divided into three main sections: Direct Call Relations, Indirect Call Relations, and Direct Call Patterns. \n\nIn the Direct Call Relations section, there are two sub-sections: C1 and C2. The C1 sub-section has 10 blue circles, while the C2 sub-section has 6 blue circles. These circles represent different call relations.\n\nMoving to the Indirect Call Relations section, it also has two sub-sections: C1 and C2. The C1 sub-section contains 10 green circles, and the C2 sub-section has 6 green circles. These green circles indicate different call relations.\n\nFinally, in the Direct Call Patterns section, there is only one sub-section, which contains 10 pink circles. These pink circles represent direct call patterns.\n\nOverall, this flowchart provides a visual representation of various call relations and patterns within a use case.",
        "reference": "Fig. 1. Figure capturing our proposed method and learning different codes for different representations of mel-spectogram"
    },
    "2202.12108v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses a network to process and analyze data. The system begins with an enhanced correlation coefficient maximization, which is used to filter out noise and enhance the quality of the data. The data is then passed through a homography matrix, which helps to align the data for further processing. The data is then sent to a warring function, where it is combined with other data from the same network. This data is then sent to a fusion RGB function, which combines the data from the different networks into a single image. The final output of the system is an encoder-decoder network, which processes the data and produces a final representation of the data.",
        "reference": "Fig. 2: Overview of the proposed fusion method."
    },
    "2101.04861v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented by the top left corner, with the direct call relations and indirect call patterns depicted in the middle and right side of the diagram respectively. The diagram also includes information on the output layer, stack of encoders, and final input representation. The module structure, lead, fundamental element, and magnetic T1 are also shown.",
        "reference": "FIG. 1. (a) Schematic illustration (top view) of the TE module made of a film of magnetic TI. A periodic array of small square hollows filled with two different dielectrics is introduced into the magnetic TI. Each hollow harbors gap-opened metallic surface states (yellow lines) and is connected in series by leads (black solid and dashed lines). Refrigeration mode is shown here. (b) Schematic geometry of the fundamental element (topological thermocouple) consisting of two connected hollows with different dielectrics (1 and 2), possessing the p- and n-types metallic surface states. d is the distance between the two adjoined surfaces. (c) Corresponding k-dependent surface band dispersions around the \u0393 point are depicted by blue lines in which \u00b5 denotes the chemical potential at equilibrium and USIA describes the structure inversion asymmetry (SIA) between the adjoined two surfaces due to band bending induced by the dielectrics."
    },
    "2107.12100v2-Figure3-1.png": {
        "prediction": "The image is a flowchart that depicts the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the training paths, network model, and path model. The Ground Truth Ranking and Predicted node sequences are also shown.",
        "reference": "Figure 3: Overview of our approach to predict in uential nodes and node sequences based on path data. We start from path data which we split into training and test sets. We learn three di erent models bases on the training data: (i) a network model containing all transitions from the training data, (ii) a multi-order generative model containing observed higher-order transitions up to a maximum order of K , which is determined by model selection, and (iii) a path model containing the full paths in the training set. Based on these models, we predict the in uence of node or node sequences according to a broad range of centrality measures. We compare the ranking of node sequences to the ground truth rankings obtained from the test paths using AUC-based evaluation."
    },
    "209217-Figure2-1.png": {
        "prediction": "The diagram depicts the architecture of BERT, a pre-trained language model. The model is designed to learn contextual representations of words and sentences by predicting missing words in masked sentences. The diagram also shows how the model is trained on a large corpus of text data. The model is then fine-tuned for specific natural language processing tasks such as question answering and sentiment analysis.",
        "reference": "Figure 2: Illustration of the mQA model architecture. We input an image and a question about the image (i.e. \u201cWhat is the cat doing?\u201d) to the model. The model is trained to generate the answer to the question (i.e. \u201cSitting on the umbrella\u201d). The weight matrix in the word embedding layers of the two LSTMs (one for the question and one for the answer) are shared. In addition, as in [25], this weight matrix is also shared, in a transposed manner, with the weight matrix in the Softmax layer. Different colors in the figure represent different components of the model. (Best viewed in color.)"
    },
    "1504097-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a system that processes text and generates summarized documents. The system takes in an input text document and preprocesses it using an argumentative summarizer. The preprocessed text is then passed through a constraints generator, which creates constraints for the summarization process. These constraints are used by the summarization generator to generate a summary of the preprocessed text. The summary is then passed through a Topos-base, which checks for coherence and consistency. If the summary passes the coherence and consistency checks, it is output as a summarized document. Otherwise, the summary is returned to the summarization generator for further processing.",
        "reference": "Fig. 2. ASDS Architecture"
    },
    "2010.15560v2-Figure9-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses case 1 and case 2. The system has two main input cases, use case 1 and use case 2. Use case 1 has two sub-cases, direct call relations and indirect call relations. Use case 2 also has two sub-cases, direct call patterns and indirect call patterns. The system processes the input through an operation called \"Input\" which is followed by another operation called \"Output\". The output layer consists of a stack of encoders and a final input representation. The system then processes the input through a series of operations such as \"Operation IV-3\", \"Operation IV-11\", and \"Operation IV-11\". These operations are followed by a masking operation and a sequence operation. Finally, the system processes the input through a series of operations including \"Operation IV-3\", \"Operation IV-11\", and \"Operation IV-11\".",
        "reference": "Fig. 9. The first best architecture."
    },
    "2202.01897v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that uses case 1 and case 2. The system has an input layer, a representation network, and an output layer. The representation network consists of three blocks, each with six layers. The first block is a feedforward neural network (FNN) with six layers. The second block is another FNN with six layers. The third block is a linear combination of the previous two blocks. The system also has a pretraining BERT architecture. The BERT model is trained to predict masked tokens in the input sequence. The system is designed to handle both direct call relations and indirect call relations. The output layer is used for the final input representation.",
        "reference": "Figure 2: The AtmoDist network used for learning the pretext task. Numbers after layer names indicate the number of filters / feature maps of an operation. The comparison network is only required during training and can be discarded afterwards."
    },
    "1045792-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT (Bidirectional Encoder Representations from Transformers). The model is pre-trained on large-scale unlabeled text data, which includes a stack of encoders and a final input representation layer. During pre-training, the model learns to predict masked words in a given sequence and next sentence pairs. This is done by masking certain words in the input and training the model to predict their original values. The architecture of BERT consists of a transformer-based feedforward neural network with multiple layers. The model's task is to predict the masked tokens from the input embeddings.",
        "reference": "Figure 3: Block diagram of the recurrent module of an LSTM network."
    },
    "2202.05262v2-Figure5-1.png": {
        "prediction": "The diagram shows a flowchart of a use case for a system that involves direct and indirect call relations. The use case is represented by the Eiffel Tower, which is located in the city of Paris. The system has two layers: Layer 1, which contains the input layer representation, and Layer 2, which contains the output layer representation. The system also has a stack of encoders, which are used to optimize the system's performance. The arrows in the diagram represent the flow of information through the system. The red arrow points to the Rome object, indicating that this is the final output of the system.",
        "reference": "Figure 5. The ROME method. To insert a fact (s, r, o\u2217) relating a subject s to an object o\u2217, a rank-one update \u039b(C\u22121k\u2217)T is applied to a single midlayer MLP projection, where (a) k\u2217 is chosen to select the last token of the subject name s, and (b) \u039b \u2208 RH is computed from v\u2217 that causes the prediction o\u2217 after text for (s, r)."
    },
    "2205.13038v2-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of the BERT pretraining process. The model is trained in two main ways: masked language modeling (MLM) and next sentence prediction (NSP). In MLM, some input tokens are randomly selected and replaced with a special [MASK] token. The model's task is to predict the original tokens from their [MASK] token representations. In NSP, the model is asked to predict whether a given pair of sentences is consecutive or not. This helps the model understand the context and meaning of words within a sentence. The diagram also shows how the model learns to represent words, segments, and positions in the input sequence. The model is trained on a large corpus of text data, which allows it to learn a wide range of linguistic patterns and relationships.",
        "reference": "Figure 1. Overview of our proposed subgraph augmentation approach. The two subgraphs in the original graph are colored in gree and orange. We first generate multi-subgraph views via stochastic augmentation. Following that we connect the augmented subgraph to the remaining part of the original graph, by adding edges that link the augmented subgraph and the whole graph. After feeding forward the whole graph into subgraph-specific GNNs, we extract the subgraph embeddings of different views, respectively (triangles and squares). Ultimately, we fuse the embeddings of different views by a pooling function and obtain the augmented subgraph embeddings (diamonds)."
    },
    "1872130-Figure1-1.png": {
        "prediction": "The diagram shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows the input images, input embeddings, and output layers. The class labels are used to calculate different errors for the three CNNs. The forward flow and backward flow are shown, along with the saliency maps and input images. The output layer is shown as well.",
        "reference": "Figure 1. The proposed method to generate the object-specific saliency maps directly from DCNNs."
    },
    "2204.06981v1-Figure8-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented by the top left corner, with the direct call relations depicted in the middle section. The indirect call patterns are shown in the bottom right corner. The diagram also includes information about the output layer, stack of encoders, and final input representation.",
        "reference": "Figure 8: A 2D diagram of the integration region. The compact binary is the wiggly curve between (0, \u03b70) and (0, \u03b7coal), the merger takes place at (0, \u03b7coal), the merger pulse arrives at the detector at (r, \u03b7f ), GW emitted by the binary moves along the light cone and emits at (r \u2032, \u03b7\u2032) a subluminal tail pulse which arrives at the detector at r at the time \u03b7 > \u03b7f after the merger pulse. GW in the two shaded regions can also source a tail pulse to (r, \u03b7). Similar post-merger first order tail radiation (emission of dashed line from the wiggly binary curve) would also be possible but is canceled by the dynamics of the process (subsection 3.5)."
    },
    "2012.03418v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart for a language model. The process starts with the SQL query, which is followed by the VBZ (verb in third person singular present). The language model then identifies a pattern and uses it to generate a candidate noun. The candidate noun is then used as input for the querying VBG (verb in gerund or present participle) to generate another candidate noun. This process continues until a final input representation is reached. The final representation is then passed through a mask sequence to generate a sequence of original word embeddings.",
        "reference": "Fig. 1. An example of a hypernym-hyponym pair in a definition: \u201csql is a language for querying databases\u201d. The definition is split into units (words and the corresponding PoS) for analysis. The word \u201clanguage\u201d and \u201cdatabases\u201d are two hypernym candidates. The PoS elements surround \u201clanguage\u201d and \u201cdatabases\u201d are different. Our model learns such features and identifies \u201clanguage\u201d as the hypernym of \u201csql\u201d."
    },
    "2107.10658v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that illustrates the architecture of BERT, a pre-trained language model. The flowchart shows the different components and their connections in the BERT model. BERT stands for Bidirectional Encoder Representations from Transformers. The model is designed to understand the context of words by using deep learning methods. It consists of an encoder, which is a transformer-based architecture, and a pre-training task. The pre-training task involves masking some of the input tokens and predicting them based on the context. This process helps the model learn the relationships between words and their context. The output layer of the BERT model is used for various natural language processing tasks such as question answering, text classification, and named entity recognition.",
        "reference": "Figure 2: Cloud architecture for Synchronous TTS."
    },
    "2102.11262v5-Figure2-1.png": {
        "prediction": "The diagram depicts a complex system with various components and their interactions. The system is divided into two main sections: the input image and the output layer. The input image is processed by a ResNet, which is a convolutional neural network (CNN) that learns to recognize features in the image. These features are then passed through a late fusion stage, where they are combined to make a final prediction.\n\nThe output layer of the system consists of a shape regularizer, which helps to ensure that the predicted shapes are well-formed and not too similar to each other. The system also has an early fusion stage, which combines the features from the ResNet and the late fusion stage before making a final prediction.\n\nThe system is designed to be robust to variations in the input data, as it can handle different types of images and variations in the features extracted from them. It is also able to learn from both labeled and unlabeled data, which makes it more versatile and capable of handling a wide range of tasks.",
        "reference": "Fig. 2: Architecture of the proposed Adversarial Shape Learning Network (ASLNet) for building extraction. We designed an explicit shape regularizer to model the shape features, and a shape discriminator to guide the segmentation network."
    },
    "2107.12979v3-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a use case for a system that involves direct and indirect call relations. The use case is represented by the top left corner of the diagram, with the direct and indirect call patterns being shown in the bottom right corner. The flowchart also includes information about the input layer, output layer, and the stack of encoders. The arrows in the diagram represent the flow of information through the system, starting from the input layer and ending at the output layer.",
        "reference": "Figure 2: The canonical microcircuit proposed by Bastos et al mapped onto the laminar connectivity of a cortical region (which comprises 6 layers). Here, for simplicity, we group layers L2 and L3 together into a broad \u2018superficial\u2019 layer and L5 and L6 together into a \u2018deep\u2019 layer. We ignore L1 entirely since there are few neurons there and they are not involved in the Bastos microcircuit. Bold lines are included in the canonoical microcircuit of Bastos et al. Dashed lines are connections which are known to exist in the cortex which are not explained by the model. Red text denotes the values which are computed in each part of the canonical microcircuit"
    },
    "2102.08893v1-Figure4-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model. The model is designed to predict the next word in a given sequence, such as a sentence or a document. The process starts with an input layer, where the text is fed into the model. The text is then passed through an encoder, which converts the text into a fixed-length vector representation. This vector representation is then passed through a transformer, which processes the information and generates a new vector representation. Finally, the output layer takes the transformed vector and predicts the next word in the sequence.\n\nThe diagram also shows the architecture of BERT (Bidirectional Encoder Representations from Transformers) in pretraining. BERT is a pre-trained language model that can be fine-tuned for various natural language processing tasks. In pretraining, BERT is trained on large amounts of unlabeled text data using a masked language modeling task. The model's task is to predict the masked tokens in the input sequence. This pretraining step allows the model to learn general language patterns and improve its performance on downstream tasks.",
        "reference": "Fig. 4. Architecture of the training phase [2]"
    },
    "2202.13313v3-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to search for a specific word or phrase in a given text, and then evaluate the context in which that word or phrase appears. The controller sampling process is used to identify the most relevant information from the text. The controller RNn (LSTM) is responsible for determining the reward based on the accuracy of the search results. The reward is calculated using a combination of relu, elu, and swish values. The output layer represents the final result of the model's evaluation.",
        "reference": "Fig. 2. Searching process of neural architecture."
    },
    "207468-Figure3-1.png": {
        "prediction": "The diagram depicts a complex network of nodes and arrows, representing various elements and their relationships. The nodes are labeled with different terms such as \"use case\", \"direct call relations\", \"indirect call patterns\", \"direct call patterns\", \"encode-sentence\", \"encode-word\", \"encode-sentence\", and \"encode-word\". The arrows connect these nodes, indicating the flow of information or the relationship between them. The diagram is quite intricate, suggesting a high level of complexity in the system it represents. It could be a representation of a software architecture, a data processing system, or any other complex system where multiple elements interact in a non-linear way.",
        "reference": "Figure 3: Hierarchical Sequence to Sequence Model with Attention."
    },
    "2102.12816v3-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a use case, with the text nodes and arrows as shown. The use case is labeled \"use case u1\" and is connected to two other cases, \"direct call relations\" and \"indirect call relations.\" The diagram also includes several other elements such as \"Trace 1,\" \"Trace 2,\" \"Prob. Distribution over Vocabulary,\" \"Prob. Distribution over Vocabulary,\" and \"Output Layer.\" The diagram appears to be related to machine learning or data processing, as it shows how information flows through different components in a system.",
        "reference": "Fig. 1. An illustration of the relationship among T , Ti and delay in slot m. In this example, Pi,m \u2212 ni,m = 2 and tm+1 \u2212 \u03c4(tm+1) = 4, which satisfies (12)."
    },
    "90264-Figure5-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses BERT, a pre-trained language model. The system takes in an input video and processes it to extract appearance information and motion information. This information is then used to generate a pooling feature vector. The system also uses a fusion method to combine the appearance and motion features. The final output layer represents the positional embeddings, which are used to predict the next word in a sequence. The system is designed to be able to run on different hardware architectures, such as CPUs, GPUs, and TPUs.",
        "reference": "Fig. 5: Illustration of late-fusion. We encode, temporally pool the appearance and motion representation separately and fuse the two probability distributions."
    },
    "2102.10461v1-Figure4-1.png": {
        "prediction": "The image is a flowchart that shows the execution traces and temporal relations of a program. The flowchart is divided into three main sections: the use case, direct call relations, and indirect call patterns. The use case section contains two sub-sections: trace 1 and trace 2. The direct call relations section contains two sub-sections: C1 and C2. The indirect call patterns section also contains two sub-sections: C1 and C2. Each sub-section has its own set of nodes and arrows, which represent different parts of the program's execution.",
        "reference": "Figure 4: CelebA architecture for the injective portion g of TRUMPET. The input size to each layer is written below it."
    },
    "2110.06804v3-Figure4-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT, a pre-trained contextual language model. The model is designed to understand the context of words and phrases in a sentence by predicting missing words or replacing certain words with random tokens. The input layer consists of word embeddings, which are then passed through a series of transformer blocks. These blocks consist of self-attention layers, which help the model understand the relationships between different words within a sentence. The output layer contains a hidden layer, which represents the final output representation of the input. The model is trained on a large corpus of text data, allowing it to learn from a wide range of examples and improve its performance over time.",
        "reference": "Figure 4. An artificial neural network"
    },
    "1339502-Figure4-1.png": {
        "prediction": "The diagram depicts a complex flowchart with multiple nodes and arrows. The flowchart is divided into two main sections: the Classification Task and the Localization Task. The Classification Task has several Convolutional Units, Pooling layers, and Localization Tasks. The Localization Task also contains Convolutional Units, Pooling layers, and Localization Losses. The diagram appears to be related to image processing or computer vision, as it shows how data moves through different layers and tasks in a convolutional neural network.",
        "reference": "Figure 4. Structure of the network. Left: Detailed components of the convolutional feature extraction and multi-level feature fusion. The \u201cConvUnit(w, h, n)\u201d represents a convolutional layer of n w\u00d7 h kernels, connected by a batch normalization layer and a ReLU layer. The \u201cUpSample(n)\u201d represents a deconvolution layer of n\u00d7 n kernels with stride n. Right: The design of multi-task module. \u201cScale&Shift\u201d is used to stretch and translate the values."
    },
    "1639981-Figure3-1.png": {
        "prediction": "The image depicts a flowchart that illustrates the use of case 1 and 2. The flowchart is divided into two main sections: the left side, which shows the execution traces and temporal relations, and the right side, which displays the direct call patterns. The left side of the flowchart features a stack of encoders, a final input representation, and an output layer. The right side of the flowchart includes a stack of encoders, a final input representation, and a positional embeddings section. The flowchart also shows various nodes such as trace 1, trace 2, and P (B follows A). Additionally, there are arrows connecting these nodes, indicating the flow of information or data in the system. Overall, the flowchart provides a visual representation of the process, structure, and relationships within the system being described.",
        "reference": "Figure 3: Left: Columnar architecture in a fully connected network, with the path through one column highlighted. Each column corresponds to a different \u03b1j . Right: Columnar architecture in a convolutional network. In this setting the w\u03b1\u2019s take linear combinations of the feature maps obtained by convolving the input with the dictionary. We make the same abuse of notation here as in the main text\u2014the vectorized filter banks must be reshaped before the convolution takes place."
    },
    "2204.05103v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of an execution trace and temporal relations. The flowchart is divided into two main parts, the left side showing the execution traces and the right side showing the temporal relations. The top part of the flowchart shows the use case \"use case u1\" with its associated \"indirect call patterns (C1, C2)\". The bottom part of the flowchart shows the \"direct call patterns (C1, C2)\". The flowchart also includes various nodes such as \"Trace 1\", \"Trace 2\", \"Direct Call Relations\", \"Indirect Call Relations\", \"Input Encoder (id-cnn)\", and \"Output Layer\". The arrows in the flowchart represent the connections between these nodes. This flowchart seems to be related to a software or system design, possibly for a machine learning model.",
        "reference": "Figure 1: Our approach with self supervised learning based on a Transformer (a) and fine-tuning strategy for learning the final emotion predictor (b)."
    },
    "2012.03152v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The first step is feature construction, which involves converting text into numerical features using techniques like word embeddings and position embeddings. The next step is classification, where an automated method is used to select a training set from the data. The selected set is then classified using a supervised learning method. After classification, accuracy analysis is performed to evaluate the performance of the model. Finally, the results are visualized, and the model is built with standard results.",
        "reference": "Fig. 2. Flowchart of experiment."
    },
    "2103.11568v4-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is designed to learn from training data and use that knowledge to make predictions on new data. The process starts with the input layer, where the raw data is fed into the model. This data is then passed through an encoder, which transforms the data into a format that can be processed by the model.\n\nNext, the data is passed through a clustering module, which groups similar data points together. The model then uses this clustering information to assign labels to the data points. These labels are used to train the model so that it can make accurate predictions on new data.\n\nThe model is then updated using a query image, which helps to refine its predictions. The updated model is then used to make predictions on new data, and the process continues in a loop until the model reaches a satisfactory level of accuracy.\n\nOverall, the diagram illustrates how a machine learning model works, from the initial input to the final output. It also shows the different components of the model, such as the encoder, clustering module, and query image, and how they work together to make accurate predictions.",
        "reference": "Fig. 2: The system pipeline of our unsupervised person re-ID method. The upper branch depicts the memory initialization stage. The training data features are assigned pseudo labels by clustering, where features of the same color belong to the same cluster. The lower branch represents the model training stage. Query features in iterative mini-batch are used to update the memory cluster representations with a momentum. The ClusterNCE loss computes the contrastive loss between query features and all cluster representations."
    },
    "2103.10685v2-Figure3-1.png": {
        "prediction": "The diagram shows the architecture of BERT in pretraining. The input to the model is a sequence of tokens, and each token is represented as an embedding vector. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The pretraining task is called next sentence prediction, where the model is trained to predict whether two sentences are consecutive or not. The log likelihood score for this task is 23.48.",
        "reference": "Fig. 3. Language model generation and language model inverse prompting scoring for generating a poem sentence."
    },
    "2012.07082v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that illustrates the pretraining of BERT. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The architecture consists of three sampled games, sampled game 1, sampled game 2, and sampled game 3. The pretraining process involves masking some of the input tokens, replacing them with a special token, and then training the model to predict the original tokens from their representation vectors. The backtracking process is also involved in this pretraining process.",
        "reference": "Figure 3: Illustration of the sampled games generated by modified SGM during its execution."
    },
    "2107.06179v2-Figure1-1.png": {
        "prediction": "The diagram shows a heart and an artery. The heart is shown with its four chambers, including the left atrium, right atrium, left ventricle, and right ventricle. The artery is shown with its three layers: tunica adventitia, tunica media, and tunica intima. The diagram also includes labels for the blood clot, coronary arteries, and cholesterol plaque.",
        "reference": "Fig. 1. Illustration of myocardial infarct caused by acute thrombus in the culprit coronary artery (see text)."
    },
    "2102.03347v1-Figure2-1.png": {
        "prediction": "The diagram illustrates the process of an attack on a bot, which is then detected by an attacker EOA and reported to a bot contract. The diagram shows the following:\n\n1. An attacker initiates an attack on a bot.\n2. The bot is compromised, and the bot contract is notified.\n3. The bot contract reports the attack to the attacker EOA.\n4. The attacker EOA detects the attack and reports it to the bot contract.\n\nThis process is shown in the diagram with arrows indicating the flow of information between the different components. The diagram also includes icons representing the different types of entities involved, such as the attacker, bot, bot contract, and attacker EOA.",
        "reference": "Figure 2: Attacker model with on-chain and off-chain parts."
    },
    "2102.06702v1-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented as a rectangle, while the direct and indirect call relations are shown with arrows. The diagram also includes the input layer, output layer, stack of encoders, and final input representation. The architecture of BERT in pretraining is also depicted, where masked tokens are replaced by the '[MASK]' token and random tokens. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. Additionally, there is a section on the mask sequence, which starts with the'mask' token and ends with the'sep' token.",
        "reference": "Fig. 3. Schematic illustration of the PCD. The gray box indicates the domain of the unit-cell model. Note that, in practice, the size of the THz antenna attached to the electrodes is much larger than the gap distance between the electrodes."
    },
    "2101.00048v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The diagram is divided into three main sections: Direct Call Relations, Indirect Call Patterns, and Direct Call Patterns. The Direct Call Relations section has two sub-sections, C1 and C2. The Indirect Call Patterns section also has two sub-sections, C1 and C2. The Direct Call Patterns section has one sub-section, C1. The diagram is designed to be helpful, honest, and harmless. It is a multi-modality conversational language model that can understand and communicate fluently in English and Chinese.",
        "reference": "Figure 1. Schematic diagram of the Slimfield microscope. a) Laser light vertically polarized at source (blue) passes through a shutter (S) and is expanded 3x by the lens pair L1 and L2, before passing through L3 which forms a telescope with the objective lens to ensure light at the sample is collimated. Finally, in the excitation path the laser light passes through a wave plate (WP) on a rotational mount, either \u03bb/2 for linear polarization or \u03bb/4 for circular polarization. The light is directed to the sample with a dichroic mirror which allows the collected fluorescence (green) to pass through it. The emission then encounters the lens L4 and is focused at the side port of the"
    },
    "2010.09890v1-Figure16-1.png": {
        "prediction": "The diagram depicts a flowchart of a system, with several components and their connections. The main components are the State Encoder, Transformer, LSTM, and MLP. The State Encoder is connected to the Transformer, which is then connected to the LSTM. The MLP is connected to the Transformer, and there is also a connection from the LSTM to the MLP. There is an arrow pointing from the State Encoder to the Transformer, indicating that the state encoder provides input to the transformer. Additionally, there is an arrow pointing from the Transformer to the LSTM, indicating that the transformer provides input to the LSTM. Finally, there is an arrow pointing from the LSTM to the MLP, indicating that the LSTM provides input to the MLP.",
        "reference": "Figure 16: Network architecture of the goal inference model, which encodes the symbolic state sequence in demonstrations and infers the count for each predicate."
    },
    "2110.07150v3-Figure1-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of using BERT for question-answering in multiple languages. The flowchart starts with \"Question in Language U\" and ends with \"Answer in Language U\". In between, there are several steps such as \"Translate to L languages (e.g., English, Bengali, Russian)\", \"Monolingual Document Retriever\", \"Monolingual Answer Selection\", \"Aggregate Candidates\", \"Cross-lingual GenQA\", and \"Answer in Language U\". The flowchart also shows a connection to \"Wikipedia Multiple Languages\" in the middle. This diagram represents how BERT can be used to answer questions in different languages by translating them, retrieving relevant documents, selecting answers, and then providing the final answer in the original language.",
        "reference": "Figure 1: Illustration of our proposed Cross-Lingual, Retrieval-based GENQA pipeline."
    },
    "2101.11878v3-Figure1-1.png": {
        "prediction": "The diagram shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also shows how components, spatial location, and map dictionary are used in the knowledge base.",
        "reference": "Figure 1: Intuitive illustration of how our model acquires knowledge during meta-learning. In particular, it learns a dictionary of component representations, which resemble individual object components. Some of these can be shared across different classes, e.g., car tires. In addition, it learns a map dictionary that contains common spatial activation patterns of components. During meta-testing, the knowledge base facilitates the learning of novel classes by re-using the already learned components and spatial activation patterns."
    },
    "2102.04335v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The process starts with the Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) tasks. In MLM, some tokens are replaced by the '[MASK]' token, and the model's task is to predict these masked tokens. In NSP, the model is trained to predict whether two sentences are consecutive in the original text or not. The model then goes through the pretraining stage, where it learns to understand the context of the input data. During the fine-tuning stage, the model is trained on a specific task, such as question-answering or sentiment analysis. Finally, the model is evaluated using a set of validation data to measure its performance.",
        "reference": "Figure 1. Research empirical model"
    },
    "2011.08706v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into two main sections: the upper section represents the use case, and the lower section shows the direct call patterns. The flowchart also includes various elements such as the input layer, output layer, stack of encoders, feature enhancement with attention, classification, and regression. The diagram provides a visual representation of the process and its components, which can be helpful in understanding the flow of information and the relationships between different parts of the system.",
        "reference": "Fig. 2. The framework of our proposed FPAENet method. ResNet-50 as the backbone to extract features. Two top-down channels are added in the FPN, and feature enhancement with attention is placed on the horizontal connection to enhance the effective information. Next, two parallel Fully Convolutional Networks to classify whether the candidate area is a lesion and locate the lesion."
    },
    "2012.13965v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the Jacobian-based iteration process. The process starts with the input layer, where the input embeddings are represented. These embeddings are then passed through the stack of encoders, which perform the encoding process. The output of the encoders is then passed through the final input representation layer. The Jacobian-based iteration is applied to the final input representation, and the difference between the current and previous representations is calculated. This difference is then used to update the input embeddings. The updated embeddings are then passed through the stack of encoders again, and the process is repeated until convergence.",
        "reference": "Fig. 1: Pipeline of our method for computing IK on general soft robots by using three learned networks \u2013 1) forward kinematics Nfk, 2) Jacobian NJ and 3) sim-to-real mapping Ns2r . According to the current configurations of robot\u2019s actuation ci, the actuation ci+1 for reaching a target position (or shape) pi+1 is computed by the Jacobian-based iteration."
    },
    "1165810-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into four sections, with each section containing different elements. The first section contains the use case, which is represented by a rectangle. The second section depicts the direct call relations, which are also represented by rectangles. The third section shows the indirect call patterns, which are represented by squares. The fourth section displays the direct call patterns, which are shown in a similar manner to the second section.\n\nThe flowchart is designed to be helpful for understanding the relationships between the different components of the system. It provides a clear and concise visual representation of how the various parts of the system interact with each other. This can help users to better understand the system's behavior and make more informed decisions about how to use it.",
        "reference": "Figure 3: Eccentricity-dependent model: Inverted pyramid with sampling points. Each circle represents a filter with its respective receptive field. For simplicity, the model is shown with 3 scales."
    },
    "2011.07995v3-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The flowchart is divided into two main sections: the upper section represents the use case, while the lower section represents the direct call patterns. The upper section includes various components such as trace 1, trace 2, and the use case. The lower section contains the direct call patterns. The flowchart also includes several nodes, including C1, C2, and C3. The diagram is designed to be helpful, honest, and harmless. It is an example of a multi-modality conversational language model that is developed by Shanghai AI Laboratory.",
        "reference": "Figure 1: Patient flowchart. BI-RADS = Breast Imaging-Reporting and Data System; AD = architectural distortion; LCC = left craniocaudal; RCC = right craniocaudal; LMLO = left mediolateral oblique; RMLO = right mediolateral oblique."
    },
    "2103.09966v1-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is pre-trained using masked language modeling and next sentence prediction tasks. In the masked language modeling task, some words in a sentence are replaced with a special token, and the model's goal is to predict the original word from its representation. In the next sentence prediction task, the model is trained to predict whether two given sentences are consecutive or not.\n\nThe architecture of the BERT model is also shown in the diagram. BERT stands for Bidirectional Encoder Representations from Transformers. It is a pre-trained deep bidirectional transformer-based language model that can be fine-tuned for various NLP tasks such as question-answering and sentiment analysis.",
        "reference": "Fig. 3: Single-line diagram of the test system."
    },
    "2102.10535v1-Figure4-1.png": {
        "prediction": "The diagram depicts a flowchart of a use case, with several nodes and arrows indicating the flow of information. The flowchart is divided into two main sections: \"Direct Call Relations\" and \"Indirect Call Patterns.\" The \"Direct Call Relations\" section contains three sub-sections: \"C1,\" \"C2,\" and \"C3.\" The \"Indirect Call Patterns\" section also has three sub-sections: \"C1,\" \"C2,\" and \"C3.\"\n\nThe flowchart begins with a node labeled \"use case u1\" and ends with a node labeled \"output layer.\" The flow of information is represented by arrows connecting the nodes, with some arrows pointing to specific sub-sections within the \"Direct Call Relations\" and \"Indirect Call Patterns\" sections.\n\nThe diagram also includes a section titled \"Pretraining BERT\" and another section titled \"Architecture of BERT in pretraining.\" These sections provide additional context and information about the flowchart's purpose and function.",
        "reference": "Figure 4. General CodeSearchNet architecture for all of our baselines. Each language is processed through different encoder mechanisms. The query encoder is shared (an NLP encoder), and the purpose of the CodeSearchNet tasks is to retrieve the most relevant code snippets subject to the natural language query."
    },
    "1087044-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of executing traces and temporal relations. The flowchart is divided into three sections: pretraining BERT, geometric labeling, and 3D reconstruction. In the pretraining BERT section, the architecture of BERT in pretraining is shown. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The geometric labeling section shows how geometric labels are used for layering, supporting, and siding. Finally, the 3D reconstruction section demonstrates how layers are used in 3D reconstruction.",
        "reference": "Figure 1: An illustration of our geometric scene parsing. Our task aims to predict the pixel-wise geometric surface labeling (first column) and the interaction relations between main regions (second column). Then the parsing result is applied to reconstruct a 3D model (third column)."
    },
    "2210.00705v2-Figure2-1.png": {
        "prediction": "The diagram shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The Transformer Encoder consists of a stack of identical layers. Each layer has two sub-layers: a self-attention mechanism and a feedforward neural network (FNN). The output of the last layer of the Transformer Encoder is used as the final representation of the input sequence.",
        "reference": "Fig. 2: An illustration of SpeechCLIP models. (a) A pre-trained HuBERT [12] extracts audio features. The features are concatenated with a learnable CLS token and fed into a transformer encoder layer to obtain a single vector representing the information of the entire sequence. The vector is then used to compute contrastive loss with the CLIP image encoder\u2019s output [23]. (b) Cascaded SpeechCLIP uses K CLS tokens to capture a small sequence of keywords from the audio signal. The keywords are batch-normalized and vector-quantized before passing to the CLIP text encoder. BN and VQ respectively denote batch normalization and vector quantization."
    },
    "2107.12847v1-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The architecture of BERT in pretraining is shown, with the model's task being to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes images of people and text nodes, such as \"use case u1\" and \"direct call relations.\" The arrows in the diagram represent the flow of information, with one arrow pointing to \"shape\" and another to \"camera.\" There are also two images of a person walking, one on the left and one on the right side of the diagram.",
        "reference": "Figure 3: The proposed local recurrent modeling approach to human mesh recovery."
    },
    "2101.06808v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT. The model is pre-trained in two phases: the global phase and the local phase. In the global phase, the model learns to predict masked tokens from their context using a feedforward neural network (FNN). The local phase involves updating the DoE based on the global phase's output. If the update is successful, the model moves to the next sentence; if not, it returns to the current sentence and updates the DoE again. The process continues until all sentences have been processed.",
        "reference": "Figure 1: An overview of the TREGO framework (detailed in Algorithm 1)."
    },
    "2202.00528v2-Figure1-1.png": {
        "prediction": "The diagram shows a feed-forward sublayer with self-attention and layer-wise encoding. The feed-forward sublayer is a part of the Transformer model, which is used in natural language processing tasks such as language translation and text summarization. The self-attention mechanism allows the model to focus on different parts of the input sequence when generating an output. The layer-wise encoding helps the model learn more complex representations of the input by progressively increasing the size of the hidden states. This architecture has been shown to be effective in many NLP tasks, including language modeling, machine translation, and text classification.",
        "reference": "Figure 1: Illustration for translation-oriented language models. X and Y denote source and target input, respectively. To enable translation, we adapt the LM self-attention mask to either the PrefixLM mask or CausalLM mask (top right), where filled black circles indicate disallowed attention. We also explore top-only encoding (Top Encoding) for PrefixLM which feeds the final-layer source encodings to generation similar to EncDec, rather than layer-wise coordinated encodings (He et al., 2018). Masks of EncDec are shown in the bottom right for comparison."
    },
    "2107.05802v2-Figure4-1.png": {
        "prediction": "The image is a flowchart that depicts the use case of a system. The flowchart is divided into two main sections, with the top section showing the direct call relations and indirect call relations. The bottom section shows the direct call patterns and indirect call patterns. The flowchart also includes various nodes such as Trace1, Trace2, and use case u1. Arrows are used to represent the flow of information or data between these nodes.",
        "reference": "Figure 4: Left panel: An illustration of measuring the width of a set S (in green) in a direction g\u0302 by identifying x,y \u2208 S in maxx,y\u2208S g\u0302 \u00b7 (y \u2212 x). The expectation of this width using random vectors g \u223c N (0, ID\u00d7D) instead of g\u0302 is twice the Gaussian width w(S). Intuitively, it is the characteristic extent of the set T over all directions rescaled by a factor between D/ \u221a D + 1 and \u221a D. Right panel: Illustration of projecting manifolds on the unit sphere and Gordon\u2019s escape theorem. The same manifold far from the sphere will have a smaller projection to it than the one that is close, and therefore it will be harder to intersect with an affine subspace."
    },
    "2011.09361v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that depicts the architecture of BERT, a pre-trained contextual language model. The model is designed to learn contextual representations of words and phrases by predicting missing words in a given sentence. The flowchart shows the different components of the BERT model, including the input embeddings, transformer layers, and output layers. It also illustrates the training process, which involves masking some of the input tokens and predicting their values from the learned representations. The diagram also includes information about the training group, validation group, and test group, as well as the dynamic-KD and static-OP components. Additionally, the fitted model and dynamic-test are shown. Overall, the image provides a detailed view of the BERT model's architecture and training process.",
        "reference": "Fig. 3: The procedure followed for splitting the datasets into training, validation and testing sets used by KD-OP."
    },
    "2010.04560v3-Figure9-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of using a case study. The flowchart starts with selecting a target device and ends with an end-user feedback. In between, there are several steps such as recommendations, explanations, and selecting a targeted persuasion fact type. The flowchart also includes reasoning, user preferences, and general context. The flowchart is designed to help users understand the process of using a case study effectively.",
        "reference": "Figure 9: Flowchart of an explainable energy recommender system."
    },
    "2110.03143v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The architecture of BERT in pretraining is shown, with the model's task being to predict masked tokens from the representation vectors it learns at the input embeddings positions. The pretraining process involves loading updated meta-parameters, updating the parameters, and updating the initial condition. The target domain, source flow, and target flow are also shown in the diagram. The diagram provides a detailed view of the BERT pretraining process.",
        "reference": "Figure 2. Overview of meta-learning strategy is presented on the left side and adaptation framework is presented on the right side. The meta-learning performs bi-level optimization, where the inner loop performs domain adaptation and the outer loop performs meta-update with respect to meta-loss. The updated meta-parameters are loaded as the initial condition of Faster-RCNN and this process is repeated. The adaptation framework performs adversarial feature alignment between source and target domain at the image and instance level."
    },
    "1292676-Figure4-1.png": {
        "prediction": "The diagram depicts a machine learning model, specifically a BERT (Bidirectional Encoder Representations from Transformers) architecture. BERT is a pre-trained contextual language representation model that can be fine-tuned for various natural language processing tasks.\n\nThe model consists of an input layer, followed by a stack of encoders and a final output layer. The encoders are responsible for processing the input and generating features. These features are then used to make predictions in the output layer.\n\nThe model is trained on a large corpus of text data using a technique called masked language modeling. In this technique, a random subset of words in the input sequence is replaced with a special token, such as [MASK]. The model's task is to predict the original words from the representation vectors it learns at the positions of the input embeddings of the [MASK] token.\n\nThe model also includes a next sentence prediction task, where the model predicts whether two given sentences follow each other or not. This task helps the model learn about the relationships between sentences.\n\nThe architecture of the BERT model is designed to capture the context of the input text, which allows it to perform well on a wide range of natural language understanding tasks.",
        "reference": "Figure 4: An overview of the LS-DQN algorithm."
    },
    "1321977-Figure3-1.png": {
        "prediction": "The diagram depicts a music generation system. The system takes in input data and uses it to train a preprocessing extractor, which then generates chord progressions. These progressions are used by the polyphonic LSTM to generate a MIDI file. The generated MIDI file is then used as input for the instrumentation process, which outputs a final representation of the music.",
        "reference": "Fig. 3. The architecture of JamBot. Chords and piano roll representations are extracted from the MIDI files in the training data (in black). The extracted chords and piano rolls are then used to train the chord and polyphonic LSTMs (in red). During music generation (in blue), the chord LSTM generates a chord progression that is used as input to the polyphonic LSTM which generates new music in MIDI format. When listening to the music, one can freely vary tempo and instrumentation."
    },
    "2010.14795v2-Figure3-1.png": {
        "prediction": "The diagram shows a process of pretraining BERT. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings of the masked tokens. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "FIG. 3: A generalized randomness-utilizing process \u03a6. If one intends to implement a certain quantum map \u03a8 utilizing a randomness source \u03c3 which has large enough min-entropy but is not a uniform random state, it could be implemented if one employs a broader notion of catalycity and allows the uncorrelated leftover randomness in the output state."
    },
    "2011.13183v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT. The architecture of the BERT model is shown in pretraining and fine-tuning stages. In the pretraining stage, the model's task is to predict the masked tokens from the input embeddings. The model learns a representation of the input by predicting the masked tokens. The pretraining stage consists of two main tasks: next sentence prediction and masked language modeling. The next sentence prediction task involves predicting whether two sentences follow each other or not. The masked language modeling task involves predicting the masked tokens from the input embeddings.\n\nIn the fine-tuning stage, the BERT model is fine-tuned for specific NLP tasks such as sentiment analysis, question-answering, and text classification. The model is trained on labeled data for these tasks using the pre-trained weights as an initialization. The fine-tuning stage consists of two main tasks: classification head and feature extractor. The classification head task involves training the model to classify the input into one of several classes. The feature extractor task involves training the model to extract features from the input that are used for classification.",
        "reference": "Figure 1: The model architecture of TinaFace. (a) Feature Extractor: ResNet-50 [11] and 6 level Feature Pyramid Network [18] to extract the multi-scale features of input image. (b) Inception block to enhance receptive field. (c) Classification Head: 5 layers FCN for classification of anchors. (d) Regression Head: 5 layers FCN for regression of anchors to ground-truth objects boxes. (e) IoU Aware Head: a single convolutional layer for IoU prediction."
    },
    "2103.00815v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The diagram has four layers, and each layer contains several nodes. The nodes are interconnected with lines, which represent connections between the different layers.\n\nThe first layer, \"input layer\", consists of 12 nodes. These nodes seem to be related to the input data, as they are connected to other layers through lines. The second layer, \"2nd layer\", also has 12 nodes. This layer is connected to the next layer, \"3rd layer\", which has 8 nodes. The fourth layer, \"4th layer\", has 6 nodes. The last layer, \"output layer\", has 4 nodes.\n\nOverall, this flowchart provides a visual representation of the BERT pretraining architecture, showing how the different layers are connected and the number of nodes in each layer.",
        "reference": "Figure 1: The graph associated to a deep neural network with input dimension 3 and 5 layers"
    },
    "2202.06743v1-Figure4-1.png": {
        "prediction": "The diagram shows the process of deformed configuration and undeformed configuration. The undeformed configuration is represented by a grid, while the deformed configuration is represented by a curved grid. The arrows in the diagram indicate the direction of the transformation from one configuration to another.",
        "reference": "Figure 4: Schematic representing the finite element discretization used for the numerical implementation of the proposed continuum formulation."
    },
    "2206.12772v1-Figure2-1.png": {
        "prediction": "The diagram shows a system for processing visual and audio data. The input to the system is an image and audio, which are processed by the Image Encoder and Audio Encoder respectively. The output of these encoders is then passed through a Localisation module, which localises the visual and audio information in space and time. The localised information is then passed through a Consistency Regularization module, which ensures that the visual and audio information is consistent with each other. Finally, the visual and audio information is passed through a Visual Sound Locaiser, which shares the visual and audio information between the two systems. This process is repeated for multiple iterations, with the final output being a representation of the input data.",
        "reference": "Figure 2: Framework Overview. We exploit a Siamese network, with two identical branches, each branch consists of an image encoder and an audio encoder. For the one branch, we perform transformations T 1vis + T 1 aud, while for the other branch, we use transformations T 2vis + T 2 aud. In this figure, T 1 vis only includes appearance transformation Tapp, while T 2 vis includes both appearance and geometric transformations Tapp + Tgeo. Both audio transformations are T\ud835\udc4e\ud835\udc62\ud835\udc51 . The framework is optimised by encouraging the audio-visual representation to be invariant to T\ud835\udc4e\ud835\udc5d\ud835\udc5d and T\ud835\udc54\ud835\udc52\ud835\udc5c , while being equivalent to T\ud835\udc54\ud835\udc52\ud835\udc5c ."
    },
    "2109.09113v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that processes data and activates quantization. The system begins with an input network, which sends data to the preprocessing stage. In the preprocessing stage, the data is filtered, and the filter output is sent to the statistics collection stage. From there, the data is sent to the statistics data representation stage, where it is represented in a specific way. The represented data is then sent through a batch normalization folding stage before reaching the threshold selection stage. At this point, the data is shifted, and negative correction is applied. The data is then sent to the activation equation stage, where it is activated. After activation, the data is sent to the quantized network, which processes the data further. Finally, the processed data is sent to the output layer, where the final result is produced.",
        "reference": "Figure 2: The HPTQ framework. Dashed lines represent statistical information passing, which include also their updates, dotted lines represent data passing and solid lines represent an updated network."
    },
    "2011.13733v1-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The relu activation is shown in yellow, and the convolution 1x1 is shown in green. The Conv 3x3 is shown in blue. The diagram also includes the input layer, output layer, and mask sequence. It appears to be a representation of a neural network architecture.",
        "reference": "FIG. 3: The overall schema of the Mini Inception-Resnet network. For the detailed modules, please refer to Figs. 4, 5 and 6."
    },
    "2101.10804v2-Figure1-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of executing traces and temporal relations. The flowchart shows the use case, direct call relations, indirect call patterns, and direct call patterns. It also includes various components such as input image, input layer, output layer, stack of encoders, final input representation, position embeddings, word embeddings, masked self-attention, add & layer norm, cross attention, linear softmax, and add & layer norm. These components are interconnected with arrows, indicating the flow of information and the relationships between them. Overall, the flowchart provides a visual representation of the process involved in executing traces and temporal relations.",
        "reference": "Fig. 1. The overall architecture of proposed CPTR model."
    },
    "2205.14647v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that illustrates the steps of a SIMDRAM framework. The diagram is divided into two main parts, with the top part showing the steps 1 and 2, and the bottom part showing step 3.\n\nStep 1 involves efficient implementation allocation to operation and uProgram generation for desired operation. This is followed by Step 2, which includes row allocation and uProgram generation for the desired operation. These steps are crucial in ensuring that the desired operations are executed efficiently using the SIMDRAM architecture.\n\nStep 3, shown in the bottom part of the diagram, involves execution according to the uProgram. This step is important as it ensures that the operations are carried out as intended after the allocation and generation processes in Steps 1 and 2.\n\nOverall, the flowchart provides a clear visual representation of the SIMDRAM framework's three-step process, emphasizing the importance of efficient operation and program generation in achieving optimal performance.",
        "reference": "Figure 2: Overview of the SIMDRAM framework."
    },
    "2101.02550v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses a combination of direct and indirect call relations. The system is designed to process input data and generate output data based on the given input layer and stack of encoders. The input layer consists of a set of input embeddings, which are used as the starting point for processing the data. The stack of encoders processes the input data through a series of modules, including the SE module, the SEP module, and the S module. The system also includes a masking mechanism, where specific tokens are replaced with a special token, such as [MASK] or [SEP]. The output layer represents the final result of the processing, which can be in the form of positional embeddings, segment embeddings, or word embeddings. The system is capable of handling different types of data, such as noise, lps, and attnet. Overall, the system provides a comprehensive approach to processing and generating data based on the given input.",
        "reference": "Fig. 2. The architecture of the first proposed ATM model, which is denoted as \u201cATMbef \u201d. From the figure, the output of the L-th LSTM layer is used to perform \u03c9, which is then used to extract the representative features at (L\u2212 1)-th LSTM layer in the SE model."
    },
    "2206.09770v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that depicts the process of object detection and tracking. The process begins with the detection of objects in a video frame, which is then localized. This information is fused to provide information about the location, speed, and category of the objects. The Real-World coordinate system is used to represent the position of the objects. The output of this process includes the object ID, location, category, speed, and heading. The diagram also shows the use of a camera for calibration and a pixel coordinate system for object detection.",
        "reference": "Fig. 2: An overview of the proposed framework for roadside vision-based traffic scene perception."
    },
    "1068967-Figure7-1.png": {
        "prediction": "The image is a flowchart that depicts the execution traces and temporal relations of a program. The flowchart is divided into several sections, including \"use case u1\", \"direct call relations\", \"indirect call relations (C1, C2)\", and \"direct call patterns (C1, C2)\". The diagram also shows the input layer, output layer, stack of encoders, and final input representation. The flowchart includes various components such as \"Conv 3x3\", \"MP 3x3\", \"Conv 2x2\", and \"Fully connected\". These components are interconnected with arrows, representing the flow of data and the sequence of operations in the program.",
        "reference": "Fig. 7: The architecture of face classifier on the shape-indexed local patches. \u201cConv\u201d means convolution, \u201cMP\u201d means max pooling, and N is the landmark number. The step size in convolution and pooling is 1 and 2, respectively."
    },
    "2012.14142v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that processes text. The system takes in an input layer, which is a sequence of words or tokens. These tokens are then passed through a stack of encoders, which transform the input into a fixed-size vector representation. This vector representation is then fed into a feedforward neural network (FNN), which further processes the information and produces an output layer. The output layer can be either a convolutional BERT (ConvBnRelU) or a decoupled BERT (DecoBnRelU). The arrows in the diagram show the flow of data through the system, with the direction of the arrow indicating the direction of the data flow. The diagram also includes labels for each component, such as \"Input layer\" and \"Output layer\", as well as labels for the encoders and the feedforward neural network.",
        "reference": "Fig. 2: The detailed structure of our HR-to-LR ultrasound image generation network."
    },
    "2109.00038v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented by the top left corner, with the direct call relations and indirect call patterns shown in the middle and right side of the diagram respectively. The diagram also includes information about the output layer, stack of encoders, and final input representation. The architecture of BERT in pretraining is depicted, where masked tokens are replaced with '[MASK]' and random tokens. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings.",
        "reference": "Figure 1. Deriving the severity measure m/r from the epidemics compartmental model. SPEIRD model is schematically shown. Transitions between the compartments are denoted by solid arrows, with the transition rates indicated above arrows. The dashed arrow from I to S indicates the interaction of I and S (infections) leading to the transition to E. The dashed arrow from P to S indicates the potential (reverse) transition from P to S due to the easing of measures. The dashed rectangles indicate parts of the model corresponding to the disease transmission (the left rectangle) and the disease outcome for the detected cases (the right rectangle). The single arrows indicate parts of the model from which the reproduction number R(t) and the severity measure (m/r) are, respectively, inferred. The total number of detected cases (D) corresponds to the sum of A, H and F and is denoted by a double arrow. Compartments are S \u2013 susceptible, P \u2013protected, E \u2013 exposed, I \u2013infected, R \u2013 recovered, A \u2013 active, H \u2013 healed, F \u2013 fatalities, D \u2013 total number of detected cases. r and m represent recovery and mortality rates of active (detected) cases."
    },
    "2202.07728v2-Figure3-1.png": {
        "prediction": "The image is a flowchart that depicts the use case of a system. The flowchart includes several nodes and arrows, representing different elements of the system's operation.\n\nStarting from the top left corner, there are two nodes labeled \"use case u1\" and \"indirect call patterns (C1, C2)\". These represent the main function of the system and the different types of calls that can be made to it.\n\nMoving downwards, we see a node labeled \"direct call relations\" with an arrow pointing towards a node labeled \"indirect call relations (C1, C2)\". This indicates that the system has direct and indirect call relations, which may have different behaviors or outcomes depending on the type of call.\n\nFurther down, there is a node labeled \"direct call patterns (C1, C2)\" with another arrow pointing towards a node labeled \"indirect call patterns (C1, C2)\". This suggests that the system has both direct and indirect call patterns, which may also have different behaviors or outcomes based on the type of call.\n\nFinally, at the bottom right corner, there is a node labeled \"Output Layer\", indicating the final stage of the system's operation where the results of the calls and their interactions are produced as output.\n\nOverall, the flowchart provides a visual representation of the system's architecture, highlighting its key components, their interconnections, and the different types of calls and patterns that can be used within the system.",
        "reference": "Figure 3. Scaling strategy. In order to scale to very large models, we propose to estimate the bounds of an intermediate layer\u2019s activations empirically by (1) Sampling N input perturbations and (2) calculating empirical bounds on the resulting activations for the layer h(\u00b7). We can then form the set PxB which is a subset of the true boundsHxB since the sampling is never exhaustive. We can then plug this set into a verified perturbation analysis method (3) and continue the forward propagation of the inputs through the rest of the network."
    },
    "2204.11695v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of the execution traces and temporal relations. The flowchart is divided into two main sections: Direct Call Relations and Indirect Call Patterns. The Direct Call Relations section has three sub-sections: Direct Call Patterns, Indirect Call Patterns, and Use Case U1. The Indirect Call Patterns section also has three sub-sections: Direct Call Patterns, Indirect Call Patterns, and Use Case U1. The diagram also includes a section on Output Layer.",
        "reference": "Figure 2: Illustration of the proposed BREM. Untrimmed videos are first fed into the backbone to generate the 1D temporal feature, which is used to construct the feature pyramid and frame-level feature. REM adopts each pyramid feature as input and generates coarse proposals and scores. Then the aligned feature is used for refinement of action location and scores. In parallel, BEM acquires the frame-level feature as input and produces the multi-scale boundary quality map for localization quality prediction."
    },
    "2207.11906v2-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The flowchart has several components, including an on-device model, streaming, mask, encoder, and non-streaming server model. The streaming component is connected to the mask, which is then linked to the encoder. The non-streaming server model is also connected to the encoder. The flowchart also includes various linear and softmax components. Additionally, there are several rectangles with text inside them, such as \"On-device model\" and \"Streaming\". This flowchart provides a visual representation of the BERT pretraining process.",
        "reference": "Fig. 1. Illustration of the proposed dual-mode ASR supernet training. When the encoder operates in the streaming mode, it is pruned by the binary mask (marked in purple). The predictor is pruned during streaming mode in the similar way, while intact during the nonstreaming mode."
    },
    "2010.06256v1-Figure3-1.png": {
        "prediction": "The image depicts a flowchart of a program. The program is designed to process strings and perform various actions based on the input. The flowchart includes several nodes, such as \"use case u1\", \"direct call relations\", \"indirect call patterns (C1, C2)\", and \"direct call patterns (C1, C2)\". These nodes are interconnected by arrows, indicating the flow of data and control through the program. The program also includes several sub-trees, such as \"SubTree\", \"SimpleNode\", \"Decorator\", and \"CompositeNode\", which are used to process the input string and generate the desired output. Overall, the flowchart provides a visual representation of the program's structure and logic, making it easier to understand how the program works.",
        "reference": "Figure 3. A meta-model for BehaviorTree.CPP (reverseengineered from its XML format)"
    },
    "2206.00770v2-Figure2-1.png": {
        "prediction": "The image is a flowchart that depicts the architecture of BERT in pretraining. The flowchart shows the process of pretraining BERT, which includes the perception, planning, scenario, and control steps. The diagram also illustrates the role of dynamic objects, detection, tracking, prediction, and classification in the pretraining process. Additionally, the flowchart highlights the importance of localizing, sensing, map data, and vehicle interface in the overall pretraining process.",
        "reference": "Fig. 2. Overview of the Autoware.Auto software stack components"
    },
    "2012.06782v1-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The first step is the input vector, which is then passed through a raw signal layer. The raw signal is then processed by a convolutional layer with 32 kernels, each of size 5x5. This is followed by a max pooling layer with a pool size of 2x2 and a stride of 2. The output of the max pooling layer is then passed through a flatten layer, which reshapes the data into a 1D vector. Finally, the data is passed through a fully connected layer with 2 neurons.",
        "reference": "Fig. 3: The proposed CNN architecture for classification of different mental tasks. Note: M denotes the number of classes."
    },
    "1026139-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a use case for a system. The flowchart is divided into three sections: input video, output captions, and output layer. The input video section shows the input video being processed by a C3D module, which is used to extract features from the video. These features are then passed through a proposal module, which generates proposals for regions of interest in the image. The output captions section shows the generated captions being passed through an LSTM module, which is used to generate the final output representation. Finally, the output layer section shows the final output being generated based on the input and the output representations.",
        "reference": "Figure 2: Complete pipeline for dense-captioning events in videos with descriptions. We first extract C3D features from the input video. These features are fed into our proposal module at varying stride to predict both short as well as long events. Each proposal, which consists of a unique start and end time and a hidden representation, is then used as input into the captioning module. Finally, this captioning model leverages context from neighboring events to generate each event description."
    },
    "2207.06011v1-Figure1-1.png": {
        "prediction": "The figure is a flowchart that illustrates the process of training a neural network. The flowchart consists of two main sections: the training pipeline and the inference pipeline. The training pipeline contains several components, including a decoder, a duration predictor, a monotonic alignment search, a projection, a speaker attractor, a spectrogram extractor, and a stochastic duration predictor. These components work together to train the neural network. The inference pipeline, on the other hand, includes a decoder, a duration predictor, a projection, a speaker attractor, a spectrogram extractor, and a transformer encoder. These components are used to make predictions using the trained neural network. Overall, the flowchart provides a detailed overview of the process involved in training and using a neural network.",
        "reference": "Figure 1: System overview of SATTS during training and inference."
    },
    "2206.04962v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a language model, specifically BERT. The architecture of BERT in pretraining is depicted. In the lowercased example sequence consisting of the segment pair \u2018he starts to speak. the nervous crowd is watch-ing him.\u2019 the tokens \u2018speak\u2019 and \u2018nervous\u2019 were sampled to be masked.'speak' is replaced by the \u2018[MASK]\u2019 token and \u2018nervous\u2019 is replaced by the random token \u2018that\u2019. The model's task is to predict the tokens \u2018speak\u2019 and \u2018nervous\u2019 from the representation vectors it learns at the positions of the input embeddings of \u2018[MASK]\u2019 and \u2018that\u2019. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Fig. 1. The overall architecture of the proposed method. The clean speech S and interference I are fed into the ECAE . The interference consists of background noises, reverberation of both speech and noise signals. After the feature combination is extracted, as the first pre-task, the latent representation of the clean speech signal is learned via ECAE . As the second pre-task, the DM and ERM are estimated in the masking module. Besides, the proposed method utilizes the speech reconstruction losses of each pre-task to train the other pre-task. After the feature maps are recovered in the decoder, the reconstructed clean spectra are obtained as the output by using DCAE . By using the learned speech representation into the mixture representation, the estimated mixtures are produced from the mixture autoencoder (MAE) with unpaired and unseen training mixture spectra Y."
    },
    "2110.05666v1-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart with several nodes and arrows. The nodes are labeled \"use case u1\", \"direct call relations\", \"indirect call relations (C1, C2)\", and \"direct call patterns (C1, C2)\". The arrows connect these nodes, indicating the flow of information or process between them. Additionally, there are labels such as \"Trace 1\", \"Trace 2\", \"P (B follows A)\", \"FNN+softmax\", \"Output Layer\", \"Stack of Encoders\", \"Final Input Representation\", \"Positional Embeddings\", \"Segment Embeddings\", \"Masked [CLS] he starts to [MASK] the that crowd is watch-ing him. [SEP]\", and \"Original [CLS] he starts to speak. [SEP] the nervous crowd is watch-ing him. [SEP]\". These labels provide further context about the flowchart's purpose and the specific details of the process it represents.",
        "reference": "Fig. 3 (Color online) Thermal model for estimation of temperature rise at at the graphite surface. See the text for details."
    },
    "2207.00422v2-Figure5-1.png": {
        "prediction": "The image is a flowchart that shows the process of pretraining BERT. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The architecture of BERT in pretraining is also shown in the diagram.",
        "reference": "Figure 5: Illustration of our personalized showcases framework for the given business. We take user historical images and textual reviews as inputs. First, we select an image set that is most relevant to a user\u2019s interest. Thenwe generate natural language explanations accordingly with a multi-modal decoder. A cross-modal contrastive loss and a personalized contrastive loss are applied between each input modality and the explanations. Last, the selected images and generated textual explanations will be organized as multi-modal explanations to users."
    },
    "2103.11913v1-Figure8-1.png": {
        "prediction": "The image depicts a flowchart of a machine learning model. The model is BERT, which stands for Bidirectional Encoder Representations from Transformers. BERT is a pre-trained language representation model that uses transformer-based deep bidirectional contextualized word embeddings.\n\nThe diagram shows the architecture of BERT in pre-training mode. In this stage, the model learns to predict masked words and next sentences from a given text. The text is segmented into two parts: input embeddings and positional embeddings. The input embeddings represent the words in the sentence, while the positional embeddings indicate the position of each word within the sentence.\n\nThe model's task is to predict the masked words and the next sentence from the learned representations at the positions of the input embeddings. This is done by using feedforward neural networks (FNNs) with softmax activation functions. The output layer represents the final input representation, and the stack of encoders is used to process the input data.\n\nThe diagram also includes an example of how the model works. It shows a sequence of text where the words \"speak\" and \"nervous\" are masked. The model's task is to predict these words based on the context provided by the surrounding words. The predicted words are then used to generate a new sentence.",
        "reference": "Figure 8: Illustration of the stencil that refers to the divergence matrix Dn,n+1."
    },
    "2210.07587v2-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the execution traces and temporal relations. The flowchart includes several nodes, such as \"use case u1\", \"direct call relations\", \"indirect call patterns (C1, C2)\", and \"direct call patterns (C1, C2)\". These nodes are interconnected by arrows, indicating the flow of information or process between them.\n\nThe flowchart also shows the use of \"PB follows A\" and \"FNN+softmax\" in the lower part. The upper part of the flowchart contains a stack of encoders and a final input representation. The encoders are represented as a series of rectangles with text inside them, while the final input representation is shown as a rectangle with a label \"Output Layer\".\n\nOverall, the flowchart provides a visual representation of a complex process or system, illustrating the relationships and interactions between different components.",
        "reference": "Figure 1: The overview of the CONENTAIL framework. By casting the classification as a nested entailment task, the model performs classification by telling if a query sentence q entails [premise example p entails hypothesis label h]. In a few-shot setting, the premise is an example sentence; in a zero-shot setting, the premise is a \u201cNULL\u201d placeholder."
    },
    "2204.00172v3-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart for a machine learning model. The flowchart begins with the source labeled \"Source Ih\" and ends with the loss supervised by the \"Unsupervised MSE loss\". In between, there are several steps such as stylization, augmentation, and adaptive joint occlusion. These steps are followed by the student, who is represented by the \"Student H4\" label. The teacher, represented by the \"Teacher H4\" label, provides predictions to the student. The flowchart also includes various other elements such as the \"source prediction\", \"reverse augmentation\", and \"student target prediction\". Overall, this flowchart represents a complex machine learning process that involves multiple stages and components.",
        "reference": "Fig. 2: An overview of our unified framework comprising a supervised branch that learns from source domain data with corresponding annotation, as well as an unsupervised branch that learns from unlabeled target domain data. We perform domain alignment both in the input-level via style-transfer with style references from the opposite domain, and the output-level of the model that guides the training on the target domain with more reliable pseudo-labels. The student model is trained by the combination of two losses, while the teacher model is updated with the exponential moving average weights of the student"
    },
    "2012.05858v3-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses BERT for language understanding. The system consists of three main parts: the WarmingNet, the ShardingNet, and the Loss. The WarmingNet is responsible for pretraining the BERT model by masking some tokens in the input sequence and predicting them. The ShardingNet is used to shard the BERT model's parameters across multiple GPUs. Finally, the Loss is calculated based on the output of the BERT model and the ground truth labels. The arrows in the diagram represent the data flow between these components.",
        "reference": "Figure 3: PCNet \u03c0\u0302 architecture and training. PCNet approximates the real project-and-capture process \u03c0 using a deep neural network (WarpingNet + ShadingNet). The inputs are a projector input image x, a camera-captured scene image (under normal light) Is, and a projector direct light mask Im. The output I\u0302x is an inferred camera-captured scene (under superimposed projection). WarpingNet consists of a learnable affine matrix \u03b8aff, thin-plate-spline (TPS) parameters \u03b8TPS and a grid refinement networkW\u03b8r . This coarse-to-fine pipeline allows WarpingNet to learn a fine-grained image sampling grid \u2126 to warp the projector input image x to the camera\u2019s canonical frontal view by \u03c6(x,\u2126), where \u03c6(\u00b7; \u00b7) is a differentiable image interpolator [16] denoted as \u2297. Then, we use the input projector direct light mask Im to exclude occluded pixels by \u03c6(x,\u2126) Im, where is element-wise multiplication. Afterwards, this warped projector image is further used to compute an intermediate rough shading image \u03c6(x,\u2126) Im Is to enforce the occlusion constraint. ShadingNet has a two-branch encoder-decoder structure to capture complex photometric transformations. In particular, it concatenates Is and \u03c6(x,\u2126) Im Is and feeds them to the middle encoder branch. Similarly, \u03c6(x,\u2126) Im is fed to the backbone encoder branch. The skip connections between the two branches model photometric interactions between the three inputs at different levels. In addition, we pass Is to the output layer through three convolutional layers. Finally, the feature maps are fused into one inferred camera-captured scene (under superimposed projection) I\u0302x by the backbone decoder."
    },
    "2102.09652v2-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the architecture of BERT, a pre-trained contextual language model. The diagram shows the input to the model and the output layer. The arrows in the flowchart represent the direction of information flow through the network. The diagram also includes various components such as the input embeddings, position embeddings, segment embeddings, and word embeddings. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram provides a visual representation of how the BERT model processes and learns from text data.",
        "reference": "Figure 1: Illustration of the staggered meshes Kocc and Kvir for a quasi-2D system."
    },
    "2207.14568v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a training stage. The training stage is divided into two parts: the pretraining stage and the fine-tuning stage.\n\nIn the pretraining stage, the BERT model is trained on a large corpus of text data to learn general language representations. This is done by masking some of the input tokens and predicting them from their contextual embeddings. The model is also trained to predict the next sentence in a given pair of sentences.\n\nAfter pretraining, the BERT model is fine-tuned for specific NLP tasks such as question answering or sentiment analysis. In the fine-tuning stage, the model is trained on task-specific datasets to adapt its learned representations to the specific task at hand. The model is fine-tuned using a combination of supervised learning and self-reinforcement learning techniques.\n\nOverall, the diagram illustrates the process of training a BERT model for various NLP tasks through a combination of pretraining and fine-tuning.",
        "reference": "Fig. 1: Framework overview: blue lines denote GAN training; yellow lines denote self re-training; black lines denote the paths which are not trainable."
    },
    "2107.02271v1-Figure7-1.png": {
        "prediction": "The image is a flowchart that illustrates a model-based data communication system. The system consists of several components, including deployment, network initialization, selection, receive-aware communication, and PDR monitoring.\n\n1. Deployment: This component initiates the process by setting up the environment for the data communication system.\n2. Network Initialization: After deployment, the network is initialized to establish connections and prepare for data transmission.\n3. Selection: In this step, a selection process is performed to choose the appropriate data to be transmitted.\n4. Receive-Aware Communication: Once the data is selected, it is transmitted over the network using a receive-aware communication method. This method ensures that the data is received correctly and efficiently by the intended recipient.\n5. PDR Monitoring: Finally, the PDR (Packet Delivery Report) is monitored to evaluate the performance of the data communication system. The PDR provides information on the success rate of data transmission, which can be used to improve the system's efficiency and reliability.\n\nOverall, the flowchart depicts a comprehensive data communication system that includes various stages, from deployment to monitoring, to ensure effective and efficient data transmission.",
        "reference": "FIGURE 7: Overview of the model-based data communication."
    },
    "2107.05792v4-Figure3-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of using case 1.1 and case 2.1. The flowchart begins with the use of case 1.1, which is followed by the use of case 2.1. The flowchart also shows the use of indirect call patterns (C1, C2) and direct call patterns (C1, C2). Additionally, there are labels and codes present in the diagram. The flowchart seems to be related to a study on machine learning refinement and classification refinement.",
        "reference": "Figure 3: Example of classification process used to analyze the contributions by GL use."
    },
    "2010.07804v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the generation of semantic information and consistency learning. The first part, \"Generation of Semantic Information\", illustrates how semantic information is generated from input data. The second part, \"Consistency Learning\", shows how consistency is learned from the generated semantic information. The process starts with input data, which is processed to generate semantic information. This information is then used to learn consistency. The consistency is then used to generate semantic information, which is then used to learn consistency again. This process continues in a loop, leading to the generation of more consistent semantic information.",
        "reference": "Figure 2: Overview of CIMON. (a) CIMON generates semantic information including refined pseudo-graph and confidence matrix for reliable guidance. (b) With data augmentation, CIMON generates two groups of semantic information. The parallel and cross semantic consistency is constructed between features and hash codes under the same group and crossing the different groups respectively. The contrastive consistency is based on two groups of hash codes."
    },
    "215238689-Figure3-1.png": {
        "prediction": "The diagram shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings of the masked tokens. P (BfollowsA) is the next sentence prediction task. FNN stands for feedforward neural network.",
        "reference": "Figure 3: The overview of the proposed CSFN-DST. It takes the current dialogue utterance, the previous dialogue state and the schema graph as the input and predicts the current dialogue state. It consists of an embedding layer, context and schema fusion networks, a slot-gate classifier and an RNN-based value decoder."
    },
    "1016639-Figure2-1.png": {
        "prediction": "The image is a flowchart that illustrates the process of using a case to search for information. The flowchart shows the use of a case, which is represented by a box with the label \"use case u1\". The flowchart also includes several other boxes and arrows, representing different steps in the process.\n\nThe first step in the process is to apply a lispstick, which is represented by a box with the label \"Apply Lispstick\". This step is followed by a search, which is represented by an arrow pointing from the \"Apply Lispstick\" box to a box with the label \"search\".\n\nAfter the search, the next step in the process is to use a CNN, which is represented by a box with the label \"CNN\". The final step in the process is to read the input data, which is represented by a box with the label \"Input Data\".\n\nThe flowchart also includes a text box that says \"a woman makeup her face\", indicating that the process may be related to a woman applying makeup to her face. Overall, the flowchart depicts a sequence of actions and decisions that are used to search for information using a case.",
        "reference": "Figure 2. Our multi-task convolutional neural network (CNN). The model is based on CNN with two task branches for classification and caption embedding respectively."
    },
    "2010.07860v3-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that involves several components, including a structured network, deep network, interaction predictor head, and basis evaluation. The system starts with the input layer, which is followed by the transformed distribution. The error distribution is then transformed to produce the final output representation. The architecture of the system includes a pretraining BERT, which is used for predicting masked tokens and next sentence pairs. The system also has an interaction predictor head, which is responsible for predicting interactions between the input and output layers. The basis evaluation component is used to evaluate the performance of the system. Overall, the system seems to be designed for natural language processing tasks, such as language modeling or text classification.",
        "reference": "Figure 1. Architecture of a deep conditional transformation model. Both the shift and interaction predictor can potentially be defined by a structured network including linear terms, (penalized) splines or other structured additive regression terms and deep neural network defined by an arbitrary network structure. While the shift predictor (C\u03a8) is a sum of both subnetwork predictions, the interaction predictor (A B) is only multiplied with a final 1-hidden unit fully-connected layer (network head, vec(\u0393)) after the structured predictors and latent features of the deep neural network are combined with the basis evaluated outcome. The shift and interaction network part together define the transformation function, which transforms the error distribution and yields the final likelihood used as loss function."
    },
    "2011.08010v1-Figure1-1.png": {
        "prediction": "The diagram depicts a machine learning model, specifically a BERT model. BERT is a transformer-based pre-training model for language understanding tasks. The model is trained on a large corpus of unlabeled text, and then fine-tuned on a specific task, such as question-answering or sentiment analysis.\n\nThe diagram shows the following stages:\n\n1. Pretraining: In this stage, the BERT model is trained on a large corpus of unlabeled text. The model learns to predict missing words in a sentence given the context.\n2. Coarse output: The pretrained BERT model is used to generate a coarse output, which is then refined in the next stage.\n3. Refinement: The coarse output is refined using a trained collector. This collector is trained to identify important information from the coarse output and use it to improve the accuracy of the final output.\n4. Fine-tuning: The refined output is fine-tuned on a specific task, such as question-answering or sentiment analysis. The fine-tuning process involves adjusting the weights of the pre-trained BERT model to better perform the specific task.\n5. Model output: The final output of the model is generated based on the fine-tuning process.\n\nThe diagram also includes a social media platform, which is used to collect data for training the BERT model. The data collected from social media is used to fine-tune the BERT model to better understand human language and perform specific language understanding tasks.",
        "reference": "Figure 1: The inference pipeline of our model. The two-stage model first generates a segmentation mask from Sentinel-2 imagery in Stage 1, and then combines Sentinel-2 imagery, initial coarse output, and crowdsourced points in Stage 2 to generate the final segmentation mask. Points collected from either a Trained Collector or Social Media model can be used interchangeably in this model."
    },
    "2210.00486v1-Figure2-1.png": {
        "prediction": "The diagram depicts a system that uses a joint model to connect two parties, P1 and P2, who are trying to establish a connection. The system has an assistant party that helps the two parties connect.\n\n1. Initially, the system randomly selects one of the parties, P1, and sends a message to the other party, P2.\n2. If P2 receives the message, it sends a response back to P1.\n3. If P2 does not receive the message within a certain time frame, it drops out of the system and the process starts again with a new random selection of P1.\n4. If P2 responds to the message, the system creates a joint model between P1 and P2.\n5. The system then randomly selects another party, P3, and sends a message to P1 and P2.\n6. If P3 receives a response from either P1 or P2, the system creates a joint model between P3 and the responding party.\n7. The system continues this process until all parties have been connected by a joint model.\n\nThis system is designed to help parties connect in a way that maximizes the number of connections while minimizing the number of dropped connections. It is a probabilistic system, meaning that the outcome of each step is determined by a random selection.",
        "reference": "Figure 2: Overview of pMPL"
    },
    "2107.10538v1-Figure4-1.png": {
        "prediction": "The image is a flowchart that shows the interaction between different agents in a system. The agents are represented by circles, and arrows indicate the flow of information between them.\n\nStarting from the top left, we have the \"voice record agent\" which records the voice input. This information is then passed on to the \"voice recognition agent\" which converts the voice input into text.\n\nNext, the \"translation agent\" takes the text and translates it into another language. The translated text is then passed on to the \"storage agent\" which stores the information.\n\nFinally, the \"voice recognition agent\" takes the stored information and converts it back into voice output. The \"voice record agent\" then records this output, completing the cycle.\n\nOverall, the flowchart shows how the system processes voice input, translates it, stores the translation, and then outputs the translation as voice.",
        "reference": "Fig. 4: MAGS model of MCCOMP+DIV."
    },
    "2202.05132v2-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of execution traces and temporal relations. The flowchart is divided into three main sections: use case 1, indirect call patterns, and direct call patterns. The diagram also includes various elements such as input layer, output layer, stack of encoders, and final input representation. The flowchart provides a visual representation of how data flows through the system and the sequence of operations that are performed.",
        "reference": "FIG. 1. (a) Representation of the operator state \u03c1op(t) [Eq. (1)]. Each qubit in Qout is prepared in a maximally entangled state (black dots) with the corresponding qubit Qin, before being time evolved under the channel Nt. (b) Illustration of the Hayden-Preskill protocol [5]. An unknown quantum state |\u03c8\u3009 is used as an input to a small subregion A, while the remaining qubits (B) are prepared in a maximally entangled state with a set of ancillas B\u2032 (circled). If the channel is perfectly scrambling then |\u03c8\u3009 can be reconstructed using the ancillas combined with a subset of output qubits C of the same size as A, regardless of which qubits are in C (qubits in D are discarded). Formally, the final state of the ancillas combined with the outputs C depends on the input state to A through the channel NA\u2192B \u2032C"
    },
    "2202.06453v1-Figure1-1.png": {
        "prediction": "The diagram depicts a flowchart with various nodes and arrows. The nodes are labeled \"use case u1\", \"direct call relations\", \"indirect call relations (C1, C2)\", and \"direct call patterns (C1, C2)\". The arrows in the flowchart represent the connections between these nodes. The flowchart is designed to illustrate the relationships and interactions among different elements of a system or process. It is used to visualize the flow of data or information through a system, as well as to identify the different components and their interconnections within a particular system.",
        "reference": "Figure 1: Simulation diagram for an interconnection between a circuit of interest and a load."
    },
    "2010.09602v1-Figure5-1.png": {
        "prediction": "The diagram depicts a complex system with several components. The system is designed to process input data and generate output based on the input. The architecture of the system includes multiple layers, each responsible for different functions. The system also includes various modules such as the encoder, decoder, and attention mechanism. The input data is processed through the encoder, which converts it into a fixed-length vector representation. This vector is then passed through the attention mechanism, which helps the model focus on important parts of the input. Finally, the output is generated by the decoder, which uses the vector representation to predict the next word in the sequence. The system is trained using a combination of supervised and unsupervised learning techniques. The training data consists of pairs of input sequences and their corresponding output sequences. The system is optimized to minimize the difference between the predicted output and the actual output. The system is also designed to handle variable-length input and output sequences.",
        "reference": "Fig. 5: Detailed architecture of proposed TTS model. Dashed line denotes feedback loop. Number in bracket denotes neural layer size. FC denotes a fully connected layer. During inference, only prior and decoder are used."
    },
    "2204.10523v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes the pretraining data, the masked language model objective, and the next sentence prediction objective. The arrows in the diagram indicate the flow of information between different parts of the model.",
        "reference": "Figure 1: The probabilistic graphical model of two-covariance PLDA"
    },
    "2011.12854v6-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The diagram is divided into three sections: the upper section, the middle section, and the lower section. The upper section contains a stack of encoders, which are used to process the input data. The middle section has a set transformer, which is responsible for transforming the data. The lower section includes a semantic explainer, which helps in explaining the output. There is also a visual explainer, which provides a visual representation of the data. Finally, there is a semantic explainer, which helps in explaining the output.",
        "reference": "Figure 3: Neuro-Symbolic XIL for improved explanations and interaction. (Top) Neuro-Symbolic Concept Learner with Slot-Attention and Set Transformer. (Bottom) Neuro-Symbolic revision pipeline with explanations of the model before (gray) and after applying the feedback (orange)."
    },
    "2207.04317v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart with the use case \"use case u1\" at the top. The flowchart is divided into two sections: \"direct call relations\" and \"indirect call relations.\" The direct call relations section has three sub-sections labeled \"C1,\" \"C2,\" and \"C3.\" The indirect call relations section also has three sub-sections labeled \"C1,\" \"C2,\" and \"C3.\" The flowchart is designed to help users understand the different types of call relations and their corresponding patterns.",
        "reference": "Figure 1: Illustration of counterfactual explanation in neural recommender systems."
    },
    "2202.02646v1-Figure1-1.png": {
        "prediction": "The diagram depicts a complex system with several components and their interactions. The system is divided into two main parts: the left side, which contains the input data claim c, abstract retrieval, and input data claim c; and the right side, which includes the label prediction, decision d, and rational selection.\n\nOn the left side, there are three main components: the input data claim c, the abstract retrieval, and the input data claim c. The input data claim c is used to provide information for the system, while the abstract retrieval is responsible for extracting relevant information from the input data. The input data claim c is also used to provide additional context to the system.\n\nOn the right side, there are four main components: the label prediction, decision d, rational selection, and the output layer. The label prediction is used to make predictions about the input data based on the information provided by the system. The decision d is used to make decisions based on the predictions made by the label prediction. The rational selection is used to select the most appropriate action based on the decision made by the decision d. Finally, the output layer is used to present the results of the system's analysis in a clear and concise manner.\n\nOverall, this diagram shows a complex system that uses multiple components to process and analyze data, make predictions, and make decisions. It is designed to be highly efficient and accurate, providing users with the most relevant and useful information possible.",
        "reference": "Figure 1: Architectural illustration of the RERRFACT scientific claim verification pipeline."
    },
    "117714-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart of a machine learning model, specifically BERT (Bidirectional Encoder Representations from Transformers). The model is designed to be pre-trained on large-scale unlabeled text data before being fine-tuned on downstream tasks. The architecture of BERT in pre-training is shown, with the model's task being to predict masked tokens from the input embeddings. The diagram also includes an encoding stage and a decoding stage, as well as a motion trigger stage. The model is trained using a combination of supervised and unsupervised learning techniques.",
        "reference": "Figure 2. Visualization of our motion-triggered model. Our model consists of an RNN with LSTM cell encoder (blue block) and a Policy Network (yellow block). At each frame, RNN will generate an anticipated intention according to a new embedded representation g and the previous hidden state h of the RNN. The policy will generate the motion-trigger decision a for next frame, based on motion representation fm and the hidden state h of the RNN. The orange circle represents the fusion operation (details in Sec. 3.2). The red and black circles represent a trigger and non-trigger decision of policy network, respectively (details in Sec. 3.3). When a = 0, fo is empty since it is not processed."
    },
    "2107.02290v1-Figure8-1.png": {
        "prediction": "The diagram shows a flowchart with two main sections. The first section, \"use case u1\", is depicted in the top left corner of the diagram and has three sub-sections: \"direct call relations\", \"indirect call relations\", and \"indirect call patterns (C1, C2)\". The second section, \"direct call patterns (C1, C2)\", is located at the bottom right corner of the diagram. The diagram also includes several nodes, such as \"Trace 1\" and \"Trace 2\", which are connected by arrows. These arrows represent the flow of information or data between different parts of the system. Overall, this flowchart seems to be a representation of a software system's architecture, where the use cases, call relations, and call patterns are all interconnected.",
        "reference": "FIGURE 8 Coaxial cable model and its cross section. The inner, black region \u03a90 models the copper wire, the white region \u03a91 the air insulator and the outer, gray region \u03a92 the conducting shield35."
    },
    "2206.13082v1-Figure2-1.png": {
        "prediction": "The image is a diagram of a network architecture. The top diagram shows the pretraining of BERT, which involves masking some tokens in the input sequence and predicting them from the representation vectors. The bottom diagram illustrates the dynamic vocal feature encoder, which consists of a fully connected layer and a fully connected network. The diagram also includes a point-to-vox map and a vox-to-point map. Overall, this image depicts a complex network structure that involves multiple layers, connections, and encoders.",
        "reference": "Fig. 2. Illustration of the PST architecture. (a) is the main component of PST, which follows an encoder-decoder pipeline. (b) is a demonstration of dynamic voxelization used in the encoding part."
    },
    "2107.08402v1-Figure2-1.png": {
        "prediction": "The image is a flowchart that shows the architecture of BERT in pretraining. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes information about the server, upload local model updates, and download global model. It seems to be related to machine learning and natural language processing.",
        "reference": "Fig. 2: Overview of Proposed Methods"
    },
    "2102.04593v1-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of a use case for a system. The system is designed to process input data, generate an image, and then output the result. The input data is processed by a generator, which produces a CNN layer 1. This layer is then passed through a discriminator, which generates a CNN layer 3. The output of the discriminator is then passed through another discriminator, which generates a CNN layer 5. Finally, the output of the second discriminator is passed through a generator, which produces the final output representation.",
        "reference": "Figure 3. DCGAN architecture"
    },
    "2012.09727v2-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented by the \"use case u1\" and \"indirect call patterns (C1, C2)\". The diagram also includes the \"direct call relations\" and \"indirect call relations\" in the form of blue circles. The arrows indicate the direction of the flow, with the text nodes and arrows depicted as shown.",
        "reference": "Fig. 1. (A) The architecture of the proposed continuous speech separation using speaker inventory. The Speaker inventory construction module forms the speaker inventory from the long mixture by using Kmeans clustering; the long mixture is split into small segments, and the speaker profile selection module selects two relevant profiles from the inventory for each segment; the speech separation module fuses the selected speaker profiles into the system for source separation. (B) Multiplicative adaptation of the selected profiles ep1 and ep2 . (C) Stitching procedure of adjacent segment outputs in a long recording."
    },
    "2101.05804v3-Figure1-1.png": {
        "prediction": "The image is a flowchart that depicts the use case of a system. The system has three main parts: pretraining, fine-tuning, and inference. In the pretraining section, the system learns to predict masked words in a given text. This is done by training on a large corpus of text data. The fine-tuning section involves taking a pre-trained model and adapting it to a specific task or domain. Finally, the inference section uses the trained model to make predictions on new, unseen data.",
        "reference": "Figure 1: Illustration of the three relevant physical regimes for a system of massive fermions [purple dots] with a long range interaction mediated by a scalar field [blue]. T and n are the characteristic fermion kinetic energy and number density, respectively. For ultrarelativistic fermions, in panel (a); and interparticle distances above the interaction range \u223c M\u22121\u03c6 , in panel (c); there are no long range effects."
    },
    "2107.02955v1-Figure2-1.png": {
        "prediction": "The diagram depicts a flowchart with the title \"use case u1\" at the top. The flowchart is divided into two main sections: \"Direct call relations\" and \"Indirect call relations.\" Each section has its own set of sub-sections, including \"C1,\" \"C2,\" and \"C4.\" These sub-sections are further broken down into smaller components such as \"Trace 1\" and \"Trace 2.\" The flowchart also features arrows connecting different components, indicating the flow of information or processes. Additionally, there is a label \"Prismatic Joint\" pointing to a specific part of the flowchart.",
        "reference": "Fig. 2. Our model for the flat elastic terrain. Each tile is connected to the floor by a spring-loaded prismatic joint."
    },
    "2207.04913v1-Figure1-1.png": {
        "prediction": "The diagram illustrates a process of adaptive inference using optimal transport. The process starts with a target, which is represented by a set of samples. These samples are then used to generate barcenters and optimal transport. The optimal transport is then fed into a coupling matrix, which is used to predict the class likelihood. Finally, a predicted class likelihood is generated.\n\nThe diagram also shows that the optimal transport is generated from a target, which is represented by a set of samples. This target is then used to generate barcenters, which are then used to generate optimal transport. The optimal transport is then fed into a coupling matrix, which is used to predict the class likelihood. A predicted class likelihood is then generated.\n\nThe diagram also shows that the optimal transport is generated from a target, which is represented by a set of samples. This target is then used to generate barcenters, which are then used to generate optimal transport. The optimal transport is then fed into a coupling matrix, which is used to predict the class likelihood. A predicted class likelihood is then generated.",
        "reference": "Fig. 1. An overview of our WDRDG framework, consisting of three components: (a) Wasserstein uncertainty set construction for each class based on the empirical Wasserstein barycenters and radius obtained from given source domains. One constraint is added to control the discriminability of LFDs; (b) distributionally robust optimization to solve for the least favorable distributions; (c) adaptive inference for target testing samples based on probability mass on LFDs and coupling matrix from optimal transportation between barycenter samples and target samples."
    },
    "2202.13626v1-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of a system that involves various components and processes. The system is designed to capture images, transfer them for segmentation, and then detect activity in the image. The flowchart also includes a section on training, which suggests that the system is being developed or improved over time. Additionally, there are several nodes in the flowchart that represent different types of actions, such as \"Request action command\" and \"User authentication.\" These actions are likely part of the system's user interface, allowing users to interact with the system and request specific actions. Overall, the diagram provides a high-level overview of the system's architecture and the various processes it involves.",
        "reference": "Figure 3: Remote control (IFTTT) of home IoT service with the centralized learning model."
    },
    "19488885-Figure4-1.png": {
        "prediction": "The image depicts a flowchart of a task-specific machine learning pipeline. The process starts with a pre-training stage, where the BERT model is trained on a large corpus of text data. This is followed by a classification stage, where the trained BERT model is used to classify input data into different categories.\n\nAfter the classification stage, the output layer represents the final output of the model. The shared embeddings and shared embeddings are then used in the task-specific BILSTM (Bi-directional Long Short-Term Memory) and task-specific MLP (Multi-Layer Perceptron) stages. These stages are designed to perform specific tasks based on the input data.\n\nFinally, the shared embeddings are used again in the shared embeddings stage, which is used for the task-specific BILSTM and task-specific MLP stages. The shared embeddings are then used as input to the task-specific BILSTM and task-specific MLP stages, which are responsible for performing the actual task.",
        "reference": "Figure 4: MTL model. Token representations are computed both by a task-specific and a shared BiLSTM. Their outputs are concatenated with the parser state embedding, identical to Figure 2, and fed into the task-specific MLP for selecting the next transition. Shared parameters are shown in blue."
    },
    "2011.10683v1-Figure5-1.png": {
        "prediction": "The image is a flowchart that illustrates the use case of a system that uses BERT, a pre-trained language model. The system takes in a user's input and processes it to provide an answer or response.\n\nThe flowchart has three main sections: Segment Template Set, Segment Callback Function, and Segment Callback Function. These sections represent the different parts of the system, including the input segment, the callback function, and the output segment.\n\nIn the Segment Template Set section, there are two sub-sections: \"Template Set\" and \"Segment\". The \"Template Set\" sub-section contains a list of templates, which are used as a reference for generating responses. The \"Segment\" sub-section contains a list of segments, which are the individual components of the input.\n\nIn the Segment Callback Function section, there is a single sub-section called \"Callback Function\". This is where the actual processing of the input occurs. The system uses the input segment to generate a response using the pre-trained BERT model. The generated response is then passed to the callback function, which processes the response and returns it to the user.\n\nFinally, in the Segment Callback Function section, there is a single sub-section called \"Callback Function\". This is where the final response is generated and returned to the user. The system uses the response from the callback function to generate a final response, which is then returned to the user.\n\nOverall, the flowchart shows how the system uses BERT to process user input and generate a response based on the pre-trained language model. It also shows how the system uses the Segment Template Set, Segment Callback Function, and Segment Callback Function to process the input and generate the final response.",
        "reference": "Figure 5: Illustration of response composition in Flow-RG."
    },
    "2103.15997v2-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The top left corner of the diagram shows a use case, which is followed by two traces (Trace 1 and Trace 2) that are connected to the use case. These traces are then linked to a direct call relations block. This block contains several call relations, including C1, C2, C3, and C4. Each of these call relations has a different color, with C1 being blue, C2 being green, C3 being red, and C4 being yellow.\n\nThe bottom right corner of the diagram shows a criss-cross attention block. This block contains several attention blocks, including P1, P2, and P3. These attention blocks have different colors as well, with P1 being pink, P2 being orange, and P3 being purple. The attention blocks are also connected to the use case, indicating their role in the overall process.",
        "reference": "Fig. 1. Diagram of the criss-cross attention module. For each position in the input feature map, the attention module generates a sparse attention map by aggregating information in the horizontal and vertical axes. After the second operation, each pixel has extracted context from all the pixels."
    },
    "2102.03921v1-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart of a system that uses BERT, a pre-trained language model. The system consists of an action generator, a pool of classifiers, and a response-action encoder network. The action generator generates actions based on the input, which is then passed through the pool of classifiers. The classifiers are used to classify the input into different categories. The output of the classifiers is then passed through the response-action encoder network, which encodes the response to the given action. The final output of the system is a representation of the input.",
        "reference": "Figure 3: Least action classifier network architecture. Blocks of solid lines are differentiable. Sample images are from dataset CIFAR-10, which is collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton[MIT License](https://www.cs.toronto.edu/ kriz/cifar.html)"
    },
    "2102.03921v1-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of a system that uses a pool of classifiers to classify an image. The system starts with an image, which is then passed through a classifier \"key\" to determine the type of classifier needed for further processing. The image is then passed to a pool of classifiers, where it is classified by the appropriate classifier based on the result from the previous step. The output of the classification process is then passed through a classifier response, which produces the final output of the system. This system can be used for various applications such as object recognition, image segmentation, and text classification.",
        "reference": "Figure 2: Markov decision process diagram [Malashin, 2019]"
    },
    "2210.05232v1-Figure4-1.png": {
        "prediction": "The diagram shows a flowchart of a machine learning model. The model is based on the BERT (Bidirectional Encoder Representations from Transformers) architecture, which is a pre-trained language representation model. The model consists of an input layer, a stack of encoders, and a final output layer. The encoders are responsible for processing the input data and extracting features. The final output layer is used to make predictions based on the extracted features. The model also includes a masking mechanism that randomly masks some of the input tokens during training. This helps the model learn to predict the masked tokens, improving its ability to understand the context of the input. Additionally, the model has a mechanism for updating the position embeddings, which are used to represent the positions of the input tokens in the sequence. These updates help the model better understand the relationships between the input tokens and their positions in the sequence. Overall, the model is designed to be able to understand natural language and generate text that is similar to human-written text.",
        "reference": "Fig. 4. An illustration of the iterative confidence-based pose estimation network."
    },
    "2107.00679v1-Figure1-1.png": {
        "prediction": "The image depicts a flowchart of a communication system. The system is divided into three layers: Layer 1, Layer 2, and Layer 3. \n\nLayer 1 consists of validators that validate the input data. Layer 2 contains non-validators, which are responsible for processing the data. Layer 3 has devices that perform specific tasks related to the communication process.\n\nThe normal network communication layer connects the layers, while the blockchain communication layer ensures secure and transparent communication between the layers. The arrows in the diagram represent the flow of data and information within the system.",
        "reference": "Fig. 1: Three-layer system architecture [7]"
    },
    "2110.00808v2-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The architecture of BERT in pretraining is shown, with the masked language model (MLM) and next sentence prediction (NSP) tasks. In the MLM task, some tokens are replaced by a special [MASK] token, and the model's task is to predict these tokens. In the NSP task, the model is trained to predict whether two sentences follow each other or not. The diagram also shows the different loss functions used in the training process, including the latent codes, cyclic loss, and adversarial loss. The latent codes are used to represent the input embeddings, while the cyclic loss and adversarial loss are used to regularize the model during training.",
        "reference": "Figure 1: Cycle-consistent world model. In the pictured situation, a sequence of top camera images is used a the input. The images are encoded frame-wise into latent states and forward predicted by the transition model. From these latent codes, reconstructed top camera images and images translated to semantic top camera images are calculated. From the translated images, cyclic latent codes are calculated. Finally, the four losses can be calculated, which enforce equations (2) and (3)."
    },
    "1245438-Figure1-1.png": {
        "prediction": "The image is a diagram of the BERT pretraining process. BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based natural language processing pre-training model and a contextual language model developed by Google. The diagram shows the architecture of BERT in pretraining, with two main tasks: masked language modeling (MLM) and next sentence prediction (NSP). MLM involves replacing some of the words in a sentence with a special token [MASK] and training the model to predict the original words. NSP involves training the model to predict whether two given sentences are consecutive or not. The diagram also includes a Convolutional Neural Network (ConvNet) and a Convolutional Block (Conv+BN), which are used in the pretraining process. Overall, the image provides a visual representation of the BERT pretraining process and its architecture.",
        "reference": "Fig. 1: The network architecture with pipe-lined components."
    },
    "407011-Figure3-1.png": {
        "prediction": "The diagram depicts a machine learning model, specifically a BERT (Bidirectional Encoder Representations from Transformers) model. The BERT model is pre-trained on a large corpus of unlabeled text and can be fine-tuned with a new task or problem. The architecture of the BERT model consists of an input layer, a stack of encoders, and a final output layer. The encoders are responsible for processing the input text and extracting features, while the final output layer produces the desired output based on the extracted features.\n\nThe diagram also shows the process of pre-training the BERT model. During pre-training, the model learns to predict masked words in a given sentence. This is done by randomly masking some words in the input text and training the model to predict these masked words. The model is trained using a combination of positive and negative sets. The positive set consists of sentences where the masked word is present, while the negative set consists of sentences where the masked word is not present. The model is trained to predict the masked word in both the positive and negative sets. The pre-training process helps the model learn contextualized representations of words that can be fine-tuned for specific tasks.\n\nIn summary, the diagram represents a BERT model's architecture and its pre-training process, which involves predicting masked words in a given sentence using a combination of positive and negative sets.",
        "reference": "Fig. 3. Illustration of a triplet network by incorporating intra-class variance into triplet embedding, in which the joint learning objective is to minimize the combination of softmax loss and triplet loss (consisting of inter-class and intra-class triplet loss)."
    },
    "2204.04591v2-Figure3-1.png": {
        "prediction": "The diagram depicts a system for encrypting and decrypting data. The process begins with the publisher sending attributes to the public key, which is then encrypted by the remote ledger. The encrypted data is then sent to the user's ABE key, which shares the attribute definition. The local ledger authenticates the permissions, and the user retrieves the data over the DND network. Finally, the data is decrypted using the user's private key.",
        "reference": "Fig. 3. Encryption and Decryption Model"
    },
    "2107.04229v1-Figure3-1.png": {
        "prediction": "The image is a flowchart that depicts the process of executing traces and temporal relations. The flowchart is divided into three sections: the first section shows the tracheal sound files, the second section shows the labelers, and the third section shows the intersection.\n\n1. Tracheal Sound Files: This section contains the tracheal sound files, which are represented by the letters \"A\" and \"B\". These files are used to follow the next sentence prediction task.\n2. Labelers: This section consists of two labelers, labeled as \"1\" and \"2\". The first labeler is associated with the first labels, while the second labeler is associated with the second labels. The first labeler is connected to the first and second labelers through an intersection, indicating that it is responsible for both sets of labels.\n3. Intersection: The intersection in this flowchart represents the point at which the first and second labelers meet. It connects the first and second labelers, indicating that they are both involved in the same process.\n\nOverall, the flowchart illustrates the process of executing traces and temporal relations, with a focus on the tracheal sound files, labelers, and their interactions.",
        "reference": "Fig. 3. Flowchart of ground-truth labels establishment."
    },
    "2107.04229v1-Figure4-1.png": {
        "prediction": "The image is a flowchart that illustrates the execution traces and temporal relations in a system. The system consists of several components, such as the use case, direct call relations, indirect call patterns, and direct call patterns. The flowchart also shows the input layer, output layer, and the stack of encoders. The diagram provides a visual representation of how the system works, with arrows indicating the direction of data flow and the relationships between different components. It is a complex system, but the flowchart helps to understand its structure and function.",
        "reference": "Fig. 4. Architecture of the CNN-BiGRU model."
    },
    "2101.10102v2-Figure3-1.png": {
        "prediction": "The diagram depicts a flowchart for a machine learning model. The input is processed by the DNN, and the output is compared to the maximum value using the PAC Model. The analyzing process is then performed, which involves the PAC-Model robustness. This is followed by the subsection 4.1 & 4.3, which represents the model learning. The diagram also shows the sampling, input, output, and score difference. It seems to be a representation of a machine learning algorithm with a focus on the PAC Model.",
        "reference": "Figure 3: Framework of PAC-model robustness analysis base on model learning"
    },
    "2010.13007v1-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of a pre-trained encoder. The input frames are processed by the pre-trained encoder, which is then followed by acoustic representations and predicted labels. The pre-trained encoder is connected to a predicted labels section, which is also connected to a probing classifier. The diagram also includes an input section that contains audio signals.",
        "reference": "Fig. 1. An illustration of the model architecture used in probing experiments."
    },
    "2012.05892v2-Figure2-1.png": {
        "prediction": "Line 1: The graph shows the relationship between temperature and voltage. As the temperature increases, the voltage also increases. Line 2: The graph shows the relationship between wavelength and voltage. As the wavelength increases, the voltage also increases. Line 3: The graph shows the relationship between temperature and wavelength. As the temperature increases, the wavelength also increases.",
        "reference": "Fig. 2. Experimental demonstration of Planck spectroscopy. (A) Schematic of the experimental setup. Thermal emission from samples on a temperature stage was collected using a lens and focused onto an infrared detector. (B) Normalized voltage vs. temperature for five different samples: a laboratory blackbody, a sapphire wafer, a fused-silica wafer, and two n-doped silicon wafers with doping levels of 1020 and 1021 cm-3. (C) The extracted response function of the measurement setup, obtained by using the blackbody reference (dotted circle), is in good agreement with the expected response function based on vendor-provided properties of the detector, heater window, and lens. (D-G) Measured emissivity of the samples using Planck spectroscopy (circles), and the corresponding spectra measured using a Fouriertransform spectrometer (solid line)."
    },
    "1446589-Figure1-1.png": {
        "prediction": "The diagram shows a flowchart of the BERT pretraining process. The architecture of BERT in pretraining is shown, with the model's task to predict the masked tokens from the representation vectors it learns at the input embeddings positions. The diagram also includes images of animals such as a tiger, dog, bear, and cat, which are part of the data used for training. The data distribution and structure transfer are also indicated in the diagram.",
        "reference": "Figure 1: Illustration of the proposed method. The manifold structure (the straight lines) in the label embedding space is transferred to the image feature space for synthesizing the virtual signature (the purple star) of an unseen class. The purple arrow points to a refined signature, which demonstrates that the synthesized virtual signature is optimized after running the Expectation-Maximization algorithm so that unseen data are assigned to labels according to the data distribution."
    },
    "2205.03991v2-Figure4.1-1.png": {
        "prediction": "The image is a flowchart that depicts the architecture of BERT, a pre-trained language model. The flowchart shows the different components of the model and their interactions. BERT consists of two main parts: the generalized G-PDE and the local PDE. The generalized G-PDE section contains the input embeddings, segment embeddings, and position embeddings. The local PDE section includes the non-local G-PDE, local PDE, and non-local laplacian. The model's task is to predict the masked tokens in the input sequence. The architecture of BERT is designed to learn contextualized representations of words, which can be used for various natural language processing tasks such as question answering, text classification, and named entity recognition.",
        "reference": "FIGURE 4.1. Overview of nonlocal diffusion processes proposed in related work [ETT15, GO09, AVMRTM10] and their interrelations to the nonlocal G-PDE (4.7). The approaches highlighted by the blue region only model the image labeling problem. Edge labels refer to the corresponding sections of the analytical ablation study."
    },
    "2012.12554v1-Figure1-1.png": {
        "prediction": "The image is a flowchart that shows the process of pretraining BERT. The model's task is to predict the masked tokens from the representation vectors it learns at the positions of the input embeddings. The diagram also includes a series of images, with two green boxes indicating areas of interest. The text in the diagram describes the different components and their functions, such as the \"interpolation module\" and the \"frame selection module.\"",
        "reference": "Figure 1: Overview of our video annotation process. A human annotator draws a box on the first frame of the video; then our guiding frame selection mechanism predicts the next frame to annotate and the process iterates. Our method automatically and accurately interpolates bounding boxes for all frames that were not directly annotated by the human. Hence, at the end of the process object annotations are generated for all frames."
    },
    "216738-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The diagram is divided into three sections: the pretraining BERT architecture, the Tensor addition, and the fused embeddings. The pretraining BERT architecture section displays the input embeddings, segment embeddings, position embeddings, and word embeddings. The Tensor addition section illustrates the tensor addition process, which includes the input embeddings, mask token, and the final input representation. Finally, the fused embeddings section shows the fused embeddings layer, which includes the convolution layer and the final output representation.",
        "reference": "Figure 2. Architecture of proposed image fusion CNN illustrated for input exposure stack with images of size h\u00d7w. The pre-fusion layers C1 and C2 that share same weights, extract low-level features from input images. The feature pairs of input images are fused into a single feature by merge layer. The fused features are input to reconstruction layers to generate fused image Yfused."
    },
    "2210.01442v1-Figure3-1.png": {
        "prediction": "The diagram shows a flowchart of an experiment control system. The system consists of an experiment, an HV control, a motion control, and a monitoring. The experiment is connected to the HV control, which in turn is connected to the motion control. The motion control is connected to the monitoring. The experiment also has thin planes and thick planes, while the monitoring has humidity sensors and temperature sensors.",
        "reference": "Figure 3: Schematic overview of the experiment control system."
    },
    "2103.04138v2-Figure2-1.png": {
        "prediction": "The diagram shows a flowchart of the execution traces and temporal relations. The use case is represented by the top left corner, and the direct call relations are shown in the middle section. The indirect call patterns are depicted in the right section. The diagram also includes the output layer and the stack of encoders. The arrows indicate the direction of the flow, with the top left corner as the starting point and the bottom right corner as the end point.",
        "reference": "Figure 2. Leading-order Feynman diagram contributing to the hard kernels Tij(u), where the local four-quark operators are represented by the black square."
    }
}